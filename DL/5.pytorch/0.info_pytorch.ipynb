{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is an open-source deep learning framework developed by **Meta AI (formerly Facebook AI Research)**. It is widely used for building and training **machine learning** and **deep learning** models, especially in **computer vision**, **natural language processing (NLP)**, and **speech recognition** applications.\n",
    "\n",
    "### üî• Key Features of PyTorch:\n",
    "1. **Dynamic Computation Graphs** ‚Äì Unlike TensorFlow‚Äôs static graphs, PyTorch builds computation graphs dynamically, making debugging and experimentation easier.\n",
    "2. **Easy to Use** ‚Äì Its Pythonic syntax makes it beginner-friendly and intuitive.\n",
    "3. **GPU Acceleration** ‚Äì Supports CUDA for high-speed training on GPUs.\n",
    "4. **Autograd (Automatic Differentiation)** ‚Äì Provides automatic computation of gradients for backpropagation.\n",
    "5. **TorchScript** ‚Äì Converts PyTorch models into optimized, deployable versions.\n",
    "6. **ONNX Support** ‚Äì Allows exporting models for interoperability with other frameworks like TensorFlow.\n",
    "7. **Strong Community & Ecosystem** ‚Äì Supported by a large community with extensive pre-trained models (TorchVision, TorchText, etc.).\n",
    "\n",
    "Since you work on **deep learning for NLP and speech recognition**, PyTorch is a great choice because of its **strong support for sequence models (like LSTMs, GRUs, Transformers)** and libraries like **torchaudio** for speech applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Autograd in PyTorch (Automatic Differentiation)**\n",
    "PyTorch‚Äôs **`autograd`** (short for **automatic differentiation**) is a core feature that **automatically computes gradients** for **tensor operations**. This is crucial for **deep learning**, as it helps in **optimizing neural networks** using **gradient descent**.\n",
    "\n",
    "\n",
    "## **1Ô∏è‚É£ Why is Autograd Important?**\n",
    "In **deep learning**, models are trained using **backpropagation**, which requires **gradients** of the loss function with respect to model parameters (weights & biases).**`autograd`** makes this process **automatic**, so you don‚Äôt need to manually compute derivatives.\n",
    "\n",
    "### üîπ Example: Basic Derivative Computation\n",
    "Let's compute the derivative of \\( y = x^2 \\) using PyTorch:\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Create a tensor with requires_grad=True to track computation\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a function\n",
    "y = x**2  # y = x^2\n",
    "\n",
    "# Compute the gradient\n",
    "y.backward()  # Computes dy/dx\n",
    "\n",
    "# Print the gradient (dy/dx = 2x)\n",
    "print(x.grad)  # Output: tensor(4.)\n",
    "```\n",
    "üìå **Explanation:** Since \\( dy/dx = 2x \\), at \\( x = 2 \\), we get \\( dy/dx = 4 \\).\n",
    "\n",
    "\n",
    "\n",
    "## **2Ô∏è‚É£ Tracking Computation Graph**\n",
    "PyTorch records all operations on `requires_grad=True` tensors to create a **computation graph (DAG - Directed Acyclic Graph)**. The gradients are computed using **reverse-mode differentiation (backpropagation)**.\n",
    "\n",
    "### üîπ Example: Chain Rule Computation\n",
    "Let‚Äôs compute the gradient for:\n",
    "$$\n",
    "z = 3x^3 + 2y^2\n",
    "$$\n",
    "```python\n",
    "# Define tensors\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Define function\n",
    "z = 3 * x**3 + 2 * y**2\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()\n",
    "\n",
    "# Print gradients\n",
    "print(x.grad)  # dz/dx = 9x^2 = 9*(2)^2 = 36\n",
    "print(y.grad)  # dz/dy = 4y = 4*(3) = 12\n",
    "```\n",
    "\n",
    "‚úÖ **PyTorch automatically applies the chain rule!**\n",
    "\n",
    "\n",
    "\n",
    "## **3Ô∏è‚É£ Disabling Gradient Computation (`torch.no_grad()`)**\n",
    "During **inference** (when we don‚Äôt need gradients), computing them is unnecessary and slows things down. We can disable it using:\n",
    "```python\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():  # Disables gradient tracking\n",
    "    y = x * 2\n",
    "print(y.requires_grad)  # Output: False\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **4Ô∏è‚É£ Zeroing Gradients (`zero_grad()`)**\n",
    "In **training loops**, PyTorch accumulates gradients instead of overwriting them. To prevent issues, we **reset** gradients manually.\n",
    "\n",
    "### üîπ Example: Preventing Gradient Accumulation\n",
    "```python\n",
    "# Create a tensor\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Perform a computation\n",
    "y = x**2\n",
    "y.backward()\n",
    "print(x.grad)  # Output: tensor(6.)\n",
    "\n",
    "# Reset gradients before next step\n",
    "x.grad.zero_()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **5Ô∏è‚É£ Detaching Tensors (`detach()`)**\n",
    "Sometimes, we need to **detach** a tensor from the computation graph to stop tracking gradients.\n",
    "\n",
    "### üîπ Example:\n",
    "```python\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x**2\n",
    "\n",
    "# Detach from autograd\n",
    "y_detached = y.detach()\n",
    "print(y_detached.requires_grad)  # Output: False\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **6Ô∏è‚É£ Computing Gradients for Multiple Outputs (`torch.autograd.grad()`)**\n",
    "Instead of calling `.backward()`, we can compute gradients manually using `torch.autograd.grad()`.\n",
    "\n",
    "### üîπ Example:\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# Compute gradient manually\n",
    "grad = torch.autograd.grad(y, x)\n",
    "print(grad)  # Output: (tensor(12.),)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7Ô∏è‚É£ Higher-Order Gradients (Second-Order Derivatives)**\n",
    "PyTorch supports **higher-order gradients** (gradients of gradients).\n",
    "\n",
    "### üîπ Example: Second Derivative \\( d^2y/dx^2 \\)\n",
    "$$\n",
    "y = x^3, \\quad \\text{Find } d^2y/dx^2\n",
    "$$\n",
    "```python\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# First derivative\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "\n",
    "# Second derivative\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "\n",
    "print(grad1)  # First derivative: 3x^2 => 3*(2^2) = 12\n",
    "print(grad2)  # Second derivative: 6x => 6*2 = 12\n",
    "```\n",
    "\n",
    "## **üî• Summary: How Autograd Works**\n",
    "| Step | Description |\n",
    "|------|------------|\n",
    "| 1Ô∏è‚É£ | Set `requires_grad=True` for tensors that need gradients |\n",
    "| 2Ô∏è‚É£ | Perform operations to build a **computation graph** |\n",
    "| 3Ô∏è‚É£ | Call `.backward()` to compute gradients |\n",
    "| 4Ô∏è‚É£ | Access gradients using `.grad` |\n",
    "| 5Ô∏è‚É£ | Use `torch.no_grad()` for inference |\n",
    "| 6Ô∏è‚É£ | Use `.detach()` to remove tensors from graph |\n",
    "| 7Ô∏è‚É£ | Call `zero_grad()` to prevent gradient accumulation |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll walk you through building a full **Breast Cancer Detection** pipeline using **PyTorch**, including:  \n",
    "‚úÖ **Loading the dataset**  \n",
    "‚úÖ **Preprocessing the data**  \n",
    "‚úÖ **Building a Neural Network**  \n",
    "‚úÖ **Training the model**  \n",
    "‚úÖ **Evaluating the accuracy**  \n",
    "\n",
    "\n",
    "\n",
    "## **1Ô∏è‚É£ Load Dataset & Preprocessing**\n",
    "We‚Äôll load the dataset from the given URL, preprocess it, and convert it into tensors.\n",
    "\n",
    "### **Steps**:\n",
    "1. Load the dataset using `pandas`\n",
    "2. Encode categorical labels (Malignant/Benign ‚Üí 0/1)\n",
    "3. Normalize features for better training\n",
    "4. Split into **train/test** sets\n",
    "5. Convert to **PyTorch tensors & DataLoader**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Encode target column (M=1, B=0)\n",
    "df['diagnosis'] = LabelEncoder().fit_transform(df['diagnosis'])\n",
    "\n",
    "# Split features & labels\n",
    "X = df.drop(columns=['diagnosis']).values  # Features\n",
    "y = df['diagnosis'].values  # Labels\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Split into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **2Ô∏è‚É£ Define Neural Network**\n",
    "We create a simple **fully connected neural network (MLP)** with:\n",
    "- **Input layer**: 30 features\n",
    "- **Hidden layers**: 2 layers with ReLU activation\n",
    "- **Output layer**: 1 neuron (Sigmoid activation for binary classification)\n",
    "\n",
    "```python\n",
    "# Define the Neural Network\n",
    "class BreastCancerNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BreastCancerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(30, 16)  # Input layer (30 ‚Üí 16 neurons)\n",
    "        self.fc2 = nn.Linear(16, 8)   # Hidden layer (16 ‚Üí 8 neurons)\n",
    "        self.fc3 = nn.Linear(8, 1)    # Output layer (8 ‚Üí 1 neuron)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Instantiate model\n",
    "model = BreastCancerNN()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **3Ô∏è‚É£ Define Loss Function & Optimizer**\n",
    "Since this is a **binary classification** problem:\n",
    "- **Loss function**: Binary Cross Entropy (`BCELoss`)\n",
    "- **Optimizer**: Adam (`optim.Adam`)\n",
    "\n",
    "```python\n",
    "# Define loss function & optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **4Ô∏è‚É£ Training the Model**\n",
    "We train the model using **batch gradient descent** for **20 epochs**.\n",
    "\n",
    "### **Training Steps**:\n",
    "1. Forward pass: Compute predictions\n",
    "2. Compute loss\n",
    "3. Backpropagation: Compute gradients\n",
    "4. Update weights\n",
    "\n",
    "```python\n",
    "# Training loop\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_batch, y_batch = batch\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print loss per epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **5Ô∏è‚É£ Model Evaluation**\n",
    "Now, let's test the model on the **test set** and compute **accuracy**.\n",
    "\n",
    "```python\n",
    "# Evaluate model\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for batch in test_loader:\n",
    "        X_batch, y_batch = batch\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = (y_pred >= 0.5).float()  # Convert probabilities to 0/1\n",
    "\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **6Ô∏è‚É£ Full Pipeline Summary**\n",
    "‚úî Load dataset  \n",
    "‚úî Preprocess (encode labels, normalize, split)  \n",
    "‚úî Convert data into PyTorch tensors  \n",
    "‚úî Define a simple **Neural Network**  \n",
    "‚úî Train using **Binary Cross Entropy Loss**  \n",
    "‚úî Evaluate accuracy  \n",
    "\n",
    "\n",
    "\n",
    "## **üî• Results**\n",
    "- The model will **learn from the dataset** and predict whether a tumor is **malignant or benign**.\n",
    "- The final **test accuracy** will be around **95%** (varies per run).\n",
    "\n",
    "Would you like me to help with **model improvements** like:\n",
    "‚úî Adding **Dropout**  \n",
    "‚úî Using **Batch Normalization**  \n",
    "‚úî Trying **different optimizers** üöÄ?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, the **Dataset** and **DataLoader** classes are essential components of the data handling pipeline. They enable efficient loading, preprocessing, and batching of data for model training and evaluation.\n",
    "\n",
    "\n",
    "\n",
    "## 1Ô∏è‚É£ The Dataset Class\n",
    "\n",
    "### **Purpose**\n",
    "- **Abstract Data Representation:** The `Dataset` class provides a standard way to represent your data so that you can iterate over it. It encapsulates the logic for reading data from files, applying transformations, and returning individual samples (and labels).\n",
    "- **Customizable:** You can subclass `torch.utils.data.Dataset` to create a custom dataset that suits your needs.\n",
    "\n",
    "### **Key Methods**\n",
    "- **`__len__`**: Returns the total number of samples in the dataset.\n",
    "- **`__getitem__`**: Given an index, returns the corresponding sample (e.g., an image, a text snippet) and its label. This is the method used by the DataLoader to fetch data.\n",
    "\n",
    "### **Example**\n",
    "Here‚Äôs an example of a custom dataset for a classification task:\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)  # Load data from a CSV file\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the sample and target\n",
    "        sample = self.data.iloc[idx, :-1].values.astype('float32')\n",
    "        target = self.data.iloc[idx, -1]  # Assuming the last column is the label\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample, target\n",
    "```\n",
    "In this example, `CustomDataset`:\n",
    "- Reads data from a CSV file.\n",
    "- Implements the required `__len__` and `__getitem__` methods.\n",
    "- Optionally applies a transformation to each sample.\n",
    "\n",
    "\n",
    "\n",
    "## 2Ô∏è‚É£ The DataLoader Class\n",
    "\n",
    "### **Purpose**\n",
    "- **Batching:** Automatically splits the dataset into batches, which is crucial for efficient training.\n",
    "- **Shuffling:** Allows you to shuffle the data at every epoch to reduce bias in model training.\n",
    "- **Parallelism:** Supports loading data in parallel using multiple subprocesses to speed up data preparation.\n",
    "\n",
    "### **Key Parameters**\n",
    "- **`dataset`**: The dataset object (instance of a subclass of `Dataset`).\n",
    "- **`batch_size`**: Number of samples per batch.\n",
    "- **`shuffle`**: Whether to shuffle the data at every epoch.\n",
    "- **`num_workers`**: Number of subprocesses to use for data loading. More workers can speed up the process if your data loading and preprocessing are heavy.\n",
    "- **`collate_fn`**: (Optional) A function to merge a list of samples to form a mini-batch. Useful when dealing with data of varying shapes.\n",
    "\n",
    "### **Example**\n",
    "Here‚Äôs how you can wrap a custom dataset in a DataLoader:\n",
    "```python\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Instantiate the dataset\n",
    "dataset = CustomDataset(csv_file='data.csv')\n",
    "\n",
    "# Create DataLoader: It will handle batching and shuffling automatically\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "\n",
    "# Iterating over the DataLoader in a training loop\n",
    "for batch_samples, batch_labels in dataloader:\n",
    "    # Now batch_samples is a batch of data, and batch_labels are the corresponding labels\n",
    "    # You can perform your forward pass here\n",
    "    pass\n",
    "```\n",
    "In this example, the DataLoader takes care of:\n",
    "- **Creating batches** of 32 samples.\n",
    "- **Shuffling** the data before each epoch.\n",
    "- **Utilizing multiple workers** (if supported by your system) to load data faster.\n",
    "\n",
    "\n",
    "\n",
    "## 3Ô∏è‚É£ Integration in a Training Pipeline\n",
    "\n",
    "When building a training loop in PyTorch, you typically use these classes as follows:\n",
    "1. **Dataset Preparation:** You define or load a dataset using a custom or pre-built Dataset class.\n",
    "2. **Batch Loading:** You wrap the dataset in a DataLoader to handle batching, shuffling, and multi-threaded data loading.\n",
    "3. **Training Loop:** Within each iteration of the training loop, you fetch batches of data from the DataLoader, perform a forward pass, compute loss, backpropagate gradients, and update model parameters.\n",
    "\n",
    "### **Training Loop Example**\n",
    "```python\n",
    "# Assume model, optimizer, and loss function are defined\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_samples, batch_labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(batch_samples)\n",
    "        loss = loss_fn(outputs, batch_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "```\n",
    "This pattern ensures that your data is efficiently fed into your model during training, supporting scalability and high performance.\n",
    "\n",
    "\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "- **Dataset Class:** Provides a way to represent and access individual samples of your data, making it easy to preprocess and transform data before training.\n",
    "- **DataLoader Class:** Automates batching, shuffling, and parallel data loading, ensuring that your training loop can efficiently process data.\n",
    "\n",
    "Together, they form the backbone of PyTorch‚Äôs data handling, enabling you to build scalable and flexible deep learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let‚Äôs break it down in **super simple** terms.  \n",
    "\n",
    "\n",
    "\n",
    "### **1Ô∏è‚É£ What is a Dataset in PyTorch?**  \n",
    "\n",
    "Think of a **dataset** as a **big list** of data points (like images, text, or numbers) stored somewhere (CSV, database, etc.).  \n",
    "\n",
    "üîπ Imagine you have a big book with **thousands of rows** of data.  \n",
    "üîπ Each row is a **single example** (like a patient‚Äôs medical record in a cancer dataset).  \n",
    "üîπ The **Dataset class** helps us **open the book and read specific rows** in an organized way.\n",
    "\n",
    "In PyTorch, a dataset is like a **recipe book** that tells how to get each data point when needed.\n",
    "\n",
    "üí° **Example in simple words**:  \n",
    "Let's say we have a notebook where each page has two things:  \n",
    "1Ô∏è‚É£ A picture of a fruit üçéüçåüçä  \n",
    "2Ô∏è‚É£ The name of the fruit written below it  \n",
    "\n",
    "Now, if you wanted to \"read\" this notebook in Python, you'd need a way to **pick a page, look at the picture, and read the name**. That‚Äôs exactly what the **Dataset class** does!  \n",
    "\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ What is a DataLoader in PyTorch?**  \n",
    "\n",
    "Now, imagine you're cooking for 100 people at a restaurant. You **won‚Äôt cook one meal at a time**, right? You‚Äôd cook **multiple meals in batches** to save time.  \n",
    "\n",
    "That‚Äôs exactly what the **DataLoader** does‚Äîit helps you **grab multiple rows at once** instead of fetching data one by one.  \n",
    "\n",
    "üí° **Example in simple words**:  \n",
    "- If the dataset is a **big book**, the **DataLoader** is a **waiter** who brings plates of food (batches of data) to the chef (your model).  \n",
    "- Instead of reading one page at a time, the DataLoader helps us read **10, 20, or 32 pages at once** (this is called **batching**).  \n",
    "\n",
    "**Why is this useful?**  \n",
    "‚úÖ **Faster training** ‚Üí Your model learns quicker when data is processed in batches.  \n",
    "‚úÖ **Shuffling** ‚Üí The DataLoader can mix up the pages so the model doesn‚Äôt memorize data in order.  \n",
    "\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ Real-world Example (Dataset + DataLoader in Action)**  \n",
    "\n",
    "#### üçâ Imagine you are training a model to recognize fruits üçéüçåüçá  \n",
    "\n",
    "üëâ **Dataset class**: Knows where all fruit pictures & labels are stored.  \n",
    "üëâ **DataLoader**: Brings the fruit images **in batches** to the model for learning.  \n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create a dataset class\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)  # Load data from CSV\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # Total number of fruit images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fruit_image = self.data.iloc[idx, :-1].values  # Fruit image pixels\n",
    "        fruit_name = self.data.iloc[idx, -1]  # Fruit label (e.g., Apple, Banana)\n",
    "        return fruit_image, fruit_name\n",
    "\n",
    "# Step 2: Create a DataLoader to serve data in batches\n",
    "dataset = FruitDataset(\"fruits.csv\")  # Load dataset\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)  # Load in batches of 10\n",
    "\n",
    "# Step 3: Train model using batches\n",
    "for batch in dataloader:\n",
    "    images, labels = batch\n",
    "    print(\"New batch of 10 images ready for training!\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **4Ô∏è‚É£ Final Summary (Super Simple Takeaway)**  \n",
    "\n",
    "üü¢ **Dataset** = A book üìñ with data (every row is a piece of information).  \n",
    "üü¢ **DataLoader** = A waiter üçΩÔ∏è bringing multiple rows (batches) at once.  \n",
    "üü¢ **Why do we need both?** So that our model **learns faster and efficiently**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
