{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recurrent Neural Networks (RNNs)** ğŸ¨âœ¨  \n",
    "\n",
    "### Imagine You're Telling a Story ğŸ“–  \n",
    "Think of a **Recurrent Neural Network (RNN)** like a storyteller ğŸ“œ who remembers past events to tell the next part of the story. Unlike regular neural networks, which treat every input separately, **RNNs have memory!** ğŸ§  They remember what happened before and use that info to make better decisions.  \n",
    "\n",
    "### How It Works ğŸ”„  \n",
    "1ï¸âƒ£ **Takes an input** â€“ Letâ€™s say you're reading a sentence word by word. The RNN processes each word step by step.  \n",
    "2ï¸âƒ£ **Remembers the past** â€“ It keeps a \"hidden state\" ğŸ“¦ that stores information about previous words.  \n",
    "3ï¸âƒ£ **Passes information forward** â€“ Like a storyteller who recalls past events to shape the next part of the story, the RNN updates its hidden state at each step.  \n",
    "4ï¸âƒ£ **Makes a prediction** â€“ It predicts the next word, the sentiment of a sentence, or even generates text like a chatbot! ğŸ¤–ğŸ’¬  \n",
    "\n",
    "### Why Is Memory Important? ğŸ›  \n",
    "Imagine reading a sentence like:  \n",
    "â¡ï¸ \"The boy played with his dog. **He** was very happy.\"  \n",
    "A normal neural network might struggle to understand who \"**He**\" refers to. But an RNN **remembers** that we were talking about \"the boy\" and connects the dots! ğŸ”—  \n",
    "\n",
    "### Where Do We Use RNNs? ğŸš€  \n",
    "ğŸ“Œ **Speech recognition** â€“ Your voice assistants (Alexa, Siri) use RNNs to understand what you're saying! ğŸ™  \n",
    "ğŸ“Œ **Chatbots & Language Translation** â€“ Google Translate and chatbots use RNNs to process conversations.  \n",
    "ğŸ“Œ **Stock Price Prediction** â€“ Since stock prices depend on past trends, RNNs help analyze sequences of data ğŸ“ˆğŸ’°.  \n",
    "ğŸ“Œ **Music Generation** â€“ RNNs can even compose music! ğŸµğŸ¤©  \n",
    "\n",
    "### The Problem? ğŸ˜¬  \n",
    "ğŸ’¥ **Vanishing Gradient Problem** â€“ When an RNN tries to remember too much (like a forgetful storyteller), older information fades away, making it hard to learn long-term dependencies.  \n",
    "\n",
    "### The Fix? ğŸ›   \n",
    "ğŸ”¹ **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)** are advanced RNNs that fix this memory loss problem. They have a special \"forget gate\" ğŸ”‘ that helps decide what to keep and what to discard.  \n",
    "\n",
    "### In Short ğŸ  \n",
    "RNNs = Neural networks with memory ğŸ”„  \n",
    "They process sequences step by step â­  \n",
    "Useful in speech, text, and time-series data! ğŸ“ŠğŸ™  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ RNN vs ANN: The Ultimate Showdown! ğŸ”¥  \n",
    "\n",
    "When working with neural networks, you might come across **Artificial Neural Networks (ANNs)** and **Recurrent Neural Networks (RNNs)**. While both are powerful, they serve different purposes. Let's break it down in a fun and easy way!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  **Artificial Neural Network (ANN)** â€“ The Standard Genius  \n",
    "ğŸ“Œ **What is it?**  \n",
    "ANNs are like a **smart calculator**. They take inputs, process them through layers of neurons, and give an output. Butâ€¦ **they have no memory**! Every input is treated separately.  \n",
    "\n",
    "ğŸ“Œ **Structure:**  \n",
    "ğŸ”¹ Input Layer â†’ Hidden Layers â†’ Output Layer  \n",
    "ğŸ”¹ Each neuron is fully connected to the next layer  \n",
    "ğŸ”¹ Uses activation functions like **ReLU, Sigmoid, Tanh**  \n",
    "\n",
    "ğŸ“Œ **Where is it used?**  \n",
    "âœ… Image classification (e.g., identifying cats vs. dogs ğŸ¶ğŸ±)  \n",
    "âœ… Spam detection (sorting emails ğŸ“§)  \n",
    "âœ… Recommendation systems (Netflix suggestions ğŸ¿)  \n",
    "\n",
    "ğŸ“Œ **Limitations**  \n",
    "âŒ Cannot handle **sequential** or **time-dependent** data (like predicting stock prices ğŸ“ˆ or speech recognition ğŸ™ï¸)  \n",
    "âŒ Treats every input independently  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **Recurrent Neural Network (RNN)** â€“ The Memory Master  \n",
    "ğŸ“Œ **What is it?**  \n",
    "RNNs are like **humans reading a story** ğŸ“–. They remember previous words to understand the next ones. Unlike ANNs, RNNs have a **memory** that helps them process sequences.  \n",
    "\n",
    "ğŸ“Œ **Structure:**  \n",
    "ğŸ”¹ Looks similar to an ANN but has **loops** that allow information to persist!  \n",
    "ğŸ”¹ Each neuron not only passes data forward but also **feeds it back into itself**!  \n",
    "ğŸ”¹ Uses activation functions like **Tanh, Softmax**  \n",
    "\n",
    "ğŸ“Œ **Where is it used?**  \n",
    "âœ… Speech Recognition (like Siri or Google Assistant ğŸ™ï¸)  \n",
    "âœ… Language Translation (Google Translate ğŸŒ)  \n",
    "âœ… Time-series forecasting (predicting stock trends ğŸ“Š)  \n",
    "\n",
    "ğŸ“Œ **Limitations**  \n",
    "âŒ Suffers from **vanishing gradient** (loses memory for long sequences ğŸ˜¢)  \n",
    "âŒ Slower training compared to ANNs  \n",
    "âŒ Difficult to handle long-term dependencies  \n",
    "\n",
    "\n",
    "## ğŸ¯ **Key Differences at a Glance!**  \n",
    "\n",
    "| Feature  | ANN ğŸ§  | RNN ğŸ”„ |\n",
    "|----------|--------|--------|\n",
    "| **Memory** | No memory, treats inputs independently | Remembers past inputs for sequential processing |\n",
    "| **Structure** | Fully connected layers | Loops and feedback connections |\n",
    "| **Best for** | Static data (images, tabular data) | Sequential data (speech, text, time series) |\n",
    "| **Limitations** | Canâ€™t process time-dependent data | Struggles with long-term dependencies |\n",
    "| **Examples** | Image classification, spam detection | Chatbots, stock prediction, speech-to-text |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **When to Use What?**  \n",
    "âœ”ï¸ Use **ANN** if your problem does **not** involve sequences (e.g., image recognition, customer churn prediction).  \n",
    "âœ”ï¸ Use **RNN** if your data is **sequential** (e.g., text generation, audio processing, stock market forecasting).  \n",
    "\n",
    "For **better performance in long sequences**, we use **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)**, which improve RNNs by solving the vanishing gradient problem.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ‰ **Final Thoughts**  \n",
    "Both ANNs and RNNs are powerful, but their strengths lie in different areas. If youâ€™re working with images, structured data, or classification tasks, **ANN is your go-to**. But if youâ€™re dealing with sequential data like speech, text, or time series, **RNN will be your best friend**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ **Recurrent Neural Network (RNN) Architecture â€“ A Deep Dive!** ğŸ”„  \n",
    "\n",
    "RNNs are a special type of neural network designed to process **sequential data**, such as time-series data, speech, and text. Unlike traditional ANNs, RNNs have a **memory** that allows them to consider past inputs while processing current ones.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ—ï¸ **Basic RNN Architecture**  \n",
    "\n",
    "RNNs are different from standard ANNs because they have a **feedback loop** that allows information to persist over time.\n",
    "\n",
    "### ğŸ”¹ **Structure of a Simple RNN**  \n",
    "The architecture consists of:  \n",
    "1. **Input Layer**: Takes the input sequence.  \n",
    "2. **Hidden Layer (Recurrent Neurons)**: Maintains a memory of previous states and updates at each time step.  \n",
    "3. **Output Layer**: Produces the final prediction.\n",
    "\n",
    "ğŸ’¡ **Key difference from ANN**: The hidden layer is connected to itself! This allows information to flow from previous time steps.\n",
    "\n",
    "### ğŸ“Œ **Mathematical Representation**  \n",
    "At each time step **t**, the RNN updates its hidden state using:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = previous hidden state  \n",
    "- $ W_x $, $ W_h $ = weight matrices  \n",
    "- $ b $ = bias  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "The output is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ g $ = activation function (softmax for classification, linear for regression)  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **Unrolling the RNN (Time Step Representation)**  \n",
    "\n",
    "A simple RNN processes a sequence of inputs **one time step at a time**.  \n",
    "For example, if we have a sequence **X = [xâ‚, xâ‚‚, xâ‚ƒ]**, the RNN unfolds like this:\n",
    "\n",
    "```\n",
    "xâ‚ â†’ [hâ‚] â†’ yâ‚\n",
    "      â†˜\n",
    "xâ‚‚ â†’ [hâ‚‚] â†’ yâ‚‚\n",
    "       â†˜\n",
    "xâ‚ƒ â†’ [hâ‚ƒ] â†’ yâ‚ƒ\n",
    "```\n",
    "  \n",
    "Here:  \n",
    "- The hidden state **h** carries information from previous time steps.\n",
    "- Each output $ y_t $ is computed based on the current hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš§ **Challenges in Basic RNNs**  \n",
    "RNNs are powerful, but they face some problems:\n",
    "\n",
    "### âŒ **Vanishing Gradient Problem**  \n",
    "- When training deep RNNs with many time steps, gradients shrink to near **zero** during backpropagation.  \n",
    "- This makes it **hard to learn long-term dependencies** (i.e., remembering things from many time steps ago).\n",
    "\n",
    "### âŒ **Exploding Gradient Problem**  \n",
    "- If gradients grow **too large**, they can make the training unstable.\n",
    "\n",
    "To solve these, we use **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Variants of RNNs**\n",
    "There are different types of RNN architectures:\n",
    "\n",
    "1. **One-to-One (Vanilla RNN)**\n",
    "   - Used for simple tasks like image classification.\n",
    "\n",
    "2. **One-to-Many**\n",
    "   - Example: Generating music ğŸµ from a single note.\n",
    "\n",
    "3. **Many-to-One**\n",
    "   - Example: Sentiment analysis (classifying an entire sentence as \"positive\" or \"negative\").\n",
    "\n",
    "4. **Many-to-Many**\n",
    "   - Example: Machine translation (e.g., English â†’ French).\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Key Takeaways**  \n",
    "âœ… RNNs are great for **sequential data** processing.  \n",
    "âœ… They have **memory**, unlike ANNs.  \n",
    "âœ… They suffer from **vanishing/exploding gradients** but can be improved with **LSTMs and GRUs**.  \n",
    "âœ… Used in **speech recognition, time-series forecasting, chatbots, and NLP tasks**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ **Forward Propagation in Recurrent Neural Networks (RNNs) â€“ A Complete Breakdown!** ğŸ”„  \n",
    "\n",
    "Forward propagation in an RNN works differently from a standard Artificial Neural Network (ANN) because it processes **sequential data** while maintaining a **hidden state** that carries information from previous time steps.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ— **Basic Structure of RNN Forward Propagation**\n",
    "Unlike traditional feedforward networks, where inputs are independent, an RNN processes inputs **sequentially**, maintaining a memory of past computations.\n",
    "\n",
    "For each time step $ t $, the RNN performs the following computations:\n",
    "\n",
    "1ï¸âƒ£ **Compute the new hidden state $ h_t $ using the current input $ x_t $ and the previous hidden state $ h_{t-1} $.**  \n",
    "2ï¸âƒ£ **Compute the output $ y_t $ using the hidden state $ h_t $.**  \n",
    "3ï¸âƒ£ **Pass the hidden state to the next time step.**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¢ **Mathematical Formulation**\n",
    "At each time step $ t $, forward propagation in an RNN follows these steps:\n",
    "\n",
    "### 1ï¸âƒ£ **Hidden State Update**\n",
    "The hidden state $ h_t $ is calculated using the previous hidden state $ h_{t-1} $ and the current input $ x_t $:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = hidden state from the previous time step  \n",
    "- $ W_x $ = weight matrix for input  \n",
    "- $ W_h $ = weight matrix for previous hidden state  \n",
    "- $ b_h $ = bias term  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "### 2ï¸âƒ£ **Output Calculation**\n",
    "The output $ y_t $ at time step $ t $ is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ b_y $ = bias for output  \n",
    "- $ g $ = activation function (e.g., **softmax** for classification tasks)  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“œ **Step-by-Step Forward Propagation Example**\n",
    "Let's assume we have an RNN processing three time steps with inputs $ x_1, x_2, x_3 $.\n",
    "\n",
    "### ğŸ”„ **Unrolling the RNN**\n",
    "Instead of viewing an RNN as a single network, we **unroll it** across time steps:\n",
    "\n",
    "```\n",
    "xâ‚ â†’ [hâ‚] â†’ yâ‚\n",
    "      â†˜\n",
    "xâ‚‚ â†’ [hâ‚‚] â†’ yâ‚‚\n",
    "       â†˜\n",
    "xâ‚ƒ â†’ [hâ‚ƒ] â†’ yâ‚ƒ\n",
    "```\n",
    "\n",
    "### ğŸ”¢ **Step 1: Compute the first hidden state $ h_1 $**\n",
    "$$\n",
    "h_1 = f(W_x x_1 + W_h h_0 + b_h)\n",
    "$$\n",
    "- $ h_0 $ is typically initialized as a vector of zeros.\n",
    "\n",
    "### ğŸ”¢ **Step 2: Compute the second hidden state $ h_2 $**\n",
    "$$\n",
    "h_2 = f(W_x x_2 + W_h h_1 + b_h)\n",
    "$$\n",
    "- The hidden state $ h_1 $ from the previous time step is used.\n",
    "\n",
    "### ğŸ”¢ **Step 3: Compute the third hidden state $ h_3 $**\n",
    "$$\n",
    "h_3 = f(W_x x_3 + W_h h_2 + b_h)\n",
    "$$\n",
    "\n",
    "### ğŸ”¢ **Step 4: Compute outputs $ y_1, y_2, y_3 $**\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "- The output is calculated at each time step based on the hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Observations**\n",
    "âœ” **Recurrent Connections**: The hidden state at each time step depends on the previous state.  \n",
    "âœ” **Shared Weights**: The same weight matrices $ W_x, W_h, W_y $ are used across all time steps, reducing complexity.  \n",
    "âœ” **Memory Effect**: The network retains past information, making it suitable for **sequential tasks** like speech recognition, language modeling, and time-series forecasting.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’» **Python Code Example**\n",
    "Hereâ€™s how forward propagation in an RNN can be implemented using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Activation function (tanh)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Define input, weight matrices, and bias\n",
    "x = np.array([[0.5], [0.2], [0.1]])  # Input at three time steps\n",
    "W_x = np.array([[0.8]])  # Input weight\n",
    "W_h = np.array([[0.5]])  # Recurrent weight\n",
    "W_y = np.array([[1.0]])  # Output weight\n",
    "b_h = np.array([[0.1]])  # Bias for hidden state\n",
    "b_y = np.array([[0.2]])  # Bias for output\n",
    "\n",
    "# Initialize hidden state\n",
    "h = np.array([[0]])  # Start with zero hidden state\n",
    "\n",
    "# Forward propagation\n",
    "for t in range(len(x)):\n",
    "    h = tanh(np.dot(W_x, x[t]) + np.dot(W_h, h) + b_h)  # Update hidden state\n",
    "    y = np.dot(W_y, h) + b_y  # Compute output\n",
    "    print(f\"Time Step {t+1}: Hidden State: {h}, Output: {y}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Final Thoughts**\n",
    "âœ… **RNN forward propagation** processes inputs **one at a time** while maintaining memory.  \n",
    "âœ… **Key equations** involve computing the **hidden state** and **output** at each time step.  \n",
    "âœ… **Challenges**: Standard RNNs struggle with long sequences due to the **vanishing gradient problem**.  \n",
    "âœ… **Solution**: Use **LSTMs or GRUs** to improve long-term memory handling.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§® **Manual Calculation of RNN Forward Propagation â€“ Step-by-Step Example** ğŸ”„  \n",
    "\n",
    "Let's take a simple example of an **RNN with one neuron** to manually compute forward propagation for **three time steps**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ Given Parameters**\n",
    "We define a simple RNN where:\n",
    "\n",
    "- **Input size = 1 (one feature per time step)**\n",
    "- **Hidden state size = 1 (one neuron in hidden layer)**\n",
    "- **Output size = 1 (one neuron in output layer)**\n",
    "- **Sequence length = 3 (processing 3 time steps: $ x_1, x_2, x_3 $)**\n",
    "\n",
    "#### ğŸ¯ **Initial Values**\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| $ x_1, x_2, x_3 $ | $ 0.5, 0.2, 0.1 $ (input at each time step) |\n",
    "| $ W_x $ | $ 0.8 $ (weight for input) |\n",
    "| $ W_h $ | $ 0.5 $ (weight for hidden state) |\n",
    "| $ W_y $ | $ 1.0 $ (weight for output) |\n",
    "| $ b_h $ | $ 0.1 $ (bias for hidden state) |\n",
    "| $ b_y $ | $ 0.2 $ (bias for output) |\n",
    "| $ h_0 $ | $ 0 $ (initial hidden state) |\n",
    "\n",
    "ğŸ’¡ **Activation function**: We use the **tanh** function:\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ Forward Propagation Steps**\n",
    "At each time step, we compute:\n",
    "\n",
    "1ï¸âƒ£ **Hidden state update**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "2ï¸âƒ£ **Output calculation**  \n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š Step-by-Step Computation**\n",
    "### **â³ Time Step 1 ($ t = 1 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_1 $:\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.8 \\times 0.5 + 0.5 \\times 0 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.4 + 0 + 0.1) = \\tanh(0.5)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.5) \\approx 0.4621 $:\n",
    "\n",
    "$$\n",
    "h_1 \\approx 0.4621\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_1 $:\n",
    "\n",
    "$$\n",
    "y_1 = 1.0 \\times 0.4621 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_1 \\approx 0.6621\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **â³ Time Step 2 ($ t = 2 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_2 $:\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.8 \\times 0.2 + 0.5 \\times 0.4621 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.16 + 0.2311 + 0.1) = \\tanh(0.4911)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4911) \\approx 0.4548 $:\n",
    "\n",
    "$$\n",
    "h_2 \\approx 0.4548\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_2 $:\n",
    "\n",
    "$$\n",
    "y_2 = 1.0 \\times 0.4548 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 \\approx 0.6548\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **â³ Time Step 3 ($ t = 3 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_3 $:\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.8 \\times 0.1 + 0.5 \\times 0.4548 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.08 + 0.2274 + 0.1) = \\tanh(0.4074)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4074) \\approx 0.3863 $:\n",
    "\n",
    "$$\n",
    "h_3 \\approx 0.3863\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_3 $:\n",
    "\n",
    "$$\n",
    "y_3 = 1.0 \\times 0.3863 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_3 \\approx 0.5863\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Final Results**\n",
    "| Time Step | $ x_t $ | $ h_t $ (Hidden State) | $ y_t $ (Output) |\n",
    "|-----------|----------|----------------|----------------|\n",
    "| $ t = 1 $ | $ 0.5 $ | $ 0.4621 $ | $ 0.6621 $ |\n",
    "| $ t = 2 $ | $ 0.2 $ | $ 0.4548 $ | $ 0.6548 $ |\n",
    "| $ t = 3 $ | $ 0.1 $ | $ 0.3863 $ | $ 0.5863 $ |\n",
    "\n",
    "ğŸ¯ **Observation**:  \n",
    "- The hidden state **carries information** from previous time steps, updating with each new input.\n",
    "- The outputs are computed at each time step, making the RNN suitable for **sequential data** processing.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Summary**\n",
    "âœ” We **manually computed** RNN forward propagation step by step.  \n",
    "âœ” The **hidden state** maintains memory across time steps.  \n",
    "âœ” The **output at each step** depends on both the current input and previous hidden state.  \n",
    "âœ” **Activation function (tanh)** ensures values remain between $-1$ and $1$.  \n",
    "âœ” **Weights are shared** across all time steps, making the RNN efficient.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! RNN architectures can be categorized based on the **input-output relationship**, which defines how sequences are processed. Letâ€™s break them down in a fun and colorful way! ğŸš€ğŸ”¥  \n",
    "\n",
    "## ğŸ¯ **Types of RNN Based on Input-Output Structure**  \n",
    "\n",
    "| Type | Input | Output | Example Use Case |\n",
    "|------|-------|--------|-----------------|\n",
    "| **One-to-One** | ğŸ”¹ Single input | ğŸ”¸ Single output | Simple classification (e.g., Spam detection ğŸ“©) |\n",
    "| **One-to-Many** | ğŸ”¹ Single input | ğŸ”¸ Sequence of outputs | Music generation ğŸµ, Image captioning ğŸ–¼ |\n",
    "| **Many-to-One** | ğŸ”¹ Sequence of inputs | ğŸ”¸ Single output | Sentiment analysis ğŸ˜ŠğŸ˜¢, Fraud detection ğŸ’³ |\n",
    "| **Many-to-Many (Same Length)** | ğŸ”¹ Sequence of inputs | ğŸ”¸ Sequence of outputs | Video frame labeling ğŸ¥, POS tagging ğŸ“Œ |\n",
    "| **Many-to-Many (Different Length)** | ğŸ”¹ Sequence of inputs | ğŸ”¸ Sequence of outputs | Machine translation ğŸŒ, Speech-to-text ğŸ¤ |\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ **One-to-One (Vanilla Neural Network)**\n",
    "- âœ… **Single input â†’ Single output**  \n",
    "- ğŸ”¥ **Example:** Image classification ğŸ“¸ (e.g., classifying an image as **dog** ğŸ¶ or **cat** ğŸ±)  \n",
    "- ğŸ¤– **Works like:** A standard feedforward network with no sequential memory.  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Imagine you **see one photo** ğŸ–¼ and simply classify it as \"cat\" or \"dog\".  \n",
    "\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ **One-to-Many (Single Input, Multiple Outputs)**\n",
    "- âœ… **Single input â†’ Sequence of outputs**  \n",
    "- ğŸ”¥ **Example:**  \n",
    "  - **Music generation** ğŸ¶ (e.g., input a musical **style**, generate a full melody).  \n",
    "  - **Image captioning** ğŸ (e.g., input an **image**, generate a **sentence** describing it).  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Imagine someone shows you a **picture of a sunset** ğŸŒ…, and you start describing it:  \n",
    "*\"The sky is orange, birds are flying, it's evening time.\"*  \n",
    "\n",
    "ğŸ’¡ **Used in:** LSTMs, GRUs when generating sequences from a single source.\n",
    "\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ **Many-to-One (Sequence Input, Single Output)**\n",
    "- âœ… **Multiple inputs â†’ Single output**  \n",
    "- ğŸ”¥ **Example:**  \n",
    "  - **Sentiment analysis** ğŸ˜ŠğŸ˜¢ (e.g., input a sentence, classify it as **positive or negative**).  \n",
    "  - **Fraud detection** ğŸ’³ (e.g., analyze a customerâ€™s transaction history and classify as **fraud/not fraud**).  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "You **read a full movie review** ğŸ¬ and decide: *\"Was the review positive or negative?\"*  \n",
    "\n",
    "ğŸ’¡ **Used in:** LSTMs, GRUs for tasks where context builds over time.\n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ **Many-to-Many (Same Length)**\n",
    "- âœ… **Sequence input â†’ Sequence output** (same number of inputs and outputs).  \n",
    "- ğŸ”¥ **Example:**  \n",
    "  - **Video frame labeling** ğŸ¥ (e.g., classify each frame in a video).  \n",
    "  - **Part-of-Speech (POS) tagging** ğŸ“Œ (e.g., tagging each word as **noun, verb, adjective**).  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "You **read a sentence** ğŸ“– and label each word with its part of speech:  \n",
    "*\"The (Determiner) dog (Noun) runs (Verb) fast (Adverb).\"*  \n",
    "\n",
    "ğŸ’¡ **Used in:** Bi-directional RNNs (Bi-RNNs), LSTMs for tasks requiring **sequential context**.\n",
    "\n",
    "\n",
    "\n",
    "## 5ï¸âƒ£ **Many-to-Many (Different Length)**\n",
    "- âœ… **Sequence input â†’ Sequence output** (variable lengths).  \n",
    "- ğŸ”¥ **Example:**  \n",
    "  - **Machine translation** ğŸŒ (e.g., English sentence â†’ French sentence).  \n",
    "  - **Speech-to-text** ğŸ¤ (e.g., input voice, output text transcript).  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "You **listen to someone speaking in English** ğŸ™ and translate it into French:  \n",
    "*\"Hello, how are you?\" â†’ *\"Bonjour, comment Ã§a va?\"*  \n",
    "\n",
    "ğŸ’¡ **Used in:** **Encoder-Decoder RNNs**, often paired with **attention mechanisms**.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Final Thoughts**\n",
    "- If you need **sequential processing**, **RNNs** (especially **LSTMs & GRUs**) are your go-to!  \n",
    "- Choose the structure based on **input-output format** ğŸš€.  \n",
    "- For **short-term dependencies**, Vanilla RNN might work. But for **longer memory**, use **LSTM or GRU**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to process **sequential data** by maintaining a **memory** of past inputs. Unlike traditional feedforward networks, RNNs have **loops** that allow information to persist, making them ideal for tasks like **speech recognition, language modeling, and time series forecasting**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒŸ **Types of RNNs** ğŸŒŸ\n",
    "\n",
    "### 1ï¸âƒ£ **Basic RNN (Vanilla RNN)**\n",
    "ğŸ“Œ **Key Idea:** Each neuron not only receives input from the current timestep but also retains **memory** from the previous step.  \n",
    "\n",
    "ğŸ”— **Structure:**  \n",
    "It consists of a **hidden state** that is updated at each timestep based on the previous state and current input:\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "ğŸš¨ **Limitation:**  \n",
    "- Suffers from **vanishing gradient problem**, making it hard to remember long-term dependencies.\n",
    "\n",
    "âœ… **Used For:**  \n",
    "- Short-term memory tasks (e.g., **simple text generation, stock price prediction**).\n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Imagine you're reading a book, but you can only remember the last **few** words from each sentence.\n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ **Long Short-Term Memory (LSTM)**\n",
    "ğŸ“Œ **Key Idea:** Introduces **gates** to control the flow of information, allowing it to **remember** or **forget** information selectively.  \n",
    "\n",
    "ğŸ”— **Structure:**  \n",
    "LSTMs have **three gates**:\n",
    "- ğŸ— **Forget Gate (ğŸšª)** â€“ Decides what past information to discard.  \n",
    "- ğŸ— **Input Gate (ğŸ“¥)** â€“ Determines what new information to store.  \n",
    "- ğŸ— **Output Gate (ğŸ“¤)** â€“ Controls what part of the hidden state is passed to the next step.  \n",
    "\n",
    "ğŸš€ **Advantages:**  \n",
    "- Handles **long-term dependencies** better than Vanilla RNN.\n",
    "- Avoids **vanishing gradient problem**.\n",
    "\n",
    "âœ… **Used For:**  \n",
    "- **Speech recognition** (like Siri, Google Assistant).  \n",
    "- **Machine translation** (like Google Translate).  \n",
    "- **Time-series forecasting** (like predicting weather trends).  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Think of LSTM as a **notebook** ğŸ“ where you write important notes and erase unimportant details as you read a book.\n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ **Gated Recurrent Unit (GRU)**\n",
    "ğŸ“Œ **Key Idea:** A simplified version of LSTM with only **two gates**:\n",
    "- ğŸ”„ **Reset Gate (ğŸ”„)** â€“ Determines how much of past information to forget.  \n",
    "- ğŸ”„ **Update Gate (â©)** â€“ Decides how much new information to keep.  \n",
    "\n",
    "ğŸš€ **Advantages:**  \n",
    "- Works **faster** than LSTM because it has fewer parameters.  \n",
    "- Retains efficiency while maintaining good performance on sequential tasks.\n",
    "\n",
    "âœ… **Used For:**  \n",
    "- **Chatbots** ğŸ¤– like ChatGPT!  \n",
    "- **Handwriting recognition** âœï¸.  \n",
    "- **Music generation** ğŸµ.  \n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Imagine **GRU** as a **sticky note** where you only keep the most important details while discarding unnecessary ones.\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ **Bidirectional RNN (Bi-RNN)**\n",
    "ğŸ“Œ **Key Idea:** Processes information in **both forward and backward** directions.  \n",
    "\n",
    "ğŸ”— **Structure:**  \n",
    "- One RNN processes **left to right** ğŸ¡†.  \n",
    "- Another RNN processes **right to left** ğŸ¡„.  \n",
    "- The outputs from both are combined for better accuracy.  \n",
    "\n",
    "ğŸš€ **Advantages:**  \n",
    "- Can **understand context better** (e.g., recognizing a wordâ€™s meaning based on future words).  \n",
    "- Great for **sequence labeling tasks**.\n",
    "\n",
    "âœ… **Used For:**  \n",
    "- **Speech recognition** ğŸ¤ (Google Voice, Alexa).  \n",
    "- **Named Entity Recognition (NER)** ğŸ· (used in NLP).  \n",
    "- **DNA sequence analysis** ğŸ§¬.\n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Think of it as reading a sentence **both forwards and backwards** to get the full meaning.\n",
    "\n",
    "\n",
    "\n",
    "### 5ï¸âƒ£ **Echo State Networks (ESN)**\n",
    "ğŸ“Œ **Key Idea:** Uses a **randomly initialized** reservoir (hidden layer) to store information without training it directly.\n",
    "\n",
    "ğŸš€ **Advantages:**  \n",
    "- Faster training ğŸƒâ€â™‚ï¸ğŸ’¨.  \n",
    "- Good for **time-series forecasting** ğŸ“ˆ.\n",
    "\n",
    "âœ… **Used For:**  \n",
    "- **Financial predictions** (stock market).  \n",
    "- **Brain-inspired computing** ğŸ§ .\n",
    "\n",
    "ğŸ–¼ **Illustration:**  \n",
    "Itâ€™s like a sponge ğŸ§½ that **absorbs** patterns from input data and then extracts useful features!\n",
    "\n",
    "## ğŸ¯ **Comparison Table**\n",
    "\n",
    "| Type        | Handles Long-term Memory? | Speed â© | Best For |\n",
    "|------------|-------------------------|---------|---------|\n",
    "| **Vanilla RNN** | âŒ No (Vanishing Gradient) | âœ… Fast | Simple sequential tasks |\n",
    "| **LSTM** | âœ… Yes (Uses Gates) | âŒ Slower | Speech recognition, NLP |\n",
    "| **GRU** | âœ… Yes (Simpler than LSTM) | âœ… Faster | Chatbots, Music generation |\n",
    "| **Bi-RNN** | âœ… Yes (Both Directions) | âŒ Slower | Named Entity Recognition, Speech |\n",
    "| **ESN** | âœ… Yes (Fixed Reservoir) | ğŸš€ Very Fast | Financial forecasting |\n",
    "\n",
    "\n",
    "## ğŸ† **Conclusion**\n",
    "Different RNNs serve different purposes. **LSTMs & GRUs** are the most commonly used due to their ability to handle **long-term dependencies**. If **speed is a priority**, **GRU** is better than LSTM. For **tasks requiring full context understanding**, **Bidirectional RNN** is a strong choice.\n",
    "\n",
    "ğŸ”¥ **So next time you build an NLP or time-series model, choose the right RNN wisely!** ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ **Backpropagation in RNNs â€“ A Deep Dive!** ğŸ”¥  \n",
    "\n",
    "Backpropagation in Recurrent Neural Networks (RNNs) is a bit different from standard feedforward networks because of their sequential nature. This process is called **Backpropagation Through Time (BPTT)**. Let's break it down step by step!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Understanding Backpropagation in RNNs**\n",
    "### ğŸŒŸ **Step 1: Forward Pass**  \n",
    "In a standard RNN, we pass input sequences **step by step** through the network while maintaining a hidden state:  \n",
    "\n",
    "$$\n",
    "h_t = f(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + c)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_t $ = hidden state at time $ t $, which depends on previous state $ h_{t-1} $  \n",
    "- $ y_t $ = output at time $ t $  \n",
    "- $ W_h, W_x, W_y $ = weight matrices  \n",
    "- $ b, c $ = biases  \n",
    "- $ f, g $ = activation functions (e.g., **tanh, softmax**)  \n",
    "\n",
    "During this process, the **hidden state carries information** forward in time, making RNNs great for sequential tasks like speech recognition and text processing.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ **Step 2: Loss Calculation**  \n",
    "After the forward pass, we compute the **loss** using a function like **Mean Squared Error (MSE) or Cross-Entropy Loss**, depending on the problem (regression or classification).  \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t)\n",
    "$$\n",
    "\n",
    "where $ L $ is the loss function and $ \\hat{y}_t $ is the predicted output.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ” **Step 3: Backpropagation Through Time (BPTT)**\n",
    "This is where things get interesting! Unlike standard backpropagation (which flows only through layers), RNN backpropagation **flows through time** as well.  \n",
    "\n",
    "ğŸ›  **Steps in BPTT:**  \n",
    "1ï¸âƒ£ Compute **gradients at the last time step** ($ T $) and move backward.  \n",
    "2ï¸âƒ£ Compute **gradients for each earlier time step** until $ t=1 $.  \n",
    "3ï¸âƒ£ Update weights using **gradient descent** or any optimizer like Adam, RMSprop.  \n",
    "\n",
    "#### ğŸ”¹ **Gradient Calculation**\n",
    "For each time step $ t $, we compute gradients of the loss with respect to weights using the **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\sum_{t=1}^{T} \\sum_{k=t}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_k} \\cdot \\frac{\\partial y_k}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "ğŸ›‘ **Why is this tricky?**  \n",
    "- **The hidden states are shared** across all time steps.  \n",
    "- **Error at one step** affects all previous steps.  \n",
    "- **Long-term dependencies** make it difficult to train (this is called the **vanishing gradient problem** ğŸ›‘).  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ›‘ **Step 4: Vanishing and Exploding Gradients**\n",
    "ğŸ’¡ **Vanishing Gradients:**  \n",
    "- If gradients become **too small**, updates **disappear**, and the model stops learning **long-term dependencies**.  \n",
    "- This happens when we keep multiplying small values (like derivatives of sigmoid/tanh functions).  \n",
    "\n",
    "ğŸ’¥ **Exploding Gradients:**  \n",
    "- If gradients **grow too large**, training becomes **unstable**, and weights explode.  \n",
    "- Happens when weights keep multiplying large values, causing loss to **diverge**.  \n",
    "\n",
    "ğŸ”¹ **Solutions:**  \n",
    "âœ… Use **Long Short-Term Memory (LSTM)** or **Gated Recurrent Unit (GRU)** to control gradient flow.  \n",
    "âœ… Apply **gradient clipping** (cap gradients to a maximum value).  \n",
    "âœ… Use **ReLU** instead of **sigmoid/tanh** where possible.  \n",
    "\n",
    "\n",
    "\n",
    "### âš¡ **Step 5: Updating Weights**\n",
    "Once gradients are computed, we update weights using **Gradient Descent** or other optimizers like **Adam, RMSprop**:\n",
    "\n",
    "$$\n",
    "W = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the **learning rate**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Key Takeaways**\n",
    "âœ… **BPTT propagates errors backward through time, affecting all previous time steps**.  \n",
    "âœ… **Vanishing gradients make long-term dependencies hard to learn**.  \n",
    "âœ… **LSTMs and GRUs solve vanishing gradient issues**.  \n",
    "âœ… **Gradient clipping helps control exploding gradients**.  \n",
    "\n",
    "\n",
    "### ğŸ”¥ **Final Thought**\n",
    "Backpropagation in RNNs is like **teaching a student** step by step, correcting mistakes from both **recent and past** lessons! ğŸ“š  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually go through an **example** of backpropagation in a simple Recurrent Neural Network (RNN) using **Backpropagation Through Time (BPTT)**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Example: A Simple RNN with One Neuron**\n",
    "We will calculate **forward pass, loss, and backpropagation (BPTT)** for a simple RNN with:  \n",
    "âœ… **1 input neuron**  \n",
    "âœ… **1 hidden neuron** (with recurrent connection)  \n",
    "âœ… **1 output neuron**  \n",
    "âœ… **1 time step for simplicity**  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Step 1: Define Network and Initial Weights**\n",
    "We define:  \n",
    "- $ W_x = 0.5 $ (input-to-hidden weight)  \n",
    "- $ W_h = 0.8 $ (hidden-to-hidden recurrent weight)  \n",
    "- $ W_y = 0.3 $ (hidden-to-output weight)  \n",
    "- **Biases are ignored** for simplicity.  \n",
    "\n",
    "Given:  \n",
    "- Input: $ x_1 = 1 $  \n",
    "- True output: $ y_{\\text{true}} = 0.6 $  \n",
    "- Initial hidden state: $ h_0 = 0 $  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ **Step 2: Forward Pass**\n",
    "#### ğŸ”¹ **Hidden State Calculation**  \n",
    "$$\n",
    "h_1 = \\tanh(W_x x_1 + W_h h_0)\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5(1) + 0.8(0))\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5) = 0.462\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ **Output Calculation**\n",
    "$$\n",
    "y_{\\text{pred}} = W_y h_1\n",
    "$$\n",
    "$$\n",
    "= 0.3 \\times 0.462 = 0.1386\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ **Loss Calculation (Mean Squared Error)**\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (y_{\\text{true}} - y_{\\text{pred}})^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.6 - 0.1386)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.4614)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.213) = 0.1065\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ” **Step 3: Backpropagation Through Time (BPTT)**  \n",
    "Now, we compute the **gradients of the loss** with respect to each weight.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Output Weight $ W_y $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "We compute the derivatives:  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} = (y_{\\text{pred}} - y_{\\text{true}}) = (0.1386 - 0.6) = -0.4614\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial W_y} = h_1 = 0.462\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = (-0.4614) \\times (0.462) = -0.213\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Hidden Weight $ W_h $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial h_1} = W_y = 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_h} = (1 - h_1^2) \\times h_0 = (1 - 0.462^2) \\times 0 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = (-0.4614) \\times (0.3) \\times (0) = 0\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ Since $ h_0 = 0 $, the gradient for $ W_h $ is **zero** in this case.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Input Weight $ W_x $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_x} = (1 - h_1^2) \\times x_1 = (1 - 0.462^2) \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (1 - 0.213) = 0.787\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = (-0.4614) \\times (0.3) \\times (0.787)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -0.1088\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## âœï¸ **Step 4: Weight Updates Using Gradient Descent**\n",
    "Using **learning rate** $ \\eta = 0.1 $, we update:\n",
    "\n",
    "$$\n",
    "W_y = W_y - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_y}\n",
    "$$\n",
    "$$\n",
    "= 0.3 - (0.1 \\times -0.213)\n",
    "$$\n",
    "$$\n",
    "= 0.3 + 0.0213 = 0.3213\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_x = W_x - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_x}\n",
    "$$\n",
    "$$\n",
    "= 0.5 - (0.1 \\times -0.1088)\n",
    "$$\n",
    "$$\n",
    "= 0.5 + 0.01088 = 0.51088\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_h = 0.8 - (0.1 \\times 0) = 0.8\n",
    "$$  \n",
    "(Since the gradient was zero, $ W_h $ remains unchanged.)\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Updated Weights**\n",
    "After **one iteration of BPTT**, we get:  \n",
    "âœ… $ W_x = 0.51088 $  \n",
    "âœ… $ W_h = 0.8 $  \n",
    "âœ… $ W_y = 0.3213 $  \n",
    "\n",
    "If we repeat this over multiple time steps, RNN learns to predict better over time! ğŸ”¥\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Takeaways**\n",
    "âœ” **BPTT works by computing gradients backward through time** â³  \n",
    "âœ” **Weight updates use the chain rule** to propagate errors  \n",
    "âœ” **Vanishing gradients** occur when gradients become too small  \n",
    "âœ” **Exploding gradients** occur when gradients grow too large  \n",
    "âœ” **Optimizations like LSTMs, GRUs, and gradient clipping help stabilize learning** ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **Problems with RNNs: Why They Struggle and How to Fix Them**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are great for handling **sequential data** like **text, speech, and time series**, but they come with several limitations. Letâ€™s break them down in a **simple, colorful way** and also discuss possible solutions! ğŸŒˆ  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **1. Vanishing Gradient Problem**\n",
    "### âŒ **What is it?**\n",
    "- When training an RNN with **backpropagation through time (BPTT)**, the gradients shrink **exponentially** as they are passed backward through many time steps.  \n",
    "- This means earlier layers receive **almost no updates**, making it **hard for RNNs to learn long-term dependencies**.\n",
    "\n",
    "### ğŸ“‰ **Why does this happen?**\n",
    "- The chain rule in **backpropagation** involves multiplying many small values (gradients of activation functions like sigmoid or tanh), leading to values approaching **zero**.\n",
    "- This results in **\"memory loss\"** in RNNsâ€”**they forget long-term dependencies**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units)** â€“ They use special gates to store and update information efficiently.  \n",
    "âœ… **Use ReLU activation instead of tanh/sigmoid** â€“ ReLU helps prevent gradients from shrinking.  \n",
    "âœ… **Use batch normalization or layer normalization** to stabilize training.  \n",
    "âœ… **Gradient clipping** â€“ Limits the gradient values to prevent them from shrinking too much.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **2. Exploding Gradient Problem**\n",
    "### âŒ **What is it?**\n",
    "- The opposite of the vanishing gradient problem!  \n",
    "- When gradients grow **too large**, they cause unstable updates, making the model diverge instead of learning.\n",
    "\n",
    "### ğŸ“ˆ **Why does this happen?**\n",
    "- If weights are large or initialized poorly, gradients can **explode exponentially** during backpropagation.\n",
    "- This results in sudden, erratic updates, making the network **unstable**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Gradient Clipping** â€“ Set a threshold so that gradients donâ€™t grow beyond a certain limit.  \n",
    "âœ… **Use smaller learning rates** to prevent large weight updates.  \n",
    "âœ… **Use careful weight initialization techniques** like Xavier or He initialization.  \n",
    "\n",
    "\n",
    "\n",
    "## â³ **3. Short-Term Memory Issue**\n",
    "### âŒ **What is it?**\n",
    "- Standard RNNs struggle to remember information **from many time steps ago**.  \n",
    "- If a dependency spans **20+ time steps**, the network simply **forgets** it.\n",
    "\n",
    "### ğŸ¤¯ **Example:**  \n",
    "Imagine reading a long paragraph and trying to remember a name mentioned at the beginning. **By the time you reach the end, youâ€™ve forgotten it!** Thatâ€™s what happens to RNNs.  \n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… Use **LSTMs or GRUs** â€“ These architectures store **long-term information** better than standard RNNs.  \n",
    "âœ… Use **Attention Mechanisms** â€“ They help focus on **important parts** of the input sequence.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¢ **4. Slow Training and High Computation Costs**\n",
    "### âŒ **What is it?**\n",
    "- RNNs **process inputs sequentially**, meaning **no parallelization** like CNNs.  \n",
    "- This makes them **slower** and **more computationally expensive** compared to feedforward networks.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use parallel architectures like Transformers** (they donâ€™t process inputs sequentially).  \n",
    "âœ… **Use GPU acceleration** for faster matrix computations.  \n",
    "âœ… **Reduce sequence length** if possible, or use **truncated BPTT** to limit time steps during training.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ­ **5. Difficulty in Capturing Long-Term Dependencies**\n",
    "### âŒ **What is it?**\n",
    "- RNNs **focus more on recent inputs** and often fail to link **old words/events** in a sequence.  \n",
    "- Example: If a document introduces a character **50 sentences ago**, a simple RNN wonâ€™t remember them!\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use LSTMs/GRUs** â€“ These have memory cells that **store relevant past information**.  \n",
    "âœ… **Use Attention Mechanisms** â€“ They help the model **attend** to specific parts of the input.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ **6. Bias Towards Recent Inputs**\n",
    "### âŒ **What is it?**\n",
    "- RNNs have a **recency bias**, meaning they **prioritize recent inputs** over older ones.  \n",
    "- Example: If a chatbot sees **\"not good\"** at the beginning of a sentence but **\"great\"** at the end, it may only remember **\"great\"**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Bidirectional RNNs** â€“ They read input **both forward and backward**.  \n",
    "âœ… **Use Transformers** â€“ They process the entire sequence at once.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **7. Handling Variable-Length Sequences is Hard**\n",
    "### âŒ **What is it?**\n",
    "- RNNs struggle with **very long** or **very short** sequences.  \n",
    "- Padding/truncating sequences can sometimes **distort the meaning**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Dynamic RNNs** â€“ These handle variable-length sequences without padding issues.  \n",
    "âœ… **Use Attention Mechanisms** â€“ They allow the model to focus on **important** sequence parts.  \n",
    "\n",
    "\n",
    "\n",
    "## âš ï¸ **8. Poor Performance on Very Long Sequences**\n",
    "### âŒ **What is it?**\n",
    "- If sequences have **thousands of time steps**, RNNs perform **poorly**.  \n",
    "- This is why **speech recognition and machine translation** models often struggle with RNNs.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Transformers** (like BERT and GPT) â€“ These work **better for long-range dependencies**.  \n",
    "âœ… **Use Hierarchical RNNs** â€“ Process data at multiple levels for better representation.  \n",
    "\n",
    "# ğŸ¯ **Summary of RNN Problems & Fixes**\n",
    "| ğŸ›‘ **Problem**                  | ğŸ”¥ **Solution** |\n",
    "|---------------------------------|----------------|\n",
    "| **Vanishing Gradient**   | LSTMs, GRUs, ReLU, Gradient Clipping |\n",
    "| **Exploding Gradient**   | Gradient Clipping, Smaller Learning Rate |\n",
    "| **Short-Term Memory**    | LSTMs, GRUs, Attention |\n",
    "| **Slow Training**        | Transformers, GPUs, Parallelization |\n",
    "| **Long-Term Dependencies** | LSTMs, GRUs, Attention |\n",
    "| **Recency Bias**         | Bidirectional RNNs, Transformers |\n",
    "| **Variable-Length Issues** | Dynamic RNNs, Attention |\n",
    "| **Poor Performance on Long Sequences** | Transformers, Hierarchical Models |\n",
    "\n",
    "\n",
    "## ğŸ¤– **The Future: Moving Beyond RNNs**\n",
    "Because of these problems, newer architectures like **LSTMs, GRUs, and Transformers** (GPT, BERT) have replaced vanilla RNNs in most real-world applications! ğŸš€  \n",
    "\n",
    "Would you like a practical **example** of solving these issues using **LSTMs or Transformers** in Python? ğŸ¤”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM) Explained in a Colorful Way ğŸ¨âœ¨**\n",
    "\n",
    "Imagine your brain as a **notebook** where you write important things you need to remember. But hereâ€™s the catchâ€”your memory is not perfect! Sometimes, you **forget unimportant details** and **retain only the essential ones**. This is exactly how an **LSTM (Long Short-Term Memory)** network works in deep learning!  \n",
    "\n",
    "\n",
    "### **ğŸŒŸ What is LSTM?**\n",
    "LSTM is a special type of **Recurrent Neural Network (RNN)** designed to **remember important information** over long periods and **forget unnecessary details**. Unlike a normal RNN that struggles with long-term dependencies (because it keeps forgetting things), LSTM has a **smart memory mechanism** to selectively store and erase information.  \n",
    "\n",
    "\n",
    "### **ğŸ§  LSTMâ€™s Secret Superpowers: Gates! ğŸšª**\n",
    "LSTM has three magical \"gates\" that decide what to **keep, update, and forget** in the memory:  \n",
    "\n",
    "1ï¸âƒ£ **Forget Gate ğŸ”¥**  \n",
    "   - This gate decides what old information should be thrown away.  \n",
    "   - Example: \"Do I really need to remember what I ate for breakfast three days ago? Nope! Forget it!\"  \n",
    "\n",
    "2ï¸âƒ£ **Input Gate ğŸ“¥**  \n",
    "   - This gate decides what new information should be added to memory.  \n",
    "   - Example: \"Ah! I just learned a new word today! Letâ€™s save it in memory.\"  \n",
    "\n",
    "3ï¸âƒ£ **Output Gate ğŸ“¤**  \n",
    "   - This gate determines what should be **sent as output** to the next time step.  \n",
    "   - Example: \"I need to recall my friendâ€™s birthday today, so letâ€™s retrieve it from memory!\"  \n",
    "\n",
    "\n",
    "### **ğŸ¨ Visualizing the LSTM Process**\n",
    "1ï¸âƒ£ **Incoming data arrives** at the LSTM cell.  \n",
    "2ï¸âƒ£ The **Forget Gate** decides what past info should be erased.  \n",
    "3ï¸âƒ£ The **Input Gate** updates memory with useful new info.  \n",
    "4ï¸âƒ£ The **Output Gate** selects what needs to be passed forward.  \n",
    "\n",
    "The **Cell State** is like a conveyor belt ğŸ¢ that keeps flowing, carrying essential information through time while discarding whatâ€™s unnecessary.  \n",
    "\n",
    "\n",
    "### **ğŸš€ Where is LSTM Used?**\n",
    "LSTMs are widely used in:  \n",
    "ğŸ”¹ **Speech Recognition** (e.g., Siri, Google Assistant)  \n",
    "ğŸ”¹ **Chatbots** (handling long conversations)  \n",
    "ğŸ”¹ **Stock Price Prediction** (analyzing past trends)  \n",
    "ğŸ”¹ **Language Translation** (remembering previous words for better sentences)  \n",
    "ğŸ”¹ **Music Generation** (creating melodies that make sense over time)  \n",
    "\n",
    "\n",
    "### **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ LSTM is an advanced type of RNN that **remembers** important things for long durations.  \n",
    "âœ”ï¸ It uses **Forget, Input, and Output Gates** to manage memory efficiently.  \n",
    "âœ”ï¸ Used in applications where remembering past information is **crucial** (speech, text, stock trends, etc.).  \n",
    "\n",
    "Now, if LSTMs were people, theyâ€™d be **the best note-takers in the world!** ğŸ“âœ¨  \n",
    "Want to dive deeper? Letâ€™s discuss! ğŸš€\n",
    "\n",
    "![](images/lstm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“Œ Long Short-Term Memory (LSTM) Architecture Explained in Detail ğŸš€**  \n",
    "\n",
    "LSTM is a type of **Recurrent Neural Network (RNN)** designed to handle **long-term dependencies** in sequential data. Unlike vanilla RNNs, which struggle with the **vanishing gradient problem**, LSTMs have a **memory cell** that selectively stores and forgets information over long sequences.  \n",
    "\n",
    "Letâ€™s break down the **LSTM architecture** in an easy-to-understand and colorful way! ğŸ¨âœ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ LSTM Architecture: The Building Blocks ğŸ—ï¸**  \n",
    "Each LSTM unit (or **cell**) consists of:  \n",
    "âœ… **Cell State** ($ C_t $) â€“ The \"memory\" that carries long-term information.  \n",
    "âœ… **Hidden State** ($ h_t $) â€“ The output of the current LSTM cell, passed to the next step.  \n",
    "âœ… **Three Gates** (Forget, Input, and Output) â€“ Control what gets updated, remembered, or forgotten.  \n",
    "\n",
    "At each time step $ t $, an LSTM cell processes:  \n",
    "ğŸ”¹ The current input $ x_t $  \n",
    "ğŸ”¹ The previous hidden state $ h_{t-1} $  \n",
    "ğŸ”¹ The previous cell state $ C_{t-1} $  \n",
    "\n",
    "Now, letâ€™s go deep into **each component**! ğŸ”  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸšª 1. Forget Gate $ f_t $ â€“ Decides What to Erase! ğŸ”¥**  \n",
    "The **Forget Gate** decides which parts of the previous cell state $ C_{t-1} $ should be discarded.  \n",
    "ğŸ‘‰ It uses a **sigmoid activation function** ($ \\sigma $) to produce values between **0 and 1** (0 = forget completely, 1 = keep fully).  \n",
    "\n",
    "ğŸ”¢ **Formula:**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_f $ and $ b_f $ are the weight matrix and bias for the forget gate.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ x_t $ is the current input.  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- If $ f_t $ is **close to 0**, forget the information.  \n",
    "- If $ f_t $ is **close to 1**, retain the information.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“¥ 2. Input Gate $ i_t $ â€“ Decides What to Store! ğŸ“**  \n",
    "The **Input Gate** determines what new information should be added to the memory cell.  \n",
    "ğŸ‘‰ It consists of:  \n",
    "âœ… A **sigmoid layer** to decide which values to update.  \n",
    "âœ… A **tanh layer** to create a candidate memory update $ \\tilde{C}_t $.  \n",
    "\n",
    "ğŸ”¢ **Formulas:**  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$  \n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- $ i_t $ controls **how much** of $ \\tilde{C}_t $ should be stored in memory.  \n",
    "- $ \\tilde{C}_t $ contains the potential **new information**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”„ 3. Update Cell State $ C_t $ â€“ The Actual Memory! ğŸ§ **  \n",
    "After **forgetting some old info** and **adding new info**, we update the **cell state**:  \n",
    "\n",
    "ğŸ”¢ **Formula:**  \n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- The **old memory $ C_{t-1} $** is reduced based on $ f_t $.  \n",
    "- The **new memory $ \\tilde{C}_t $** is added based on $ i_t $.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“¤ 4. Output Gate $ o_t $ â€“ Decides the Final Output! ğŸ“Š**  \n",
    "The **Output Gate** determines what the **hidden state** $ h_t $ (the output of the LSTM cell) should be.  \n",
    "\n",
    "ğŸ”¢ **Formulas:**  \n",
    "$$\n",
    "o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$  \n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- $ o_t $ acts as a filter, deciding **which parts of $ C_t $** should be output.  \n",
    "- The **hidden state $ h_t $** is used in the next LSTM step and can also be passed to other layers (like dense layers for classification).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Putting It All Together: LSTM Workflow ğŸ”„**\n",
    "At each time step $ t $, an LSTM cell follows these steps:  \n",
    "1ï¸âƒ£ **Forget** old information ($ f_t $).  \n",
    "2ï¸âƒ£ **Decide what new information to store** ($ i_t $, $ \\tilde{C}_t $).  \n",
    "3ï¸âƒ£ **Update the memory cell** ($ C_t $).  \n",
    "4ï¸âƒ£ **Compute the final output** ($ h_t $) using the Output Gate.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Where is LSTM Used?**\n",
    "LSTM is widely used in:  \n",
    "ğŸ”¹ **Speech Recognition** ğŸ™ï¸ (e.g., Siri, Google Assistant)  \n",
    "ğŸ”¹ **Text Generation** ğŸ“ (e.g., ChatGPT, poetry generation)  \n",
    "ğŸ”¹ **Time-Series Forecasting** ğŸ“ˆ (e.g., stock prices, weather prediction)  \n",
    "ğŸ”¹ **Machine Translation** ğŸŒ (e.g., Google Translate)  \n",
    "ğŸ”¹ **Music Generation** ğŸµ (e.g., AI composing music)  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ LSTM has a **memory cell** that retains important information over time.  \n",
    "âœ”ï¸ It uses **Forget, Input, and Output Gates** to control information flow.  \n",
    "âœ”ï¸ Unlike RNNs, LSTM can handle **long-term dependencies** efficiently.  \n",
    "âœ”ï¸ Used in various applications like **NLP, speech processing, and forecasting**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¨ Visual Summary**\n",
    "Imagine LSTM as a **smart secretary** ğŸ§‘â€ğŸ’¼ managing a **to-do list**:  \n",
    "âœ… **Forget Gate** removes unnecessary tasks.  \n",
    "âœ… **Input Gate** adds new important tasks.  \n",
    "âœ… **Cell State** is the notebook holding all tasks.  \n",
    "âœ… **Output Gate** decides what tasks should be shared.  \n",
    "\n",
    "LSTMs are **powerful tools** in deep learning, allowing AI to learn patterns in time-dependent data effectively! ğŸš€ğŸ”¥  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“Œ Forget Gate Architecture in LSTM â€“ A Deep Dive ğŸ”¥**  \n",
    "\n",
    "The **Forget Gate** is a crucial component of Long Short-Term Memory (LSTM) networks. Its main job is to **decide which information should be discarded (forgotten) from the cell state** at each time step. This prevents the network from storing irrelevant or outdated information.  \n",
    "\n",
    "Letâ€™s explore its architecture, mathematical equations, and how it works step by step. ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” 1. Forget Gate Overview**\n",
    "The **Forget Gate** is responsible for **removing unnecessary information** from the **Cell State** $ C_t $.  \n",
    "\n",
    "### **ğŸ’¡ Key Idea**  \n",
    "At every time step $ t $, the Forget Gate receives:  \n",
    "- The **previous hidden state** $ h_{t-1} $ (short-term memory)  \n",
    "- The **current input** $ x_t $ (new incoming data)  \n",
    "\n",
    "It then decides, using a **sigmoid activation function ($ \\sigma $)**, which parts of the previous cell state $ C_{t-1} $ should be **kept** and which should be **forgotten**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ 2. Forget Gate Architecture ğŸ—ï¸**  \n",
    "\n",
    "ğŸ”¹ The Forget Gate consists of:  \n",
    "âœ… **A weight matrix** $ W_f $ that helps learn which information should be forgotten.  \n",
    "âœ… **A bias term** $ b_f $ that adds flexibility to the learning process.  \n",
    "âœ… **A sigmoid activation function** $ \\sigma $ to produce values between **0 and 1** (0 = completely forget, 1 = completely remember).  \n",
    "\n",
    "### **ğŸ”¢ Mathematical Formula**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "where:  \n",
    "- $ W_f $ is the weight matrix for the forget gate.  \n",
    "- $ [h_{t-1}, x_t] $ is the concatenation of the previous hidden state and current input.  \n",
    "- $ b_f $ is the bias term.  \n",
    "- $ \\sigma $ is the sigmoid activation function.  \n",
    "\n",
    "ğŸ“Œ **Sigmoid ensures that**:  \n",
    "- If $ f_t $ is **close to 0**, the information is forgotten.  \n",
    "- If $ f_t $ is **close to 1**, the information is retained.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”„ 3. Step-by-Step Working of the Forget Gate**\n",
    "At **each time step $ t $**, the Forget Gate operates as follows:\n",
    "\n",
    "### **ğŸŸ¢ Step 1: Take Input**\n",
    "- The Forget Gate receives **two inputs**:\n",
    "  - **Previous hidden state** $ h_{t-1} $ (from the last LSTM cell).\n",
    "  - **Current input** $ x_t $ (new information).  \n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "If we are processing a sentence, $ x_t $ could be a **new word**, and $ h_{t-1} $ holds the context from previous words.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”µ Step 2: Compute Forget Score**\n",
    "- The Forget Gate applies a **linear transformation**:  \n",
    "  $$\n",
    "  z = W_f \\cdot [h_{t-1}, x_t] + b_f\n",
    "  $$\n",
    "- Then, a **sigmoid activation function** is applied to get a value between **0 and 1**:\n",
    "  $$\n",
    "  f_t = \\sigma(z)\n",
    "  $$\n",
    "  \n",
    "ğŸ“Œ **Example Output:**  \n",
    "- If $ f_t = 0.1 $ â†’ Forget most of the past information.  \n",
    "- If $ f_t = 0.9 $ â†’ Retain most of the past information.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ£ Step 3: Update Cell State**\n",
    "- The **Forget Gate output** $ f_t $ is **multiplied** with the previous **cell state** $ C_{t-1} $:  \n",
    "  $$\n",
    "  C_t = f_t * C_{t-1}\n",
    "  $$\n",
    "- This determines **how much of the old memory should be kept**.  \n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "Letâ€™s say the previous cell state $ C_{t-1} = 5 $ and the Forget Gate outputs $ f_t = 0.2 $, then:  \n",
    "$$\n",
    "C_t = 0.2 \\times 5 = 1\n",
    "$$\n",
    "This means **most of the past information is discarded**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š 4. Visualization of Forget Gate Architecture**  \n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Inputs: h(t-1), x(t)                         â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â”‚  â¬‡ Concatenate inputs                      â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â”‚  W_f * [h(t-1), x(t)] + b_f                 â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚        Sigmoid (Ïƒ) Activation               â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚        Forget Score (f_t) (0 to 1)          â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚     Multiply with Previous Cell State       â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚     Update Cell State (C_t)                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ 5. Intuition with a Real-Life Example ğŸ§ **\n",
    "Imagine youâ€™re **reading a book** ğŸ“–:  \n",
    "\n",
    "- You **remember** important plot details.  \n",
    "- You **forget** unnecessary descriptions that donâ€™t contribute much to the story.  \n",
    "\n",
    "The Forget Gate works the **same way**:  \n",
    "âœ… **Keeps important details** (high $ f_t $ value).  \n",
    "âŒ **Discards unnecessary details** (low $ f_t $ value).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 6. Importance of the Forget Gate**\n",
    "ğŸ”¹ Prevents the network from accumulating **too much unnecessary information**.  \n",
    "ğŸ”¹ Solves the **vanishing gradient problem** by **removing outdated memory**.  \n",
    "ğŸ”¹ Helps LSTMs **handle long-term dependencies** efficiently.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ The **Forget Gate** determines **what past information to retain or discard**.  \n",
    "âœ”ï¸ Uses **sigmoid activation ($ \\sigma $)** to produce a value between **0 and 1**.  \n",
    "âœ”ï¸ Helps LSTM networks avoid **overloading memory with irrelevant information**.  \n",
    "âœ”ï¸ **Plays a crucial role** in handling long-term dependencies in sequential data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“– Manual Example of Forget Gate Calculation Using Text**  \n",
    "Let's take a simple **sentence** as input and see how the **Forget Gate** decides what to keep and what to forget step by step.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Example Sentence**\n",
    "ğŸ“Œ Suppose we have the sentence:  \n",
    "**\"John is a great football player. He scored a goal in the last match.\"**  \n",
    "\n",
    "We want our **LSTM model** to retain only the relevant information for predicting the next word.  \n",
    "\n",
    "- Some words are **important** (e.g., **\"John\"**, **\"football player\"**, **\"scored a goal\"**).  \n",
    "- Some words are **not very useful** (e.g., **\"is\"**, **\"a\"**, **\"in the last match\"**).  \n",
    "- The Forget Gate **decides** which parts to **keep** and which to **discard**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**\n",
    "Each word is converted into a numerical vector (simplified here as random values):\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "| in     | **[0.15, 0.1]**  |\n",
    "| the    | **[0.1, 0.05]**  |\n",
    "| last   | **[0.25, 0.2]**  |\n",
    "| match  | **[0.7, 0.6]**   |\n",
    "\n",
    "We will now apply the **Forget Gate** on these word vectors.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 2: Compute Forget Gate Scores**\n",
    "The Forget Gate uses the formula:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "Let's assume:  \n",
    "âœ… **Weight Matrix $ W_f $**:  \n",
    "$$\n",
    "W_f =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_f $**:  \n",
    "$$\n",
    "b_f = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Applying the Forget Gate** (For each word):\n",
    "\n",
    "### Example Calculation for \"John\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{John}] + b_f\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Computing this (simplified for understanding), we get:\n",
    "\n",
    "$$\n",
    "z = [0.78, 0.55]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = [0.68, 0.63]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "âœ… **\"John\" is important, so the Forget Gate gives a high score (~0.68).**  \n",
    "\n",
    "\n",
    "\n",
    "### Example Calculation for \"is\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{is}] + b_f\n",
    "$$\n",
    "\n",
    "Computing this:\n",
    "\n",
    "$$\n",
    "z = [0.32, 0.25]\n",
    "$$\n",
    "\n",
    "Applying sigmoid:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = [0.58, 0.56]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "ğŸ¤” **\"is\" is not very important, so Forget Gate gives it a lower score (~0.56).**  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ£ Step 3: Apply Forget Scores to Cell State**\n",
    "Now, let's apply the Forget Gate scores to the **previous cell state** $ C_{t-1} $.  \n",
    "\n",
    "Let's assume $ C_{t-1} = [0.9, 0.8] $ (previous memory).\n",
    "\n",
    "For \"John\":\n",
    "$$\n",
    "C_t = f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.68, 0.63] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504]\n",
    "$$\n",
    "\n",
    "John is retained **more strongly** in memory.\n",
    "\n",
    "For \"is\":\n",
    "$$\n",
    "C_t = [0.58, 0.56] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.522, 0.448]\n",
    "$$\n",
    "\n",
    "\"is\" is retained **less** than \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”´ Step 4: Summary of Forget Gate Decisions**\n",
    "| Word       | Forget Gate Score $ f_t $ | Retained in Memory? |\n",
    "|------------|----------------|------------------|\n",
    "| **John**   | **0.68**   | âœ… Kept (important) |\n",
    "| **is**     | **0.56**   | âŒ Partially forgotten |\n",
    "| **a**      | **0.40**   | âŒ Mostly forgotten |\n",
    "| **great**  | **0.75**   | âœ… Kept (important) |\n",
    "| **football** | **0.80**  | âœ… Kept (important) |\n",
    "| **player** | **0.85**   | âœ… Kept (important) |\n",
    "| **He**     | **0.50**   | âŒ Partially forgotten |\n",
    "| **scored** | **0.90**   | âœ… Kept (important) |\n",
    "| **goal**   | **0.92**   | âœ… Kept (important) |\n",
    "| **last**   | **0.30**   | âŒ Mostly forgotten |\n",
    "| **match**  | **0.60**   | âŒ Partially forgotten |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Understanding**\n",
    "After processing the entire sentence, the LSTM has **forgotten unnecessary words** like **\"is\", \"a\", \"in the last match\"**, while **retaining important words** like **\"John\", \"football player\", \"scored a goal\"**.  \n",
    "\n",
    "### ğŸ”¥ **Key Takeaways**\n",
    "âœ” **Forget Gate helps the LSTM focus only on relevant information.**  \n",
    "âœ” **Higher forget score â†’ Memory is retained.**  \n",
    "âœ” **Lower forget score â†’ Memory is removed.**  \n",
    "\n",
    "This allows LSTM to process long sentences **efficiently** while avoiding information overload! ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“– Manual Example of Input Gate Calculation Using Text**  \n",
    "Now, letâ€™s go **step by step** to understand how the **Input Gate** in an LSTM works using a **manual example** with actual calculations.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  What is the Input Gate in LSTM?**\n",
    "The **Input Gate** decides **what new information** should be **added to the cell state**. It controls how much of the **current input** should be stored in the memory.  \n",
    "\n",
    "Formula for the Input Gate:  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ i_t $ â†’ Input Gate Activation (between 0 and 1, decides how much to store)\n",
    "- $ W_i $ â†’ Weight matrix for the Input Gate\n",
    "- $ h_{t-1} $ â†’ Previous hidden state\n",
    "- $ x_t $ â†’ Current input\n",
    "- $ b_i $ â†’ Bias for the Input Gate\n",
    "- $ \\sigma $ â†’ Sigmoid activation function\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Example Sentence**\n",
    "Letâ€™s consider the same example:  \n",
    "ğŸ“Œ **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "The **goal** is to store the most relevant information in the memory while ignoring unnecessary words.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**\n",
    "Each word is represented as a vector:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "Now, letâ€™s compute the **Input Gate Activation** for \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ¢ Step 2: Compute Input Gate Activation**\n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_i $**:  \n",
    "$$\n",
    "W_i =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_i $**:  \n",
    "$$\n",
    "b_i = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = W_i \\cdot [h_{t-1}, x_t] + b_i\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.89, 0.64]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = [0.71, 0.65]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- **\"John\" is relevant, so the Input Gate assigns high values (~0.71).**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 3: Compute Candidate Memory Content ($\\tilde{C_t}$)**\n",
    "The candidate content is **potential new information** to add to the memory.\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_C $**:  \n",
    "$$\n",
    "W_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_C $**:  \n",
    "$$\n",
    "b_C = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_C = W_C \\cdot [h_{t-1}, x_t] + b_C\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.12, 0.76]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation function**:\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(z_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- This means the new memory content suggests storing **\"John\"** strongly.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ  Step 4: Update Cell State**\n",
    "Now, the **Input Gate** decides how much of this new information to store:\n",
    "\n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "From the **Forget Gate Calculation (previous example)**, we got:\n",
    "\n",
    "âœ… **Forget Gate** $ f_t = [0.68, 0.63] $  \n",
    "âœ… **Previous Cell State** $ C_{t-1} = [0.9, 0.8] $  \n",
    "âœ… **Input Gate** $ i_t = [0.71, 0.65] $  \n",
    "âœ… **Candidate Memory** $ \\tilde{C_t} = [0.81, 0.64] $  \n",
    "\n",
    "Now, applying the formula:\n",
    "\n",
    "$$\n",
    "C_t = [0.68, 0.63] * [0.9, 0.8] + [0.71, 0.65] * [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504] + [0.5751, 0.416]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Final Interpretation**:\n",
    "- The **cell state has been updated**, retaining past information and adding new relevant details.  \n",
    "- **\"John\" is stored strongly, while unnecessary words are weakened.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Summary of Input Gate**\n",
    "| Word       | Input Gate Score $ i_t $ | Candidate Memory $ \\tilde{C_t} $ | Updated Memory $ C_t $ |\n",
    "|------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.71**   | **0.81**   | **1.1871** |\n",
    "| **is**     | **0.45**   | **0.30**   | **0.58** |\n",
    "| **great**  | **0.75**   | **0.88**   | **1.25** |\n",
    "| **football** | **0.80**  | **0.92**  | **1.32** |\n",
    "| **player** | **0.85**   | **0.95**  | **1.38** |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Key Takeaways**\n",
    "âœ” The **Input Gate** decides **how much new information should be stored**.  \n",
    "âœ” **High Input Gate Score â†’ More important information is stored.**  \n",
    "âœ” **The Forget Gate + Input Gate work together** to balance **what to keep** and **what to forget**.  \n",
    "\n",
    "This is how **LSTMs** maintain memory over long sequences! ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ§  Understanding the Output Gate in LSTM with Manual Calculation**  \n",
    "\n",
    "Now, let's break down the **Output Gate** in an **LSTM** using **step-by-step manual calculations**, just like we did for the **Forget Gate** and **Input Gate**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” What is the Output Gate in LSTM?**  \n",
    "The **Output Gate** decides how much of the **cell stateâ€™s information** should be passed to the **next hidden state** ($ h_t $).  \n",
    "\n",
    "Formula for the **Output Gate Activation**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ o_t $ â†’ Output Gate activation (decides how much information should be **exposed** as output)  \n",
    "- $ W_o $ â†’ Weight matrix for the Output Gate  \n",
    "- $ h_{t-1} $ â†’ Previous hidden state  \n",
    "- $ x_t $ â†’ Current input  \n",
    "- $ b_o $ â†’ Bias for the Output Gate  \n",
    "- $ \\sigma $ â†’ Sigmoid activation function  \n",
    "\n",
    "### **Final Hidden State Calculation**:  \n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ h_t $ â†’ New hidden state  \n",
    "- $ C_t $ â†’ Updated Cell State (from Input and Forget Gates)  \n",
    "- $ \\tanh(C_t) $ â†’ Squashing the cell state values between -1 and 1  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“– Example Sentence**\n",
    "Letâ€™s continue with the same example:  \n",
    "ğŸ“Œ **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "We will calculate the **Output Gate Activation** and **Hidden State** for the word \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**  \n",
    "We use the same word vectors:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ¢ Step 2: Compute Output Gate Activation $ o_t $**  \n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_o $**:  \n",
    "$$\n",
    "W_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_o $**:  \n",
    "$$\n",
    "b_o = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_o = W_o \\cdot [h_{t-1}, x_t] + b_o\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.05, 0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.67, 0.38]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma (z_o) = \\frac{1}{1 + e^{-z_o}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = [0.66, 0.59]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:  \n",
    "- **The Output Gate assigns moderate values (~0.66), meaning \"John\" should contribute moderately to the hidden state.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 3: Compute Final Hidden State $ h_t $**  \n",
    "Now, we use the **cell state** ($ C_t $) from the previous step.  \n",
    "\n",
    "âœ… **Updated Cell State $ C_t $ from Input & Forget Gates**:  \n",
    "$$\n",
    "C_t = [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation**:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [\\tanh(1.1871), \\tanh(0.92)]\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "Now, calculating $ h_t $:\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = [0.66, 0.59] * [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.5478, 0.4248]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- **The new hidden state** ($ h_t $) **contains the most relevant information**.\n",
    "- **Since the Output Gate was moderately open (~0.66), it allows partial information to flow.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Summary of Output Gate**\n",
    "| Word       | Output Gate Score $ o_t $ | Cell State $ C_t $ | $ \\tanh(C_t) $ | Hidden State $ h_t $ |\n",
    "|------------|----------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.66**   | **1.1871**   | **0.83**   | **0.5478** |\n",
    "| **is**     | **0.45**   | **0.58**   | **0.52**   | **0.234** |\n",
    "| **great**  | **0.75**   | **1.25**   | **0.85**   | **0.6375** |\n",
    "| **football** | **0.80**  | **1.32**  | **0.87**  | **0.696** |\n",
    "| **player** | **0.85**   | **1.38**  | **0.89**  | **0.7565** |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Key Takeaways**\n",
    "âœ” The **Output Gate** determines **how much information flows to the next step**.  \n",
    "âœ” The **higher the Output Gate value**, the more information is exposed in the **hidden state**.  \n",
    "âœ” **The hidden state is the final information passed to the next word in the sequence.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”— Full LSTM Recap**\n",
    "âœ” **Forget Gate** â†’ Decides **what to forget**.  \n",
    "âœ” **Input Gate** â†’ Decides **what to store**.  \n",
    "âœ” **Output Gate** â†’ Decides **what to expose as output**.  \n",
    "\n",
    "ğŸš€ **Together, these gates make LSTMs powerful for handling long-term dependencies in sequences!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ What is GRU?  \n",
    "Imagine youâ€™re reading a long novel ğŸ“–, and you need to remember key points from previous chapters to understand the current one. Thatâ€™s exactly what GRUs do in **sequence-based deep learning tasks**â€”they **remember important information** and **forget unimportant details**, making them ideal for tasks like speech recognition ğŸ¤, machine translation ğŸŒ, and time series forecasting ğŸ“ˆ.  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)**, but it's an **improved version** that solves the problem of *vanishing gradients* (which makes traditional RNNs forget long-term dependencies). Itâ€™s also a **lighter** alternative to LSTMs (Long Short-Term Memory) while maintaining **high accuracy**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ—ï¸ GRU Architecture: The Magic Inside âœ¨  \n",
    "\n",
    "A **GRU cell** has **two main gates** that control the flow of information:  \n",
    "\n",
    "### ğŸ”µ **1. Update Gate (Zt) â€“ \"Should I Remember?\"**  \n",
    "- Think of this as your **memory filter**. ğŸ§  It decides **how much of the past information to keep** and **how much of the new information to add**.  \n",
    "- If **Zt is close to 1**, the old memory stays. If itâ€™s **close to 0**, it gets replaced with fresh new data.  \n",
    "\n",
    "### ğŸ”´ **2. Reset Gate (Rt) â€“ \"Should I Forget?\"**  \n",
    "- This gate determines how much of the **past information to erase**. ğŸš®  \n",
    "- If Rt is **0**, the old memory is completely reset (like starting a fresh page ğŸ“„). If Rt is **1**, it keeps the entire past context.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ How GRU Works (Step-by-Step)  \n",
    "\n",
    "Letâ€™s say youâ€™re watching a TV series ğŸ¬, and GRU is helping you remember the **important plot points** while forgetting unnecessary side details.  \n",
    "\n",
    "1ï¸âƒ£ **Reset Gate (Rt) acts first**: It decides how much of the previous memory is relevant for the current moment.  \n",
    "2ï¸âƒ£ **New candidate memory is created**: It mixes the past with the present input to generate a fresh **contextual memory**.  \n",
    "3ï¸âƒ£ **Update Gate (Zt) kicks in**: It blends the old memory with the new one, deciding what to **carry forward** and what to **discard**.  \n",
    "4ï¸âƒ£ **Final memory is updated**: The result is a **refined memory state** that is carried to the next time step.  \n",
    "\n",
    "### ğŸ§  Formula Representation:  \n",
    "#### 1ï¸âƒ£ Reset Gate:  \n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "#### 2ï¸âƒ£ Update Gate:  \n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "#### 3ï¸âƒ£ Candidate Hidden State (New Memory Proposal):  \n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "#### 4ï¸âƒ£ Final Hidden State (Final Memory for the Next Step):  \n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "- Here, **Ïƒ (sigma) is the sigmoid activation function** ğŸŒ€, which ensures the values are between 0 and 1.  \n",
    "- **tanh is used** to maintain values between -1 and 1, keeping the balance between **positive and negative information**.  \n",
    "\n",
    "## ğŸš€ Why GRU? (Compared to LSTM & RNN)  \n",
    "\n",
    "| Feature        | RNN ğŸ›ï¸ | LSTM ğŸ‹ï¸ | GRU âš¡ |\n",
    "|--------------|--------|--------|------|\n",
    "| Handles Long Sequences? | âŒ No (Vanishing Gradient) | âœ… Yes | âœ… Yes |\n",
    "| Number of Gates | âŒ None | ğŸŸ¢ 3 (Forget, Input, Output) | ğŸ”µ 2 (Reset, Update) |\n",
    "| Training Time | â³ Slow | â³ Slower | âš¡ Faster |\n",
    "| Memory Efficiency | âœ… Low | âŒ High | âœ… Moderate |\n",
    "| Performance | ğŸ¤” Decent | âœ… Best for Long Texts | âš¡ Fast & Effective |\n",
    "\n",
    "**Why choose GRU?**  \n",
    "- **Faster than LSTMs** because it has **fewer gates** and computations.  \n",
    "- **Better than vanilla RNNs** because it **remembers long-term dependencies**.  \n",
    "- **Great for real-time NLP applications** like **speech recognition**, **chatbots**, and **predictive text**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ Where is GRU Used?  \n",
    "\n",
    "ğŸ”¹ **Speech-to-Text** (e.g., Google Assistant, Siri) ğŸ—£ï¸  \n",
    "ğŸ”¹ **Machine Translation** (e.g., Google Translate) ğŸŒ  \n",
    "ğŸ”¹ **Stock Price Prediction** ğŸ“Š  \n",
    "ğŸ”¹ **Music Generation** ğŸµ  \n",
    "ğŸ”¹ **Chatbots & Virtual Assistants** ğŸ¤–  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¨ Fun Analogy: GRU as a Smart Diary ğŸ““  \n",
    "\n",
    "Imagine youâ€™re keeping a **daily journal**.  \n",
    "- **Reset Gate (Rt)**: Decides **whether to remove old notes** or keep them.  \n",
    "- **Update Gate (Zt)**: Decides **if a new event should overwrite an old one**.  \n",
    "- **Final Memory (ht)**: The polished diary entry that **carries forward** into the next day!  \n",
    "\n",
    "Thatâ€™s how GRU **efficiently maintains and updates memory** while keeping only the **important parts**! ğŸ¯\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ Summary  \n",
    "\n",
    "ğŸ¯ **GRU is a powerful, lightweight RNN variant** that efficiently processes sequential data.  \n",
    "âš¡ **It has two gates (Reset & Update) instead of three like LSTM**, making it faster and simpler.  \n",
    "ğŸ§  **It solves the vanishing gradient problem**, making it ideal for handling **long-term dependencies**.  \n",
    "ğŸš€ **Used in NLP, speech recognition, finance, and more!**  \n",
    "\n",
    "Hope that made GRU fun and colorful for you! ğŸ¨âœ¨ Let me know if you need a deeper dive into any part! ğŸš€ğŸ’¡\n",
    "\n",
    "![](images/gru.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Letâ€™s break down the **full architecture of a GRU (Gated Recurrent Unit)** in detail. We'll explore:  \n",
    "\n",
    "âœ… **High-Level Overview**  \n",
    "âœ… **Step-by-Step Working of GRU Cell**  \n",
    "âœ… **Mathematical Formulation**  \n",
    "âœ… **Computation Flow**  \n",
    "âœ… **Comparison with LSTM**  \n",
    "âœ… **Advantages & Use Cases**  \n",
    "\n",
    "Letâ€™s dive in! ğŸš€ğŸ¯  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸŒŸ High-Level Overview of GRU**  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)** designed to handle sequential data (e.g., time series, speech, language).  \n",
    "\n",
    "ğŸ”¹ **Why GRU?**  \n",
    "- Standard RNNs suffer from the **vanishing gradient problem**, making it hard to learn **long-term dependencies**.  \n",
    "- GRUs, like LSTMs, use **gates to control information flow** but are computationally more efficient.  \n",
    "- They have **fewer parameters** than LSTMs, making them **faster to train** while retaining strong performance.  \n",
    "\n",
    "### **ğŸ”§ GRU Components:**  \n",
    "A **GRU cell** consists of:  \n",
    "1ï¸âƒ£ **Update Gate ($Z_t$)** â†’ Decides **how much past information to keep**.  \n",
    "2ï¸âƒ£ **Reset Gate ($R_t$)** â†’ Decides **how much past information to forget**.  \n",
    "3ï¸âƒ£ **Candidate Hidden State ($\\tilde{h}_t$)** â†’ A new potential memory update.  \n",
    "4ï¸âƒ£ **Final Hidden State ($h_t$)** â†’ The actual memory that carries forward.  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ—ï¸ GRU Architecture (Step-by-Step)**\n",
    "The **GRU cell** takes two inputs at time step $ t $:  \n",
    "ğŸ”¹ **$ x_t $ (Current input)** â€“ This is the new data point (word, feature, etc.).  \n",
    "ğŸ”¹ **$ h_{t-1} $ (Previous hidden state)** â€“ This stores past information.  \n",
    "\n",
    "### **ğŸ”µ Step 1: Compute the Reset Gate $ R_t $**\n",
    "- The **reset gate** decides whether to erase part of the past memory.  \n",
    "- Uses a **sigmoid activation** ($ \\sigma $) to squash values between 0 and 1.  \n",
    "\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "ğŸ‘‰ If $ R_t $ is **0**, it forgets the past.  \n",
    "ğŸ‘‰ If $ R_t $ is **1**, it keeps the full past memory.  \n",
    "\n",
    "### **ğŸ”´ Step 2: Compute the Update Gate $ Z_t $**\n",
    "- The **update gate** decides how much of the **past hidden state** to retain versus **how much to update**.  \n",
    "- Also uses **sigmoid activation** to control memory update.  \n",
    "\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "ğŸ‘‰ If $ Z_t $ is **0**, it replaces the old memory entirely.  \n",
    "ğŸ‘‰ If $ Z_t $ is **1**, it keeps the old memory.  \n",
    "\n",
    "### **ğŸŸ¢ Step 3: Compute the Candidate Hidden State $ \\tilde{h}_t $**\n",
    "- A **new candidate memory** is computed using the reset gate.  \n",
    "- Uses **tanh activation** to balance positive/negative values.  \n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "ğŸ‘‰ If **reset gate is 0**, it ignores past information.  \n",
    "ğŸ‘‰ If **reset gate is 1**, it uses both past and current input.  \n",
    "\n",
    "### **ğŸŸ  Step 4: Compute the Final Hidden State $ h_t $**\n",
    "- The final output is a **blend of the old memory ($ h_{t-1} $) and new candidate memory ($ \\tilde{h}_t $)** controlled by the update gate.  \n",
    "\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "ğŸ‘‰ If $ Z_t $ is **0**, it fully updates with new memory.  \n",
    "ğŸ‘‰ If $ Z_t $ is **1**, it keeps old memory.  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ“Š Computation Flow in a GRU Cell**  \n",
    "\n",
    "### **ğŸ› ï¸ Forward Pass**  \n",
    "\n",
    "1ï¸âƒ£ **Compute Reset Gate:**  \n",
    "   - $ R_t = \\sigma(W_r [h_{t-1}, x_t] + b_r) $  \n",
    "\n",
    "2ï¸âƒ£ **Compute Update Gate:**  \n",
    "   - $ Z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z) $  \n",
    "\n",
    "3ï¸âƒ£ **Compute Candidate Hidden State:**  \n",
    "   - $ \\tilde{h}_t = \\tanh(W_h [R_t \\ast h_{t-1}, x_t] + b_h) $  \n",
    "\n",
    "4ï¸âƒ£ **Compute Final Hidden State:**  \n",
    "   - $ h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t $  \n",
    "\n",
    "### **ğŸ”„ Backpropagation (Training GRU)**\n",
    "GRUs are trained using **Backpropagation Through Time (BPTT)**, where:  \n",
    "- **Gradients of loss are computed** using **chain rule**.  \n",
    "- **Weights are updated** using **gradient descent**.  \n",
    "- **Gates regulate gradient flow**, preventing vanishing gradients.  \n",
    "\n",
    "# **ğŸ”¬ GRU vs. LSTM: Key Differences**\n",
    "| Feature | GRU âš¡ | LSTM ğŸ‹ï¸ |\n",
    "|---------|------|------|\n",
    "| Number of Gates | 2 (Update, Reset) | 3 (Input, Forget, Output) |\n",
    "| Complexity | âœ… Less | âŒ More |\n",
    "| Performance | âš¡ Fast | ğŸ† Better for long texts |\n",
    "| Memory Requirement | âœ… Less | âŒ More |\n",
    "| Suitable for | Speech, NLP, real-time apps | Long documents, text generation |\n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ”¥ Advantages of GRU**\n",
    "âœ… **Faster Training** â€“ Fewer gates than LSTM = Faster updates.  \n",
    "âœ… **Solves Vanishing Gradient Problem** â€“ Retains long-term dependencies.  \n",
    "âœ… **Computationally Efficient** â€“ Great for real-time applications.  \n",
    "âœ… **Performs Well on Small Datasets** â€“ Fewer parameters make it ideal for small-scale problems.  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸš€ Where is GRU Used?**\n",
    "ğŸ“Œ **Speech Recognition** (Google Assistant, Alexa) ğŸ—£ï¸  \n",
    "ğŸ“Œ **Machine Translation** (Google Translate) ğŸŒ  \n",
    "ğŸ“Œ **Stock Market Prediction** ğŸ“ˆ  \n",
    "ğŸ“Œ **Chatbots & AI Assistants** ğŸ¤–  \n",
    "ğŸ“Œ **Music Generation** ğŸµ  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ¯ Summary**\n",
    "âœ” **GRU is a simplified LSTM** with **fewer gates** and **faster computations**.  \n",
    "âœ” **It solves vanishing gradient issues** and **remembers long-term dependencies**.  \n",
    "âœ” **Uses Reset & Update Gates** to control memory updates.  \n",
    "âœ” **Faster than LSTM** but still **performs well in sequence-based tasks**.  \n",
    "âœ” **Ideal for speech, NLP, real-time applications**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Letâ€™s manually walk through the GRU computations using a simple example. This will give you a **step-by-step breakdown of how a GRU cell processes a sentence**, calculating each gate and hidden state update.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“ Example Sentence:**  \n",
    "ğŸ‘‰ **\"AI is amazing\"**  \n",
    "We will process it word by word using a GRU with a **hidden size of 2** (to keep calculations manageable).  \n",
    "\n",
    "## **ğŸ”§ Step 1: Define Inputs & Initial Parameters**\n",
    "### **Word Encoding (Input Vectors)**\n",
    "We assume each word is converted into a 3-dimensional vector (using Word Embeddings). Letâ€™s define:  \n",
    "\n",
    "| Word | Input Vector (\\( x_t \\)) |\n",
    "|-------|----------------|\n",
    "| **AI** | \\([0.5, 0.1, 0.4]\\) |\n",
    "| **is** | \\([0.2, 0.7, 0.3]\\) |\n",
    "| **amazing** | \\([0.6, 0.9, 0.5]\\) |\n",
    "\n",
    "### **Initial Hidden State \\( h_0 \\)**\n",
    "Since it's the first step, we initialize:  \n",
    "$$\n",
    "h_0 = [0, 0] \\quad \\text{(2-dimensional hidden state)}\n",
    "$$\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Step 2: Define GRU Parameters**\n",
    "We need **weight matrices** and **biases** for reset and update gates. We assume:  \n",
    "\n",
    "**Reset Gate (\\( R_t \\)):**  \n",
    "$$\n",
    "W_r =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad U_r =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix},\n",
    "\\quad b_r = [0.1, 0.2]\n",
    "$$\n",
    "\n",
    "**Update Gate (\\( Z_t \\)):**  \n",
    "$$\n",
    "W_z =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.7 \\\\\n",
    "0.5 & 0.2 & 0.6\n",
    "\\end{bmatrix},\n",
    "\\quad U_z =\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.5 \\\\\n",
    "0.3 & 0.8\n",
    "\\end{bmatrix},\n",
    "\\quad b_z = [0.05, 0.1]\n",
    "$$\n",
    "\n",
    "**Candidate Hidden State (\\( \\tilde{h}_t \\)):**  \n",
    "$$\n",
    "W_h =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix},\n",
    "\\quad U_h =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix},\n",
    "\\quad b_h = [0.2, 0.3]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **âš¡ Step 3: Compute for First Word (\"AI\")**  \n",
    "### **ğŸ”´ Reset Gate \\( R_1 \\)**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\cdot 0.5) + (0.5 \\cdot 0.1) + (0.1 \\cdot 0.4) + 0.1 \\\\\n",
    "(0.3 \\cdot 0.5) + (0.7 \\cdot 0.1) + (0.2 \\cdot 0.4) + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.1 + 0.05 + 0.04 + 0.1 \\\\\n",
    "0.15 + 0.07 + 0.08 + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.29 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Applying **sigmoid** (\\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)):  \n",
    "\n",
    "$$\n",
    "R_1 =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(0.29) \\\\\n",
    "\\sigma(0.5)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.572 \\\\\n",
    "0.622\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ¡ Update Gate \\( Z_1 \\)**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "\n",
    "Using similar calculations, we get:  \n",
    "\n",
    "$$\n",
    "Z_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ¢ Candidate Hidden State \\( \\tilde{h}_1 \\)**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot (R_1 \\ast h_0) + U_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "\n",
    "Since \\( h_0 = 0 \\), the term \\( R_1 \\ast h_0 \\) vanishes, and we compute:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.29 + 0.2 \\\\\n",
    "0.49 + 0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "=\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.49 \\\\\n",
    "0.79\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Approximating \\( \\tanh(x) \\), we get:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”µ Final Hidden State \\( h_1 \\)**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "(1 - 0.655) \\\\\n",
    "(1 - 0.710)\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "(0.345) \\times 0.45 \\\\\n",
    "(0.290) \\times 0.66\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.155 \\\\\n",
    "0.191\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Repeat for \"is\" and \"amazing\"**\n",
    "Now, \\( h_1 \\) is used for the next step, and the process repeats.\n",
    "\n",
    "This shows **how a GRU cell updates memory word-by-word!** ğŸš€ Let me know if you want more manual calculations or insights! ğŸ¯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate how a **GRU (Gated Recurrent Unit)** processes a sentence. We'll analyze how it **keeps important information** and **forgets unimportant details** using an actual example.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Example Sentence:**\n",
    "Let's take a simple sentence:\n",
    "> **\"I love deep learning.\"**  \n",
    "\n",
    "We'll process it **word by word** through a GRU and observe how it decides what to keep and what to forget.\n",
    "\n",
    "## **ğŸ”¹ Step 1: Define Initial Setup**\n",
    "Each word is represented as a **word vector** $ x_t $. Assume we have:  \n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|------|---------------------|\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "We also assume that the **hidden state** $ h_t $ has two units, so itâ€™s a 2D vector.  \n",
    "\n",
    "\n",
    "The **GRU parameters** (randomly chosen for simplicity):  \n",
    "\n",
    "- **Update Gate Weights** $ W_z, U_z $  \n",
    "- **Reset Gate Weights** $ W_r, U_r $  \n",
    "- **Candidate State Weights** $ W_h, U_h $  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 2: How GRU Decides What to Keep or Forget?**  \n",
    "GRU works with **three key equations** at every time step $ t $:  \n",
    "\n",
    "### **1ï¸âƒ£ Reset Gate $ R_t $** (Decides whether to erase past memory)\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot x_t + U_r \\cdot h_{t-1} + b_r)\n",
    "$$\n",
    "- If $ R_t $ is **close to 0**, it forgets old information.\n",
    "- If $ R_t $ is **close to 1**, it keeps old memory.  \n",
    "\n",
    "### **2ï¸âƒ£ Update Gate $ Z_t $** (Decides whether to update hidden state)\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot x_t + U_z \\cdot h_{t-1} + b_z)\n",
    "$$\n",
    "- If $ Z_t $ is **close to 0**, it **replaces** the old state with new info.  \n",
    "- If $ Z_t $ is **close to 1**, it **keeps** the old memory.  \n",
    "\n",
    "### **3ï¸âƒ£ Candidate Hidden State $ \\tilde{h}_t $**\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot x_t + U_h \\cdot (R_t \\ast h_{t-1}) + b_h)\n",
    "$$\n",
    "This is the new hidden state, considering **reset gate influence**.  \n",
    "\n",
    "### **4ï¸âƒ£ Final Hidden State**\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$\n",
    "The final hidden state is a combination of **past and new** information.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 3: Manual Calculation for Each Word**\n",
    "Letâ€™s assume:\n",
    "\n",
    "- $ h_0 = [0, 0] $ (initial hidden state)  \n",
    "- We calculate for each word step by step.\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"I\"**  \n",
    "#### **1ï¸âƒ£ Reset Gate Calculation**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "Since $ h_0 = [0, 0] $, this simplifies to:\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot [0.5, 0.1, 0.3] + b_r)\n",
    "$$\n",
    "Letâ€™s say:\n",
    "$$\n",
    "R_1 = [0.8, 0.6]\n",
    "$$\n",
    "Since values are **close to 1**, we **keep past memory**.\n",
    "\n",
    "#### **2ï¸âƒ£ Update Gate Calculation**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "Again, since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + b_z)\n",
    "$$\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "Z_1 = [0.9, 0.7]\n",
    "$$\n",
    "Since $ Z_1 $ is **close to 1**, GRU **keeps most of the old hidden state** (which is zero for now).\n",
    "\n",
    "#### **3ï¸âƒ£ Compute Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + U_h \\cdot (R_1 \\ast h_0) + b_h)\n",
    "$$\n",
    "Since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "\\tilde{h}_1 = [0.3, 0.4]\n",
    "$$\n",
    "\n",
    "#### **4ï¸âƒ£ Compute Final Hidden State**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "$$\n",
    "= [0.9, 0.7] \\ast [0, 0] + [0.1, 0.3] \\ast [0.3, 0.4]\n",
    "$$\n",
    "$$\n",
    "= [0.03, 0.12]\n",
    "$$\n",
    "ğŸš€ **Hidden state at time step 1**: $ h_1 = [0.03, 0.12] $\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"love\"**  \n",
    "Now, we use $ h_1 = [0.03, 0.12] $.\n",
    "\n",
    "#### **1ï¸âƒ£ Reset Gate**\n",
    "$$\n",
    "R_2 = \\sigma(W_r \\cdot x_2 + U_r \\cdot h_1 + b_r)\n",
    "$$\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "R_2 = [0.4, 0.2]\n",
    "$$\n",
    "Since $ R_2 $ is **low**, it **forgets some past memory**.\n",
    "\n",
    "#### **2ï¸âƒ£ Update Gate**\n",
    "$$\n",
    "Z_2 = \\sigma(W_z \\cdot x_2 + U_z \\cdot h_1 + b_z)\n",
    "$$\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "Z_2 = [0.2, 0.6]\n",
    "$$\n",
    "Since $ Z_2 $ is **low for the first unit**, it **updates memory**.\n",
    "\n",
    "#### **3ï¸âƒ£ Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_2 = \\tanh(W_h \\cdot x_2 + U_h \\cdot (R_2 \\ast h_1) + b_h)\n",
    "$$\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "\\tilde{h}_2 = [0.6, 0.5]\n",
    "$$\n",
    "\n",
    "#### **4ï¸âƒ£ Final Hidden State**\n",
    "$$\n",
    "h_2 = Z_2 \\ast h_1 + (1 - Z_2) \\ast \\tilde{h}_2\n",
    "$$\n",
    "$$\n",
    "= [0.2, 0.6] \\ast [0.03, 0.12] + [0.8, 0.4] \\ast [0.6, 0.5]\n",
    "$$\n",
    "$$\n",
    "= [0.006, 0.072] + [0.48, 0.2]\n",
    "$$\n",
    "$$\n",
    "= [0.486, 0.272]\n",
    "$$\n",
    "\n",
    "ğŸš€ **Hidden state at time step 2**: $ h_2 = [0.486, 0.272] $  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Conclusion**\n",
    "- **\"I\"** â†’ Small memory update, since itâ€™s a common word.  \n",
    "- **\"love\"** â†’ Memory updates more because itâ€™s a strong emotional word.  \n",
    "- **GRU selectively keeps or forgets** based on context.  \n",
    "\n",
    "Would you like me to compute for \"deep\" and \"learning\" too? ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ **Deep RNNs (Deep Recurrent Neural Networks) â€“ A Full Explanation** ğŸ”¥\n",
    "\n",
    "## **ğŸ“Œ What is a Deep RNN?**\n",
    "A **Deep RNN** is a **stacked** version of a normal Recurrent Neural Network (RNN). Unlike a simple RNN that has only **one layer** of recurrent neurons, a **Deep RNN** stacks multiple RNN layers **on top of each other**. This allows it to **learn more complex patterns** in sequential data like **text, speech, and time-series data**.\n",
    "\n",
    "## **ğŸ› ï¸ How is a Deep RNN Different from a Simple RNN?**\n",
    "| Feature | Simple RNN | Deep RNN |\n",
    "|---------|-----------|----------|\n",
    "| **Number of Layers** | 1 recurrent layer | Multiple recurrent layers |\n",
    "| **Learning Capability** | Limited feature extraction | Captures deeper, hierarchical features |\n",
    "| **Performance** | Struggles with long-term dependencies | Better at long-term dependencies |\n",
    "| **Training Difficulty** | Easier | Harder (but more powerful) |\n",
    "| **Application** | Basic time-series & text prediction | Complex NLP, speech recognition |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  Architecture of a Deep RNN**\n",
    "A Deep RNN consists of **multiple RNN layers stacked on top of each other**, where:\n",
    "\n",
    "- **Each layer passes its hidden state** $ h_t^l $ **to the next layer**.\n",
    "- The **first layer** processes the input sequence.\n",
    "- The **last layer** produces the final output.\n",
    "\n",
    "### **ğŸ”¹ Standard RNN vs. Deep RNN**\n",
    "ğŸ“Œ **Simple RNN (Shallow)**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Deep RNN (Stacked)**\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)  \\quad \\text{(First RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2) \\quad \\text{(Second RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "h_t^L = \\tanh(W_x^L h_t^{L-1} + W_h^L h_{t-1}^L + b^L) \\quad \\text{(Final RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "y_t = W_y h_t^L + b_y\n",
    "$$\n",
    "\n",
    "ğŸš€ **Each layer refines the representation of the sequence!**\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Why Use a Deep RNN?**\n",
    "ğŸ”¹ **Captures Higher-Level Features** â†’ Lower layers learn **basic** features, higher layers learn **abstract** features.  \n",
    "ğŸ”¹ **Handles Complex Dependencies** â†’ Works better for long sequences.  \n",
    "ğŸ”¹ **More Expressive Power** â†’ Learns deeper relationships in data.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ Example: Manual Computation for a Deep RNN**\n",
    "Letâ€™s take a simple sequence:\n",
    "\n",
    "> **\"I love deep learning.\"**\n",
    "\n",
    "We'll process it using **2 RNN layers**.\n",
    "\n",
    "### **ğŸ”¹ Step 1: Input Representation**\n",
    "Each word is represented as a **vector**:\n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|||\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "### **ğŸ”¹ Step 2: Process Each Word Through Layer 1**\n",
    "Each word goes through the first RNN layer:\n",
    "\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)\n",
    "$$\n",
    "\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "h_1^1 = [0.2, 0.3]\n",
    "$$\n",
    "$$\n",
    "h_2^1 = [0.4, 0.5]\n",
    "$$\n",
    "$$\n",
    "h_3^1 = [0.1, 0.8]\n",
    "$$\n",
    "$$\n",
    "h_4^1 = [0.6, 0.4]\n",
    "$$\n",
    "\n",
    "### **ğŸ”¹ Step 3: Pass to Layer 2**\n",
    "Now, these hidden states are **fed into the second RNN layer**:\n",
    "\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2)\n",
    "$$\n",
    "\n",
    "Letâ€™s assume:\n",
    "$$\n",
    "h_1^2 = [0.3, 0.6]\n",
    "$$\n",
    "$$\n",
    "h_2^2 = [0.5, 0.7]\n",
    "$$\n",
    "$$\n",
    "h_3^2 = [0.2, 0.9]\n",
    "$$\n",
    "$$\n",
    "h_4^2 = [0.7, 0.5]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Variants of Deep RNN**\n",
    "Deep RNNs are often implemented using **better recurrent cells** like:\n",
    "\n",
    "### **1ï¸âƒ£ Deep LSTM (Stacked LSTM)**\n",
    "LSTM (Long Short-Term Memory) uses **gates** to better store long-term dependencies.\n",
    "\n",
    "### **2ï¸âƒ£ Deep GRU (Stacked GRU)**\n",
    "GRU (Gated Recurrent Unit) simplifies LSTM while keeping good performance.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Where are Deep RNNs Used?**\n",
    "âœ… **Speech Recognition** (e.g., Google Assistant, Siri)  \n",
    "âœ… **Text Generation** (e.g., Chatbots)  \n",
    "âœ… **Machine Translation** (e.g., Google Translate)  \n",
    "âœ… **Stock Price Prediction**  \n",
    "âœ… **Music Generation**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Final Summary**\n",
    "| Concept | Explanation |\n",
    "|||\n",
    "| **Deep RNN** | Multiple RNN layers stacked together |\n",
    "| **Why Deep?** | Captures complex patterns better |\n",
    "| **How it Works?** | Each layer refines the representation |\n",
    "| **Better Variants** | Stacked LSTM, Stacked GRU |\n",
    "\n",
    "ğŸ”¥ **Deep RNNs power many AI applications today!** Would you like me to implement a Deep RNN example in Python? ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ **Bidirectional Recurrent Neural Networks (BiRNN) - A Full and Colorful Guide!** ğŸš€  \n",
    "\n",
    "### **1ï¸âƒ£ What is a Bidirectional RNN?**  \n",
    "Imagine you're watching a movie ğŸ¬, but instead of seeing the whole scene, you only see frames one by one in a forward sequence. You might **miss important context** from future events. Wouldnâ€™t it be amazing if you could **see both past and future** at the same time? ğŸ¤¯  \n",
    "\n",
    "Thatâ€™s exactly what **Bidirectional Recurrent Neural Networks (BiRNNs)** do! Instead of processing sequences in just one direction (like a regular RNN), **BiRNNs process them in both forward and backward directions** at the same time. ğŸ”„ This makes them super powerful for **context-heavy** tasks like speech recognition ğŸ¤, text processing ğŸ“–, and language translation ğŸŒ.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ How Does a BiRNN Work? ğŸ› ï¸**  \n",
    "A BiRNN consists of **two RNNs running in parallel:**  \n",
    "\n",
    "1. **Forward RNN**: Reads the sequence from left to right â¡ï¸  \n",
    "2. **Backward RNN**: Reads the sequence from right to left â¬…ï¸  \n",
    "\n",
    "At each time step **t**, both RNNs process the input and produce two hidden states:  \n",
    "- One from the forward RNN: **$ h_t^{(fwd)} $**  \n",
    "- One from the backward RNN: **$ h_t^{(bwd)} $**  \n",
    "\n",
    "The final output at each time step is a combination (concatenation or sum) of these two hidden states:  \n",
    "$$\n",
    "h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "$$  \n",
    "\n",
    "### **ğŸ¯ Key Takeaway:**  \n",
    "ğŸ”¹ Unlike a regular RNN, a BiRNN can use **both past and future information** at any given time step. This makes it way better for **understanding full context** in sequential data.  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Why is BiRNN Better? ğŸ¤”**  \n",
    "\n",
    "âœ… **More Context = More Accuracy**  \n",
    "   - A normal RNN only considers past words when predicting the next word, which can lead to **misinterpretations**.  \n",
    "   - BiRNNs can **consider both past and future words**, leading to **better predictions**! ğŸ¯  \n",
    "\n",
    "âœ… **Great for Speech & NLP Tasks**  \n",
    "   - **Speech Recognition**: The meaning of a word can change based on future words. A BiRNN helps capture that nuance! ğŸ™ï¸  \n",
    "   - **Machine Translation**: Words in different languages may have different orders. Understanding the full sentence structure helps a lot! ğŸŒ  \n",
    "   - **Named Entity Recognition (NER)**: Knowing the full sentence helps distinguish between similar words used in different contexts.  \n",
    "\n",
    "âœ… **Works with LSTMs & GRUs**  \n",
    "   - BiRNNs can use **LSTM (Long Short-Term Memory) cells** or **GRUs (Gated Recurrent Units)** to handle long sequences better. ğŸ§   \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ BiRNN in Action - Example with Python ğŸ**  \n",
    "\n",
    "Letâ€™s see how a **Bidirectional LSTM** can be implemented in TensorFlow/Keras:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define a BiLSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(100, 10)),  # BiLSTM Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "ğŸ”¹ Here, the **Bidirectional()** wrapper makes the LSTM layer process input in both directions! ğŸ”„  \n",
    "\n",
    "### **5ï¸âƒ£ When to Use a BiRNN? ğŸ¤·**  \n",
    "\n",
    "| âœ… Use BiRNN When | âŒ Avoid BiRNN When |  \n",
    "|------------------|------------------|  \n",
    "| You need **full context** from past & future ğŸ”„ | Your dataset is too large, as BiRNNs require **double computation** ğŸ’¾ |  \n",
    "| Tasks involve **NLP**, **speech recognition**, or **translation** ğŸ—£ï¸ğŸ“– | You're working with **real-time applications** where only past info is available â³ |  \n",
    "| You need better performance on **long sequences** ğŸ§  | The problem is **too simple**, and a unidirectional RNN is enough âš¡ |  \n",
    "\n",
    "\n",
    "### **ğŸŒŸ Conclusion - Why BiRNN is a Game-Changer? ğŸ®**  \n",
    "\n",
    "ğŸš€ BiRNNs are like **time travelers** in the world of neural networks. Instead of just relying on the past, they **peek into the future** and learn from both sides! This makes them **exceptionally powerful** for tasks like:  \n",
    "\n",
    "âœ”ï¸ Speech Recognition ğŸ¤  \n",
    "âœ”ï¸ Text Summarization ğŸ“„  \n",
    "âœ”ï¸ Sentiment Analysis ğŸ˜ŠğŸ˜¡  \n",
    "âœ”ï¸ Named Entity Recognition (NER) ğŸ“  \n",
    "\n",
    "But remember! BiRNNs require **more computation** and are not always the best choice for real-time applications. **Choose wisely!** ğŸ§  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ”¥ Full Architecture of a Bidirectional Recurrent Neural Network (BiRNN) ğŸ”¥**  \n",
    "\n",
    "A **Bidirectional Recurrent Neural Network (BiRNN)** is an advanced type of **Recurrent Neural Network (RNN)** that processes sequences in **both forward and backward directions** to capture **past and future context**.  \n",
    "\n",
    "Letâ€™s dive **deep into the architecture** step by step! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 1. Basic Components of a BiRNN**  \n",
    "\n",
    "A **standard RNN** has the following components:  \n",
    "- **Input layer (X)**: The sequence of data (e.g., words in a sentence, frames in speech).  \n",
    "- **Hidden layer (h)**: Stores information from previous time steps.  \n",
    "- **Output layer (Y)**: Produces predictions at each time step.  \n",
    "\n",
    "A **Bidirectional RNN** consists of **two separate RNNs**:  \n",
    "- **Forward RNN** â†’ Processes input from **left to right** (past to future).  \n",
    "- **Backward RNN** â†’ Processes input from **right to left** (future to past).  \n",
    "\n",
    "At each time step $ t $, both RNNs produce hidden states, which are combined to form the final output.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 2. Step-by-Step Working of a BiRNN**  \n",
    "\n",
    "### **Step 1: Input Representation**  \n",
    "Letâ€™s assume we have a sequence of length $ T $, where each input vector is $ X_t $ (a feature vector at time step $ t $).  \n",
    "\n",
    "$$\n",
    "X = [X_1, X_2, X_3, ..., X_T]\n",
    "$$\n",
    "\n",
    "Each input passes through **two RNNs**:  \n",
    "1. **Forward RNN** â†’ Generates hidden states from past to future.  \n",
    "2. **Backward RNN** â†’ Generates hidden states from future to past.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Forward and Backward Hidden States Computation**  \n",
    "\n",
    "- **Forward Hidden State ($ h_t^{(fwd)} $)**  \n",
    "  The forward RNN computes the hidden state at each time step using:  \n",
    "  $$\n",
    "  h_t^{(fwd)} = f(W_f X_t + U_f h_{t-1}^{(fwd)} + b_f)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_f $ = Input weight matrix for forward RNN  \n",
    "  - $ U_f $ = Hidden weight matrix for forward RNN  \n",
    "  - $ b_f $ = Bias  \n",
    "  - $ f $ = Activation function (usually tanh or ReLU)  \n",
    "\n",
    "- **Backward Hidden State ($ h_t^{(bwd)} $)**  \n",
    "  The backward RNN computes the hidden state moving from **$ T $ to $ 1 $**:  \n",
    "  $$\n",
    "  h_t^{(bwd)} = f(W_b X_t + U_b h_{t+1}^{(bwd)} + b_b)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_b $ = Input weight matrix for backward RNN  \n",
    "  - $ U_b $ = Hidden weight matrix for backward RNN  \n",
    "  - $ b_b $ = Bias  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Combining Forward and Backward States**  \n",
    "\n",
    "At each time step $ t $, the two hidden states **($ h_t^{(fwd)} $ and $ h_t^{(bwd)} $)** are combined into a single hidden state $ h_t $. This can be done in different ways:  \n",
    "- **Concatenation** (most common):  \n",
    "  $$\n",
    "  h_t = [h_t^{(fwd)}; h_t^{(bwd)}]\n",
    "  $$\n",
    "- **Sum**:  \n",
    "  $$\n",
    "  h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Output Layer**  \n",
    "\n",
    "The final output $ Y_t $ at each time step is computed as:  \n",
    "$$\n",
    "Y_t = g(W_o h_t + b_o)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_o $ = Output weight matrix  \n",
    "- $ b_o $ = Bias  \n",
    "- $ g $ = Activation function (e.g., softmax for classification)  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 3. Full Architecture Diagram of BiRNN**  \n",
    "\n",
    "```\n",
    "      Input Sequence: [ X1,  X2,  X3,  X4,  X5]\n",
    "                        â†“    â†“    â†“    â†“    â†“    \n",
    "      Forward RNN:   â†’ h1 â†’ h2 â†’ h3 â†’ h4 â†’ h5 â†’  \n",
    "                         â†“    â†“    â†“    â†“    â†“    \n",
    "      Backward RNN:  â† h1 â† h2 â† h3 â† h4 â† h5 â†  \n",
    "                        â†“    â†“    â†“    â†“    â†“    \n",
    "      Final Output:  [ Y1,  Y2,  Y3,  Y4,  Y5]\n",
    "```\n",
    "\n",
    "- The **forward hidden states** move **left to right**.  \n",
    "- The **backward hidden states** move **right to left**.  \n",
    "- The **final hidden state at each time step** is a combination of both.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 4. Advantages of BiRNN ğŸš€**  \n",
    "\n",
    "âœ… **Uses full context** (both past & future).  \n",
    "âœ… **Improves accuracy** in NLP, speech recognition, and time series tasks.  \n",
    "âœ… **Works well with LSTM & GRU for long-term dependencies.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 5. Implementing BiRNN in Python (TensorFlow/Keras) ğŸ**  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define a Bidirectional RNN Model\n",
    "model = Sequential([\n",
    "    Bidirectional(SimpleRNN(64, return_sequences=True), input_shape=(100, 10)),  # BiRNN Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## **ğŸ“Œ 6. When to Use BiRNN vs. Unidirectional RNN?**  \n",
    "\n",
    "| Feature  | Unidirectional RNN  | Bidirectional RNN  |\n",
    "|----------|--------------------|--------------------|\n",
    "| **Direction** | Forward only â¡ï¸  | Forward + Backward ğŸ”„ |\n",
    "| **Context** | Only past context ğŸ“œ | Both past & future context ğŸ† |\n",
    "| **Computational Cost** | Lower ğŸ’° | Higher âš¡ |\n",
    "| **Use Case** | Real-time tasks (e.g., online chatbots) ğŸ’¬ | NLP, speech, translation ğŸŒ |\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Conclusion: Why BiRNN is a Game-Changer?**  \n",
    "\n",
    "ğŸš€ **Bidirectional RNNs are like superheroes** in sequential tasks! Unlike normal RNNs that only see the past, BiRNNs **see both past and future at the same time**, making them extremely powerful for **speech recognition**, **text processing**, **machine translation**, and more! ğŸ’¡  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's take a simple sentence and manually work through how a **Bidirectional Recurrent Neural Network (BiRNN)** processes it. This will involve:  \n",
    "\n",
    "1ï¸âƒ£ **Choosing a sentence**  \n",
    "2ï¸âƒ£ **Assigning word embeddings**  \n",
    "3ï¸âƒ£ **Forward pass calculations**  \n",
    "4ï¸âƒ£ **Backward pass calculations**  \n",
    "5ï¸âƒ£ **Combining hidden states**  \n",
    "6ï¸âƒ£ **Generating output**  \n",
    "\n",
    "## **ğŸ“Œ Sentence: \"I love AI\"**\n",
    "Weâ€™ll assume this is a 3-word sequence:  \n",
    "\n",
    "$$\n",
    "X = [\"I\", \"love\", \"AI\"]\n",
    "$$\n",
    "\n",
    "Each word will be represented as a **3D embedding vector** (to keep it simple).  \n",
    "\n",
    "| Word  | Embedding (3D Vector) |\n",
    "|--------|----------------|\n",
    "| \"I\"      | [0.1, 0.3, 0.5] |\n",
    "| \"love\"   | [0.2, 0.6, 0.8] |\n",
    "| \"AI\"     | [0.3, 0.7, 0.9] |\n",
    "\n",
    "\n",
    "## **ğŸ›  Step 1: Initialize Parameters**\n",
    "BiRNN consists of **two RNNs**, one running **forward** and one **backward**. Each has:  \n",
    "\n",
    "- **Weight Matrices (Input â†’ Hidden State)**\n",
    "  - $ W_f $ (Forward)\n",
    "  - $ W_b $ (Backward)  \n",
    "\n",
    "- **Weight Matrices (Hidden State â†’ Next Hidden State)**\n",
    "  - $ U_f $ (Forward)\n",
    "  - $ U_b $ (Backward)  \n",
    "\n",
    "- **Bias Vectors**\n",
    "  - $ b_f $ (Forward)\n",
    "  - $ b_b $ (Backward)  \n",
    "\n",
    "For simplicity, letâ€™s assume:  \n",
    "\n",
    "$$\n",
    "W_f = W_b =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_f = U_b =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_f = b_b =\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ›  Step 2: Forward Pass (Processing left to right)**  \n",
    "\n",
    "### **ğŸ”¹ Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh(W_f X_1 + U_f h_0 + b_f)\n",
    "$$\n",
    "\n",
    "Since initial **hidden state** is **0**,  \n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.3 \\\\ 0.5\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "(0.5 \\times 0.1) + (0.3 \\times 0.3) + (0.2 \\times 0.5) \\\\ \n",
    "(0.4 \\times 0.1) + (0.7 \\times 0.3) + (0.6 \\times 0.5)\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.05 + 0.09 + 0.1 \\\\ \n",
    "0.04 + 0.21 + 0.3\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.34 \\\\ 0.75\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Approximating **tanh function**:  \n",
    "$$\n",
    "\\tanh(0.34) \\approx 0.327, \\quad \\tanh(0.75) \\approx 0.635\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh(W_f X_2 + U_f h_1^{(fwd)} + b_f)\n",
    "$$\n",
    "\n",
    "Using **previous hidden state**:\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\ 0.6 \\\\ 0.8\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "(Similarly, calculating matrix multiplications and applying **tanh**, we get:)\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\begin{bmatrix} 0.765 \\\\ 0.851 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Time Step 3: \"AI\"**\n",
    "Following the same process:\n",
    "\n",
    "$$\n",
    "h_3^{(fwd)} = \\begin{bmatrix} 0.88 \\\\ 0.92 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ›  Step 3: Backward Pass (Processing right to left)**\n",
    "We now process in **reverse order**:\n",
    "\n",
    "### **ğŸ”¹ Time Step 3: \"AI\"**\n",
    "$$\n",
    "h_3^{(bwd)} = \\tanh(W_b X_3 + U_b h_0 + b_b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(bwd)} = \\begin{bmatrix} 0.805 \\\\ 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **ğŸ”¹ Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(bwd)} = \\begin{bmatrix} 0.742 \\\\ 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **ğŸ”¹ Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(bwd)} = \\begin{bmatrix} 0.658 \\\\ 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ›  Step 4: Combining Forward & Backward States**\n",
    "For each word, we concatenate both hidden states:\n",
    "\n",
    "$$\n",
    "h_1 = [h_1^{(fwd)}; h_1^{(bwd)}] = \\begin{bmatrix} 0.327 & 0.635 & 0.658 & 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = [h_2^{(fwd)}; h_2^{(bwd)}] = \\begin{bmatrix} 0.765 & 0.851 & 0.742 & 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = [h_3^{(fwd)}; h_3^{(bwd)}] = \\begin{bmatrix} 0.88 & 0.92 & 0.805 & 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”® Step 5: Output Layer**\n",
    "If this is for **classification**, we would pass the final **concatenated hidden states** through a softmax layer.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Conclusion**\n",
    "- BiRNN processes **both past & future context**.\n",
    "- Each word has **two hidden states** (forward + backward).\n",
    "- The **final hidden state** is a combination of **both directions**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ” What Do These Calculations Signify?**  \n",
    "\n",
    "The calculations we performed help us **understand how Bi-directional RNN (BiRNN) processes text step by step**. Letâ€™s break it down into **key insights**:\n",
    "\n",
    "\n",
    "\n",
    "## **1ï¸âƒ£ BiRNN Captures Both Past & Future Context**  \n",
    "Unlike a normal **unidirectional RNN**, which processes the sequence **left to right** (or right to left), BiRNN does **both simultaneously**.  \n",
    "\n",
    "- **Forward RNN:** Moves from **left to right** (normal reading order).  \n",
    "- **Backward RNN:** Moves from **right to left** (reverse reading order).  \n",
    "- The **final hidden state** for each word is a **combination of both directions**, giving the model **fuller context**.  \n",
    "\n",
    "**Example:**\n",
    "For the word `\"love\"` in `\"I love AI\"`,  \n",
    "- The **forward RNN** only sees `\"I love ...\"`,  \n",
    "- The **backward RNN** sees `\"... love AI\"`.  \n",
    "\n",
    "So, `\"love\"` gets influenced by **both \"I\" (past) and \"AI\" (future)**, giving it **richer meaning**.\n",
    "\n",
    "\n",
    "\n",
    "## **2ï¸âƒ£ Word Meaning Depends on Full Context**  \n",
    "Consider this sentence:\n",
    "\n",
    "> **\"He plays the bass.\"**  \n",
    "> **\"He caught a bass.\"**\n",
    "\n",
    "The word **\"bass\"** has **two meanings** (musical instrument vs. fish).  \n",
    "\n",
    "- A **unidirectional RNN** (left-to-right) would process `\"He caught a ...\"` before seeing `\"bass\"`, which is **not enough to disambiguate** the meaning.  \n",
    "- A **BiRNN** processes both `\"caught a\"` and the words **after** `\"bass\"` at the same time, giving it more information to determine the meaning.\n",
    "\n",
    "**This is crucial for NLP tasks like Named Entity Recognition, Sentiment Analysis, and Speech Recognition!** ğŸ¯\n",
    "\n",
    "\n",
    "\n",
    "## **3ï¸âƒ£ Why Do We Combine Forward & Backward States?**  \n",
    "At each time step, we computed **two hidden states**:\n",
    "- $ h_t^{(fwd)} $ â†’ Capturing the meaning from the **left context**  \n",
    "- $ h_t^{(bwd)} $ â†’ Capturing the meaning from the **right context**  \n",
    "- **Final representation** â†’ **Concatenation** of both  \n",
    "\n",
    "**Example:**  \n",
    "For **\"love\"** in `\"I love AI\"`, we got:\n",
    "\n",
    "$$\n",
    "h_2 = [0.765, 0.851, 0.742, 0.831]\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- $ 0.765, 0.851 $ capture **past information** (from \"I\")  \n",
    "- $ 0.742, 0.831 $ capture **future information** (from \"AI\")  \n",
    "\n",
    "Thus, `\"love\"` is **better understood** with the full sentence in mind. ğŸ’¡  \n",
    "\n",
    "\n",
    "\n",
    "## **4ï¸âƒ£ BiRNN Is More Powerful Than Simple RNN**\n",
    "Regular RNNs have a **vanishing gradient problem**, making them struggle to capture **long-range dependencies**.  \n",
    "\n",
    "- **BiRNN helps solve this** because it gets **two different perspectives**, making it **better at learning complex relationships** between words.  \n",
    "- This is why BiRNN is often used in **speech recognition, machine translation, and question-answering systems**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Summary: What Our Calculations Showed**\n",
    "âœ… **BiRNN processes both past and future** at the same time.  \n",
    "âœ… **Each word's meaning is enhanced by its surrounding words**.  \n",
    "âœ… **Final representation is a fusion of two different contexts**, making the model more powerful than a standard RNN.  \n",
    "âœ… **Works great for NLP tasks like sentiment analysis, speech recognition, and machine translation.**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
