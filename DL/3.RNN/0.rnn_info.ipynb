{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recurrent Neural Networks (RNNs)** 🎨✨  \n",
    "\n",
    "### Imagine You're Telling a Story 📖  \n",
    "Think of a **Recurrent Neural Network (RNN)** like a storyteller 📜 who remembers past events to tell the next part of the story. Unlike regular neural networks, which treat every input separately, **RNNs have memory!** 🧠 They remember what happened before and use that info to make better decisions.  \n",
    "\n",
    "### How It Works 🔄  \n",
    "1️⃣ **Takes an input** – Let’s say you're reading a sentence word by word. The RNN processes each word step by step.  \n",
    "2️⃣ **Remembers the past** – It keeps a \"hidden state\" 📦 that stores information about previous words.  \n",
    "3️⃣ **Passes information forward** – Like a storyteller who recalls past events to shape the next part of the story, the RNN updates its hidden state at each step.  \n",
    "4️⃣ **Makes a prediction** – It predicts the next word, the sentiment of a sentence, or even generates text like a chatbot! 🤖💬  \n",
    "\n",
    "### Why Is Memory Important? 🏛  \n",
    "Imagine reading a sentence like:  \n",
    "➡️ \"The boy played with his dog. **He** was very happy.\"  \n",
    "A normal neural network might struggle to understand who \"**He**\" refers to. But an RNN **remembers** that we were talking about \"the boy\" and connects the dots! 🔗  \n",
    "\n",
    "### Where Do We Use RNNs? 🚀  \n",
    "📌 **Speech recognition** – Your voice assistants (Alexa, Siri) use RNNs to understand what you're saying! 🎙  \n",
    "📌 **Chatbots & Language Translation** – Google Translate and chatbots use RNNs to process conversations.  \n",
    "📌 **Stock Price Prediction** – Since stock prices depend on past trends, RNNs help analyze sequences of data 📈💰.  \n",
    "📌 **Music Generation** – RNNs can even compose music! 🎵🤩  \n",
    "\n",
    "### The Problem? 😬  \n",
    "💥 **Vanishing Gradient Problem** – When an RNN tries to remember too much (like a forgetful storyteller), older information fades away, making it hard to learn long-term dependencies.  \n",
    "\n",
    "### The Fix? 🛠  \n",
    "🔹 **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)** are advanced RNNs that fix this memory loss problem. They have a special \"forget gate\" 🔑 that helps decide what to keep and what to discard.  \n",
    "\n",
    "### In Short 🏁  \n",
    "RNNs = Neural networks with memory 🔄  \n",
    "They process sequences step by step ⏭  \n",
    "Useful in speech, text, and time-series data! 📊🎙  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔥 RNN vs ANN: The Ultimate Showdown! 🔥  \n",
    "\n",
    "When working with neural networks, you might come across **Artificial Neural Networks (ANNs)** and **Recurrent Neural Networks (RNNs)**. While both are powerful, they serve different purposes. Let's break it down in a fun and easy way!  \n",
    "\n",
    "\n",
    "\n",
    "## 🧠 **Artificial Neural Network (ANN)** – The Standard Genius  \n",
    "📌 **What is it?**  \n",
    "ANNs are like a **smart calculator**. They take inputs, process them through layers of neurons, and give an output. But… **they have no memory**! Every input is treated separately.  \n",
    "\n",
    "📌 **Structure:**  \n",
    "🔹 Input Layer → Hidden Layers → Output Layer  \n",
    "🔹 Each neuron is fully connected to the next layer  \n",
    "🔹 Uses activation functions like **ReLU, Sigmoid, Tanh**  \n",
    "\n",
    "📌 **Where is it used?**  \n",
    "✅ Image classification (e.g., identifying cats vs. dogs 🐶🐱)  \n",
    "✅ Spam detection (sorting emails 📧)  \n",
    "✅ Recommendation systems (Netflix suggestions 🍿)  \n",
    "\n",
    "📌 **Limitations**  \n",
    "❌ Cannot handle **sequential** or **time-dependent** data (like predicting stock prices 📈 or speech recognition 🎙️)  \n",
    "❌ Treats every input independently  \n",
    "\n",
    "\n",
    "\n",
    "## 🔄 **Recurrent Neural Network (RNN)** – The Memory Master  \n",
    "📌 **What is it?**  \n",
    "RNNs are like **humans reading a story** 📖. They remember previous words to understand the next ones. Unlike ANNs, RNNs have a **memory** that helps them process sequences.  \n",
    "\n",
    "📌 **Structure:**  \n",
    "🔹 Looks similar to an ANN but has **loops** that allow information to persist!  \n",
    "🔹 Each neuron not only passes data forward but also **feeds it back into itself**!  \n",
    "🔹 Uses activation functions like **Tanh, Softmax**  \n",
    "\n",
    "📌 **Where is it used?**  \n",
    "✅ Speech Recognition (like Siri or Google Assistant 🎙️)  \n",
    "✅ Language Translation (Google Translate 🌍)  \n",
    "✅ Time-series forecasting (predicting stock trends 📊)  \n",
    "\n",
    "📌 **Limitations**  \n",
    "❌ Suffers from **vanishing gradient** (loses memory for long sequences 😢)  \n",
    "❌ Slower training compared to ANNs  \n",
    "❌ Difficult to handle long-term dependencies  \n",
    "\n",
    "\n",
    "## 🎯 **Key Differences at a Glance!**  \n",
    "\n",
    "| Feature  | ANN 🧠 | RNN 🔄 |\n",
    "|----------|--------|--------|\n",
    "| **Memory** | No memory, treats inputs independently | Remembers past inputs for sequential processing |\n",
    "| **Structure** | Fully connected layers | Loops and feedback connections |\n",
    "| **Best for** | Static data (images, tabular data) | Sequential data (speech, text, time series) |\n",
    "| **Limitations** | Can’t process time-dependent data | Struggles with long-term dependencies |\n",
    "| **Examples** | Image classification, spam detection | Chatbots, stock prediction, speech-to-text |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **When to Use What?**  \n",
    "✔️ Use **ANN** if your problem does **not** involve sequences (e.g., image recognition, customer churn prediction).  \n",
    "✔️ Use **RNN** if your data is **sequential** (e.g., text generation, audio processing, stock market forecasting).  \n",
    "\n",
    "For **better performance in long sequences**, we use **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)**, which improve RNNs by solving the vanishing gradient problem.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎉 **Final Thoughts**  \n",
    "Both ANNs and RNNs are powerful, but their strengths lie in different areas. If you’re working with images, structured data, or classification tasks, **ANN is your go-to**. But if you’re dealing with sequential data like speech, text, or time series, **RNN will be your best friend**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 **Recurrent Neural Network (RNN) Architecture – A Deep Dive!** 🔄  \n",
    "\n",
    "RNNs are a special type of neural network designed to process **sequential data**, such as time-series data, speech, and text. Unlike traditional ANNs, RNNs have a **memory** that allows them to consider past inputs while processing current ones.\n",
    "\n",
    "\n",
    "\n",
    "## 🏗️ **Basic RNN Architecture**  \n",
    "\n",
    "RNNs are different from standard ANNs because they have a **feedback loop** that allows information to persist over time.\n",
    "\n",
    "### 🔹 **Structure of a Simple RNN**  \n",
    "The architecture consists of:  \n",
    "1. **Input Layer**: Takes the input sequence.  \n",
    "2. **Hidden Layer (Recurrent Neurons)**: Maintains a memory of previous states and updates at each time step.  \n",
    "3. **Output Layer**: Produces the final prediction.\n",
    "\n",
    "💡 **Key difference from ANN**: The hidden layer is connected to itself! This allows information to flow from previous time steps.\n",
    "\n",
    "### 📌 **Mathematical Representation**  \n",
    "At each time step **t**, the RNN updates its hidden state using:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = previous hidden state  \n",
    "- $ W_x $, $ W_h $ = weight matrices  \n",
    "- $ b $ = bias  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "The output is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ g $ = activation function (softmax for classification, linear for regression)  \n",
    "\n",
    "\n",
    "\n",
    "## 🔄 **Unrolling the RNN (Time Step Representation)**  \n",
    "\n",
    "A simple RNN processes a sequence of inputs **one time step at a time**.  \n",
    "For example, if we have a sequence **X = [x₁, x₂, x₃]**, the RNN unfolds like this:\n",
    "\n",
    "```\n",
    "x₁ → [h₁] → y₁\n",
    "      ↘\n",
    "x₂ → [h₂] → y₂\n",
    "       ↘\n",
    "x₃ → [h₃] → y₃\n",
    "```\n",
    "  \n",
    "Here:  \n",
    "- The hidden state **h** carries information from previous time steps.\n",
    "- Each output $ y_t $ is computed based on the current hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## 🚧 **Challenges in Basic RNNs**  \n",
    "RNNs are powerful, but they face some problems:\n",
    "\n",
    "### ❌ **Vanishing Gradient Problem**  \n",
    "- When training deep RNNs with many time steps, gradients shrink to near **zero** during backpropagation.  \n",
    "- This makes it **hard to learn long-term dependencies** (i.e., remembering things from many time steps ago).\n",
    "\n",
    "### ❌ **Exploding Gradient Problem**  \n",
    "- If gradients grow **too large**, they can make the training unstable.\n",
    "\n",
    "To solve these, we use **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)**.\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Variants of RNNs**\n",
    "There are different types of RNN architectures:\n",
    "\n",
    "1. **One-to-One (Vanilla RNN)**\n",
    "   - Used for simple tasks like image classification.\n",
    "\n",
    "2. **One-to-Many**\n",
    "   - Example: Generating music 🎵 from a single note.\n",
    "\n",
    "3. **Many-to-One**\n",
    "   - Example: Sentiment analysis (classifying an entire sentence as \"positive\" or \"negative\").\n",
    "\n",
    "4. **Many-to-Many**\n",
    "   - Example: Machine translation (e.g., English → French).\n",
    "\n",
    "\n",
    "\n",
    "## 🏆 **Key Takeaways**  \n",
    "✅ RNNs are great for **sequential data** processing.  \n",
    "✅ They have **memory**, unlike ANNs.  \n",
    "✅ They suffer from **vanishing/exploding gradients** but can be improved with **LSTMs and GRUs**.  \n",
    "✅ Used in **speech recognition, time-series forecasting, chatbots, and NLP tasks**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔄 **Forward Propagation in Recurrent Neural Networks (RNNs) – A Complete Breakdown!** 🔄  \n",
    "\n",
    "Forward propagation in an RNN works differently from a standard Artificial Neural Network (ANN) because it processes **sequential data** while maintaining a **hidden state** that carries information from previous time steps.\n",
    "\n",
    "\n",
    "\n",
    "## 🏗 **Basic Structure of RNN Forward Propagation**\n",
    "Unlike traditional feedforward networks, where inputs are independent, an RNN processes inputs **sequentially**, maintaining a memory of past computations.\n",
    "\n",
    "For each time step $ t $, the RNN performs the following computations:\n",
    "\n",
    "1️⃣ **Compute the new hidden state $ h_t $ using the current input $ x_t $ and the previous hidden state $ h_{t-1} $.**  \n",
    "2️⃣ **Compute the output $ y_t $ using the hidden state $ h_t $.**  \n",
    "3️⃣ **Pass the hidden state to the next time step.**  \n",
    "\n",
    "\n",
    "\n",
    "## 🔢 **Mathematical Formulation**\n",
    "At each time step $ t $, forward propagation in an RNN follows these steps:\n",
    "\n",
    "### 1️⃣ **Hidden State Update**\n",
    "The hidden state $ h_t $ is calculated using the previous hidden state $ h_{t-1} $ and the current input $ x_t $:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = hidden state from the previous time step  \n",
    "- $ W_x $ = weight matrix for input  \n",
    "- $ W_h $ = weight matrix for previous hidden state  \n",
    "- $ b_h $ = bias term  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "### 2️⃣ **Output Calculation**\n",
    "The output $ y_t $ at time step $ t $ is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ b_y $ = bias for output  \n",
    "- $ g $ = activation function (e.g., **softmax** for classification tasks)  \n",
    "\n",
    "\n",
    "\n",
    "## 📜 **Step-by-Step Forward Propagation Example**\n",
    "Let's assume we have an RNN processing three time steps with inputs $ x_1, x_2, x_3 $.\n",
    "\n",
    "### 🔄 **Unrolling the RNN**\n",
    "Instead of viewing an RNN as a single network, we **unroll it** across time steps:\n",
    "\n",
    "```\n",
    "x₁ → [h₁] → y₁\n",
    "      ↘\n",
    "x₂ → [h₂] → y₂\n",
    "       ↘\n",
    "x₃ → [h₃] → y₃\n",
    "```\n",
    "\n",
    "### 🔢 **Step 1: Compute the first hidden state $ h_1 $**\n",
    "$$\n",
    "h_1 = f(W_x x_1 + W_h h_0 + b_h)\n",
    "$$\n",
    "- $ h_0 $ is typically initialized as a vector of zeros.\n",
    "\n",
    "### 🔢 **Step 2: Compute the second hidden state $ h_2 $**\n",
    "$$\n",
    "h_2 = f(W_x x_2 + W_h h_1 + b_h)\n",
    "$$\n",
    "- The hidden state $ h_1 $ from the previous time step is used.\n",
    "\n",
    "### 🔢 **Step 3: Compute the third hidden state $ h_3 $**\n",
    "$$\n",
    "h_3 = f(W_x x_3 + W_h h_2 + b_h)\n",
    "$$\n",
    "\n",
    "### 🔢 **Step 4: Compute outputs $ y_1, y_2, y_3 $**\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "- The output is calculated at each time step based on the hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Key Observations**\n",
    "✔ **Recurrent Connections**: The hidden state at each time step depends on the previous state.  \n",
    "✔ **Shared Weights**: The same weight matrices $ W_x, W_h, W_y $ are used across all time steps, reducing complexity.  \n",
    "✔ **Memory Effect**: The network retains past information, making it suitable for **sequential tasks** like speech recognition, language modeling, and time-series forecasting.  \n",
    "\n",
    "\n",
    "\n",
    "## 💻 **Python Code Example**\n",
    "Here’s how forward propagation in an RNN can be implemented using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Activation function (tanh)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Define input, weight matrices, and bias\n",
    "x = np.array([[0.5], [0.2], [0.1]])  # Input at three time steps\n",
    "W_x = np.array([[0.8]])  # Input weight\n",
    "W_h = np.array([[0.5]])  # Recurrent weight\n",
    "W_y = np.array([[1.0]])  # Output weight\n",
    "b_h = np.array([[0.1]])  # Bias for hidden state\n",
    "b_y = np.array([[0.2]])  # Bias for output\n",
    "\n",
    "# Initialize hidden state\n",
    "h = np.array([[0]])  # Start with zero hidden state\n",
    "\n",
    "# Forward propagation\n",
    "for t in range(len(x)):\n",
    "    h = tanh(np.dot(W_x, x[t]) + np.dot(W_h, h) + b_h)  # Update hidden state\n",
    "    y = np.dot(W_y, h) + b_y  # Compute output\n",
    "    print(f\"Time Step {t+1}: Hidden State: {h}, Output: {y}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Final Thoughts**\n",
    "✅ **RNN forward propagation** processes inputs **one at a time** while maintaining memory.  \n",
    "✅ **Key equations** involve computing the **hidden state** and **output** at each time step.  \n",
    "✅ **Challenges**: Standard RNNs struggle with long sequences due to the **vanishing gradient problem**.  \n",
    "✅ **Solution**: Use **LSTMs or GRUs** to improve long-term memory handling.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧮 **Manual Calculation of RNN Forward Propagation – Step-by-Step Example** 🔄  \n",
    "\n",
    "Let's take a simple example of an **RNN with one neuron** to manually compute forward propagation for **three time steps**.\n",
    "\n",
    "\n",
    "\n",
    "## **📝 Given Parameters**\n",
    "We define a simple RNN where:\n",
    "\n",
    "- **Input size = 1 (one feature per time step)**\n",
    "- **Hidden state size = 1 (one neuron in hidden layer)**\n",
    "- **Output size = 1 (one neuron in output layer)**\n",
    "- **Sequence length = 3 (processing 3 time steps: $ x_1, x_2, x_3 $)**\n",
    "\n",
    "#### 🎯 **Initial Values**\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| $ x_1, x_2, x_3 $ | $ 0.5, 0.2, 0.1 $ (input at each time step) |\n",
    "| $ W_x $ | $ 0.8 $ (weight for input) |\n",
    "| $ W_h $ | $ 0.5 $ (weight for hidden state) |\n",
    "| $ W_y $ | $ 1.0 $ (weight for output) |\n",
    "| $ b_h $ | $ 0.1 $ (bias for hidden state) |\n",
    "| $ b_y $ | $ 0.2 $ (bias for output) |\n",
    "| $ h_0 $ | $ 0 $ (initial hidden state) |\n",
    "\n",
    "💡 **Activation function**: We use the **tanh** function:\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📝 Forward Propagation Steps**\n",
    "At each time step, we compute:\n",
    "\n",
    "1️⃣ **Hidden state update**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "2️⃣ **Output calculation**  \n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📊 Step-by-Step Computation**\n",
    "### **⏳ Time Step 1 ($ t = 1 $)**\n",
    "#### 🔹 Compute hidden state $ h_1 $:\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.8 \\times 0.5 + 0.5 \\times 0 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.4 + 0 + 0.1) = \\tanh(0.5)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.5) \\approx 0.4621 $:\n",
    "\n",
    "$$\n",
    "h_1 \\approx 0.4621\n",
    "$$\n",
    "\n",
    "#### 🔹 Compute output $ y_1 $:\n",
    "\n",
    "$$\n",
    "y_1 = 1.0 \\times 0.4621 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_1 \\approx 0.6621\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **⏳ Time Step 2 ($ t = 2 $)**\n",
    "#### 🔹 Compute hidden state $ h_2 $:\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.8 \\times 0.2 + 0.5 \\times 0.4621 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.16 + 0.2311 + 0.1) = \\tanh(0.4911)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4911) \\approx 0.4548 $:\n",
    "\n",
    "$$\n",
    "h_2 \\approx 0.4548\n",
    "$$\n",
    "\n",
    "#### 🔹 Compute output $ y_2 $:\n",
    "\n",
    "$$\n",
    "y_2 = 1.0 \\times 0.4548 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 \\approx 0.6548\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **⏳ Time Step 3 ($ t = 3 $)**\n",
    "#### 🔹 Compute hidden state $ h_3 $:\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.8 \\times 0.1 + 0.5 \\times 0.4548 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.08 + 0.2274 + 0.1) = \\tanh(0.4074)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4074) \\approx 0.3863 $:\n",
    "\n",
    "$$\n",
    "h_3 \\approx 0.3863\n",
    "$$\n",
    "\n",
    "#### 🔹 Compute output $ y_3 $:\n",
    "\n",
    "$$\n",
    "y_3 = 1.0 \\times 0.3863 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_3 \\approx 0.5863\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Final Results**\n",
    "| Time Step | $ x_t $ | $ h_t $ (Hidden State) | $ y_t $ (Output) |\n",
    "|-----------|----------|----------------|----------------|\n",
    "| $ t = 1 $ | $ 0.5 $ | $ 0.4621 $ | $ 0.6621 $ |\n",
    "| $ t = 2 $ | $ 0.2 $ | $ 0.4548 $ | $ 0.6548 $ |\n",
    "| $ t = 3 $ | $ 0.1 $ | $ 0.3863 $ | $ 0.5863 $ |\n",
    "\n",
    "🎯 **Observation**:  \n",
    "- The hidden state **carries information** from previous time steps, updating with each new input.\n",
    "- The outputs are computed at each time step, making the RNN suitable for **sequential data** processing.\n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Summary**\n",
    "✔ We **manually computed** RNN forward propagation step by step.  \n",
    "✔ The **hidden state** maintains memory across time steps.  \n",
    "✔ The **output at each step** depends on both the current input and previous hidden state.  \n",
    "✔ **Activation function (tanh)** ensures values remain between $-1$ and $1$.  \n",
    "✔ **Weights are shared** across all time steps, making the RNN efficient.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! RNN architectures can be categorized based on the **input-output relationship**, which defines how sequences are processed. Let’s break them down in a fun and colorful way! 🚀🔥  \n",
    "\n",
    "## 🎯 **Types of RNN Based on Input-Output Structure**  \n",
    "\n",
    "| Type | Input | Output | Example Use Case |\n",
    "|------|-------|--------|-----------------|\n",
    "| **One-to-One** | 🔹 Single input | 🔸 Single output | Simple classification (e.g., Spam detection 📩) |\n",
    "| **One-to-Many** | 🔹 Single input | 🔸 Sequence of outputs | Music generation 🎵, Image captioning 🖼 |\n",
    "| **Many-to-One** | 🔹 Sequence of inputs | 🔸 Single output | Sentiment analysis 😊😢, Fraud detection 💳 |\n",
    "| **Many-to-Many (Same Length)** | 🔹 Sequence of inputs | 🔸 Sequence of outputs | Video frame labeling 🎥, POS tagging 📌 |\n",
    "| **Many-to-Many (Different Length)** | 🔹 Sequence of inputs | 🔸 Sequence of outputs | Machine translation 🌍, Speech-to-text 🎤 |\n",
    "\n",
    "\n",
    "## 1️⃣ **One-to-One (Vanilla Neural Network)**\n",
    "- ✅ **Single input → Single output**  \n",
    "- 🔥 **Example:** Image classification 📸 (e.g., classifying an image as **dog** 🐶 or **cat** 🐱)  \n",
    "- 🤖 **Works like:** A standard feedforward network with no sequential memory.  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Imagine you **see one photo** 🖼 and simply classify it as \"cat\" or \"dog\".  \n",
    "\n",
    "\n",
    "\n",
    "## 2️⃣ **One-to-Many (Single Input, Multiple Outputs)**\n",
    "- ✅ **Single input → Sequence of outputs**  \n",
    "- 🔥 **Example:**  \n",
    "  - **Music generation** 🎶 (e.g., input a musical **style**, generate a full melody).  \n",
    "  - **Image captioning** 🏞 (e.g., input an **image**, generate a **sentence** describing it).  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Imagine someone shows you a **picture of a sunset** 🌅, and you start describing it:  \n",
    "*\"The sky is orange, birds are flying, it's evening time.\"*  \n",
    "\n",
    "💡 **Used in:** LSTMs, GRUs when generating sequences from a single source.\n",
    "\n",
    "\n",
    "\n",
    "## 3️⃣ **Many-to-One (Sequence Input, Single Output)**\n",
    "- ✅ **Multiple inputs → Single output**  \n",
    "- 🔥 **Example:**  \n",
    "  - **Sentiment analysis** 😊😢 (e.g., input a sentence, classify it as **positive or negative**).  \n",
    "  - **Fraud detection** 💳 (e.g., analyze a customer’s transaction history and classify as **fraud/not fraud**).  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "You **read a full movie review** 🎬 and decide: *\"Was the review positive or negative?\"*  \n",
    "\n",
    "💡 **Used in:** LSTMs, GRUs for tasks where context builds over time.\n",
    "\n",
    "\n",
    "\n",
    "## 4️⃣ **Many-to-Many (Same Length)**\n",
    "- ✅ **Sequence input → Sequence output** (same number of inputs and outputs).  \n",
    "- 🔥 **Example:**  \n",
    "  - **Video frame labeling** 🎥 (e.g., classify each frame in a video).  \n",
    "  - **Part-of-Speech (POS) tagging** 📌 (e.g., tagging each word as **noun, verb, adjective**).  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "You **read a sentence** 📖 and label each word with its part of speech:  \n",
    "*\"The (Determiner) dog (Noun) runs (Verb) fast (Adverb).\"*  \n",
    "\n",
    "💡 **Used in:** Bi-directional RNNs (Bi-RNNs), LSTMs for tasks requiring **sequential context**.\n",
    "\n",
    "\n",
    "\n",
    "## 5️⃣ **Many-to-Many (Different Length)**\n",
    "- ✅ **Sequence input → Sequence output** (variable lengths).  \n",
    "- 🔥 **Example:**  \n",
    "  - **Machine translation** 🌍 (e.g., English sentence → French sentence).  \n",
    "  - **Speech-to-text** 🎤 (e.g., input voice, output text transcript).  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "You **listen to someone speaking in English** 🎙 and translate it into French:  \n",
    "*\"Hello, how are you?\" → *\"Bonjour, comment ça va?\"*  \n",
    "\n",
    "💡 **Used in:** **Encoder-Decoder RNNs**, often paired with **attention mechanisms**.\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Final Thoughts**\n",
    "- If you need **sequential processing**, **RNNs** (especially **LSTMs & GRUs**) are your go-to!  \n",
    "- Choose the structure based on **input-output format** 🚀.  \n",
    "- For **short-term dependencies**, Vanilla RNN might work. But for **longer memory**, use **LSTM or GRU**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of neural network designed to process **sequential data** by maintaining a **memory** of past inputs. Unlike traditional feedforward networks, RNNs have **loops** that allow information to persist, making them ideal for tasks like **speech recognition, language modeling, and time series forecasting**.\n",
    "\n",
    "\n",
    "\n",
    "## 🌟 **Types of RNNs** 🌟\n",
    "\n",
    "### 1️⃣ **Basic RNN (Vanilla RNN)**\n",
    "📌 **Key Idea:** Each neuron not only receives input from the current timestep but also retains **memory** from the previous step.  \n",
    "\n",
    "🔗 **Structure:**  \n",
    "It consists of a **hidden state** that is updated at each timestep based on the previous state and current input:\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "🚨 **Limitation:**  \n",
    "- Suffers from **vanishing gradient problem**, making it hard to remember long-term dependencies.\n",
    "\n",
    "✅ **Used For:**  \n",
    "- Short-term memory tasks (e.g., **simple text generation, stock price prediction**).\n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Imagine you're reading a book, but you can only remember the last **few** words from each sentence.\n",
    "\n",
    "\n",
    "\n",
    "### 2️⃣ **Long Short-Term Memory (LSTM)**\n",
    "📌 **Key Idea:** Introduces **gates** to control the flow of information, allowing it to **remember** or **forget** information selectively.  \n",
    "\n",
    "🔗 **Structure:**  \n",
    "LSTMs have **three gates**:\n",
    "- 🏗 **Forget Gate (🚪)** – Decides what past information to discard.  \n",
    "- 🏗 **Input Gate (📥)** – Determines what new information to store.  \n",
    "- 🏗 **Output Gate (📤)** – Controls what part of the hidden state is passed to the next step.  \n",
    "\n",
    "🚀 **Advantages:**  \n",
    "- Handles **long-term dependencies** better than Vanilla RNN.\n",
    "- Avoids **vanishing gradient problem**.\n",
    "\n",
    "✅ **Used For:**  \n",
    "- **Speech recognition** (like Siri, Google Assistant).  \n",
    "- **Machine translation** (like Google Translate).  \n",
    "- **Time-series forecasting** (like predicting weather trends).  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Think of LSTM as a **notebook** 📝 where you write important notes and erase unimportant details as you read a book.\n",
    "\n",
    "\n",
    "\n",
    "### 3️⃣ **Gated Recurrent Unit (GRU)**\n",
    "📌 **Key Idea:** A simplified version of LSTM with only **two gates**:\n",
    "- 🔄 **Reset Gate (🔄)** – Determines how much of past information to forget.  \n",
    "- 🔄 **Update Gate (⏩)** – Decides how much new information to keep.  \n",
    "\n",
    "🚀 **Advantages:**  \n",
    "- Works **faster** than LSTM because it has fewer parameters.  \n",
    "- Retains efficiency while maintaining good performance on sequential tasks.\n",
    "\n",
    "✅ **Used For:**  \n",
    "- **Chatbots** 🤖 like ChatGPT!  \n",
    "- **Handwriting recognition** ✍️.  \n",
    "- **Music generation** 🎵.  \n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Imagine **GRU** as a **sticky note** where you only keep the most important details while discarding unnecessary ones.\n",
    "\n",
    "\n",
    "\n",
    "### 4️⃣ **Bidirectional RNN (Bi-RNN)**\n",
    "📌 **Key Idea:** Processes information in **both forward and backward** directions.  \n",
    "\n",
    "🔗 **Structure:**  \n",
    "- One RNN processes **left to right** 🡆.  \n",
    "- Another RNN processes **right to left** 🡄.  \n",
    "- The outputs from both are combined for better accuracy.  \n",
    "\n",
    "🚀 **Advantages:**  \n",
    "- Can **understand context better** (e.g., recognizing a word’s meaning based on future words).  \n",
    "- Great for **sequence labeling tasks**.\n",
    "\n",
    "✅ **Used For:**  \n",
    "- **Speech recognition** 🎤 (Google Voice, Alexa).  \n",
    "- **Named Entity Recognition (NER)** 🏷 (used in NLP).  \n",
    "- **DNA sequence analysis** 🧬.\n",
    "\n",
    "🖼 **Illustration:**  \n",
    "Think of it as reading a sentence **both forwards and backwards** to get the full meaning.\n",
    "\n",
    "\n",
    "\n",
    "### 5️⃣ **Echo State Networks (ESN)**\n",
    "📌 **Key Idea:** Uses a **randomly initialized** reservoir (hidden layer) to store information without training it directly.\n",
    "\n",
    "🚀 **Advantages:**  \n",
    "- Faster training 🏃‍♂️💨.  \n",
    "- Good for **time-series forecasting** 📈.\n",
    "\n",
    "✅ **Used For:**  \n",
    "- **Financial predictions** (stock market).  \n",
    "- **Brain-inspired computing** 🧠.\n",
    "\n",
    "🖼 **Illustration:**  \n",
    "It’s like a sponge 🧽 that **absorbs** patterns from input data and then extracts useful features!\n",
    "\n",
    "## 🎯 **Comparison Table**\n",
    "\n",
    "| Type        | Handles Long-term Memory? | Speed ⏩ | Best For |\n",
    "|------------|-------------------------|---------|---------|\n",
    "| **Vanilla RNN** | ❌ No (Vanishing Gradient) | ✅ Fast | Simple sequential tasks |\n",
    "| **LSTM** | ✅ Yes (Uses Gates) | ❌ Slower | Speech recognition, NLP |\n",
    "| **GRU** | ✅ Yes (Simpler than LSTM) | ✅ Faster | Chatbots, Music generation |\n",
    "| **Bi-RNN** | ✅ Yes (Both Directions) | ❌ Slower | Named Entity Recognition, Speech |\n",
    "| **ESN** | ✅ Yes (Fixed Reservoir) | 🚀 Very Fast | Financial forecasting |\n",
    "\n",
    "\n",
    "## 🏆 **Conclusion**\n",
    "Different RNNs serve different purposes. **LSTMs & GRUs** are the most commonly used due to their ability to handle **long-term dependencies**. If **speed is a priority**, **GRU** is better than LSTM. For **tasks requiring full context understanding**, **Bidirectional RNN** is a strong choice.\n",
    "\n",
    "🔥 **So next time you build an NLP or time-series model, choose the right RNN wisely!** 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔥 **Backpropagation in RNNs – A Deep Dive!** 🔥  \n",
    "\n",
    "Backpropagation in Recurrent Neural Networks (RNNs) is a bit different from standard feedforward networks because of their sequential nature. This process is called **Backpropagation Through Time (BPTT)**. Let's break it down step by step!  \n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Understanding Backpropagation in RNNs**\n",
    "### 🌟 **Step 1: Forward Pass**  \n",
    "In a standard RNN, we pass input sequences **step by step** through the network while maintaining a hidden state:  \n",
    "\n",
    "$$\n",
    "h_t = f(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + c)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_t $ = hidden state at time $ t $, which depends on previous state $ h_{t-1} $  \n",
    "- $ y_t $ = output at time $ t $  \n",
    "- $ W_h, W_x, W_y $ = weight matrices  \n",
    "- $ b, c $ = biases  \n",
    "- $ f, g $ = activation functions (e.g., **tanh, softmax**)  \n",
    "\n",
    "During this process, the **hidden state carries information** forward in time, making RNNs great for sequential tasks like speech recognition and text processing.  \n",
    "\n",
    "\n",
    "\n",
    "### 🔄 **Step 2: Loss Calculation**  \n",
    "After the forward pass, we compute the **loss** using a function like **Mean Squared Error (MSE) or Cross-Entropy Loss**, depending on the problem (regression or classification).  \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t)\n",
    "$$\n",
    "\n",
    "where $ L $ is the loss function and $ \\hat{y}_t $ is the predicted output.\n",
    "\n",
    "\n",
    "\n",
    "### 🔁 **Step 3: Backpropagation Through Time (BPTT)**\n",
    "This is where things get interesting! Unlike standard backpropagation (which flows only through layers), RNN backpropagation **flows through time** as well.  \n",
    "\n",
    "🛠 **Steps in BPTT:**  \n",
    "1️⃣ Compute **gradients at the last time step** ($ T $) and move backward.  \n",
    "2️⃣ Compute **gradients for each earlier time step** until $ t=1 $.  \n",
    "3️⃣ Update weights using **gradient descent** or any optimizer like Adam, RMSprop.  \n",
    "\n",
    "#### 🔹 **Gradient Calculation**\n",
    "For each time step $ t $, we compute gradients of the loss with respect to weights using the **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\sum_{t=1}^{T} \\sum_{k=t}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_k} \\cdot \\frac{\\partial y_k}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "🛑 **Why is this tricky?**  \n",
    "- **The hidden states are shared** across all time steps.  \n",
    "- **Error at one step** affects all previous steps.  \n",
    "- **Long-term dependencies** make it difficult to train (this is called the **vanishing gradient problem** 🛑).  \n",
    "\n",
    "\n",
    "\n",
    "### 🛑 **Step 4: Vanishing and Exploding Gradients**\n",
    "💡 **Vanishing Gradients:**  \n",
    "- If gradients become **too small**, updates **disappear**, and the model stops learning **long-term dependencies**.  \n",
    "- This happens when we keep multiplying small values (like derivatives of sigmoid/tanh functions).  \n",
    "\n",
    "💥 **Exploding Gradients:**  \n",
    "- If gradients **grow too large**, training becomes **unstable**, and weights explode.  \n",
    "- Happens when weights keep multiplying large values, causing loss to **diverge**.  \n",
    "\n",
    "🔹 **Solutions:**  \n",
    "✅ Use **Long Short-Term Memory (LSTM)** or **Gated Recurrent Unit (GRU)** to control gradient flow.  \n",
    "✅ Apply **gradient clipping** (cap gradients to a maximum value).  \n",
    "✅ Use **ReLU** instead of **sigmoid/tanh** where possible.  \n",
    "\n",
    "\n",
    "\n",
    "### ⚡ **Step 5: Updating Weights**\n",
    "Once gradients are computed, we update weights using **Gradient Descent** or other optimizers like **Adam, RMSprop**:\n",
    "\n",
    "$$\n",
    "W = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the **learning rate**.\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Key Takeaways**\n",
    "✅ **BPTT propagates errors backward through time, affecting all previous time steps**.  \n",
    "✅ **Vanishing gradients make long-term dependencies hard to learn**.  \n",
    "✅ **LSTMs and GRUs solve vanishing gradient issues**.  \n",
    "✅ **Gradient clipping helps control exploding gradients**.  \n",
    "\n",
    "\n",
    "### 🔥 **Final Thought**\n",
    "Backpropagation in RNNs is like **teaching a student** step by step, correcting mistakes from both **recent and past** lessons! 📚  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually go through an **example** of backpropagation in a simple Recurrent Neural Network (RNN) using **Backpropagation Through Time (BPTT)**.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Example: A Simple RNN with One Neuron**\n",
    "We will calculate **forward pass, loss, and backpropagation (BPTT)** for a simple RNN with:  \n",
    "✅ **1 input neuron**  \n",
    "✅ **1 hidden neuron** (with recurrent connection)  \n",
    "✅ **1 output neuron**  \n",
    "✅ **1 time step for simplicity**  \n",
    "\n",
    "\n",
    "\n",
    "### 🎯 **Step 1: Define Network and Initial Weights**\n",
    "We define:  \n",
    "- $ W_x = 0.5 $ (input-to-hidden weight)  \n",
    "- $ W_h = 0.8 $ (hidden-to-hidden recurrent weight)  \n",
    "- $ W_y = 0.3 $ (hidden-to-output weight)  \n",
    "- **Biases are ignored** for simplicity.  \n",
    "\n",
    "Given:  \n",
    "- Input: $ x_1 = 1 $  \n",
    "- True output: $ y_{\\text{true}} = 0.6 $  \n",
    "- Initial hidden state: $ h_0 = 0 $  \n",
    "\n",
    "\n",
    "\n",
    "### 🔄 **Step 2: Forward Pass**\n",
    "#### 🔹 **Hidden State Calculation**  \n",
    "$$\n",
    "h_1 = \\tanh(W_x x_1 + W_h h_0)\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5(1) + 0.8(0))\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5) = 0.462\n",
    "$$\n",
    "\n",
    "#### 🔹 **Output Calculation**\n",
    "$$\n",
    "y_{\\text{pred}} = W_y h_1\n",
    "$$\n",
    "$$\n",
    "= 0.3 \\times 0.462 = 0.1386\n",
    "$$\n",
    "\n",
    "#### 🔹 **Loss Calculation (Mean Squared Error)**\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (y_{\\text{true}} - y_{\\text{pred}})^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.6 - 0.1386)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.4614)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.213) = 0.1065\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## 🔁 **Step 3: Backpropagation Through Time (BPTT)**  \n",
    "Now, we compute the **gradients of the loss** with respect to each weight.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Gradient of Loss w.r.t Output Weight $ W_y $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "We compute the derivatives:  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} = (y_{\\text{pred}} - y_{\\text{true}}) = (0.1386 - 0.6) = -0.4614\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial W_y} = h_1 = 0.462\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = (-0.4614) \\times (0.462) = -0.213\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Gradient of Loss w.r.t Hidden Weight $ W_h $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial h_1} = W_y = 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_h} = (1 - h_1^2) \\times h_0 = (1 - 0.462^2) \\times 0 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = (-0.4614) \\times (0.3) \\times (0) = 0\n",
    "$$\n",
    "\n",
    "👉 Since $ h_0 = 0 $, the gradient for $ W_h $ is **zero** in this case.\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Gradient of Loss w.r.t Input Weight $ W_x $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_x} = (1 - h_1^2) \\times x_1 = (1 - 0.462^2) \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (1 - 0.213) = 0.787\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = (-0.4614) \\times (0.3) \\times (0.787)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -0.1088\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## ✏️ **Step 4: Weight Updates Using Gradient Descent**\n",
    "Using **learning rate** $ \\eta = 0.1 $, we update:\n",
    "\n",
    "$$\n",
    "W_y = W_y - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_y}\n",
    "$$\n",
    "$$\n",
    "= 0.3 - (0.1 \\times -0.213)\n",
    "$$\n",
    "$$\n",
    "= 0.3 + 0.0213 = 0.3213\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_x = W_x - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_x}\n",
    "$$\n",
    "$$\n",
    "= 0.5 - (0.1 \\times -0.1088)\n",
    "$$\n",
    "$$\n",
    "= 0.5 + 0.01088 = 0.51088\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_h = 0.8 - (0.1 \\times 0) = 0.8\n",
    "$$  \n",
    "(Since the gradient was zero, $ W_h $ remains unchanged.)\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Final Updated Weights**\n",
    "After **one iteration of BPTT**, we get:  \n",
    "✅ $ W_x = 0.51088 $  \n",
    "✅ $ W_h = 0.8 $  \n",
    "✅ $ W_y = 0.3213 $  \n",
    "\n",
    "If we repeat this over multiple time steps, RNN learns to predict better over time! 🔥\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Key Takeaways**\n",
    "✔ **BPTT works by computing gradients backward through time** ⏳  \n",
    "✔ **Weight updates use the chain rule** to propagate errors  \n",
    "✔ **Vanishing gradients** occur when gradients become too small  \n",
    "✔ **Exploding gradients** occur when gradients grow too large  \n",
    "✔ **Optimizations like LSTMs, GRUs, and gradient clipping help stabilize learning** 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 **Problems with RNNs: Why They Struggle and How to Fix Them**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are great for handling **sequential data** like **text, speech, and time series**, but they come with several limitations. Let’s break them down in a **simple, colorful way** and also discuss possible solutions! 🌈  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **1. Vanishing Gradient Problem**\n",
    "### ❌ **What is it?**\n",
    "- When training an RNN with **backpropagation through time (BPTT)**, the gradients shrink **exponentially** as they are passed backward through many time steps.  \n",
    "- This means earlier layers receive **almost no updates**, making it **hard for RNNs to learn long-term dependencies**.\n",
    "\n",
    "### 📉 **Why does this happen?**\n",
    "- The chain rule in **backpropagation** involves multiplying many small values (gradients of activation functions like sigmoid or tanh), leading to values approaching **zero**.\n",
    "- This results in **\"memory loss\"** in RNNs—**they forget long-term dependencies**.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units)** – They use special gates to store and update information efficiently.  \n",
    "✅ **Use ReLU activation instead of tanh/sigmoid** – ReLU helps prevent gradients from shrinking.  \n",
    "✅ **Use batch normalization or layer normalization** to stabilize training.  \n",
    "✅ **Gradient clipping** – Limits the gradient values to prevent them from shrinking too much.  \n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **2. Exploding Gradient Problem**\n",
    "### ❌ **What is it?**\n",
    "- The opposite of the vanishing gradient problem!  \n",
    "- When gradients grow **too large**, they cause unstable updates, making the model diverge instead of learning.\n",
    "\n",
    "### 📈 **Why does this happen?**\n",
    "- If weights are large or initialized poorly, gradients can **explode exponentially** during backpropagation.\n",
    "- This results in sudden, erratic updates, making the network **unstable**.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Gradient Clipping** – Set a threshold so that gradients don’t grow beyond a certain limit.  \n",
    "✅ **Use smaller learning rates** to prevent large weight updates.  \n",
    "✅ **Use careful weight initialization techniques** like Xavier or He initialization.  \n",
    "\n",
    "\n",
    "\n",
    "## ⏳ **3. Short-Term Memory Issue**\n",
    "### ❌ **What is it?**\n",
    "- Standard RNNs struggle to remember information **from many time steps ago**.  \n",
    "- If a dependency spans **20+ time steps**, the network simply **forgets** it.\n",
    "\n",
    "### 🤯 **Example:**  \n",
    "Imagine reading a long paragraph and trying to remember a name mentioned at the beginning. **By the time you reach the end, you’ve forgotten it!** That’s what happens to RNNs.  \n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ Use **LSTMs or GRUs** – These architectures store **long-term information** better than standard RNNs.  \n",
    "✅ Use **Attention Mechanisms** – They help focus on **important parts** of the input sequence.  \n",
    "\n",
    "\n",
    "\n",
    "## 🐢 **4. Slow Training and High Computation Costs**\n",
    "### ❌ **What is it?**\n",
    "- RNNs **process inputs sequentially**, meaning **no parallelization** like CNNs.  \n",
    "- This makes them **slower** and **more computationally expensive** compared to feedforward networks.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use parallel architectures like Transformers** (they don’t process inputs sequentially).  \n",
    "✅ **Use GPU acceleration** for faster matrix computations.  \n",
    "✅ **Reduce sequence length** if possible, or use **truncated BPTT** to limit time steps during training.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎭 **5. Difficulty in Capturing Long-Term Dependencies**\n",
    "### ❌ **What is it?**\n",
    "- RNNs **focus more on recent inputs** and often fail to link **old words/events** in a sequence.  \n",
    "- Example: If a document introduces a character **50 sentences ago**, a simple RNN won’t remember them!\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use LSTMs/GRUs** – These have memory cells that **store relevant past information**.  \n",
    "✅ **Use Attention Mechanisms** – They help the model **attend** to specific parts of the input.  \n",
    "\n",
    "\n",
    "\n",
    "## 💡 **6. Bias Towards Recent Inputs**\n",
    "### ❌ **What is it?**\n",
    "- RNNs have a **recency bias**, meaning they **prioritize recent inputs** over older ones.  \n",
    "- Example: If a chatbot sees **\"not good\"** at the beginning of a sentence but **\"great\"** at the end, it may only remember **\"great\"**.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use Bidirectional RNNs** – They read input **both forward and backward**.  \n",
    "✅ **Use Transformers** – They process the entire sequence at once.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔄 **7. Handling Variable-Length Sequences is Hard**\n",
    "### ❌ **What is it?**\n",
    "- RNNs struggle with **very long** or **very short** sequences.  \n",
    "- Padding/truncating sequences can sometimes **distort the meaning**.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use Dynamic RNNs** – These handle variable-length sequences without padding issues.  \n",
    "✅ **Use Attention Mechanisms** – They allow the model to focus on **important** sequence parts.  \n",
    "\n",
    "\n",
    "\n",
    "## ⚠️ **8. Poor Performance on Very Long Sequences**\n",
    "### ❌ **What is it?**\n",
    "- If sequences have **thousands of time steps**, RNNs perform **poorly**.  \n",
    "- This is why **speech recognition and machine translation** models often struggle with RNNs.\n",
    "\n",
    "### 🛠 **How to fix it?**\n",
    "✅ **Use Transformers** (like BERT and GPT) – These work **better for long-range dependencies**.  \n",
    "✅ **Use Hierarchical RNNs** – Process data at multiple levels for better representation.  \n",
    "\n",
    "# 🎯 **Summary of RNN Problems & Fixes**\n",
    "| 🛑 **Problem**                  | 🔥 **Solution** |\n",
    "|---------------------------------|----------------|\n",
    "| **Vanishing Gradient**   | LSTMs, GRUs, ReLU, Gradient Clipping |\n",
    "| **Exploding Gradient**   | Gradient Clipping, Smaller Learning Rate |\n",
    "| **Short-Term Memory**    | LSTMs, GRUs, Attention |\n",
    "| **Slow Training**        | Transformers, GPUs, Parallelization |\n",
    "| **Long-Term Dependencies** | LSTMs, GRUs, Attention |\n",
    "| **Recency Bias**         | Bidirectional RNNs, Transformers |\n",
    "| **Variable-Length Issues** | Dynamic RNNs, Attention |\n",
    "| **Poor Performance on Long Sequences** | Transformers, Hierarchical Models |\n",
    "\n",
    "\n",
    "## 🤖 **The Future: Moving Beyond RNNs**\n",
    "Because of these problems, newer architectures like **LSTMs, GRUs, and Transformers** (GPT, BERT) have replaced vanilla RNNs in most real-world applications! 🚀  \n",
    "\n",
    "Would you like a practical **example** of solving these issues using **LSTMs or Transformers** in Python? 🤔\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM) Explained in a Colorful Way 🎨✨**\n",
    "\n",
    "Imagine your brain as a **notebook** where you write important things you need to remember. But here’s the catch—your memory is not perfect! Sometimes, you **forget unimportant details** and **retain only the essential ones**. This is exactly how an **LSTM (Long Short-Term Memory)** network works in deep learning!  \n",
    "\n",
    "\n",
    "### **🌟 What is LSTM?**\n",
    "LSTM is a special type of **Recurrent Neural Network (RNN)** designed to **remember important information** over long periods and **forget unnecessary details**. Unlike a normal RNN that struggles with long-term dependencies (because it keeps forgetting things), LSTM has a **smart memory mechanism** to selectively store and erase information.  \n",
    "\n",
    "\n",
    "### **🧠 LSTM’s Secret Superpowers: Gates! 🚪**\n",
    "LSTM has three magical \"gates\" that decide what to **keep, update, and forget** in the memory:  \n",
    "\n",
    "1️⃣ **Forget Gate 🔥**  \n",
    "   - This gate decides what old information should be thrown away.  \n",
    "   - Example: \"Do I really need to remember what I ate for breakfast three days ago? Nope! Forget it!\"  \n",
    "\n",
    "2️⃣ **Input Gate 📥**  \n",
    "   - This gate decides what new information should be added to memory.  \n",
    "   - Example: \"Ah! I just learned a new word today! Let’s save it in memory.\"  \n",
    "\n",
    "3️⃣ **Output Gate 📤**  \n",
    "   - This gate determines what should be **sent as output** to the next time step.  \n",
    "   - Example: \"I need to recall my friend’s birthday today, so let’s retrieve it from memory!\"  \n",
    "\n",
    "\n",
    "### **🎨 Visualizing the LSTM Process**\n",
    "1️⃣ **Incoming data arrives** at the LSTM cell.  \n",
    "2️⃣ The **Forget Gate** decides what past info should be erased.  \n",
    "3️⃣ The **Input Gate** updates memory with useful new info.  \n",
    "4️⃣ The **Output Gate** selects what needs to be passed forward.  \n",
    "\n",
    "The **Cell State** is like a conveyor belt 🎢 that keeps flowing, carrying essential information through time while discarding what’s unnecessary.  \n",
    "\n",
    "\n",
    "### **🚀 Where is LSTM Used?**\n",
    "LSTMs are widely used in:  \n",
    "🔹 **Speech Recognition** (e.g., Siri, Google Assistant)  \n",
    "🔹 **Chatbots** (handling long conversations)  \n",
    "🔹 **Stock Price Prediction** (analyzing past trends)  \n",
    "🔹 **Language Translation** (remembering previous words for better sentences)  \n",
    "🔹 **Music Generation** (creating melodies that make sense over time)  \n",
    "\n",
    "\n",
    "### **🔑 Key Takeaways**\n",
    "✔️ LSTM is an advanced type of RNN that **remembers** important things for long durations.  \n",
    "✔️ It uses **Forget, Input, and Output Gates** to manage memory efficiently.  \n",
    "✔️ Used in applications where remembering past information is **crucial** (speech, text, stock trends, etc.).  \n",
    "\n",
    "Now, if LSTMs were people, they’d be **the best note-takers in the world!** 📝✨  \n",
    "Want to dive deeper? Let’s discuss! 🚀\n",
    "\n",
    "![](images/lstm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📌 Long Short-Term Memory (LSTM) Architecture Explained in Detail 🚀**  \n",
    "\n",
    "LSTM is a type of **Recurrent Neural Network (RNN)** designed to handle **long-term dependencies** in sequential data. Unlike vanilla RNNs, which struggle with the **vanishing gradient problem**, LSTMs have a **memory cell** that selectively stores and forgets information over long sequences.  \n",
    "\n",
    "Let’s break down the **LSTM architecture** in an easy-to-understand and colorful way! 🎨✨  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ LSTM Architecture: The Building Blocks 🏗️**  \n",
    "Each LSTM unit (or **cell**) consists of:  \n",
    "✅ **Cell State** ($ C_t $) – The \"memory\" that carries long-term information.  \n",
    "✅ **Hidden State** ($ h_t $) – The output of the current LSTM cell, passed to the next step.  \n",
    "✅ **Three Gates** (Forget, Input, and Output) – Control what gets updated, remembered, or forgotten.  \n",
    "\n",
    "At each time step $ t $, an LSTM cell processes:  \n",
    "🔹 The current input $ x_t $  \n",
    "🔹 The previous hidden state $ h_{t-1} $  \n",
    "🔹 The previous cell state $ C_{t-1} $  \n",
    "\n",
    "Now, let’s go deep into **each component**! 🔍  \n",
    "\n",
    "\n",
    "\n",
    "### **🚪 1. Forget Gate $ f_t $ – Decides What to Erase! 🔥**  \n",
    "The **Forget Gate** decides which parts of the previous cell state $ C_{t-1} $ should be discarded.  \n",
    "👉 It uses a **sigmoid activation function** ($ \\sigma $) to produce values between **0 and 1** (0 = forget completely, 1 = keep fully).  \n",
    "\n",
    "🔢 **Formula:**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_f $ and $ b_f $ are the weight matrix and bias for the forget gate.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ x_t $ is the current input.  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- If $ f_t $ is **close to 0**, forget the information.  \n",
    "- If $ f_t $ is **close to 1**, retain the information.  \n",
    "\n",
    "\n",
    "\n",
    "### **📥 2. Input Gate $ i_t $ – Decides What to Store! 📝**  \n",
    "The **Input Gate** determines what new information should be added to the memory cell.  \n",
    "👉 It consists of:  \n",
    "✅ A **sigmoid layer** to decide which values to update.  \n",
    "✅ A **tanh layer** to create a candidate memory update $ \\tilde{C}_t $.  \n",
    "\n",
    "🔢 **Formulas:**  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$  \n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- $ i_t $ controls **how much** of $ \\tilde{C}_t $ should be stored in memory.  \n",
    "- $ \\tilde{C}_t $ contains the potential **new information**.  \n",
    "\n",
    "\n",
    "\n",
    "### **🔄 3. Update Cell State $ C_t $ – The Actual Memory! 🧠**  \n",
    "After **forgetting some old info** and **adding new info**, we update the **cell state**:  \n",
    "\n",
    "🔢 **Formula:**  \n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- The **old memory $ C_{t-1} $** is reduced based on $ f_t $.  \n",
    "- The **new memory $ \\tilde{C}_t $** is added based on $ i_t $.  \n",
    "\n",
    "\n",
    "\n",
    "### **📤 4. Output Gate $ o_t $ – Decides the Final Output! 📊**  \n",
    "The **Output Gate** determines what the **hidden state** $ h_t $ (the output of the LSTM cell) should be.  \n",
    "\n",
    "🔢 **Formulas:**  \n",
    "$$\n",
    "o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$  \n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- $ o_t $ acts as a filter, deciding **which parts of $ C_t $** should be output.  \n",
    "- The **hidden state $ h_t $** is used in the next LSTM step and can also be passed to other layers (like dense layers for classification).  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Putting It All Together: LSTM Workflow 🔄**\n",
    "At each time step $ t $, an LSTM cell follows these steps:  \n",
    "1️⃣ **Forget** old information ($ f_t $).  \n",
    "2️⃣ **Decide what new information to store** ($ i_t $, $ \\tilde{C}_t $).  \n",
    "3️⃣ **Update the memory cell** ($ C_t $).  \n",
    "4️⃣ **Compute the final output** ($ h_t $) using the Output Gate.  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Where is LSTM Used?**\n",
    "LSTM is widely used in:  \n",
    "🔹 **Speech Recognition** 🎙️ (e.g., Siri, Google Assistant)  \n",
    "🔹 **Text Generation** 📝 (e.g., ChatGPT, poetry generation)  \n",
    "🔹 **Time-Series Forecasting** 📈 (e.g., stock prices, weather prediction)  \n",
    "🔹 **Machine Translation** 🌍 (e.g., Google Translate)  \n",
    "🔹 **Music Generation** 🎵 (e.g., AI composing music)  \n",
    "\n",
    "\n",
    "\n",
    "## **🔑 Key Takeaways**\n",
    "✔️ LSTM has a **memory cell** that retains important information over time.  \n",
    "✔️ It uses **Forget, Input, and Output Gates** to control information flow.  \n",
    "✔️ Unlike RNNs, LSTM can handle **long-term dependencies** efficiently.  \n",
    "✔️ Used in various applications like **NLP, speech processing, and forecasting**.  \n",
    "\n",
    "\n",
    "\n",
    "### **🎨 Visual Summary**\n",
    "Imagine LSTM as a **smart secretary** 🧑‍💼 managing a **to-do list**:  \n",
    "✅ **Forget Gate** removes unnecessary tasks.  \n",
    "✅ **Input Gate** adds new important tasks.  \n",
    "✅ **Cell State** is the notebook holding all tasks.  \n",
    "✅ **Output Gate** decides what tasks should be shared.  \n",
    "\n",
    "LSTMs are **powerful tools** in deep learning, allowing AI to learn patterns in time-dependent data effectively! 🚀🔥  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📌 Forget Gate Architecture in LSTM – A Deep Dive 🔥**  \n",
    "\n",
    "The **Forget Gate** is a crucial component of Long Short-Term Memory (LSTM) networks. Its main job is to **decide which information should be discarded (forgotten) from the cell state** at each time step. This prevents the network from storing irrelevant or outdated information.  \n",
    "\n",
    "Let’s explore its architecture, mathematical equations, and how it works step by step. 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## **🔎 1. Forget Gate Overview**\n",
    "The **Forget Gate** is responsible for **removing unnecessary information** from the **Cell State** $ C_t $.  \n",
    "\n",
    "### **💡 Key Idea**  \n",
    "At every time step $ t $, the Forget Gate receives:  \n",
    "- The **previous hidden state** $ h_{t-1} $ (short-term memory)  \n",
    "- The **current input** $ x_t $ (new incoming data)  \n",
    "\n",
    "It then decides, using a **sigmoid activation function ($ \\sigma $)**, which parts of the previous cell state $ C_{t-1} $ should be **kept** and which should be **forgotten**.\n",
    "\n",
    "\n",
    "\n",
    "## **📐 2. Forget Gate Architecture 🏗️**  \n",
    "\n",
    "🔹 The Forget Gate consists of:  \n",
    "✅ **A weight matrix** $ W_f $ that helps learn which information should be forgotten.  \n",
    "✅ **A bias term** $ b_f $ that adds flexibility to the learning process.  \n",
    "✅ **A sigmoid activation function** $ \\sigma $ to produce values between **0 and 1** (0 = completely forget, 1 = completely remember).  \n",
    "\n",
    "### **🔢 Mathematical Formula**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "where:  \n",
    "- $ W_f $ is the weight matrix for the forget gate.  \n",
    "- $ [h_{t-1}, x_t] $ is the concatenation of the previous hidden state and current input.  \n",
    "- $ b_f $ is the bias term.  \n",
    "- $ \\sigma $ is the sigmoid activation function.  \n",
    "\n",
    "📌 **Sigmoid ensures that**:  \n",
    "- If $ f_t $ is **close to 0**, the information is forgotten.  \n",
    "- If $ f_t $ is **close to 1**, the information is retained.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔄 3. Step-by-Step Working of the Forget Gate**\n",
    "At **each time step $ t $**, the Forget Gate operates as follows:\n",
    "\n",
    "### **🟢 Step 1: Take Input**\n",
    "- The Forget Gate receives **two inputs**:\n",
    "  - **Previous hidden state** $ h_{t-1} $ (from the last LSTM cell).\n",
    "  - **Current input** $ x_t $ (new information).  \n",
    "\n",
    "📌 **Example:**  \n",
    "If we are processing a sentence, $ x_t $ could be a **new word**, and $ h_{t-1} $ holds the context from previous words.\n",
    "\n",
    "\n",
    "\n",
    "### **🔵 Step 2: Compute Forget Score**\n",
    "- The Forget Gate applies a **linear transformation**:  \n",
    "  $$\n",
    "  z = W_f \\cdot [h_{t-1}, x_t] + b_f\n",
    "  $$\n",
    "- Then, a **sigmoid activation function** is applied to get a value between **0 and 1**:\n",
    "  $$\n",
    "  f_t = \\sigma(z)\n",
    "  $$\n",
    "  \n",
    "📌 **Example Output:**  \n",
    "- If $ f_t = 0.1 $ → Forget most of the past information.  \n",
    "- If $ f_t = 0.9 $ → Retain most of the past information.  \n",
    "\n",
    "\n",
    "\n",
    "### **🟣 Step 3: Update Cell State**\n",
    "- The **Forget Gate output** $ f_t $ is **multiplied** with the previous **cell state** $ C_{t-1} $:  \n",
    "  $$\n",
    "  C_t = f_t * C_{t-1}\n",
    "  $$\n",
    "- This determines **how much of the old memory should be kept**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "Let’s say the previous cell state $ C_{t-1} = 5 $ and the Forget Gate outputs $ f_t = 0.2 $, then:  \n",
    "$$\n",
    "C_t = 0.2 \\times 5 = 1\n",
    "$$\n",
    "This means **most of the past information is discarded**.\n",
    "\n",
    "\n",
    "\n",
    "## **📊 4. Visualization of Forget Gate Architecture**  \n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────────┐\n",
    "    │ Inputs: h(t-1), x(t)                         │\n",
    "    │                                             │\n",
    "    │  ⬇ Concatenate inputs                      │\n",
    "    │                                             │\n",
    "    │  W_f * [h(t-1), x(t)] + b_f                 │\n",
    "    │           ⬇                                 │\n",
    "    │        Sigmoid (σ) Activation               │\n",
    "    │           ⬇                                 │\n",
    "    │        Forget Score (f_t) (0 to 1)          │\n",
    "    │           ⬇                                 │\n",
    "    │     Multiply with Previous Cell State       │\n",
    "    │           ⬇                                 │\n",
    "    │     Update Cell State (C_t)                 │\n",
    "    └─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 5. Intuition with a Real-Life Example 🧠**\n",
    "Imagine you’re **reading a book** 📖:  \n",
    "\n",
    "- You **remember** important plot details.  \n",
    "- You **forget** unnecessary descriptions that don’t contribute much to the story.  \n",
    "\n",
    "The Forget Gate works the **same way**:  \n",
    "✅ **Keeps important details** (high $ f_t $ value).  \n",
    "❌ **Discards unnecessary details** (low $ f_t $ value).  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 6. Importance of the Forget Gate**\n",
    "🔹 Prevents the network from accumulating **too much unnecessary information**.  \n",
    "🔹 Solves the **vanishing gradient problem** by **removing outdated memory**.  \n",
    "🔹 Helps LSTMs **handle long-term dependencies** efficiently.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔑 Key Takeaways**\n",
    "✔️ The **Forget Gate** determines **what past information to retain or discard**.  \n",
    "✔️ Uses **sigmoid activation ($ \\sigma $)** to produce a value between **0 and 1**.  \n",
    "✔️ Helps LSTM networks avoid **overloading memory with irrelevant information**.  \n",
    "✔️ **Plays a crucial role** in handling long-term dependencies in sequential data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📖 Manual Example of Forget Gate Calculation Using Text**  \n",
    "Let's take a simple **sentence** as input and see how the **Forget Gate** decides what to keep and what to forget step by step.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Example Sentence**\n",
    "📌 Suppose we have the sentence:  \n",
    "**\"John is a great football player. He scored a goal in the last match.\"**  \n",
    "\n",
    "We want our **LSTM model** to retain only the relevant information for predicting the next word.  \n",
    "\n",
    "- Some words are **important** (e.g., **\"John\"**, **\"football player\"**, **\"scored a goal\"**).  \n",
    "- Some words are **not very useful** (e.g., **\"is\"**, **\"a\"**, **\"in the last match\"**).  \n",
    "- The Forget Gate **decides** which parts to **keep** and which to **discard**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**\n",
    "Each word is converted into a numerical vector (simplified here as random values):\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "| in     | **[0.15, 0.1]**  |\n",
    "| the    | **[0.1, 0.05]**  |\n",
    "| last   | **[0.25, 0.2]**  |\n",
    "| match  | **[0.7, 0.6]**   |\n",
    "\n",
    "We will now apply the **Forget Gate** on these word vectors.\n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 2: Compute Forget Gate Scores**\n",
    "The Forget Gate uses the formula:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "Let's assume:  \n",
    "✅ **Weight Matrix $ W_f $**:  \n",
    "$$\n",
    "W_f =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_f $**:  \n",
    "$$\n",
    "b_f = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Applying the Forget Gate** (For each word):\n",
    "\n",
    "### Example Calculation for \"John\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{John}] + b_f\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Computing this (simplified for understanding), we get:\n",
    "\n",
    "$$\n",
    "z = [0.78, 0.55]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = [0.68, 0.63]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "✅ **\"John\" is important, so the Forget Gate gives a high score (~0.68).**  \n",
    "\n",
    "\n",
    "\n",
    "### Example Calculation for \"is\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{is}] + b_f\n",
    "$$\n",
    "\n",
    "Computing this:\n",
    "\n",
    "$$\n",
    "z = [0.32, 0.25]\n",
    "$$\n",
    "\n",
    "Applying sigmoid:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = [0.58, 0.56]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "🤔 **\"is\" is not very important, so Forget Gate gives it a lower score (~0.56).**  \n",
    "\n",
    "\n",
    "\n",
    "### **🟣 Step 3: Apply Forget Scores to Cell State**\n",
    "Now, let's apply the Forget Gate scores to the **previous cell state** $ C_{t-1} $.  \n",
    "\n",
    "Let's assume $ C_{t-1} = [0.9, 0.8] $ (previous memory).\n",
    "\n",
    "For \"John\":\n",
    "$$\n",
    "C_t = f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.68, 0.63] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504]\n",
    "$$\n",
    "\n",
    "John is retained **more strongly** in memory.\n",
    "\n",
    "For \"is\":\n",
    "$$\n",
    "C_t = [0.58, 0.56] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.522, 0.448]\n",
    "$$\n",
    "\n",
    "\"is\" is retained **less** than \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🔴 Step 4: Summary of Forget Gate Decisions**\n",
    "| Word       | Forget Gate Score $ f_t $ | Retained in Memory? |\n",
    "|------------|----------------|------------------|\n",
    "| **John**   | **0.68**   | ✅ Kept (important) |\n",
    "| **is**     | **0.56**   | ❌ Partially forgotten |\n",
    "| **a**      | **0.40**   | ❌ Mostly forgotten |\n",
    "| **great**  | **0.75**   | ✅ Kept (important) |\n",
    "| **football** | **0.80**  | ✅ Kept (important) |\n",
    "| **player** | **0.85**   | ✅ Kept (important) |\n",
    "| **He**     | **0.50**   | ❌ Partially forgotten |\n",
    "| **scored** | **0.90**   | ✅ Kept (important) |\n",
    "| **goal**   | **0.92**   | ✅ Kept (important) |\n",
    "| **last**   | **0.30**   | ❌ Mostly forgotten |\n",
    "| **match**  | **0.60**   | ❌ Partially forgotten |\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Understanding**\n",
    "After processing the entire sentence, the LSTM has **forgotten unnecessary words** like **\"is\", \"a\", \"in the last match\"**, while **retaining important words** like **\"John\", \"football player\", \"scored a goal\"**.  \n",
    "\n",
    "### 🔥 **Key Takeaways**\n",
    "✔ **Forget Gate helps the LSTM focus only on relevant information.**  \n",
    "✔ **Higher forget score → Memory is retained.**  \n",
    "✔ **Lower forget score → Memory is removed.**  \n",
    "\n",
    "This allows LSTM to process long sentences **efficiently** while avoiding information overload! 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📖 Manual Example of Input Gate Calculation Using Text**  \n",
    "Now, let’s go **step by step** to understand how the **Input Gate** in an LSTM works using a **manual example** with actual calculations.  \n",
    "\n",
    "\n",
    "\n",
    "## **🧠 What is the Input Gate in LSTM?**\n",
    "The **Input Gate** decides **what new information** should be **added to the cell state**. It controls how much of the **current input** should be stored in the memory.  \n",
    "\n",
    "Formula for the Input Gate:  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ i_t $ → Input Gate Activation (between 0 and 1, decides how much to store)\n",
    "- $ W_i $ → Weight matrix for the Input Gate\n",
    "- $ h_{t-1} $ → Previous hidden state\n",
    "- $ x_t $ → Current input\n",
    "- $ b_i $ → Bias for the Input Gate\n",
    "- $ \\sigma $ → Sigmoid activation function\n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Example Sentence**\n",
    "Let’s consider the same example:  \n",
    "📌 **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "The **goal** is to store the most relevant information in the memory while ignoring unnecessary words.\n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**\n",
    "Each word is represented as a vector:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "Now, let’s compute the **Input Gate Activation** for \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🟢 Step 2: Compute Input Gate Activation**\n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_i $**:  \n",
    "$$\n",
    "W_i =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_i $**:  \n",
    "$$\n",
    "b_i = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = W_i \\cdot [h_{t-1}, x_t] + b_i\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.89, 0.64]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = [0.71, 0.65]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- **\"John\" is relevant, so the Input Gate assigns high values (~0.71).**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 3: Compute Candidate Memory Content ($\\tilde{C_t}$)**\n",
    "The candidate content is **potential new information** to add to the memory.\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_C $**:  \n",
    "$$\n",
    "W_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_C $**:  \n",
    "$$\n",
    "b_C = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_C = W_C \\cdot [h_{t-1}, x_t] + b_C\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.12, 0.76]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation function**:\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(z_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- This means the new memory content suggests storing **\"John\"** strongly.\n",
    "\n",
    "\n",
    "\n",
    "## **🟠 Step 4: Update Cell State**\n",
    "Now, the **Input Gate** decides how much of this new information to store:\n",
    "\n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "From the **Forget Gate Calculation (previous example)**, we got:\n",
    "\n",
    "✅ **Forget Gate** $ f_t = [0.68, 0.63] $  \n",
    "✅ **Previous Cell State** $ C_{t-1} = [0.9, 0.8] $  \n",
    "✅ **Input Gate** $ i_t = [0.71, 0.65] $  \n",
    "✅ **Candidate Memory** $ \\tilde{C_t} = [0.81, 0.64] $  \n",
    "\n",
    "Now, applying the formula:\n",
    "\n",
    "$$\n",
    "C_t = [0.68, 0.63] * [0.9, 0.8] + [0.71, 0.65] * [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504] + [0.5751, 0.416]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "📌 **Final Interpretation**:\n",
    "- The **cell state has been updated**, retaining past information and adding new relevant details.  \n",
    "- **\"John\" is stored strongly, while unnecessary words are weakened.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Summary of Input Gate**\n",
    "| Word       | Input Gate Score $ i_t $ | Candidate Memory $ \\tilde{C_t} $ | Updated Memory $ C_t $ |\n",
    "|------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.71**   | **0.81**   | **1.1871** |\n",
    "| **is**     | **0.45**   | **0.30**   | **0.58** |\n",
    "| **great**  | **0.75**   | **0.88**   | **1.25** |\n",
    "| **football** | **0.80**  | **0.92**  | **1.32** |\n",
    "| **player** | **0.85**   | **0.95**  | **1.38** |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Key Takeaways**\n",
    "✔ The **Input Gate** decides **how much new information should be stored**.  \n",
    "✔ **High Input Gate Score → More important information is stored.**  \n",
    "✔ **The Forget Gate + Input Gate work together** to balance **what to keep** and **what to forget**.  \n",
    "\n",
    "This is how **LSTMs** maintain memory over long sequences! 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🧠 Understanding the Output Gate in LSTM with Manual Calculation**  \n",
    "\n",
    "Now, let's break down the **Output Gate** in an **LSTM** using **step-by-step manual calculations**, just like we did for the **Forget Gate** and **Input Gate**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 What is the Output Gate in LSTM?**  \n",
    "The **Output Gate** decides how much of the **cell state’s information** should be passed to the **next hidden state** ($ h_t $).  \n",
    "\n",
    "Formula for the **Output Gate Activation**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ o_t $ → Output Gate activation (decides how much information should be **exposed** as output)  \n",
    "- $ W_o $ → Weight matrix for the Output Gate  \n",
    "- $ h_{t-1} $ → Previous hidden state  \n",
    "- $ x_t $ → Current input  \n",
    "- $ b_o $ → Bias for the Output Gate  \n",
    "- $ \\sigma $ → Sigmoid activation function  \n",
    "\n",
    "### **Final Hidden State Calculation**:  \n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ h_t $ → New hidden state  \n",
    "- $ C_t $ → Updated Cell State (from Input and Forget Gates)  \n",
    "- $ \\tanh(C_t) $ → Squashing the cell state values between -1 and 1  \n",
    "\n",
    "\n",
    "\n",
    "## **📖 Example Sentence**\n",
    "Let’s continue with the same example:  \n",
    "📌 **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "We will calculate the **Output Gate Activation** and **Hidden State** for the word \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**  \n",
    "We use the same word vectors:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "\n",
    "\n",
    "## **🟢 Step 2: Compute Output Gate Activation $ o_t $**  \n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_o $**:  \n",
    "$$\n",
    "W_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_o $**:  \n",
    "$$\n",
    "b_o = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_o = W_o \\cdot [h_{t-1}, x_t] + b_o\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.05, 0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.67, 0.38]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma (z_o) = \\frac{1}{1 + e^{-z_o}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = [0.66, 0.59]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:  \n",
    "- **The Output Gate assigns moderate values (~0.66), meaning \"John\" should contribute moderately to the hidden state.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 3: Compute Final Hidden State $ h_t $**  \n",
    "Now, we use the **cell state** ($ C_t $) from the previous step.  \n",
    "\n",
    "✅ **Updated Cell State $ C_t $ from Input & Forget Gates**:  \n",
    "$$\n",
    "C_t = [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation**:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [\\tanh(1.1871), \\tanh(0.92)]\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "Now, calculating $ h_t $:\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = [0.66, 0.59] * [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.5478, 0.4248]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- **The new hidden state** ($ h_t $) **contains the most relevant information**.\n",
    "- **Since the Output Gate was moderately open (~0.66), it allows partial information to flow.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Summary of Output Gate**\n",
    "| Word       | Output Gate Score $ o_t $ | Cell State $ C_t $ | $ \\tanh(C_t) $ | Hidden State $ h_t $ |\n",
    "|------------|----------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.66**   | **1.1871**   | **0.83**   | **0.5478** |\n",
    "| **is**     | **0.45**   | **0.58**   | **0.52**   | **0.234** |\n",
    "| **great**  | **0.75**   | **1.25**   | **0.85**   | **0.6375** |\n",
    "| **football** | **0.80**  | **1.32**  | **0.87**  | **0.696** |\n",
    "| **player** | **0.85**   | **1.38**  | **0.89**  | **0.7565** |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Key Takeaways**\n",
    "✔ The **Output Gate** determines **how much information flows to the next step**.  \n",
    "✔ The **higher the Output Gate value**, the more information is exposed in the **hidden state**.  \n",
    "✔ **The hidden state is the final information passed to the next word in the sequence.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔗 Full LSTM Recap**\n",
    "✔ **Forget Gate** → Decides **what to forget**.  \n",
    "✔ **Input Gate** → Decides **what to store**.  \n",
    "✔ **Output Gate** → Decides **what to expose as output**.  \n",
    "\n",
    "🚀 **Together, these gates make LSTMs powerful for handling long-term dependencies in sequences!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 What is GRU?  \n",
    "Imagine you’re reading a long novel 📖, and you need to remember key points from previous chapters to understand the current one. That’s exactly what GRUs do in **sequence-based deep learning tasks**—they **remember important information** and **forget unimportant details**, making them ideal for tasks like speech recognition 🎤, machine translation 🌎, and time series forecasting 📈.  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)**, but it's an **improved version** that solves the problem of *vanishing gradients* (which makes traditional RNNs forget long-term dependencies). It’s also a **lighter** alternative to LSTMs (Long Short-Term Memory) while maintaining **high accuracy**.\n",
    "\n",
    "\n",
    "\n",
    "## 🏗️ GRU Architecture: The Magic Inside ✨  \n",
    "\n",
    "A **GRU cell** has **two main gates** that control the flow of information:  \n",
    "\n",
    "### 🔵 **1. Update Gate (Zt) – \"Should I Remember?\"**  \n",
    "- Think of this as your **memory filter**. 🧠 It decides **how much of the past information to keep** and **how much of the new information to add**.  \n",
    "- If **Zt is close to 1**, the old memory stays. If it’s **close to 0**, it gets replaced with fresh new data.  \n",
    "\n",
    "### 🔴 **2. Reset Gate (Rt) – \"Should I Forget?\"**  \n",
    "- This gate determines how much of the **past information to erase**. 🚮  \n",
    "- If Rt is **0**, the old memory is completely reset (like starting a fresh page 📄). If Rt is **1**, it keeps the entire past context.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 How GRU Works (Step-by-Step)  \n",
    "\n",
    "Let’s say you’re watching a TV series 🎬, and GRU is helping you remember the **important plot points** while forgetting unnecessary side details.  \n",
    "\n",
    "1️⃣ **Reset Gate (Rt) acts first**: It decides how much of the previous memory is relevant for the current moment.  \n",
    "2️⃣ **New candidate memory is created**: It mixes the past with the present input to generate a fresh **contextual memory**.  \n",
    "3️⃣ **Update Gate (Zt) kicks in**: It blends the old memory with the new one, deciding what to **carry forward** and what to **discard**.  \n",
    "4️⃣ **Final memory is updated**: The result is a **refined memory state** that is carried to the next time step.  \n",
    "\n",
    "### 🧠 Formula Representation:  \n",
    "#### 1️⃣ Reset Gate:  \n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "#### 2️⃣ Update Gate:  \n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "#### 3️⃣ Candidate Hidden State (New Memory Proposal):  \n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "#### 4️⃣ Final Hidden State (Final Memory for the Next Step):  \n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "- Here, **σ (sigma) is the sigmoid activation function** 🌀, which ensures the values are between 0 and 1.  \n",
    "- **tanh is used** to maintain values between -1 and 1, keeping the balance between **positive and negative information**.  \n",
    "\n",
    "## 🚀 Why GRU? (Compared to LSTM & RNN)  \n",
    "\n",
    "| Feature        | RNN 🏛️ | LSTM 🏋️ | GRU ⚡ |\n",
    "|--------------|--------|--------|------|\n",
    "| Handles Long Sequences? | ❌ No (Vanishing Gradient) | ✅ Yes | ✅ Yes |\n",
    "| Number of Gates | ❌ None | 🟢 3 (Forget, Input, Output) | 🔵 2 (Reset, Update) |\n",
    "| Training Time | ⏳ Slow | ⏳ Slower | ⚡ Faster |\n",
    "| Memory Efficiency | ✅ Low | ❌ High | ✅ Moderate |\n",
    "| Performance | 🤔 Decent | ✅ Best for Long Texts | ⚡ Fast & Effective |\n",
    "\n",
    "**Why choose GRU?**  \n",
    "- **Faster than LSTMs** because it has **fewer gates** and computations.  \n",
    "- **Better than vanilla RNNs** because it **remembers long-term dependencies**.  \n",
    "- **Great for real-time NLP applications** like **speech recognition**, **chatbots**, and **predictive text**.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 Where is GRU Used?  \n",
    "\n",
    "🔹 **Speech-to-Text** (e.g., Google Assistant, Siri) 🗣️  \n",
    "🔹 **Machine Translation** (e.g., Google Translate) 🌎  \n",
    "🔹 **Stock Price Prediction** 📊  \n",
    "🔹 **Music Generation** 🎵  \n",
    "🔹 **Chatbots & Virtual Assistants** 🤖  \n",
    "\n",
    "\n",
    "\n",
    "## 🎨 Fun Analogy: GRU as a Smart Diary 📓  \n",
    "\n",
    "Imagine you’re keeping a **daily journal**.  \n",
    "- **Reset Gate (Rt)**: Decides **whether to remove old notes** or keep them.  \n",
    "- **Update Gate (Zt)**: Decides **if a new event should overwrite an old one**.  \n",
    "- **Final Memory (ht)**: The polished diary entry that **carries forward** into the next day!  \n",
    "\n",
    "That’s how GRU **efficiently maintains and updates memory** while keeping only the **important parts**! 🎯\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 Summary  \n",
    "\n",
    "🎯 **GRU is a powerful, lightweight RNN variant** that efficiently processes sequential data.  \n",
    "⚡ **It has two gates (Reset & Update) instead of three like LSTM**, making it faster and simpler.  \n",
    "🧠 **It solves the vanishing gradient problem**, making it ideal for handling **long-term dependencies**.  \n",
    "🚀 **Used in NLP, speech recognition, finance, and more!**  \n",
    "\n",
    "Hope that made GRU fun and colorful for you! 🎨✨ Let me know if you need a deeper dive into any part! 🚀💡\n",
    "\n",
    "![](images/gru.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let’s break down the **full architecture of a GRU (Gated Recurrent Unit)** in detail. We'll explore:  \n",
    "\n",
    "✅ **High-Level Overview**  \n",
    "✅ **Step-by-Step Working of GRU Cell**  \n",
    "✅ **Mathematical Formulation**  \n",
    "✅ **Computation Flow**  \n",
    "✅ **Comparison with LSTM**  \n",
    "✅ **Advantages & Use Cases**  \n",
    "\n",
    "Let’s dive in! 🚀🎯  \n",
    "\n",
    "\n",
    "\n",
    "# **🌟 High-Level Overview of GRU**  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)** designed to handle sequential data (e.g., time series, speech, language).  \n",
    "\n",
    "🔹 **Why GRU?**  \n",
    "- Standard RNNs suffer from the **vanishing gradient problem**, making it hard to learn **long-term dependencies**.  \n",
    "- GRUs, like LSTMs, use **gates to control information flow** but are computationally more efficient.  \n",
    "- They have **fewer parameters** than LSTMs, making them **faster to train** while retaining strong performance.  \n",
    "\n",
    "### **🔧 GRU Components:**  \n",
    "A **GRU cell** consists of:  \n",
    "1️⃣ **Update Gate ($Z_t$)** → Decides **how much past information to keep**.  \n",
    "2️⃣ **Reset Gate ($R_t$)** → Decides **how much past information to forget**.  \n",
    "3️⃣ **Candidate Hidden State ($\\tilde{h}_t$)** → A new potential memory update.  \n",
    "4️⃣ **Final Hidden State ($h_t$)** → The actual memory that carries forward.  \n",
    "\n",
    "\n",
    "\n",
    "# **🏗️ GRU Architecture (Step-by-Step)**\n",
    "The **GRU cell** takes two inputs at time step $ t $:  \n",
    "🔹 **$ x_t $ (Current input)** – This is the new data point (word, feature, etc.).  \n",
    "🔹 **$ h_{t-1} $ (Previous hidden state)** – This stores past information.  \n",
    "\n",
    "### **🔵 Step 1: Compute the Reset Gate $ R_t $**\n",
    "- The **reset gate** decides whether to erase part of the past memory.  \n",
    "- Uses a **sigmoid activation** ($ \\sigma $) to squash values between 0 and 1.  \n",
    "\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "👉 If $ R_t $ is **0**, it forgets the past.  \n",
    "👉 If $ R_t $ is **1**, it keeps the full past memory.  \n",
    "\n",
    "### **🔴 Step 2: Compute the Update Gate $ Z_t $**\n",
    "- The **update gate** decides how much of the **past hidden state** to retain versus **how much to update**.  \n",
    "- Also uses **sigmoid activation** to control memory update.  \n",
    "\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "👉 If $ Z_t $ is **0**, it replaces the old memory entirely.  \n",
    "👉 If $ Z_t $ is **1**, it keeps the old memory.  \n",
    "\n",
    "### **🟢 Step 3: Compute the Candidate Hidden State $ \\tilde{h}_t $**\n",
    "- A **new candidate memory** is computed using the reset gate.  \n",
    "- Uses **tanh activation** to balance positive/negative values.  \n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "👉 If **reset gate is 0**, it ignores past information.  \n",
    "👉 If **reset gate is 1**, it uses both past and current input.  \n",
    "\n",
    "### **🟠 Step 4: Compute the Final Hidden State $ h_t $**\n",
    "- The final output is a **blend of the old memory ($ h_{t-1} $) and new candidate memory ($ \\tilde{h}_t $)** controlled by the update gate.  \n",
    "\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "👉 If $ Z_t $ is **0**, it fully updates with new memory.  \n",
    "👉 If $ Z_t $ is **1**, it keeps old memory.  \n",
    "\n",
    "\n",
    "\n",
    "# **📊 Computation Flow in a GRU Cell**  \n",
    "\n",
    "### **🛠️ Forward Pass**  \n",
    "\n",
    "1️⃣ **Compute Reset Gate:**  \n",
    "   - $ R_t = \\sigma(W_r [h_{t-1}, x_t] + b_r) $  \n",
    "\n",
    "2️⃣ **Compute Update Gate:**  \n",
    "   - $ Z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z) $  \n",
    "\n",
    "3️⃣ **Compute Candidate Hidden State:**  \n",
    "   - $ \\tilde{h}_t = \\tanh(W_h [R_t \\ast h_{t-1}, x_t] + b_h) $  \n",
    "\n",
    "4️⃣ **Compute Final Hidden State:**  \n",
    "   - $ h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t $  \n",
    "\n",
    "### **🔄 Backpropagation (Training GRU)**\n",
    "GRUs are trained using **Backpropagation Through Time (BPTT)**, where:  \n",
    "- **Gradients of loss are computed** using **chain rule**.  \n",
    "- **Weights are updated** using **gradient descent**.  \n",
    "- **Gates regulate gradient flow**, preventing vanishing gradients.  \n",
    "\n",
    "# **🔬 GRU vs. LSTM: Key Differences**\n",
    "| Feature | GRU ⚡ | LSTM 🏋️ |\n",
    "|---------|------|------|\n",
    "| Number of Gates | 2 (Update, Reset) | 3 (Input, Forget, Output) |\n",
    "| Complexity | ✅ Less | ❌ More |\n",
    "| Performance | ⚡ Fast | 🏆 Better for long texts |\n",
    "| Memory Requirement | ✅ Less | ❌ More |\n",
    "| Suitable for | Speech, NLP, real-time apps | Long documents, text generation |\n",
    "\n",
    "\n",
    "\n",
    "# **🔥 Advantages of GRU**\n",
    "✅ **Faster Training** – Fewer gates than LSTM = Faster updates.  \n",
    "✅ **Solves Vanishing Gradient Problem** – Retains long-term dependencies.  \n",
    "✅ **Computationally Efficient** – Great for real-time applications.  \n",
    "✅ **Performs Well on Small Datasets** – Fewer parameters make it ideal for small-scale problems.  \n",
    "\n",
    "\n",
    "\n",
    "# **🚀 Where is GRU Used?**\n",
    "📌 **Speech Recognition** (Google Assistant, Alexa) 🗣️  \n",
    "📌 **Machine Translation** (Google Translate) 🌍  \n",
    "📌 **Stock Market Prediction** 📈  \n",
    "📌 **Chatbots & AI Assistants** 🤖  \n",
    "📌 **Music Generation** 🎵  \n",
    "\n",
    "\n",
    "\n",
    "# **🎯 Summary**\n",
    "✔ **GRU is a simplified LSTM** with **fewer gates** and **faster computations**.  \n",
    "✔ **It solves vanishing gradient issues** and **remembers long-term dependencies**.  \n",
    "✔ **Uses Reset & Update Gates** to control memory updates.  \n",
    "✔ **Faster than LSTM** but still **performs well in sequence-based tasks**.  \n",
    "✔ **Ideal for speech, NLP, real-time applications**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let’s manually walk through the GRU computations using a simple example. This will give you a **step-by-step breakdown of how a GRU cell processes a sentence**, calculating each gate and hidden state update.  \n",
    "\n",
    "\n",
    "\n",
    "### **📝 Example Sentence:**  \n",
    "👉 **\"AI is amazing\"**  \n",
    "We will process it word by word using a GRU with a **hidden size of 2** (to keep calculations manageable).  \n",
    "\n",
    "## **🔧 Step 1: Define Inputs & Initial Parameters**\n",
    "### **Word Encoding (Input Vectors)**\n",
    "We assume each word is converted into a 3-dimensional vector (using Word Embeddings). Let’s define:  \n",
    "\n",
    "| Word | Input Vector (\\( x_t \\)) |\n",
    "|-------|----------------|\n",
    "| **AI** | \\([0.5, 0.1, 0.4]\\) |\n",
    "| **is** | \\([0.2, 0.7, 0.3]\\) |\n",
    "| **amazing** | \\([0.6, 0.9, 0.5]\\) |\n",
    "\n",
    "### **Initial Hidden State \\( h_0 \\)**\n",
    "Since it's the first step, we initialize:  \n",
    "$$\n",
    "h_0 = [0, 0] \\quad \\text{(2-dimensional hidden state)}\n",
    "$$\n",
    "\n",
    "\n",
    "## **🛠️ Step 2: Define GRU Parameters**\n",
    "We need **weight matrices** and **biases** for reset and update gates. We assume:  \n",
    "\n",
    "**Reset Gate (\\( R_t \\)):**  \n",
    "$$\n",
    "W_r =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad U_r =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix},\n",
    "\\quad b_r = [0.1, 0.2]\n",
    "$$\n",
    "\n",
    "**Update Gate (\\( Z_t \\)):**  \n",
    "$$\n",
    "W_z =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.7 \\\\\n",
    "0.5 & 0.2 & 0.6\n",
    "\\end{bmatrix},\n",
    "\\quad U_z =\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.5 \\\\\n",
    "0.3 & 0.8\n",
    "\\end{bmatrix},\n",
    "\\quad b_z = [0.05, 0.1]\n",
    "$$\n",
    "\n",
    "**Candidate Hidden State (\\( \\tilde{h}_t \\)):**  \n",
    "$$\n",
    "W_h =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix},\n",
    "\\quad U_h =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix},\n",
    "\\quad b_h = [0.2, 0.3]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **⚡ Step 3: Compute for First Word (\"AI\")**  \n",
    "### **🔴 Reset Gate \\( R_1 \\)**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\cdot 0.5) + (0.5 \\cdot 0.1) + (0.1 \\cdot 0.4) + 0.1 \\\\\n",
    "(0.3 \\cdot 0.5) + (0.7 \\cdot 0.1) + (0.2 \\cdot 0.4) + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.1 + 0.05 + 0.04 + 0.1 \\\\\n",
    "0.15 + 0.07 + 0.08 + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.29 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Applying **sigmoid** (\\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)):  \n",
    "\n",
    "$$\n",
    "R_1 =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(0.29) \\\\\n",
    "\\sigma(0.5)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.572 \\\\\n",
    "0.622\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🟡 Update Gate \\( Z_1 \\)**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "\n",
    "Using similar calculations, we get:  \n",
    "\n",
    "$$\n",
    "Z_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🟢 Candidate Hidden State \\( \\tilde{h}_1 \\)**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot (R_1 \\ast h_0) + U_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "\n",
    "Since \\( h_0 = 0 \\), the term \\( R_1 \\ast h_0 \\) vanishes, and we compute:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.29 + 0.2 \\\\\n",
    "0.49 + 0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "=\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.49 \\\\\n",
    "0.79\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Approximating \\( \\tanh(x) \\), we get:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔵 Final Hidden State \\( h_1 \\)**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "(1 - 0.655) \\\\\n",
    "(1 - 0.710)\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "(0.345) \\times 0.45 \\\\\n",
    "(0.290) \\times 0.66\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.155 \\\\\n",
    "0.191\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Repeat for \"is\" and \"amazing\"**\n",
    "Now, \\( h_1 \\) is used for the next step, and the process repeats.\n",
    "\n",
    "This shows **how a GRU cell updates memory word-by-word!** 🚀 Let me know if you want more manual calculations or insights! 🎯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate how a **GRU (Gated Recurrent Unit)** processes a sentence. We'll analyze how it **keeps important information** and **forgets unimportant details** using an actual example.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Example Sentence:**\n",
    "Let's take a simple sentence:\n",
    "> **\"I love deep learning.\"**  \n",
    "\n",
    "We'll process it **word by word** through a GRU and observe how it decides what to keep and what to forget.\n",
    "\n",
    "## **🔹 Step 1: Define Initial Setup**\n",
    "Each word is represented as a **word vector** $ x_t $. Assume we have:  \n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|------|---------------------|\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "We also assume that the **hidden state** $ h_t $ has two units, so it’s a 2D vector.  \n",
    "\n",
    "\n",
    "The **GRU parameters** (randomly chosen for simplicity):  \n",
    "\n",
    "- **Update Gate Weights** $ W_z, U_z $  \n",
    "- **Reset Gate Weights** $ W_r, U_r $  \n",
    "- **Candidate State Weights** $ W_h, U_h $  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 2: How GRU Decides What to Keep or Forget?**  \n",
    "GRU works with **three key equations** at every time step $ t $:  \n",
    "\n",
    "### **1️⃣ Reset Gate $ R_t $** (Decides whether to erase past memory)\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot x_t + U_r \\cdot h_{t-1} + b_r)\n",
    "$$\n",
    "- If $ R_t $ is **close to 0**, it forgets old information.\n",
    "- If $ R_t $ is **close to 1**, it keeps old memory.  \n",
    "\n",
    "### **2️⃣ Update Gate $ Z_t $** (Decides whether to update hidden state)\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot x_t + U_z \\cdot h_{t-1} + b_z)\n",
    "$$\n",
    "- If $ Z_t $ is **close to 0**, it **replaces** the old state with new info.  \n",
    "- If $ Z_t $ is **close to 1**, it **keeps** the old memory.  \n",
    "\n",
    "### **3️⃣ Candidate Hidden State $ \\tilde{h}_t $**\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot x_t + U_h \\cdot (R_t \\ast h_{t-1}) + b_h)\n",
    "$$\n",
    "This is the new hidden state, considering **reset gate influence**.  \n",
    "\n",
    "### **4️⃣ Final Hidden State**\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$\n",
    "The final hidden state is a combination of **past and new** information.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 3: Manual Calculation for Each Word**\n",
    "Let’s assume:\n",
    "\n",
    "- $ h_0 = [0, 0] $ (initial hidden state)  \n",
    "- We calculate for each word step by step.\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"I\"**  \n",
    "#### **1️⃣ Reset Gate Calculation**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "Since $ h_0 = [0, 0] $, this simplifies to:\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot [0.5, 0.1, 0.3] + b_r)\n",
    "$$\n",
    "Let’s say:\n",
    "$$\n",
    "R_1 = [0.8, 0.6]\n",
    "$$\n",
    "Since values are **close to 1**, we **keep past memory**.\n",
    "\n",
    "#### **2️⃣ Update Gate Calculation**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "Again, since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + b_z)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "Z_1 = [0.9, 0.7]\n",
    "$$\n",
    "Since $ Z_1 $ is **close to 1**, GRU **keeps most of the old hidden state** (which is zero for now).\n",
    "\n",
    "#### **3️⃣ Compute Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + U_h \\cdot (R_1 \\ast h_0) + b_h)\n",
    "$$\n",
    "Since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "\\tilde{h}_1 = [0.3, 0.4]\n",
    "$$\n",
    "\n",
    "#### **4️⃣ Compute Final Hidden State**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "$$\n",
    "= [0.9, 0.7] \\ast [0, 0] + [0.1, 0.3] \\ast [0.3, 0.4]\n",
    "$$\n",
    "$$\n",
    "= [0.03, 0.12]\n",
    "$$\n",
    "🚀 **Hidden state at time step 1**: $ h_1 = [0.03, 0.12] $\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"love\"**  \n",
    "Now, we use $ h_1 = [0.03, 0.12] $.\n",
    "\n",
    "#### **1️⃣ Reset Gate**\n",
    "$$\n",
    "R_2 = \\sigma(W_r \\cdot x_2 + U_r \\cdot h_1 + b_r)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "R_2 = [0.4, 0.2]\n",
    "$$\n",
    "Since $ R_2 $ is **low**, it **forgets some past memory**.\n",
    "\n",
    "#### **2️⃣ Update Gate**\n",
    "$$\n",
    "Z_2 = \\sigma(W_z \\cdot x_2 + U_z \\cdot h_1 + b_z)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "Z_2 = [0.2, 0.6]\n",
    "$$\n",
    "Since $ Z_2 $ is **low for the first unit**, it **updates memory**.\n",
    "\n",
    "#### **3️⃣ Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_2 = \\tanh(W_h \\cdot x_2 + U_h \\cdot (R_2 \\ast h_1) + b_h)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "\\tilde{h}_2 = [0.6, 0.5]\n",
    "$$\n",
    "\n",
    "#### **4️⃣ Final Hidden State**\n",
    "$$\n",
    "h_2 = Z_2 \\ast h_1 + (1 - Z_2) \\ast \\tilde{h}_2\n",
    "$$\n",
    "$$\n",
    "= [0.2, 0.6] \\ast [0.03, 0.12] + [0.8, 0.4] \\ast [0.6, 0.5]\n",
    "$$\n",
    "$$\n",
    "= [0.006, 0.072] + [0.48, 0.2]\n",
    "$$\n",
    "$$\n",
    "= [0.486, 0.272]\n",
    "$$\n",
    "\n",
    "🚀 **Hidden state at time step 2**: $ h_2 = [0.486, 0.272] $  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Conclusion**\n",
    "- **\"I\"** → Small memory update, since it’s a common word.  \n",
    "- **\"love\"** → Memory updates more because it’s a strong emotional word.  \n",
    "- **GRU selectively keeps or forgets** based on context.  \n",
    "\n",
    "Would you like me to compute for \"deep\" and \"learning\" too? 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 **Deep RNNs (Deep Recurrent Neural Networks) – A Full Explanation** 🔥\n",
    "\n",
    "## **📌 What is a Deep RNN?**\n",
    "A **Deep RNN** is a **stacked** version of a normal Recurrent Neural Network (RNN). Unlike a simple RNN that has only **one layer** of recurrent neurons, a **Deep RNN** stacks multiple RNN layers **on top of each other**. This allows it to **learn more complex patterns** in sequential data like **text, speech, and time-series data**.\n",
    "\n",
    "## **🛠️ How is a Deep RNN Different from a Simple RNN?**\n",
    "| Feature | Simple RNN | Deep RNN |\n",
    "|---------|-----------|----------|\n",
    "| **Number of Layers** | 1 recurrent layer | Multiple recurrent layers |\n",
    "| **Learning Capability** | Limited feature extraction | Captures deeper, hierarchical features |\n",
    "| **Performance** | Struggles with long-term dependencies | Better at long-term dependencies |\n",
    "| **Training Difficulty** | Easier | Harder (but more powerful) |\n",
    "| **Application** | Basic time-series & text prediction | Complex NLP, speech recognition |\n",
    "\n",
    "\n",
    "\n",
    "## **🧠 Architecture of a Deep RNN**\n",
    "A Deep RNN consists of **multiple RNN layers stacked on top of each other**, where:\n",
    "\n",
    "- **Each layer passes its hidden state** $ h_t^l $ **to the next layer**.\n",
    "- The **first layer** processes the input sequence.\n",
    "- The **last layer** produces the final output.\n",
    "\n",
    "### **🔹 Standard RNN vs. Deep RNN**\n",
    "📌 **Simple RNN (Shallow)**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "📌 **Deep RNN (Stacked)**\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)  \\quad \\text{(First RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2) \\quad \\text{(Second RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "h_t^L = \\tanh(W_x^L h_t^{L-1} + W_h^L h_{t-1}^L + b^L) \\quad \\text{(Final RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "y_t = W_y h_t^L + b_y\n",
    "$$\n",
    "\n",
    "🚀 **Each layer refines the representation of the sequence!**\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Why Use a Deep RNN?**\n",
    "🔹 **Captures Higher-Level Features** → Lower layers learn **basic** features, higher layers learn **abstract** features.  \n",
    "🔹 **Handles Complex Dependencies** → Works better for long sequences.  \n",
    "🔹 **More Expressive Power** → Learns deeper relationships in data.\n",
    "\n",
    "\n",
    "\n",
    "## **📝 Example: Manual Computation for a Deep RNN**\n",
    "Let’s take a simple sequence:\n",
    "\n",
    "> **\"I love deep learning.\"**\n",
    "\n",
    "We'll process it using **2 RNN layers**.\n",
    "\n",
    "### **🔹 Step 1: Input Representation**\n",
    "Each word is represented as a **vector**:\n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|||\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "### **🔹 Step 2: Process Each Word Through Layer 1**\n",
    "Each word goes through the first RNN layer:\n",
    "\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "$$\n",
    "h_1^1 = [0.2, 0.3]\n",
    "$$\n",
    "$$\n",
    "h_2^1 = [0.4, 0.5]\n",
    "$$\n",
    "$$\n",
    "h_3^1 = [0.1, 0.8]\n",
    "$$\n",
    "$$\n",
    "h_4^1 = [0.6, 0.4]\n",
    "$$\n",
    "\n",
    "### **🔹 Step 3: Pass to Layer 2**\n",
    "Now, these hidden states are **fed into the second RNN layer**:\n",
    "\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "$$\n",
    "h_1^2 = [0.3, 0.6]\n",
    "$$\n",
    "$$\n",
    "h_2^2 = [0.5, 0.7]\n",
    "$$\n",
    "$$\n",
    "h_3^2 = [0.2, 0.9]\n",
    "$$\n",
    "$$\n",
    "h_4^2 = [0.7, 0.5]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Variants of Deep RNN**\n",
    "Deep RNNs are often implemented using **better recurrent cells** like:\n",
    "\n",
    "### **1️⃣ Deep LSTM (Stacked LSTM)**\n",
    "LSTM (Long Short-Term Memory) uses **gates** to better store long-term dependencies.\n",
    "\n",
    "### **2️⃣ Deep GRU (Stacked GRU)**\n",
    "GRU (Gated Recurrent Unit) simplifies LSTM while keeping good performance.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Where are Deep RNNs Used?**\n",
    "✅ **Speech Recognition** (e.g., Google Assistant, Siri)  \n",
    "✅ **Text Generation** (e.g., Chatbots)  \n",
    "✅ **Machine Translation** (e.g., Google Translate)  \n",
    "✅ **Stock Price Prediction**  \n",
    "✅ **Music Generation**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔎 Final Summary**\n",
    "| Concept | Explanation |\n",
    "|||\n",
    "| **Deep RNN** | Multiple RNN layers stacked together |\n",
    "| **Why Deep?** | Captures complex patterns better |\n",
    "| **How it Works?** | Each layer refines the representation |\n",
    "| **Better Variants** | Stacked LSTM, Stacked GRU |\n",
    "\n",
    "🔥 **Deep RNNs power many AI applications today!** Would you like me to implement a Deep RNN example in Python? 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 **Bidirectional Recurrent Neural Networks (BiRNN) - A Full and Colorful Guide!** 🚀  \n",
    "\n",
    "### **1️⃣ What is a Bidirectional RNN?**  \n",
    "Imagine you're watching a movie 🎬, but instead of seeing the whole scene, you only see frames one by one in a forward sequence. You might **miss important context** from future events. Wouldn’t it be amazing if you could **see both past and future** at the same time? 🤯  \n",
    "\n",
    "That’s exactly what **Bidirectional Recurrent Neural Networks (BiRNNs)** do! Instead of processing sequences in just one direction (like a regular RNN), **BiRNNs process them in both forward and backward directions** at the same time. 🔄 This makes them super powerful for **context-heavy** tasks like speech recognition 🎤, text processing 📖, and language translation 🌍.  \n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ How Does a BiRNN Work? 🛠️**  \n",
    "A BiRNN consists of **two RNNs running in parallel:**  \n",
    "\n",
    "1. **Forward RNN**: Reads the sequence from left to right ➡️  \n",
    "2. **Backward RNN**: Reads the sequence from right to left ⬅️  \n",
    "\n",
    "At each time step **t**, both RNNs process the input and produce two hidden states:  \n",
    "- One from the forward RNN: **$ h_t^{(fwd)} $**  \n",
    "- One from the backward RNN: **$ h_t^{(bwd)} $**  \n",
    "\n",
    "The final output at each time step is a combination (concatenation or sum) of these two hidden states:  \n",
    "$$\n",
    "h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "$$  \n",
    "\n",
    "### **🎯 Key Takeaway:**  \n",
    "🔹 Unlike a regular RNN, a BiRNN can use **both past and future information** at any given time step. This makes it way better for **understanding full context** in sequential data.  \n",
    "\n",
    "\n",
    "\n",
    "### **3️⃣ Why is BiRNN Better? 🤔**  \n",
    "\n",
    "✅ **More Context = More Accuracy**  \n",
    "   - A normal RNN only considers past words when predicting the next word, which can lead to **misinterpretations**.  \n",
    "   - BiRNNs can **consider both past and future words**, leading to **better predictions**! 🎯  \n",
    "\n",
    "✅ **Great for Speech & NLP Tasks**  \n",
    "   - **Speech Recognition**: The meaning of a word can change based on future words. A BiRNN helps capture that nuance! 🎙️  \n",
    "   - **Machine Translation**: Words in different languages may have different orders. Understanding the full sentence structure helps a lot! 🌍  \n",
    "   - **Named Entity Recognition (NER)**: Knowing the full sentence helps distinguish between similar words used in different contexts.  \n",
    "\n",
    "✅ **Works with LSTMs & GRUs**  \n",
    "   - BiRNNs can use **LSTM (Long Short-Term Memory) cells** or **GRUs (Gated Recurrent Units)** to handle long sequences better. 🧠  \n",
    "\n",
    "\n",
    "\n",
    "### **4️⃣ BiRNN in Action - Example with Python 🐍**  \n",
    "\n",
    "Let’s see how a **Bidirectional LSTM** can be implemented in TensorFlow/Keras:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define a BiLSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(100, 10)),  # BiLSTM Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "🔹 Here, the **Bidirectional()** wrapper makes the LSTM layer process input in both directions! 🔄  \n",
    "\n",
    "### **5️⃣ When to Use a BiRNN? 🤷**  \n",
    "\n",
    "| ✅ Use BiRNN When | ❌ Avoid BiRNN When |  \n",
    "|------------------|------------------|  \n",
    "| You need **full context** from past & future 🔄 | Your dataset is too large, as BiRNNs require **double computation** 💾 |  \n",
    "| Tasks involve **NLP**, **speech recognition**, or **translation** 🗣️📖 | You're working with **real-time applications** where only past info is available ⏳ |  \n",
    "| You need better performance on **long sequences** 🧠 | The problem is **too simple**, and a unidirectional RNN is enough ⚡ |  \n",
    "\n",
    "\n",
    "### **🌟 Conclusion - Why BiRNN is a Game-Changer? 🎮**  \n",
    "\n",
    "🚀 BiRNNs are like **time travelers** in the world of neural networks. Instead of just relying on the past, they **peek into the future** and learn from both sides! This makes them **exceptionally powerful** for tasks like:  \n",
    "\n",
    "✔️ Speech Recognition 🎤  \n",
    "✔️ Text Summarization 📄  \n",
    "✔️ Sentiment Analysis 😊😡  \n",
    "✔️ Named Entity Recognition (NER) 📍  \n",
    "\n",
    "But remember! BiRNNs require **more computation** and are not always the best choice for real-time applications. **Choose wisely!** 🧐  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔥 Full Architecture of a Bidirectional Recurrent Neural Network (BiRNN) 🔥**  \n",
    "\n",
    "A **Bidirectional Recurrent Neural Network (BiRNN)** is an advanced type of **Recurrent Neural Network (RNN)** that processes sequences in **both forward and backward directions** to capture **past and future context**.  \n",
    "\n",
    "Let’s dive **deep into the architecture** step by step! 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 1. Basic Components of a BiRNN**  \n",
    "\n",
    "A **standard RNN** has the following components:  \n",
    "- **Input layer (X)**: The sequence of data (e.g., words in a sentence, frames in speech).  \n",
    "- **Hidden layer (h)**: Stores information from previous time steps.  \n",
    "- **Output layer (Y)**: Produces predictions at each time step.  \n",
    "\n",
    "A **Bidirectional RNN** consists of **two separate RNNs**:  \n",
    "- **Forward RNN** → Processes input from **left to right** (past to future).  \n",
    "- **Backward RNN** → Processes input from **right to left** (future to past).  \n",
    "\n",
    "At each time step $ t $, both RNNs produce hidden states, which are combined to form the final output.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 2. Step-by-Step Working of a BiRNN**  \n",
    "\n",
    "### **Step 1: Input Representation**  \n",
    "Let’s assume we have a sequence of length $ T $, where each input vector is $ X_t $ (a feature vector at time step $ t $).  \n",
    "\n",
    "$$\n",
    "X = [X_1, X_2, X_3, ..., X_T]\n",
    "$$\n",
    "\n",
    "Each input passes through **two RNNs**:  \n",
    "1. **Forward RNN** → Generates hidden states from past to future.  \n",
    "2. **Backward RNN** → Generates hidden states from future to past.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Forward and Backward Hidden States Computation**  \n",
    "\n",
    "- **Forward Hidden State ($ h_t^{(fwd)} $)**  \n",
    "  The forward RNN computes the hidden state at each time step using:  \n",
    "  $$\n",
    "  h_t^{(fwd)} = f(W_f X_t + U_f h_{t-1}^{(fwd)} + b_f)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_f $ = Input weight matrix for forward RNN  \n",
    "  - $ U_f $ = Hidden weight matrix for forward RNN  \n",
    "  - $ b_f $ = Bias  \n",
    "  - $ f $ = Activation function (usually tanh or ReLU)  \n",
    "\n",
    "- **Backward Hidden State ($ h_t^{(bwd)} $)**  \n",
    "  The backward RNN computes the hidden state moving from **$ T $ to $ 1 $**:  \n",
    "  $$\n",
    "  h_t^{(bwd)} = f(W_b X_t + U_b h_{t+1}^{(bwd)} + b_b)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_b $ = Input weight matrix for backward RNN  \n",
    "  - $ U_b $ = Hidden weight matrix for backward RNN  \n",
    "  - $ b_b $ = Bias  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Combining Forward and Backward States**  \n",
    "\n",
    "At each time step $ t $, the two hidden states **($ h_t^{(fwd)} $ and $ h_t^{(bwd)} $)** are combined into a single hidden state $ h_t $. This can be done in different ways:  \n",
    "- **Concatenation** (most common):  \n",
    "  $$\n",
    "  h_t = [h_t^{(fwd)}; h_t^{(bwd)}]\n",
    "  $$\n",
    "- **Sum**:  \n",
    "  $$\n",
    "  h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Output Layer**  \n",
    "\n",
    "The final output $ Y_t $ at each time step is computed as:  \n",
    "$$\n",
    "Y_t = g(W_o h_t + b_o)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_o $ = Output weight matrix  \n",
    "- $ b_o $ = Bias  \n",
    "- $ g $ = Activation function (e.g., softmax for classification)  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 3. Full Architecture Diagram of BiRNN**  \n",
    "\n",
    "```\n",
    "      Input Sequence: [ X1,  X2,  X3,  X4,  X5]\n",
    "                        ↓    ↓    ↓    ↓    ↓    \n",
    "      Forward RNN:   → h1 → h2 → h3 → h4 → h5 →  \n",
    "                         ↓    ↓    ↓    ↓    ↓    \n",
    "      Backward RNN:  ← h1 ← h2 ← h3 ← h4 ← h5 ←  \n",
    "                        ↓    ↓    ↓    ↓    ↓    \n",
    "      Final Output:  [ Y1,  Y2,  Y3,  Y4,  Y5]\n",
    "```\n",
    "\n",
    "- The **forward hidden states** move **left to right**.  \n",
    "- The **backward hidden states** move **right to left**.  \n",
    "- The **final hidden state at each time step** is a combination of both.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 4. Advantages of BiRNN 🚀**  \n",
    "\n",
    "✅ **Uses full context** (both past & future).  \n",
    "✅ **Improves accuracy** in NLP, speech recognition, and time series tasks.  \n",
    "✅ **Works well with LSTM & GRU for long-term dependencies.**  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 5. Implementing BiRNN in Python (TensorFlow/Keras) 🐍**  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define a Bidirectional RNN Model\n",
    "model = Sequential([\n",
    "    Bidirectional(SimpleRNN(64, return_sequences=True), input_shape=(100, 10)),  # BiRNN Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## **📌 6. When to Use BiRNN vs. Unidirectional RNN?**  \n",
    "\n",
    "| Feature  | Unidirectional RNN  | Bidirectional RNN  |\n",
    "|----------|--------------------|--------------------|\n",
    "| **Direction** | Forward only ➡️  | Forward + Backward 🔄 |\n",
    "| **Context** | Only past context 📜 | Both past & future context 🏆 |\n",
    "| **Computational Cost** | Lower 💰 | Higher ⚡ |\n",
    "| **Use Case** | Real-time tasks (e.g., online chatbots) 💬 | NLP, speech, translation 🌍 |\n",
    "\n",
    "\n",
    "## **🔥 Conclusion: Why BiRNN is a Game-Changer?**  \n",
    "\n",
    "🚀 **Bidirectional RNNs are like superheroes** in sequential tasks! Unlike normal RNNs that only see the past, BiRNNs **see both past and future at the same time**, making them extremely powerful for **speech recognition**, **text processing**, **machine translation**, and more! 💡  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's take a simple sentence and manually work through how a **Bidirectional Recurrent Neural Network (BiRNN)** processes it. This will involve:  \n",
    "\n",
    "1️⃣ **Choosing a sentence**  \n",
    "2️⃣ **Assigning word embeddings**  \n",
    "3️⃣ **Forward pass calculations**  \n",
    "4️⃣ **Backward pass calculations**  \n",
    "5️⃣ **Combining hidden states**  \n",
    "6️⃣ **Generating output**  \n",
    "\n",
    "## **📌 Sentence: \"I love AI\"**\n",
    "We’ll assume this is a 3-word sequence:  \n",
    "\n",
    "$$\n",
    "X = [\"I\", \"love\", \"AI\"]\n",
    "$$\n",
    "\n",
    "Each word will be represented as a **3D embedding vector** (to keep it simple).  \n",
    "\n",
    "| Word  | Embedding (3D Vector) |\n",
    "|--------|----------------|\n",
    "| \"I\"      | [0.1, 0.3, 0.5] |\n",
    "| \"love\"   | [0.2, 0.6, 0.8] |\n",
    "| \"AI\"     | [0.3, 0.7, 0.9] |\n",
    "\n",
    "\n",
    "## **🛠 Step 1: Initialize Parameters**\n",
    "BiRNN consists of **two RNNs**, one running **forward** and one **backward**. Each has:  \n",
    "\n",
    "- **Weight Matrices (Input → Hidden State)**\n",
    "  - $ W_f $ (Forward)\n",
    "  - $ W_b $ (Backward)  \n",
    "\n",
    "- **Weight Matrices (Hidden State → Next Hidden State)**\n",
    "  - $ U_f $ (Forward)\n",
    "  - $ U_b $ (Backward)  \n",
    "\n",
    "- **Bias Vectors**\n",
    "  - $ b_f $ (Forward)\n",
    "  - $ b_b $ (Backward)  \n",
    "\n",
    "For simplicity, let’s assume:  \n",
    "\n",
    "$$\n",
    "W_f = W_b =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_f = U_b =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_f = b_b =\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 2: Forward Pass (Processing left to right)**  \n",
    "\n",
    "### **🔹 Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh(W_f X_1 + U_f h_0 + b_f)\n",
    "$$\n",
    "\n",
    "Since initial **hidden state** is **0**,  \n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.3 \\\\ 0.5\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "(0.5 \\times 0.1) + (0.3 \\times 0.3) + (0.2 \\times 0.5) \\\\ \n",
    "(0.4 \\times 0.1) + (0.7 \\times 0.3) + (0.6 \\times 0.5)\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.05 + 0.09 + 0.1 \\\\ \n",
    "0.04 + 0.21 + 0.3\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.34 \\\\ 0.75\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Approximating **tanh function**:  \n",
    "$$\n",
    "\\tanh(0.34) \\approx 0.327, \\quad \\tanh(0.75) \\approx 0.635\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh(W_f X_2 + U_f h_1^{(fwd)} + b_f)\n",
    "$$\n",
    "\n",
    "Using **previous hidden state**:\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\ 0.6 \\\\ 0.8\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "(Similarly, calculating matrix multiplications and applying **tanh**, we get:)\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\begin{bmatrix} 0.765 \\\\ 0.851 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Time Step 3: \"AI\"**\n",
    "Following the same process:\n",
    "\n",
    "$$\n",
    "h_3^{(fwd)} = \\begin{bmatrix} 0.88 \\\\ 0.92 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 3: Backward Pass (Processing right to left)**\n",
    "We now process in **reverse order**:\n",
    "\n",
    "### **🔹 Time Step 3: \"AI\"**\n",
    "$$\n",
    "h_3^{(bwd)} = \\tanh(W_b X_3 + U_b h_0 + b_b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(bwd)} = \\begin{bmatrix} 0.805 \\\\ 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **🔹 Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(bwd)} = \\begin{bmatrix} 0.742 \\\\ 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **🔹 Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(bwd)} = \\begin{bmatrix} 0.658 \\\\ 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 4: Combining Forward & Backward States**\n",
    "For each word, we concatenate both hidden states:\n",
    "\n",
    "$$\n",
    "h_1 = [h_1^{(fwd)}; h_1^{(bwd)}] = \\begin{bmatrix} 0.327 & 0.635 & 0.658 & 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = [h_2^{(fwd)}; h_2^{(bwd)}] = \\begin{bmatrix} 0.765 & 0.851 & 0.742 & 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = [h_3^{(fwd)}; h_3^{(bwd)}] = \\begin{bmatrix} 0.88 & 0.92 & 0.805 & 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🔮 Step 5: Output Layer**\n",
    "If this is for **classification**, we would pass the final **concatenated hidden states** through a softmax layer.\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Conclusion**\n",
    "- BiRNN processes **both past & future context**.\n",
    "- Each word has **two hidden states** (forward + backward).\n",
    "- The **final hidden state** is a combination of **both directions**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🔍 What Do These Calculations Signify?**  \n",
    "\n",
    "The calculations we performed help us **understand how Bi-directional RNN (BiRNN) processes text step by step**. Let’s break it down into **key insights**:\n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ BiRNN Captures Both Past & Future Context**  \n",
    "Unlike a normal **unidirectional RNN**, which processes the sequence **left to right** (or right to left), BiRNN does **both simultaneously**.  \n",
    "\n",
    "- **Forward RNN:** Moves from **left to right** (normal reading order).  \n",
    "- **Backward RNN:** Moves from **right to left** (reverse reading order).  \n",
    "- The **final hidden state** for each word is a **combination of both directions**, giving the model **fuller context**.  \n",
    "\n",
    "**Example:**\n",
    "For the word `\"love\"` in `\"I love AI\"`,  \n",
    "- The **forward RNN** only sees `\"I love ...\"`,  \n",
    "- The **backward RNN** sees `\"... love AI\"`.  \n",
    "\n",
    "So, `\"love\"` gets influenced by **both \"I\" (past) and \"AI\" (future)**, giving it **richer meaning**.\n",
    "\n",
    "\n",
    "\n",
    "## **2️⃣ Word Meaning Depends on Full Context**  \n",
    "Consider this sentence:\n",
    "\n",
    "> **\"He plays the bass.\"**  \n",
    "> **\"He caught a bass.\"**\n",
    "\n",
    "The word **\"bass\"** has **two meanings** (musical instrument vs. fish).  \n",
    "\n",
    "- A **unidirectional RNN** (left-to-right) would process `\"He caught a ...\"` before seeing `\"bass\"`, which is **not enough to disambiguate** the meaning.  \n",
    "- A **BiRNN** processes both `\"caught a\"` and the words **after** `\"bass\"` at the same time, giving it more information to determine the meaning.\n",
    "\n",
    "**This is crucial for NLP tasks like Named Entity Recognition, Sentiment Analysis, and Speech Recognition!** 🎯\n",
    "\n",
    "\n",
    "\n",
    "## **3️⃣ Why Do We Combine Forward & Backward States?**  \n",
    "At each time step, we computed **two hidden states**:\n",
    "- $ h_t^{(fwd)} $ → Capturing the meaning from the **left context**  \n",
    "- $ h_t^{(bwd)} $ → Capturing the meaning from the **right context**  \n",
    "- **Final representation** → **Concatenation** of both  \n",
    "\n",
    "**Example:**  \n",
    "For **\"love\"** in `\"I love AI\"`, we got:\n",
    "\n",
    "$$\n",
    "h_2 = [0.765, 0.851, 0.742, 0.831]\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- $ 0.765, 0.851 $ capture **past information** (from \"I\")  \n",
    "- $ 0.742, 0.831 $ capture **future information** (from \"AI\")  \n",
    "\n",
    "Thus, `\"love\"` is **better understood** with the full sentence in mind. 💡  \n",
    "\n",
    "\n",
    "\n",
    "## **4️⃣ BiRNN Is More Powerful Than Simple RNN**\n",
    "Regular RNNs have a **vanishing gradient problem**, making them struggle to capture **long-range dependencies**.  \n",
    "\n",
    "- **BiRNN helps solve this** because it gets **two different perspectives**, making it **better at learning complex relationships** between words.  \n",
    "- This is why BiRNN is often used in **speech recognition, machine translation, and question-answering systems**.\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Summary: What Our Calculations Showed**\n",
    "✅ **BiRNN processes both past and future** at the same time.  \n",
    "✅ **Each word's meaning is enhanced by its surrounding words**.  \n",
    "✅ **Final representation is a fusion of two different contexts**, making the model more powerful than a standard RNN.  \n",
    "✅ **Works great for NLP tasks like sentiment analysis, speech recognition, and machine translation.**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
