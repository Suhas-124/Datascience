{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM) Explained in a Colorful Way 🎨✨**\n",
    "\n",
    "Imagine your brain as a **notebook** where you write important things you need to remember. But here’s the catch—your memory is not perfect! Sometimes, you **forget unimportant details** and **retain only the essential ones**. This is exactly how an **LSTM (Long Short-Term Memory)** network works in deep learning!  \n",
    "\n",
    "\n",
    "### **🌟 What is LSTM?**\n",
    "LSTM is a special type of **Recurrent Neural Network (RNN)** designed to **remember important information** over long periods and **forget unnecessary details**. Unlike a normal RNN that struggles with long-term dependencies (because it keeps forgetting things), LSTM has a **smart memory mechanism** to selectively store and erase information.  \n",
    "\n",
    "\n",
    "### **🧠 LSTM’s Secret Superpowers: Gates! 🚪**\n",
    "LSTM has three magical \"gates\" that decide what to **keep, update, and forget** in the memory:  \n",
    "\n",
    "1️⃣ **Forget Gate 🔥**  \n",
    "   - This gate decides what old information should be thrown away.  \n",
    "   - Example: \"Do I really need to remember what I ate for breakfast three days ago? Nope! Forget it!\"  \n",
    "\n",
    "2️⃣ **Input Gate 📥**  \n",
    "   - This gate decides what new information should be added to memory.  \n",
    "   - Example: \"Ah! I just learned a new word today! Let’s save it in memory.\"  \n",
    "\n",
    "3️⃣ **Output Gate 📤**  \n",
    "   - This gate determines what should be **sent as output** to the next time step.  \n",
    "   - Example: \"I need to recall my friend’s birthday today, so let’s retrieve it from memory!\"  \n",
    "\n",
    "\n",
    "### **🎨 Visualizing the LSTM Process**\n",
    "1️⃣ **Incoming data arrives** at the LSTM cell.  \n",
    "2️⃣ The **Forget Gate** decides what past info should be erased.  \n",
    "3️⃣ The **Input Gate** updates memory with useful new info.  \n",
    "4️⃣ The **Output Gate** selects what needs to be passed forward.  \n",
    "\n",
    "The **Cell State** is like a conveyor belt 🎢 that keeps flowing, carrying essential information through time while discarding what’s unnecessary.  \n",
    "\n",
    "\n",
    "### **🚀 Where is LSTM Used?**\n",
    "LSTMs are widely used in:  \n",
    "🔹 **Speech Recognition** (e.g., Siri, Google Assistant)  \n",
    "🔹 **Chatbots** (handling long conversations)  \n",
    "🔹 **Stock Price Prediction** (analyzing past trends)  \n",
    "🔹 **Language Translation** (remembering previous words for better sentences)  \n",
    "🔹 **Music Generation** (creating melodies that make sense over time)  \n",
    "\n",
    "\n",
    "### **🔑 Key Takeaways**\n",
    "✔️ LSTM is an advanced type of RNN that **remembers** important things for long durations.  \n",
    "✔️ It uses **Forget, Input, and Output Gates** to manage memory efficiently.  \n",
    "✔️ Used in applications where remembering past information is **crucial** (speech, text, stock trends, etc.).  \n",
    "\n",
    "Now, if LSTMs were people, they’d be **the best note-takers in the world!** 📝✨  \n",
    "Want to dive deeper? Let’s discuss! 🚀\n",
    "\n",
    "![](lstm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📌 Long Short-Term Memory (LSTM) Architecture Explained in Detail 🚀**  \n",
    "\n",
    "LSTM is a type of **Recurrent Neural Network (RNN)** designed to handle **long-term dependencies** in sequential data. Unlike vanilla RNNs, which struggle with the **vanishing gradient problem**, LSTMs have a **memory cell** that selectively stores and forgets information over long sequences.  \n",
    "\n",
    "Let’s break down the **LSTM architecture** in an easy-to-understand and colorful way! 🎨✨  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ LSTM Architecture: The Building Blocks 🏗️**  \n",
    "Each LSTM unit (or **cell**) consists of:  \n",
    "✅ **Cell State** ($ C_t $) – The \"memory\" that carries long-term information.  \n",
    "✅ **Hidden State** ($ h_t $) – The output of the current LSTM cell, passed to the next step.  \n",
    "✅ **Three Gates** (Forget, Input, and Output) – Control what gets updated, remembered, or forgotten.  \n",
    "\n",
    "At each time step $ t $, an LSTM cell processes:  \n",
    "🔹 The current input $ x_t $  \n",
    "🔹 The previous hidden state $ h_{t-1} $  \n",
    "🔹 The previous cell state $ C_{t-1} $  \n",
    "\n",
    "Now, let’s go deep into **each component**! 🔍  \n",
    "\n",
    "\n",
    "\n",
    "### **🚪 1. Forget Gate $ f_t $ – Decides What to Erase! 🔥**  \n",
    "The **Forget Gate** decides which parts of the previous cell state $ C_{t-1} $ should be discarded.  \n",
    "👉 It uses a **sigmoid activation function** ($ \\sigma $) to produce values between **0 and 1** (0 = forget completely, 1 = keep fully).  \n",
    "\n",
    "🔢 **Formula:**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_f $ and $ b_f $ are the weight matrix and bias for the forget gate.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ x_t $ is the current input.  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- If $ f_t $ is **close to 0**, forget the information.  \n",
    "- If $ f_t $ is **close to 1**, retain the information.  \n",
    "\n",
    "\n",
    "\n",
    "### **📥 2. Input Gate $ i_t $ – Decides What to Store! 📝**  \n",
    "The **Input Gate** determines what new information should be added to the memory cell.  \n",
    "👉 It consists of:  \n",
    "✅ A **sigmoid layer** to decide which values to update.  \n",
    "✅ A **tanh layer** to create a candidate memory update $ \\tilde{C}_t $.  \n",
    "\n",
    "🔢 **Formulas:**  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$  \n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- $ i_t $ controls **how much** of $ \\tilde{C}_t $ should be stored in memory.  \n",
    "- $ \\tilde{C}_t $ contains the potential **new information**.  \n",
    "\n",
    "\n",
    "\n",
    "### **🔄 3. Update Cell State $ C_t $ – The Actual Memory! 🧠**  \n",
    "After **forgetting some old info** and **adding new info**, we update the **cell state**:  \n",
    "\n",
    "🔢 **Formula:**  \n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- The **old memory $ C_{t-1} $** is reduced based on $ f_t $.  \n",
    "- The **new memory $ \\tilde{C}_t $** is added based on $ i_t $.  \n",
    "\n",
    "\n",
    "\n",
    "### **📤 4. Output Gate $ o_t $ – Decides the Final Output! 📊**  \n",
    "The **Output Gate** determines what the **hidden state** $ h_t $ (the output of the LSTM cell) should be.  \n",
    "\n",
    "🔢 **Formulas:**  \n",
    "$$\n",
    "o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$  \n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$  \n",
    "\n",
    "📌 **Intuition:**  \n",
    "- $ o_t $ acts as a filter, deciding **which parts of $ C_t $** should be output.  \n",
    "- The **hidden state $ h_t $** is used in the next LSTM step and can also be passed to other layers (like dense layers for classification).  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Putting It All Together: LSTM Workflow 🔄**\n",
    "At each time step $ t $, an LSTM cell follows these steps:  \n",
    "1️⃣ **Forget** old information ($ f_t $).  \n",
    "2️⃣ **Decide what new information to store** ($ i_t $, $ \\tilde{C}_t $).  \n",
    "3️⃣ **Update the memory cell** ($ C_t $).  \n",
    "4️⃣ **Compute the final output** ($ h_t $) using the Output Gate.  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Where is LSTM Used?**\n",
    "LSTM is widely used in:  \n",
    "🔹 **Speech Recognition** 🎙️ (e.g., Siri, Google Assistant)  \n",
    "🔹 **Text Generation** 📝 (e.g., ChatGPT, poetry generation)  \n",
    "🔹 **Time-Series Forecasting** 📈 (e.g., stock prices, weather prediction)  \n",
    "🔹 **Machine Translation** 🌍 (e.g., Google Translate)  \n",
    "🔹 **Music Generation** 🎵 (e.g., AI composing music)  \n",
    "\n",
    "\n",
    "\n",
    "## **🔑 Key Takeaways**\n",
    "✔️ LSTM has a **memory cell** that retains important information over time.  \n",
    "✔️ It uses **Forget, Input, and Output Gates** to control information flow.  \n",
    "✔️ Unlike RNNs, LSTM can handle **long-term dependencies** efficiently.  \n",
    "✔️ Used in various applications like **NLP, speech processing, and forecasting**.  \n",
    "\n",
    "\n",
    "\n",
    "### **🎨 Visual Summary**\n",
    "Imagine LSTM as a **smart secretary** 🧑‍💼 managing a **to-do list**:  \n",
    "✅ **Forget Gate** removes unnecessary tasks.  \n",
    "✅ **Input Gate** adds new important tasks.  \n",
    "✅ **Cell State** is the notebook holding all tasks.  \n",
    "✅ **Output Gate** decides what tasks should be shared.  \n",
    "\n",
    "LSTMs are **powerful tools** in deep learning, allowing AI to learn patterns in time-dependent data effectively! 🚀🔥  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📌 Forget Gate Architecture in LSTM – A Deep Dive 🔥**  \n",
    "\n",
    "The **Forget Gate** is a crucial component of Long Short-Term Memory (LSTM) networks. Its main job is to **decide which information should be discarded (forgotten) from the cell state** at each time step. This prevents the network from storing irrelevant or outdated information.  \n",
    "\n",
    "Let’s explore its architecture, mathematical equations, and how it works step by step. 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## **🔎 1. Forget Gate Overview**\n",
    "The **Forget Gate** is responsible for **removing unnecessary information** from the **Cell State** $ C_t $.  \n",
    "\n",
    "### **💡 Key Idea**  \n",
    "At every time step $ t $, the Forget Gate receives:  \n",
    "- The **previous hidden state** $ h_{t-1} $ (short-term memory)  \n",
    "- The **current input** $ x_t $ (new incoming data)  \n",
    "\n",
    "It then decides, using a **sigmoid activation function ($ \\sigma $)**, which parts of the previous cell state $ C_{t-1} $ should be **kept** and which should be **forgotten**.\n",
    "\n",
    "\n",
    "\n",
    "## **📐 2. Forget Gate Architecture 🏗️**  \n",
    "\n",
    "🔹 The Forget Gate consists of:  \n",
    "✅ **A weight matrix** $ W_f $ that helps learn which information should be forgotten.  \n",
    "✅ **A bias term** $ b_f $ that adds flexibility to the learning process.  \n",
    "✅ **A sigmoid activation function** $ \\sigma $ to produce values between **0 and 1** (0 = completely forget, 1 = completely remember).  \n",
    "\n",
    "### **🔢 Mathematical Formula**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "where:  \n",
    "- $ W_f $ is the weight matrix for the forget gate.  \n",
    "- $ [h_{t-1}, x_t] $ is the concatenation of the previous hidden state and current input.  \n",
    "- $ b_f $ is the bias term.  \n",
    "- $ \\sigma $ is the sigmoid activation function.  \n",
    "\n",
    "📌 **Sigmoid ensures that**:  \n",
    "- If $ f_t $ is **close to 0**, the information is forgotten.  \n",
    "- If $ f_t $ is **close to 1**, the information is retained.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔄 3. Step-by-Step Working of the Forget Gate**\n",
    "At **each time step $ t $**, the Forget Gate operates as follows:\n",
    "\n",
    "### **🟢 Step 1: Take Input**\n",
    "- The Forget Gate receives **two inputs**:\n",
    "  - **Previous hidden state** $ h_{t-1} $ (from the last LSTM cell).\n",
    "  - **Current input** $ x_t $ (new information).  \n",
    "\n",
    "📌 **Example:**  \n",
    "If we are processing a sentence, $ x_t $ could be a **new word**, and $ h_{t-1} $ holds the context from previous words.\n",
    "\n",
    "\n",
    "\n",
    "### **🔵 Step 2: Compute Forget Score**\n",
    "- The Forget Gate applies a **linear transformation**:  \n",
    "  $$\n",
    "  z = W_f \\cdot [h_{t-1}, x_t] + b_f\n",
    "  $$\n",
    "- Then, a **sigmoid activation function** is applied to get a value between **0 and 1**:\n",
    "  $$\n",
    "  f_t = \\sigma(z)\n",
    "  $$\n",
    "  \n",
    "📌 **Example Output:**  \n",
    "- If $ f_t = 0.1 $ → Forget most of the past information.  \n",
    "- If $ f_t = 0.9 $ → Retain most of the past information.  \n",
    "\n",
    "\n",
    "\n",
    "### **🟣 Step 3: Update Cell State**\n",
    "- The **Forget Gate output** $ f_t $ is **multiplied** with the previous **cell state** $ C_{t-1} $:  \n",
    "  $$\n",
    "  C_t = f_t * C_{t-1}\n",
    "  $$\n",
    "- This determines **how much of the old memory should be kept**.  \n",
    "\n",
    "📌 **Example:**  \n",
    "Let’s say the previous cell state $ C_{t-1} = 5 $ and the Forget Gate outputs $ f_t = 0.2 $, then:  \n",
    "$$\n",
    "C_t = 0.2 \\times 5 = 1\n",
    "$$\n",
    "This means **most of the past information is discarded**.\n",
    "\n",
    "\n",
    "\n",
    "## **📊 4. Visualization of Forget Gate Architecture**  \n",
    "\n",
    "```\n",
    "    ┌─────────────────────────────────────────────┐\n",
    "    │ Inputs: h(t-1), x(t)                         │\n",
    "    │                                             │\n",
    "    │  ⬇ Concatenate inputs                      │\n",
    "    │                                             │\n",
    "    │  W_f * [h(t-1), x(t)] + b_f                 │\n",
    "    │           ⬇                                 │\n",
    "    │        Sigmoid (σ) Activation               │\n",
    "    │           ⬇                                 │\n",
    "    │        Forget Score (f_t) (0 to 1)          │\n",
    "    │           ⬇                                 │\n",
    "    │     Multiply with Previous Cell State       │\n",
    "    │           ⬇                                 │\n",
    "    │     Update Cell State (C_t)                 │\n",
    "    └─────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 5. Intuition with a Real-Life Example 🧠**\n",
    "Imagine you’re **reading a book** 📖:  \n",
    "\n",
    "- You **remember** important plot details.  \n",
    "- You **forget** unnecessary descriptions that don’t contribute much to the story.  \n",
    "\n",
    "The Forget Gate works the **same way**:  \n",
    "✅ **Keeps important details** (high $ f_t $ value).  \n",
    "❌ **Discards unnecessary details** (low $ f_t $ value).  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 6. Importance of the Forget Gate**\n",
    "🔹 Prevents the network from accumulating **too much unnecessary information**.  \n",
    "🔹 Solves the **vanishing gradient problem** by **removing outdated memory**.  \n",
    "🔹 Helps LSTMs **handle long-term dependencies** efficiently.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔑 Key Takeaways**\n",
    "✔️ The **Forget Gate** determines **what past information to retain or discard**.  \n",
    "✔️ Uses **sigmoid activation ($ \\sigma $)** to produce a value between **0 and 1**.  \n",
    "✔️ Helps LSTM networks avoid **overloading memory with irrelevant information**.  \n",
    "✔️ **Plays a crucial role** in handling long-term dependencies in sequential data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📖 Manual Example of Forget Gate Calculation Using Text**  \n",
    "Let's take a simple **sentence** as input and see how the **Forget Gate** decides what to keep and what to forget step by step.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Example Sentence**\n",
    "📌 Suppose we have the sentence:  \n",
    "**\"John is a great football player. He scored a goal in the last match.\"**  \n",
    "\n",
    "We want our **LSTM model** to retain only the relevant information for predicting the next word.  \n",
    "\n",
    "- Some words are **important** (e.g., **\"John\"**, **\"football player\"**, **\"scored a goal\"**).  \n",
    "- Some words are **not very useful** (e.g., **\"is\"**, **\"a\"**, **\"in the last match\"**).  \n",
    "- The Forget Gate **decides** which parts to **keep** and which to **discard**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**\n",
    "Each word is converted into a numerical vector (simplified here as random values):\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "| in     | **[0.15, 0.1]**  |\n",
    "| the    | **[0.1, 0.05]**  |\n",
    "| last   | **[0.25, 0.2]**  |\n",
    "| match  | **[0.7, 0.6]**   |\n",
    "\n",
    "We will now apply the **Forget Gate** on these word vectors.\n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 2: Compute Forget Gate Scores**\n",
    "The Forget Gate uses the formula:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "Let's assume:  \n",
    "✅ **Weight Matrix $ W_f $**:  \n",
    "$$\n",
    "W_f =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_f $**:  \n",
    "$$\n",
    "b_f = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Applying the Forget Gate** (For each word):\n",
    "\n",
    "### Example Calculation for \"John\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{John}] + b_f\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Computing this (simplified for understanding), we get:\n",
    "\n",
    "$$\n",
    "z = [0.78, 0.55]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = [0.68, 0.63]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "✅ **\"John\" is important, so the Forget Gate gives a high score (~0.68).**  \n",
    "\n",
    "\n",
    "\n",
    "### Example Calculation for \"is\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{is}] + b_f\n",
    "$$\n",
    "\n",
    "Computing this:\n",
    "\n",
    "$$\n",
    "z = [0.32, 0.25]\n",
    "$$\n",
    "\n",
    "Applying sigmoid:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = [0.58, 0.56]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "🤔 **\"is\" is not very important, so Forget Gate gives it a lower score (~0.56).**  \n",
    "\n",
    "\n",
    "\n",
    "### **🟣 Step 3: Apply Forget Scores to Cell State**\n",
    "Now, let's apply the Forget Gate scores to the **previous cell state** $ C_{t-1} $.  \n",
    "\n",
    "Let's assume $ C_{t-1} = [0.9, 0.8] $ (previous memory).\n",
    "\n",
    "For \"John\":\n",
    "$$\n",
    "C_t = f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.68, 0.63] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504]\n",
    "$$\n",
    "\n",
    "John is retained **more strongly** in memory.\n",
    "\n",
    "For \"is\":\n",
    "$$\n",
    "C_t = [0.58, 0.56] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.522, 0.448]\n",
    "$$\n",
    "\n",
    "\"is\" is retained **less** than \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🔴 Step 4: Summary of Forget Gate Decisions**\n",
    "| Word       | Forget Gate Score $ f_t $ | Retained in Memory? |\n",
    "|------------|----------------|------------------|\n",
    "| **John**   | **0.68**   | ✅ Kept (important) |\n",
    "| **is**     | **0.56**   | ❌ Partially forgotten |\n",
    "| **a**      | **0.40**   | ❌ Mostly forgotten |\n",
    "| **great**  | **0.75**   | ✅ Kept (important) |\n",
    "| **football** | **0.80**  | ✅ Kept (important) |\n",
    "| **player** | **0.85**   | ✅ Kept (important) |\n",
    "| **He**     | **0.50**   | ❌ Partially forgotten |\n",
    "| **scored** | **0.90**   | ✅ Kept (important) |\n",
    "| **goal**   | **0.92**   | ✅ Kept (important) |\n",
    "| **last**   | **0.30**   | ❌ Mostly forgotten |\n",
    "| **match**  | **0.60**   | ❌ Partially forgotten |\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Understanding**\n",
    "After processing the entire sentence, the LSTM has **forgotten unnecessary words** like **\"is\", \"a\", \"in the last match\"**, while **retaining important words** like **\"John\", \"football player\", \"scored a goal\"**.  \n",
    "\n",
    "### 🔥 **Key Takeaways**\n",
    "✔ **Forget Gate helps the LSTM focus only on relevant information.**  \n",
    "✔ **Higher forget score → Memory is retained.**  \n",
    "✔ **Lower forget score → Memory is removed.**  \n",
    "\n",
    "This allows LSTM to process long sentences **efficiently** while avoiding information overload! 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📖 Manual Example of Input Gate Calculation Using Text**  \n",
    "Now, let’s go **step by step** to understand how the **Input Gate** in an LSTM works using a **manual example** with actual calculations.  \n",
    "\n",
    "\n",
    "\n",
    "## **🧠 What is the Input Gate in LSTM?**\n",
    "The **Input Gate** decides **what new information** should be **added to the cell state**. It controls how much of the **current input** should be stored in the memory.  \n",
    "\n",
    "Formula for the Input Gate:  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ i_t $ → Input Gate Activation (between 0 and 1, decides how much to store)\n",
    "- $ W_i $ → Weight matrix for the Input Gate\n",
    "- $ h_{t-1} $ → Previous hidden state\n",
    "- $ x_t $ → Current input\n",
    "- $ b_i $ → Bias for the Input Gate\n",
    "- $ \\sigma $ → Sigmoid activation function\n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Example Sentence**\n",
    "Let’s consider the same example:  \n",
    "📌 **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "The **goal** is to store the most relevant information in the memory while ignoring unnecessary words.\n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**\n",
    "Each word is represented as a vector:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "Now, let’s compute the **Input Gate Activation** for \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🟢 Step 2: Compute Input Gate Activation**\n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_i $**:  \n",
    "$$\n",
    "W_i =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_i $**:  \n",
    "$$\n",
    "b_i = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = W_i \\cdot [h_{t-1}, x_t] + b_i\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.89, 0.64]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = [0.71, 0.65]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- **\"John\" is relevant, so the Input Gate assigns high values (~0.71).**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 3: Compute Candidate Memory Content ($\\tilde{C_t}$)**\n",
    "The candidate content is **potential new information** to add to the memory.\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_C $**:  \n",
    "$$\n",
    "W_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_C $**:  \n",
    "$$\n",
    "b_C = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_C = W_C \\cdot [h_{t-1}, x_t] + b_C\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.12, 0.76]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation function**:\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(z_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- This means the new memory content suggests storing **\"John\"** strongly.\n",
    "\n",
    "\n",
    "\n",
    "## **🟠 Step 4: Update Cell State**\n",
    "Now, the **Input Gate** decides how much of this new information to store:\n",
    "\n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "From the **Forget Gate Calculation (previous example)**, we got:\n",
    "\n",
    "✅ **Forget Gate** $ f_t = [0.68, 0.63] $  \n",
    "✅ **Previous Cell State** $ C_{t-1} = [0.9, 0.8] $  \n",
    "✅ **Input Gate** $ i_t = [0.71, 0.65] $  \n",
    "✅ **Candidate Memory** $ \\tilde{C_t} = [0.81, 0.64] $  \n",
    "\n",
    "Now, applying the formula:\n",
    "\n",
    "$$\n",
    "C_t = [0.68, 0.63] * [0.9, 0.8] + [0.71, 0.65] * [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504] + [0.5751, 0.416]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "📌 **Final Interpretation**:\n",
    "- The **cell state has been updated**, retaining past information and adding new relevant details.  \n",
    "- **\"John\" is stored strongly, while unnecessary words are weakened.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Summary of Input Gate**\n",
    "| Word       | Input Gate Score $ i_t $ | Candidate Memory $ \\tilde{C_t} $ | Updated Memory $ C_t $ |\n",
    "|------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.71**   | **0.81**   | **1.1871** |\n",
    "| **is**     | **0.45**   | **0.30**   | **0.58** |\n",
    "| **great**  | **0.75**   | **0.88**   | **1.25** |\n",
    "| **football** | **0.80**  | **0.92**  | **1.32** |\n",
    "| **player** | **0.85**   | **0.95**  | **1.38** |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Key Takeaways**\n",
    "✔ The **Input Gate** decides **how much new information should be stored**.  \n",
    "✔ **High Input Gate Score → More important information is stored.**  \n",
    "✔ **The Forget Gate + Input Gate work together** to balance **what to keep** and **what to forget**.  \n",
    "\n",
    "This is how **LSTMs** maintain memory over long sequences! 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🧠 Understanding the Output Gate in LSTM with Manual Calculation**  \n",
    "\n",
    "Now, let's break down the **Output Gate** in an **LSTM** using **step-by-step manual calculations**, just like we did for the **Forget Gate** and **Input Gate**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 What is the Output Gate in LSTM?**  \n",
    "The **Output Gate** decides how much of the **cell state’s information** should be passed to the **next hidden state** ($ h_t $).  \n",
    "\n",
    "Formula for the **Output Gate Activation**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ o_t $ → Output Gate activation (decides how much information should be **exposed** as output)  \n",
    "- $ W_o $ → Weight matrix for the Output Gate  \n",
    "- $ h_{t-1} $ → Previous hidden state  \n",
    "- $ x_t $ → Current input  \n",
    "- $ b_o $ → Bias for the Output Gate  \n",
    "- $ \\sigma $ → Sigmoid activation function  \n",
    "\n",
    "### **Final Hidden State Calculation**:  \n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ h_t $ → New hidden state  \n",
    "- $ C_t $ → Updated Cell State (from Input and Forget Gates)  \n",
    "- $ \\tanh(C_t) $ → Squashing the cell state values between -1 and 1  \n",
    "\n",
    "\n",
    "\n",
    "## **📖 Example Sentence**\n",
    "Let’s continue with the same example:  \n",
    "📌 **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "We will calculate the **Output Gate Activation** and **Hidden State** for the word \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **🔢 Step 1: Assign Word Vectors**  \n",
    "We use the same word vectors:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "\n",
    "\n",
    "## **🟢 Step 2: Compute Output Gate Activation $ o_t $**  \n",
    "Let’s assume:\n",
    "\n",
    "✅ **Weight Matrix $ W_o $**:  \n",
    "$$\n",
    "W_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "✅ **Bias $ b_o $**:  \n",
    "$$\n",
    "b_o = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "✅ **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "✅ **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_o = W_o \\cdot [h_{t-1}, x_t] + b_o\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.05, 0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.67, 0.38]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma (z_o) = \\frac{1}{1 + e^{-z_o}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = [0.66, 0.59]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:  \n",
    "- **The Output Gate assigns moderate values (~0.66), meaning \"John\" should contribute moderately to the hidden state.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 3: Compute Final Hidden State $ h_t $**  \n",
    "Now, we use the **cell state** ($ C_t $) from the previous step.  \n",
    "\n",
    "✅ **Updated Cell State $ C_t $ from Input & Forget Gates**:  \n",
    "$$\n",
    "C_t = [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation**:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [\\tanh(1.1871), \\tanh(0.92)]\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "Now, calculating $ h_t $:\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = [0.66, 0.59] * [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.5478, 0.4248]\n",
    "$$\n",
    "\n",
    "📌 **Interpretation**:\n",
    "- **The new hidden state** ($ h_t $) **contains the most relevant information**.\n",
    "- **Since the Output Gate was moderately open (~0.66), it allows partial information to flow.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Summary of Output Gate**\n",
    "| Word       | Output Gate Score $ o_t $ | Cell State $ C_t $ | $ \\tanh(C_t) $ | Hidden State $ h_t $ |\n",
    "|------------|----------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.66**   | **1.1871**   | **0.83**   | **0.5478** |\n",
    "| **is**     | **0.45**   | **0.58**   | **0.52**   | **0.234** |\n",
    "| **great**  | **0.75**   | **1.25**   | **0.85**   | **0.6375** |\n",
    "| **football** | **0.80**  | **1.32**  | **0.87**  | **0.696** |\n",
    "| **player** | **0.85**   | **1.38**  | **0.89**  | **0.7565** |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Key Takeaways**\n",
    "✔ The **Output Gate** determines **how much information flows to the next step**.  \n",
    "✔ The **higher the Output Gate value**, the more information is exposed in the **hidden state**.  \n",
    "✔ **The hidden state is the final information passed to the next word in the sequence.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔗 Full LSTM Recap**\n",
    "✔ **Forget Gate** → Decides **what to forget**.  \n",
    "✔ **Input Gate** → Decides **what to store**.  \n",
    "✔ **Output Gate** → Decides **what to expose as output**.  \n",
    "\n",
    "🚀 **Together, these gates make LSTMs powerful for handling long-term dependencies in sequences!**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
