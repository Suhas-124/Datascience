{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Long Short-Term Memory (LSTM) Explained in a Colorful Way ğŸ¨âœ¨**\n",
    "\n",
    "Imagine your brain as a **notebook** where you write important things you need to remember. But hereâ€™s the catchâ€”your memory is not perfect! Sometimes, you **forget unimportant details** and **retain only the essential ones**. This is exactly how an **LSTM (Long Short-Term Memory)** network works in deep learning!  \n",
    "\n",
    "\n",
    "### **ğŸŒŸ What is LSTM?**\n",
    "LSTM is a special type of **Recurrent Neural Network (RNN)** designed to **remember important information** over long periods and **forget unnecessary details**. Unlike a normal RNN that struggles with long-term dependencies (because it keeps forgetting things), LSTM has a **smart memory mechanism** to selectively store and erase information.  \n",
    "\n",
    "\n",
    "### **ğŸ§  LSTMâ€™s Secret Superpowers: Gates! ğŸšª**\n",
    "LSTM has three magical \"gates\" that decide what to **keep, update, and forget** in the memory:  \n",
    "\n",
    "1ï¸âƒ£ **Forget Gate ğŸ”¥**  \n",
    "   - This gate decides what old information should be thrown away.  \n",
    "   - Example: \"Do I really need to remember what I ate for breakfast three days ago? Nope! Forget it!\"  \n",
    "\n",
    "2ï¸âƒ£ **Input Gate ğŸ“¥**  \n",
    "   - This gate decides what new information should be added to memory.  \n",
    "   - Example: \"Ah! I just learned a new word today! Letâ€™s save it in memory.\"  \n",
    "\n",
    "3ï¸âƒ£ **Output Gate ğŸ“¤**  \n",
    "   - This gate determines what should be **sent as output** to the next time step.  \n",
    "   - Example: \"I need to recall my friendâ€™s birthday today, so letâ€™s retrieve it from memory!\"  \n",
    "\n",
    "\n",
    "### **ğŸ¨ Visualizing the LSTM Process**\n",
    "1ï¸âƒ£ **Incoming data arrives** at the LSTM cell.  \n",
    "2ï¸âƒ£ The **Forget Gate** decides what past info should be erased.  \n",
    "3ï¸âƒ£ The **Input Gate** updates memory with useful new info.  \n",
    "4ï¸âƒ£ The **Output Gate** selects what needs to be passed forward.  \n",
    "\n",
    "The **Cell State** is like a conveyor belt ğŸ¢ that keeps flowing, carrying essential information through time while discarding whatâ€™s unnecessary.  \n",
    "\n",
    "\n",
    "### **ğŸš€ Where is LSTM Used?**\n",
    "LSTMs are widely used in:  \n",
    "ğŸ”¹ **Speech Recognition** (e.g., Siri, Google Assistant)  \n",
    "ğŸ”¹ **Chatbots** (handling long conversations)  \n",
    "ğŸ”¹ **Stock Price Prediction** (analyzing past trends)  \n",
    "ğŸ”¹ **Language Translation** (remembering previous words for better sentences)  \n",
    "ğŸ”¹ **Music Generation** (creating melodies that make sense over time)  \n",
    "\n",
    "\n",
    "### **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ LSTM is an advanced type of RNN that **remembers** important things for long durations.  \n",
    "âœ”ï¸ It uses **Forget, Input, and Output Gates** to manage memory efficiently.  \n",
    "âœ”ï¸ Used in applications where remembering past information is **crucial** (speech, text, stock trends, etc.).  \n",
    "\n",
    "Now, if LSTMs were people, theyâ€™d be **the best note-takers in the world!** ğŸ“âœ¨  \n",
    "Want to dive deeper? Letâ€™s discuss! ğŸš€\n",
    "\n",
    "![](lstm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“Œ Long Short-Term Memory (LSTM) Architecture Explained in Detail ğŸš€**  \n",
    "\n",
    "LSTM is a type of **Recurrent Neural Network (RNN)** designed to handle **long-term dependencies** in sequential data. Unlike vanilla RNNs, which struggle with the **vanishing gradient problem**, LSTMs have a **memory cell** that selectively stores and forgets information over long sequences.  \n",
    "\n",
    "Letâ€™s break down the **LSTM architecture** in an easy-to-understand and colorful way! ğŸ¨âœ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ LSTM Architecture: The Building Blocks ğŸ—ï¸**  \n",
    "Each LSTM unit (or **cell**) consists of:  \n",
    "âœ… **Cell State** ($ C_t $) â€“ The \"memory\" that carries long-term information.  \n",
    "âœ… **Hidden State** ($ h_t $) â€“ The output of the current LSTM cell, passed to the next step.  \n",
    "âœ… **Three Gates** (Forget, Input, and Output) â€“ Control what gets updated, remembered, or forgotten.  \n",
    "\n",
    "At each time step $ t $, an LSTM cell processes:  \n",
    "ğŸ”¹ The current input $ x_t $  \n",
    "ğŸ”¹ The previous hidden state $ h_{t-1} $  \n",
    "ğŸ”¹ The previous cell state $ C_{t-1} $  \n",
    "\n",
    "Now, letâ€™s go deep into **each component**! ğŸ”  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸšª 1. Forget Gate $ f_t $ â€“ Decides What to Erase! ğŸ”¥**  \n",
    "The **Forget Gate** decides which parts of the previous cell state $ C_{t-1} $ should be discarded.  \n",
    "ğŸ‘‰ It uses a **sigmoid activation function** ($ \\sigma $) to produce values between **0 and 1** (0 = forget completely, 1 = keep fully).  \n",
    "\n",
    "ğŸ”¢ **Formula:**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_f $ and $ b_f $ are the weight matrix and bias for the forget gate.  \n",
    "- $ h_{t-1} $ is the previous hidden state.  \n",
    "- $ x_t $ is the current input.  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- If $ f_t $ is **close to 0**, forget the information.  \n",
    "- If $ f_t $ is **close to 1**, retain the information.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“¥ 2. Input Gate $ i_t $ â€“ Decides What to Store! ğŸ“**  \n",
    "The **Input Gate** determines what new information should be added to the memory cell.  \n",
    "ğŸ‘‰ It consists of:  \n",
    "âœ… A **sigmoid layer** to decide which values to update.  \n",
    "âœ… A **tanh layer** to create a candidate memory update $ \\tilde{C}_t $.  \n",
    "\n",
    "ğŸ”¢ **Formulas:**  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$  \n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- $ i_t $ controls **how much** of $ \\tilde{C}_t $ should be stored in memory.  \n",
    "- $ \\tilde{C}_t $ contains the potential **new information**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”„ 3. Update Cell State $ C_t $ â€“ The Actual Memory! ğŸ§ **  \n",
    "After **forgetting some old info** and **adding new info**, we update the **cell state**:  \n",
    "\n",
    "ğŸ”¢ **Formula:**  \n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- The **old memory $ C_{t-1} $** is reduced based on $ f_t $.  \n",
    "- The **new memory $ \\tilde{C}_t $** is added based on $ i_t $.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“¤ 4. Output Gate $ o_t $ â€“ Decides the Final Output! ğŸ“Š**  \n",
    "The **Output Gate** determines what the **hidden state** $ h_t $ (the output of the LSTM cell) should be.  \n",
    "\n",
    "ğŸ”¢ **Formulas:**  \n",
    "$$\n",
    "o_t = \\sigma (W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$  \n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Intuition:**  \n",
    "- $ o_t $ acts as a filter, deciding **which parts of $ C_t $** should be output.  \n",
    "- The **hidden state $ h_t $** is used in the next LSTM step and can also be passed to other layers (like dense layers for classification).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Putting It All Together: LSTM Workflow ğŸ”„**\n",
    "At each time step $ t $, an LSTM cell follows these steps:  \n",
    "1ï¸âƒ£ **Forget** old information ($ f_t $).  \n",
    "2ï¸âƒ£ **Decide what new information to store** ($ i_t $, $ \\tilde{C}_t $).  \n",
    "3ï¸âƒ£ **Update the memory cell** ($ C_t $).  \n",
    "4ï¸âƒ£ **Compute the final output** ($ h_t $) using the Output Gate.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Where is LSTM Used?**\n",
    "LSTM is widely used in:  \n",
    "ğŸ”¹ **Speech Recognition** ğŸ™ï¸ (e.g., Siri, Google Assistant)  \n",
    "ğŸ”¹ **Text Generation** ğŸ“ (e.g., ChatGPT, poetry generation)  \n",
    "ğŸ”¹ **Time-Series Forecasting** ğŸ“ˆ (e.g., stock prices, weather prediction)  \n",
    "ğŸ”¹ **Machine Translation** ğŸŒ (e.g., Google Translate)  \n",
    "ğŸ”¹ **Music Generation** ğŸµ (e.g., AI composing music)  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ LSTM has a **memory cell** that retains important information over time.  \n",
    "âœ”ï¸ It uses **Forget, Input, and Output Gates** to control information flow.  \n",
    "âœ”ï¸ Unlike RNNs, LSTM can handle **long-term dependencies** efficiently.  \n",
    "âœ”ï¸ Used in various applications like **NLP, speech processing, and forecasting**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¨ Visual Summary**\n",
    "Imagine LSTM as a **smart secretary** ğŸ§‘â€ğŸ’¼ managing a **to-do list**:  \n",
    "âœ… **Forget Gate** removes unnecessary tasks.  \n",
    "âœ… **Input Gate** adds new important tasks.  \n",
    "âœ… **Cell State** is the notebook holding all tasks.  \n",
    "âœ… **Output Gate** decides what tasks should be shared.  \n",
    "\n",
    "LSTMs are **powerful tools** in deep learning, allowing AI to learn patterns in time-dependent data effectively! ğŸš€ğŸ”¥  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“Œ Forget Gate Architecture in LSTM â€“ A Deep Dive ğŸ”¥**  \n",
    "\n",
    "The **Forget Gate** is a crucial component of Long Short-Term Memory (LSTM) networks. Its main job is to **decide which information should be discarded (forgotten) from the cell state** at each time step. This prevents the network from storing irrelevant or outdated information.  \n",
    "\n",
    "Letâ€™s explore its architecture, mathematical equations, and how it works step by step. ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” 1. Forget Gate Overview**\n",
    "The **Forget Gate** is responsible for **removing unnecessary information** from the **Cell State** $ C_t $.  \n",
    "\n",
    "### **ğŸ’¡ Key Idea**  \n",
    "At every time step $ t $, the Forget Gate receives:  \n",
    "- The **previous hidden state** $ h_{t-1} $ (short-term memory)  \n",
    "- The **current input** $ x_t $ (new incoming data)  \n",
    "\n",
    "It then decides, using a **sigmoid activation function ($ \\sigma $)**, which parts of the previous cell state $ C_{t-1} $ should be **kept** and which should be **forgotten**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ 2. Forget Gate Architecture ğŸ—ï¸**  \n",
    "\n",
    "ğŸ”¹ The Forget Gate consists of:  \n",
    "âœ… **A weight matrix** $ W_f $ that helps learn which information should be forgotten.  \n",
    "âœ… **A bias term** $ b_f $ that adds flexibility to the learning process.  \n",
    "âœ… **A sigmoid activation function** $ \\sigma $ to produce values between **0 and 1** (0 = completely forget, 1 = completely remember).  \n",
    "\n",
    "### **ğŸ”¢ Mathematical Formula**  \n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "where:  \n",
    "- $ W_f $ is the weight matrix for the forget gate.  \n",
    "- $ [h_{t-1}, x_t] $ is the concatenation of the previous hidden state and current input.  \n",
    "- $ b_f $ is the bias term.  \n",
    "- $ \\sigma $ is the sigmoid activation function.  \n",
    "\n",
    "ğŸ“Œ **Sigmoid ensures that**:  \n",
    "- If $ f_t $ is **close to 0**, the information is forgotten.  \n",
    "- If $ f_t $ is **close to 1**, the information is retained.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”„ 3. Step-by-Step Working of the Forget Gate**\n",
    "At **each time step $ t $**, the Forget Gate operates as follows:\n",
    "\n",
    "### **ğŸŸ¢ Step 1: Take Input**\n",
    "- The Forget Gate receives **two inputs**:\n",
    "  - **Previous hidden state** $ h_{t-1} $ (from the last LSTM cell).\n",
    "  - **Current input** $ x_t $ (new information).  \n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "If we are processing a sentence, $ x_t $ could be a **new word**, and $ h_{t-1} $ holds the context from previous words.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”µ Step 2: Compute Forget Score**\n",
    "- The Forget Gate applies a **linear transformation**:  \n",
    "  $$\n",
    "  z = W_f \\cdot [h_{t-1}, x_t] + b_f\n",
    "  $$\n",
    "- Then, a **sigmoid activation function** is applied to get a value between **0 and 1**:\n",
    "  $$\n",
    "  f_t = \\sigma(z)\n",
    "  $$\n",
    "  \n",
    "ğŸ“Œ **Example Output:**  \n",
    "- If $ f_t = 0.1 $ â†’ Forget most of the past information.  \n",
    "- If $ f_t = 0.9 $ â†’ Retain most of the past information.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ£ Step 3: Update Cell State**\n",
    "- The **Forget Gate output** $ f_t $ is **multiplied** with the previous **cell state** $ C_{t-1} $:  \n",
    "  $$\n",
    "  C_t = f_t * C_{t-1}\n",
    "  $$\n",
    "- This determines **how much of the old memory should be kept**.  \n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "Letâ€™s say the previous cell state $ C_{t-1} = 5 $ and the Forget Gate outputs $ f_t = 0.2 $, then:  \n",
    "$$\n",
    "C_t = 0.2 \\times 5 = 1\n",
    "$$\n",
    "This means **most of the past information is discarded**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š 4. Visualization of Forget Gate Architecture**  \n",
    "\n",
    "```\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Inputs: h(t-1), x(t)                         â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â”‚  â¬‡ Concatenate inputs                      â”‚\n",
    "    â”‚                                             â”‚\n",
    "    â”‚  W_f * [h(t-1), x(t)] + b_f                 â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚        Sigmoid (Ïƒ) Activation               â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚        Forget Score (f_t) (0 to 1)          â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚     Multiply with Previous Cell State       â”‚\n",
    "    â”‚           â¬‡                                 â”‚\n",
    "    â”‚     Update Cell State (C_t)                 â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ 5. Intuition with a Real-Life Example ğŸ§ **\n",
    "Imagine youâ€™re **reading a book** ğŸ“–:  \n",
    "\n",
    "- You **remember** important plot details.  \n",
    "- You **forget** unnecessary descriptions that donâ€™t contribute much to the story.  \n",
    "\n",
    "The Forget Gate works the **same way**:  \n",
    "âœ… **Keeps important details** (high $ f_t $ value).  \n",
    "âŒ **Discards unnecessary details** (low $ f_t $ value).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ 6. Importance of the Forget Gate**\n",
    "ğŸ”¹ Prevents the network from accumulating **too much unnecessary information**.  \n",
    "ğŸ”¹ Solves the **vanishing gradient problem** by **removing outdated memory**.  \n",
    "ğŸ”¹ Helps LSTMs **handle long-term dependencies** efficiently.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”‘ Key Takeaways**\n",
    "âœ”ï¸ The **Forget Gate** determines **what past information to retain or discard**.  \n",
    "âœ”ï¸ Uses **sigmoid activation ($ \\sigma $)** to produce a value between **0 and 1**.  \n",
    "âœ”ï¸ Helps LSTM networks avoid **overloading memory with irrelevant information**.  \n",
    "âœ”ï¸ **Plays a crucial role** in handling long-term dependencies in sequential data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“– Manual Example of Forget Gate Calculation Using Text**  \n",
    "Let's take a simple **sentence** as input and see how the **Forget Gate** decides what to keep and what to forget step by step.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Example Sentence**\n",
    "ğŸ“Œ Suppose we have the sentence:  \n",
    "**\"John is a great football player. He scored a goal in the last match.\"**  \n",
    "\n",
    "We want our **LSTM model** to retain only the relevant information for predicting the next word.  \n",
    "\n",
    "- Some words are **important** (e.g., **\"John\"**, **\"football player\"**, **\"scored a goal\"**).  \n",
    "- Some words are **not very useful** (e.g., **\"is\"**, **\"a\"**, **\"in the last match\"**).  \n",
    "- The Forget Gate **decides** which parts to **keep** and which to **discard**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**\n",
    "Each word is converted into a numerical vector (simplified here as random values):\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| a      | **[0.1, 0.05]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "| in     | **[0.15, 0.1]**  |\n",
    "| the    | **[0.1, 0.05]**  |\n",
    "| last   | **[0.25, 0.2]**  |\n",
    "| match  | **[0.7, 0.6]**   |\n",
    "\n",
    "We will now apply the **Forget Gate** on these word vectors.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 2: Compute Forget Gate Scores**\n",
    "The Forget Gate uses the formula:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "\n",
    "Let's assume:  \n",
    "âœ… **Weight Matrix $ W_f $**:  \n",
    "$$\n",
    "W_f =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_f $**:  \n",
    "$$\n",
    "b_f = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Applying the Forget Gate** (For each word):\n",
    "\n",
    "### Example Calculation for \"John\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{John}] + b_f\n",
    "$$\n",
    "\n",
    "$$\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Computing this (simplified for understanding), we get:\n",
    "\n",
    "$$\n",
    "z = [0.78, 0.55]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_t = [0.68, 0.63]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "âœ… **\"John\" is important, so the Forget Gate gives a high score (~0.68).**  \n",
    "\n",
    "\n",
    "\n",
    "### Example Calculation for \"is\":\n",
    "$$\n",
    "z = W_f \\cdot [h_{t-1}, x_{is}] + b_f\n",
    "$$\n",
    "\n",
    "Computing this:\n",
    "\n",
    "$$\n",
    "z = [0.32, 0.25]\n",
    "$$\n",
    "\n",
    "Applying sigmoid:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma (z) = [0.58, 0.56]\n",
    "$$\n",
    "\n",
    "Interpretation:  \n",
    "ğŸ¤” **\"is\" is not very important, so Forget Gate gives it a lower score (~0.56).**  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŸ£ Step 3: Apply Forget Scores to Cell State**\n",
    "Now, let's apply the Forget Gate scores to the **previous cell state** $ C_{t-1} $.  \n",
    "\n",
    "Let's assume $ C_{t-1} = [0.9, 0.8] $ (previous memory).\n",
    "\n",
    "For \"John\":\n",
    "$$\n",
    "C_t = f_t * C_{t-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.68, 0.63] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504]\n",
    "$$\n",
    "\n",
    "John is retained **more strongly** in memory.\n",
    "\n",
    "For \"is\":\n",
    "$$\n",
    "C_t = [0.58, 0.56] * [0.9, 0.8]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.522, 0.448]\n",
    "$$\n",
    "\n",
    "\"is\" is retained **less** than \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”´ Step 4: Summary of Forget Gate Decisions**\n",
    "| Word       | Forget Gate Score $ f_t $ | Retained in Memory? |\n",
    "|------------|----------------|------------------|\n",
    "| **John**   | **0.68**   | âœ… Kept (important) |\n",
    "| **is**     | **0.56**   | âŒ Partially forgotten |\n",
    "| **a**      | **0.40**   | âŒ Mostly forgotten |\n",
    "| **great**  | **0.75**   | âœ… Kept (important) |\n",
    "| **football** | **0.80**  | âœ… Kept (important) |\n",
    "| **player** | **0.85**   | âœ… Kept (important) |\n",
    "| **He**     | **0.50**   | âŒ Partially forgotten |\n",
    "| **scored** | **0.90**   | âœ… Kept (important) |\n",
    "| **goal**   | **0.92**   | âœ… Kept (important) |\n",
    "| **last**   | **0.30**   | âŒ Mostly forgotten |\n",
    "| **match**  | **0.60**   | âŒ Partially forgotten |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Understanding**\n",
    "After processing the entire sentence, the LSTM has **forgotten unnecessary words** like **\"is\", \"a\", \"in the last match\"**, while **retaining important words** like **\"John\", \"football player\", \"scored a goal\"**.  \n",
    "\n",
    "### ğŸ”¥ **Key Takeaways**\n",
    "âœ” **Forget Gate helps the LSTM focus only on relevant information.**  \n",
    "âœ” **Higher forget score â†’ Memory is retained.**  \n",
    "âœ” **Lower forget score â†’ Memory is removed.**  \n",
    "\n",
    "This allows LSTM to process long sentences **efficiently** while avoiding information overload! ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“– Manual Example of Input Gate Calculation Using Text**  \n",
    "Now, letâ€™s go **step by step** to understand how the **Input Gate** in an LSTM works using a **manual example** with actual calculations.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  What is the Input Gate in LSTM?**\n",
    "The **Input Gate** decides **what new information** should be **added to the cell state**. It controls how much of the **current input** should be stored in the memory.  \n",
    "\n",
    "Formula for the Input Gate:  \n",
    "$$\n",
    "i_t = \\sigma (W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ i_t $ â†’ Input Gate Activation (between 0 and 1, decides how much to store)\n",
    "- $ W_i $ â†’ Weight matrix for the Input Gate\n",
    "- $ h_{t-1} $ â†’ Previous hidden state\n",
    "- $ x_t $ â†’ Current input\n",
    "- $ b_i $ â†’ Bias for the Input Gate\n",
    "- $ \\sigma $ â†’ Sigmoid activation function\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Example Sentence**\n",
    "Letâ€™s consider the same example:  \n",
    "ğŸ“Œ **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "The **goal** is to store the most relevant information in the memory while ignoring unnecessary words.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**\n",
    "Each word is represented as a vector:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "Now, letâ€™s compute the **Input Gate Activation** for \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ¢ Step 2: Compute Input Gate Activation**\n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_i $**:  \n",
    "$$\n",
    "W_i =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_i $**:  \n",
    "$$\n",
    "b_i = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = W_i \\cdot [h_{t-1}, x_t] + b_i\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.4 \\\\\n",
    "0.3 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.89, 0.64]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "i_t = \\sigma (z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "i_t = [0.71, 0.65]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- **\"John\" is relevant, so the Input Gate assigns high values (~0.71).**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 3: Compute Candidate Memory Content ($\\tilde{C_t}$)**\n",
    "The candidate content is **potential new information** to add to the memory.\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "\n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_C $**:  \n",
    "$$\n",
    "W_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_C $**:  \n",
    "$$\n",
    "b_C = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_C = W_C \\cdot [h_{t-1}, x_t] + b_C\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_C =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.5 \\\\\n",
    "0.4 & 0.3\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1, 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.12, 0.76]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation function**:\n",
    "\n",
    "$$\n",
    "\\tilde{C_t} = \\tanh(z_C)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- This means the new memory content suggests storing **\"John\"** strongly.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ  Step 4: Update Cell State**\n",
    "Now, the **Input Gate** decides how much of this new information to store:\n",
    "\n",
    "$$\n",
    "C_t = f_t * C_{t-1} + i_t * \\tilde{C_t}\n",
    "$$\n",
    "\n",
    "From the **Forget Gate Calculation (previous example)**, we got:\n",
    "\n",
    "âœ… **Forget Gate** $ f_t = [0.68, 0.63] $  \n",
    "âœ… **Previous Cell State** $ C_{t-1} = [0.9, 0.8] $  \n",
    "âœ… **Input Gate** $ i_t = [0.71, 0.65] $  \n",
    "âœ… **Candidate Memory** $ \\tilde{C_t} = [0.81, 0.64] $  \n",
    "\n",
    "Now, applying the formula:\n",
    "\n",
    "$$\n",
    "C_t = [0.68, 0.63] * [0.9, 0.8] + [0.71, 0.65] * [0.81, 0.64]\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "= [0.612, 0.504] + [0.5751, 0.416]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Final Interpretation**:\n",
    "- The **cell state has been updated**, retaining past information and adding new relevant details.  \n",
    "- **\"John\" is stored strongly, while unnecessary words are weakened.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Summary of Input Gate**\n",
    "| Word       | Input Gate Score $ i_t $ | Candidate Memory $ \\tilde{C_t} $ | Updated Memory $ C_t $ |\n",
    "|------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.71**   | **0.81**   | **1.1871** |\n",
    "| **is**     | **0.45**   | **0.30**   | **0.58** |\n",
    "| **great**  | **0.75**   | **0.88**   | **1.25** |\n",
    "| **football** | **0.80**  | **0.92**  | **1.32** |\n",
    "| **player** | **0.85**   | **0.95**  | **1.38** |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Key Takeaways**\n",
    "âœ” The **Input Gate** decides **how much new information should be stored**.  \n",
    "âœ” **High Input Gate Score â†’ More important information is stored.**  \n",
    "âœ” **The Forget Gate + Input Gate work together** to balance **what to keep** and **what to forget**.  \n",
    "\n",
    "This is how **LSTMs** maintain memory over long sequences! ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ§  Understanding the Output Gate in LSTM with Manual Calculation**  \n",
    "\n",
    "Now, let's break down the **Output Gate** in an **LSTM** using **step-by-step manual calculations**, just like we did for the **Forget Gate** and **Input Gate**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” What is the Output Gate in LSTM?**  \n",
    "The **Output Gate** decides how much of the **cell stateâ€™s information** should be passed to the **next hidden state** ($ h_t $).  \n",
    "\n",
    "Formula for the **Output Gate Activation**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ o_t $ â†’ Output Gate activation (decides how much information should be **exposed** as output)  \n",
    "- $ W_o $ â†’ Weight matrix for the Output Gate  \n",
    "- $ h_{t-1} $ â†’ Previous hidden state  \n",
    "- $ x_t $ â†’ Current input  \n",
    "- $ b_o $ â†’ Bias for the Output Gate  \n",
    "- $ \\sigma $ â†’ Sigmoid activation function  \n",
    "\n",
    "### **Final Hidden State Calculation**:  \n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ h_t $ â†’ New hidden state  \n",
    "- $ C_t $ â†’ Updated Cell State (from Input and Forget Gates)  \n",
    "- $ \\tanh(C_t) $ â†’ Squashing the cell state values between -1 and 1  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“– Example Sentence**\n",
    "Letâ€™s continue with the same example:  \n",
    "ğŸ“Œ **\"John is a great football player. He scored a goal.\"**  \n",
    "\n",
    "We will calculate the **Output Gate Activation** and **Hidden State** for the word \"John.\"\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ Step 1: Assign Word Vectors**  \n",
    "We use the same word vectors:\n",
    "\n",
    "| Word  | Word Vector Representation (Simplified) |\n",
    "|--------|----------------------------|\n",
    "| John   | **[0.8, 0.5]**   |\n",
    "| is     | **[0.2, 0.1]**   |\n",
    "| great  | **[0.9, 0.7]**   |\n",
    "| football | **[0.7, 0.6]**   |\n",
    "| player | **[0.85, 0.75]**  |\n",
    "| He     | **[0.3, 0.2]**   |\n",
    "| scored | **[0.95, 0.85]**  |\n",
    "| goal   | **[0.9, 0.8]**   |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŸ¢ Step 2: Compute Output Gate Activation $ o_t $**  \n",
    "Letâ€™s assume:\n",
    "\n",
    "âœ… **Weight Matrix $ W_o $**:  \n",
    "$$\n",
    "W_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "âœ… **Bias $ b_o $**:  \n",
    "$$\n",
    "b_o = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "âœ… **Previous Hidden State $ h_{t-1} $**:  \n",
    "$$\n",
    "h_{t-1} = [0.5, 0.4]\n",
    "$$\n",
    "\n",
    "âœ… **Current Input $ x_{John} $**:  \n",
    "$$\n",
    "x_t = [0.8, 0.5]\n",
    "$$\n",
    "\n",
    "$$\n",
    "z_o = W_o \\cdot [h_{t-1}, x_t] + b_o\n",
    "$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$\n",
    "z_o =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 \\\\\n",
    "0.2 & 0.1\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5, 0.4, 0.8, 0.5\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.05, 0.05\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.67, 0.38]\n",
    "$$\n",
    "\n",
    "Applying **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "o_t = \\sigma (z_o) = \\frac{1}{1 + e^{-z_o}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "o_t = [0.66, 0.59]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:  \n",
    "- **The Output Gate assigns moderate values (~0.66), meaning \"John\" should contribute moderately to the hidden state.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 3: Compute Final Hidden State $ h_t $**  \n",
    "Now, we use the **cell state** ($ C_t $) from the previous step.  \n",
    "\n",
    "âœ… **Updated Cell State $ C_t $ from Input & Forget Gates**:  \n",
    "$$\n",
    "C_t = [1.1871, 0.92]\n",
    "$$\n",
    "\n",
    "Applying **tanh activation**:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [\\tanh(1.1871), \\tanh(0.92)]\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "\\tanh(C_t) = [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "Now, calculating $ h_t $:\n",
    "\n",
    "$$\n",
    "h_t = o_t * \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_t = [0.66, 0.59] * [0.83, 0.72]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.5478, 0.4248]\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Interpretation**:\n",
    "- **The new hidden state** ($ h_t $) **contains the most relevant information**.\n",
    "- **Since the Output Gate was moderately open (~0.66), it allows partial information to flow.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Summary of Output Gate**\n",
    "| Word       | Output Gate Score $ o_t $ | Cell State $ C_t $ | $ \\tanh(C_t) $ | Hidden State $ h_t $ |\n",
    "|------------|----------------|----------------|----------------|----------------|\n",
    "| **John**   | **0.66**   | **1.1871**   | **0.83**   | **0.5478** |\n",
    "| **is**     | **0.45**   | **0.58**   | **0.52**   | **0.234** |\n",
    "| **great**  | **0.75**   | **1.25**   | **0.85**   | **0.6375** |\n",
    "| **football** | **0.80**  | **1.32**  | **0.87**  | **0.696** |\n",
    "| **player** | **0.85**   | **1.38**  | **0.89**  | **0.7565** |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Key Takeaways**\n",
    "âœ” The **Output Gate** determines **how much information flows to the next step**.  \n",
    "âœ” The **higher the Output Gate value**, the more information is exposed in the **hidden state**.  \n",
    "âœ” **The hidden state is the final information passed to the next word in the sequence.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”— Full LSTM Recap**\n",
    "âœ” **Forget Gate** â†’ Decides **what to forget**.  \n",
    "âœ” **Input Gate** â†’ Decides **what to store**.  \n",
    "âœ” **Output Gate** â†’ Decides **what to expose as output**.  \n",
    "\n",
    "ğŸš€ **Together, these gates make LSTMs powerful for handling long-term dependencies in sequences!**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
