{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ **Backpropagation in RNNs â€“ A Deep Dive!** ğŸ”¥  \n",
    "\n",
    "Backpropagation in Recurrent Neural Networks (RNNs) is a bit different from standard feedforward networks because of their sequential nature. This process is called **Backpropagation Through Time (BPTT)**. Let's break it down step by step!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Understanding Backpropagation in RNNs**\n",
    "### ğŸŒŸ **Step 1: Forward Pass**  \n",
    "In a standard RNN, we pass input sequences **step by step** through the network while maintaining a hidden state:  \n",
    "\n",
    "$$\n",
    "h_t = f(W_h h_{t-1} + W_x x_t + b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + c)\n",
    "$$\n",
    "\n",
    "where:  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_t $ = hidden state at time $ t $, which depends on previous state $ h_{t-1} $  \n",
    "- $ y_t $ = output at time $ t $  \n",
    "- $ W_h, W_x, W_y $ = weight matrices  \n",
    "- $ b, c $ = biases  \n",
    "- $ f, g $ = activation functions (e.g., **tanh, softmax**)  \n",
    "\n",
    "During this process, the **hidden state carries information** forward in time, making RNNs great for sequential tasks like speech recognition and text processing.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ **Step 2: Loss Calculation**  \n",
    "After the forward pass, we compute the **loss** using a function like **Mean Squared Error (MSE) or Cross-Entropy Loss**, depending on the problem (regression or classification).  \n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{t=1}^{T} L(y_t, \\hat{y}_t)\n",
    "$$\n",
    "\n",
    "where $ L $ is the loss function and $ \\hat{y}_t $ is the predicted output.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ” **Step 3: Backpropagation Through Time (BPTT)**\n",
    "This is where things get interesting! Unlike standard backpropagation (which flows only through layers), RNN backpropagation **flows through time** as well.  \n",
    "\n",
    "ğŸ›  **Steps in BPTT:**  \n",
    "1ï¸âƒ£ Compute **gradients at the last time step** ($ T $) and move backward.  \n",
    "2ï¸âƒ£ Compute **gradients for each earlier time step** until $ t=1 $.  \n",
    "3ï¸âƒ£ Update weights using **gradient descent** or any optimizer like Adam, RMSprop.  \n",
    "\n",
    "#### ğŸ”¹ **Gradient Calculation**\n",
    "For each time step $ t $, we compute gradients of the loss with respect to weights using the **chain rule**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\sum_{t=1}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_t} \\cdot \\frac{\\partial y_t}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\sum_{t=1}^{T} \\sum_{k=t}^{T} \\frac{\\partial \\mathcal{L}}{\\partial y_k} \\cdot \\frac{\\partial y_k}{\\partial h_k} \\cdot \\frac{\\partial h_k}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "ğŸ›‘ **Why is this tricky?**  \n",
    "- **The hidden states are shared** across all time steps.  \n",
    "- **Error at one step** affects all previous steps.  \n",
    "- **Long-term dependencies** make it difficult to train (this is called the **vanishing gradient problem** ğŸ›‘).  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ›‘ **Step 4: Vanishing and Exploding Gradients**\n",
    "ğŸ’¡ **Vanishing Gradients:**  \n",
    "- If gradients become **too small**, updates **disappear**, and the model stops learning **long-term dependencies**.  \n",
    "- This happens when we keep multiplying small values (like derivatives of sigmoid/tanh functions).  \n",
    "\n",
    "ğŸ’¥ **Exploding Gradients:**  \n",
    "- If gradients **grow too large**, training becomes **unstable**, and weights explode.  \n",
    "- Happens when weights keep multiplying large values, causing loss to **diverge**.  \n",
    "\n",
    "ğŸ”¹ **Solutions:**  \n",
    "âœ… Use **Long Short-Term Memory (LSTM)** or **Gated Recurrent Unit (GRU)** to control gradient flow.  \n",
    "âœ… Apply **gradient clipping** (cap gradients to a maximum value).  \n",
    "âœ… Use **ReLU** instead of **sigmoid/tanh** where possible.  \n",
    "\n",
    "\n",
    "\n",
    "### âš¡ **Step 5: Updating Weights**\n",
    "Once gradients are computed, we update weights using **Gradient Descent** or other optimizers like **Adam, RMSprop**:\n",
    "\n",
    "$$\n",
    "W = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "$$\n",
    "\n",
    "where $ \\eta $ is the **learning rate**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Key Takeaways**\n",
    "âœ… **BPTT propagates errors backward through time, affecting all previous time steps**.  \n",
    "âœ… **Vanishing gradients make long-term dependencies hard to learn**.  \n",
    "âœ… **LSTMs and GRUs solve vanishing gradient issues**.  \n",
    "âœ… **Gradient clipping helps control exploding gradients**.  \n",
    "\n",
    "\n",
    "### ğŸ”¥ **Final Thought**\n",
    "Backpropagation in RNNs is like **teaching a student** step by step, correcting mistakes from both **recent and past** lessons! ğŸ“š  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually go through an **example** of backpropagation in a simple Recurrent Neural Network (RNN) using **Backpropagation Through Time (BPTT)**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Example: A Simple RNN with One Neuron**\n",
    "We will calculate **forward pass, loss, and backpropagation (BPTT)** for a simple RNN with:  \n",
    "âœ… **1 input neuron**  \n",
    "âœ… **1 hidden neuron** (with recurrent connection)  \n",
    "âœ… **1 output neuron**  \n",
    "âœ… **1 time step for simplicity**  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Step 1: Define Network and Initial Weights**\n",
    "We define:  \n",
    "- $ W_x = 0.5 $ (input-to-hidden weight)  \n",
    "- $ W_h = 0.8 $ (hidden-to-hidden recurrent weight)  \n",
    "- $ W_y = 0.3 $ (hidden-to-output weight)  \n",
    "- **Biases are ignored** for simplicity.  \n",
    "\n",
    "Given:  \n",
    "- Input: $ x_1 = 1 $  \n",
    "- True output: $ y_{\\text{true}} = 0.6 $  \n",
    "- Initial hidden state: $ h_0 = 0 $  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ **Step 2: Forward Pass**\n",
    "#### ğŸ”¹ **Hidden State Calculation**  \n",
    "$$\n",
    "h_1 = \\tanh(W_x x_1 + W_h h_0)\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5(1) + 0.8(0))\n",
    "$$\n",
    "$$\n",
    "= \\tanh(0.5) = 0.462\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ **Output Calculation**\n",
    "$$\n",
    "y_{\\text{pred}} = W_y h_1\n",
    "$$\n",
    "$$\n",
    "= 0.3 \\times 0.462 = 0.1386\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ **Loss Calculation (Mean Squared Error)**\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (y_{\\text{true}} - y_{\\text{pred}})^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.6 - 0.1386)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.4614)^2\n",
    "$$\n",
    "$$\n",
    "= \\frac{1}{2} (0.213) = 0.1065\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ” **Step 3: Backpropagation Through Time (BPTT)**  \n",
    "Now, we compute the **gradients of the loss** with respect to each weight.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Output Weight $ W_y $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial W_y}\n",
    "$$\n",
    "\n",
    "We compute the derivatives:  \n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} = (y_{\\text{pred}} - y_{\\text{true}}) = (0.1386 - 0.6) = -0.4614\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial W_y} = h_1 = 0.462\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_y} = (-0.4614) \\times (0.462) = -0.213\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Hidden Weight $ W_h $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_{\\text{pred}}}{\\partial h_1} = W_y = 0.3\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_h} = (1 - h_1^2) \\times h_0 = (1 - 0.462^2) \\times 0 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_h} = (-0.4614) \\times (0.3) \\times (0) = 0\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ Since $ h_0 = 0 $, the gradient for $ W_h $ is **zero** in this case.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Gradient of Loss w.r.t Input Weight $ W_x $**\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = \\frac{\\partial \\mathcal{L}}{\\partial y_{\\text{pred}}} \\times \\frac{\\partial y_{\\text{pred}}}{\\partial h_1} \\times \\frac{\\partial h_1}{\\partial W_x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_1}{\\partial W_x} = (1 - h_1^2) \\times x_1 = (1 - 0.462^2) \\times 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (1 - 0.213) = 0.787\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_x} = (-0.4614) \\times (0.3) \\times (0.787)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= -0.1088\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## âœï¸ **Step 4: Weight Updates Using Gradient Descent**\n",
    "Using **learning rate** $ \\eta = 0.1 $, we update:\n",
    "\n",
    "$$\n",
    "W_y = W_y - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_y}\n",
    "$$\n",
    "$$\n",
    "= 0.3 - (0.1 \\times -0.213)\n",
    "$$\n",
    "$$\n",
    "= 0.3 + 0.0213 = 0.3213\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_x = W_x - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W_x}\n",
    "$$\n",
    "$$\n",
    "= 0.5 - (0.1 \\times -0.1088)\n",
    "$$\n",
    "$$\n",
    "= 0.5 + 0.01088 = 0.51088\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_h = 0.8 - (0.1 \\times 0) = 0.8\n",
    "$$  \n",
    "(Since the gradient was zero, $ W_h $ remains unchanged.)\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Updated Weights**\n",
    "After **one iteration of BPTT**, we get:  \n",
    "âœ… $ W_x = 0.51088 $  \n",
    "âœ… $ W_h = 0.8 $  \n",
    "âœ… $ W_y = 0.3213 $  \n",
    "\n",
    "If we repeat this over multiple time steps, RNN learns to predict better over time! ğŸ”¥\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Takeaways**\n",
    "âœ” **BPTT works by computing gradients backward through time** â³  \n",
    "âœ” **Weight updates use the chain rule** to propagate errors  \n",
    "âœ” **Vanishing gradients** occur when gradients become too small  \n",
    "âœ” **Exploding gradients** occur when gradients grow too large  \n",
    "âœ” **Optimizations like LSTMs, GRUs, and gradient clipping help stabilize learning** ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **Problems with RNNs: Why They Struggle and How to Fix Them**\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are great for handling **sequential data** like **text, speech, and time series**, but they come with several limitations. Letâ€™s break them down in a **simple, colorful way** and also discuss possible solutions! ğŸŒˆ  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **1. Vanishing Gradient Problem**\n",
    "### âŒ **What is it?**\n",
    "- When training an RNN with **backpropagation through time (BPTT)**, the gradients shrink **exponentially** as they are passed backward through many time steps.  \n",
    "- This means earlier layers receive **almost no updates**, making it **hard for RNNs to learn long-term dependencies**.\n",
    "\n",
    "### ğŸ“‰ **Why does this happen?**\n",
    "- The chain rule in **backpropagation** involves multiplying many small values (gradients of activation functions like sigmoid or tanh), leading to values approaching **zero**.\n",
    "- This results in **\"memory loss\"** in RNNsâ€”**they forget long-term dependencies**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use LSTMs (Long Short-Term Memory) or GRUs (Gated Recurrent Units)** â€“ They use special gates to store and update information efficiently.  \n",
    "âœ… **Use ReLU activation instead of tanh/sigmoid** â€“ ReLU helps prevent gradients from shrinking.  \n",
    "âœ… **Use batch normalization or layer normalization** to stabilize training.  \n",
    "âœ… **Gradient clipping** â€“ Limits the gradient values to prevent them from shrinking too much.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **2. Exploding Gradient Problem**\n",
    "### âŒ **What is it?**\n",
    "- The opposite of the vanishing gradient problem!  \n",
    "- When gradients grow **too large**, they cause unstable updates, making the model diverge instead of learning.\n",
    "\n",
    "### ğŸ“ˆ **Why does this happen?**\n",
    "- If weights are large or initialized poorly, gradients can **explode exponentially** during backpropagation.\n",
    "- This results in sudden, erratic updates, making the network **unstable**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Gradient Clipping** â€“ Set a threshold so that gradients donâ€™t grow beyond a certain limit.  \n",
    "âœ… **Use smaller learning rates** to prevent large weight updates.  \n",
    "âœ… **Use careful weight initialization techniques** like Xavier or He initialization.  \n",
    "\n",
    "\n",
    "\n",
    "## â³ **3. Short-Term Memory Issue**\n",
    "### âŒ **What is it?**\n",
    "- Standard RNNs struggle to remember information **from many time steps ago**.  \n",
    "- If a dependency spans **20+ time steps**, the network simply **forgets** it.\n",
    "\n",
    "### ğŸ¤¯ **Example:**  \n",
    "Imagine reading a long paragraph and trying to remember a name mentioned at the beginning. **By the time you reach the end, youâ€™ve forgotten it!** Thatâ€™s what happens to RNNs.  \n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… Use **LSTMs or GRUs** â€“ These architectures store **long-term information** better than standard RNNs.  \n",
    "âœ… Use **Attention Mechanisms** â€“ They help focus on **important parts** of the input sequence.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¢ **4. Slow Training and High Computation Costs**\n",
    "### âŒ **What is it?**\n",
    "- RNNs **process inputs sequentially**, meaning **no parallelization** like CNNs.  \n",
    "- This makes them **slower** and **more computationally expensive** compared to feedforward networks.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use parallel architectures like Transformers** (they donâ€™t process inputs sequentially).  \n",
    "âœ… **Use GPU acceleration** for faster matrix computations.  \n",
    "âœ… **Reduce sequence length** if possible, or use **truncated BPTT** to limit time steps during training.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ­ **5. Difficulty in Capturing Long-Term Dependencies**\n",
    "### âŒ **What is it?**\n",
    "- RNNs **focus more on recent inputs** and often fail to link **old words/events** in a sequence.  \n",
    "- Example: If a document introduces a character **50 sentences ago**, a simple RNN wonâ€™t remember them!\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use LSTMs/GRUs** â€“ These have memory cells that **store relevant past information**.  \n",
    "âœ… **Use Attention Mechanisms** â€“ They help the model **attend** to specific parts of the input.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ **6. Bias Towards Recent Inputs**\n",
    "### âŒ **What is it?**\n",
    "- RNNs have a **recency bias**, meaning they **prioritize recent inputs** over older ones.  \n",
    "- Example: If a chatbot sees **\"not good\"** at the beginning of a sentence but **\"great\"** at the end, it may only remember **\"great\"**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Bidirectional RNNs** â€“ They read input **both forward and backward**.  \n",
    "âœ… **Use Transformers** â€“ They process the entire sequence at once.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **7. Handling Variable-Length Sequences is Hard**\n",
    "### âŒ **What is it?**\n",
    "- RNNs struggle with **very long** or **very short** sequences.  \n",
    "- Padding/truncating sequences can sometimes **distort the meaning**.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Dynamic RNNs** â€“ These handle variable-length sequences without padding issues.  \n",
    "âœ… **Use Attention Mechanisms** â€“ They allow the model to focus on **important** sequence parts.  \n",
    "\n",
    "\n",
    "\n",
    "## âš ï¸ **8. Poor Performance on Very Long Sequences**\n",
    "### âŒ **What is it?**\n",
    "- If sequences have **thousands of time steps**, RNNs perform **poorly**.  \n",
    "- This is why **speech recognition and machine translation** models often struggle with RNNs.\n",
    "\n",
    "### ğŸ›  **How to fix it?**\n",
    "âœ… **Use Transformers** (like BERT and GPT) â€“ These work **better for long-range dependencies**.  \n",
    "âœ… **Use Hierarchical RNNs** â€“ Process data at multiple levels for better representation.  \n",
    "\n",
    "# ğŸ¯ **Summary of RNN Problems & Fixes**\n",
    "| ğŸ›‘ **Problem**                  | ğŸ”¥ **Solution** |\n",
    "|---------------------------------|----------------|\n",
    "| **Vanishing Gradient**   | LSTMs, GRUs, ReLU, Gradient Clipping |\n",
    "| **Exploding Gradient**   | Gradient Clipping, Smaller Learning Rate |\n",
    "| **Short-Term Memory**    | LSTMs, GRUs, Attention |\n",
    "| **Slow Training**        | Transformers, GPUs, Parallelization |\n",
    "| **Long-Term Dependencies** | LSTMs, GRUs, Attention |\n",
    "| **Recency Bias**         | Bidirectional RNNs, Transformers |\n",
    "| **Variable-Length Issues** | Dynamic RNNs, Attention |\n",
    "| **Poor Performance on Long Sequences** | Transformers, Hierarchical Models |\n",
    "\n",
    "\n",
    "## ğŸ¤– **The Future: Moving Beyond RNNs**\n",
    "Because of these problems, newer architectures like **LSTMs, GRUs, and Transformers** (GPT, BERT) have replaced vanilla RNNs in most real-world applications! ğŸš€  \n",
    "\n",
    "Would you like a practical **example** of solving these issues using **LSTMs or Transformers** in Python? ğŸ¤”\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
