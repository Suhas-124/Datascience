{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 **Deep RNNs (Deep Recurrent Neural Networks) – A Full Explanation** 🔥\n",
    "\n",
    "## **📌 What is a Deep RNN?**\n",
    "A **Deep RNN** is a **stacked** version of a normal Recurrent Neural Network (RNN). Unlike a simple RNN that has only **one layer** of recurrent neurons, a **Deep RNN** stacks multiple RNN layers **on top of each other**. This allows it to **learn more complex patterns** in sequential data like **text, speech, and time-series data**.\n",
    "\n",
    "## **🛠️ How is a Deep RNN Different from a Simple RNN?**\n",
    "| Feature | Simple RNN | Deep RNN |\n",
    "|---------|-----------|----------|\n",
    "| **Number of Layers** | 1 recurrent layer | Multiple recurrent layers |\n",
    "| **Learning Capability** | Limited feature extraction | Captures deeper, hierarchical features |\n",
    "| **Performance** | Struggles with long-term dependencies | Better at long-term dependencies |\n",
    "| **Training Difficulty** | Easier | Harder (but more powerful) |\n",
    "| **Application** | Basic time-series & text prediction | Complex NLP, speech recognition |\n",
    "\n",
    "\n",
    "\n",
    "## **🧠 Architecture of a Deep RNN**\n",
    "A Deep RNN consists of **multiple RNN layers stacked on top of each other**, where:\n",
    "\n",
    "- **Each layer passes its hidden state** $ h_t^l $ **to the next layer**.\n",
    "- The **first layer** processes the input sequence.\n",
    "- The **last layer** produces the final output.\n",
    "\n",
    "### **🔹 Standard RNN vs. Deep RNN**\n",
    "📌 **Simple RNN (Shallow)**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "📌 **Deep RNN (Stacked)**\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)  \\quad \\text{(First RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2) \\quad \\text{(Second RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "h_t^L = \\tanh(W_x^L h_t^{L-1} + W_h^L h_{t-1}^L + b^L) \\quad \\text{(Final RNN Layer)}\n",
    "$$\n",
    "$$\n",
    "y_t = W_y h_t^L + b_y\n",
    "$$\n",
    "\n",
    "🚀 **Each layer refines the representation of the sequence!**\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Why Use a Deep RNN?**\n",
    "🔹 **Captures Higher-Level Features** → Lower layers learn **basic** features, higher layers learn **abstract** features.  \n",
    "🔹 **Handles Complex Dependencies** → Works better for long sequences.  \n",
    "🔹 **More Expressive Power** → Learns deeper relationships in data.\n",
    "\n",
    "\n",
    "\n",
    "## **📝 Example: Manual Computation for a Deep RNN**\n",
    "Let’s take a simple sequence:\n",
    "\n",
    "> **\"I love deep learning.\"**\n",
    "\n",
    "We'll process it using **2 RNN layers**.\n",
    "\n",
    "### **🔹 Step 1: Input Representation**\n",
    "Each word is represented as a **vector**:\n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|||\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "### **🔹 Step 2: Process Each Word Through Layer 1**\n",
    "Each word goes through the first RNN layer:\n",
    "\n",
    "$$\n",
    "h_t^1 = \\tanh(W_x^1 x_t + W_h^1 h_{t-1}^1 + b^1)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "$$\n",
    "h_1^1 = [0.2, 0.3]\n",
    "$$\n",
    "$$\n",
    "h_2^1 = [0.4, 0.5]\n",
    "$$\n",
    "$$\n",
    "h_3^1 = [0.1, 0.8]\n",
    "$$\n",
    "$$\n",
    "h_4^1 = [0.6, 0.4]\n",
    "$$\n",
    "\n",
    "### **🔹 Step 3: Pass to Layer 2**\n",
    "Now, these hidden states are **fed into the second RNN layer**:\n",
    "\n",
    "$$\n",
    "h_t^2 = \\tanh(W_x^2 h_t^1 + W_h^2 h_{t-1}^2 + b^2)\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "$$\n",
    "h_1^2 = [0.3, 0.6]\n",
    "$$\n",
    "$$\n",
    "h_2^2 = [0.5, 0.7]\n",
    "$$\n",
    "$$\n",
    "h_3^2 = [0.2, 0.9]\n",
    "$$\n",
    "$$\n",
    "h_4^2 = [0.7, 0.5]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Variants of Deep RNN**\n",
    "Deep RNNs are often implemented using **better recurrent cells** like:\n",
    "\n",
    "### **1️⃣ Deep LSTM (Stacked LSTM)**\n",
    "LSTM (Long Short-Term Memory) uses **gates** to better store long-term dependencies.\n",
    "\n",
    "### **2️⃣ Deep GRU (Stacked GRU)**\n",
    "GRU (Gated Recurrent Unit) simplifies LSTM while keeping good performance.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Where are Deep RNNs Used?**\n",
    "✅ **Speech Recognition** (e.g., Google Assistant, Siri)  \n",
    "✅ **Text Generation** (e.g., Chatbots)  \n",
    "✅ **Machine Translation** (e.g., Google Translate)  \n",
    "✅ **Stock Price Prediction**  \n",
    "✅ **Music Generation**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔎 Final Summary**\n",
    "| Concept | Explanation |\n",
    "|||\n",
    "| **Deep RNN** | Multiple RNN layers stacked together |\n",
    "| **Why Deep?** | Captures complex patterns better |\n",
    "| **How it Works?** | Each layer refines the representation |\n",
    "| **Better Variants** | Stacked LSTM, Stacked GRU |\n",
    "\n",
    "🔥 **Deep RNNs power many AI applications today!** Would you like me to implement a Deep RNN example in Python? 🚀\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
