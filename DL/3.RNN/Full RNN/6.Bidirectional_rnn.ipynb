{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 **Bidirectional Recurrent Neural Networks (BiRNN) - A Full and Colorful Guide!** 🚀  \n",
    "\n",
    "### **1️⃣ What is a Bidirectional RNN?**  \n",
    "Imagine you're watching a movie 🎬, but instead of seeing the whole scene, you only see frames one by one in a forward sequence. You might **miss important context** from future events. Wouldn’t it be amazing if you could **see both past and future** at the same time? 🤯  \n",
    "\n",
    "That’s exactly what **Bidirectional Recurrent Neural Networks (BiRNNs)** do! Instead of processing sequences in just one direction (like a regular RNN), **BiRNNs process them in both forward and backward directions** at the same time. 🔄 This makes them super powerful for **context-heavy** tasks like speech recognition 🎤, text processing 📖, and language translation 🌍.  \n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ How Does a BiRNN Work? 🛠️**  \n",
    "A BiRNN consists of **two RNNs running in parallel:**  \n",
    "\n",
    "1. **Forward RNN**: Reads the sequence from left to right ➡️  \n",
    "2. **Backward RNN**: Reads the sequence from right to left ⬅️  \n",
    "\n",
    "At each time step **t**, both RNNs process the input and produce two hidden states:  \n",
    "- One from the forward RNN: **$ h_t^{(fwd)} $**  \n",
    "- One from the backward RNN: **$ h_t^{(bwd)} $**  \n",
    "\n",
    "The final output at each time step is a combination (concatenation or sum) of these two hidden states:  \n",
    "$$\n",
    "h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "$$  \n",
    "\n",
    "### **🎯 Key Takeaway:**  \n",
    "🔹 Unlike a regular RNN, a BiRNN can use **both past and future information** at any given time step. This makes it way better for **understanding full context** in sequential data.  \n",
    "\n",
    "\n",
    "\n",
    "### **3️⃣ Why is BiRNN Better? 🤔**  \n",
    "\n",
    "✅ **More Context = More Accuracy**  \n",
    "   - A normal RNN only considers past words when predicting the next word, which can lead to **misinterpretations**.  \n",
    "   - BiRNNs can **consider both past and future words**, leading to **better predictions**! 🎯  \n",
    "\n",
    "✅ **Great for Speech & NLP Tasks**  \n",
    "   - **Speech Recognition**: The meaning of a word can change based on future words. A BiRNN helps capture that nuance! 🎙️  \n",
    "   - **Machine Translation**: Words in different languages may have different orders. Understanding the full sentence structure helps a lot! 🌍  \n",
    "   - **Named Entity Recognition (NER)**: Knowing the full sentence helps distinguish between similar words used in different contexts.  \n",
    "\n",
    "✅ **Works with LSTMs & GRUs**  \n",
    "   - BiRNNs can use **LSTM (Long Short-Term Memory) cells** or **GRUs (Gated Recurrent Units)** to handle long sequences better. 🧠  \n",
    "\n",
    "\n",
    "\n",
    "### **4️⃣ BiRNN in Action - Example with Python 🐍**  \n",
    "\n",
    "Let’s see how a **Bidirectional LSTM** can be implemented in TensorFlow/Keras:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define a BiLSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(100, 10)),  # BiLSTM Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "🔹 Here, the **Bidirectional()** wrapper makes the LSTM layer process input in both directions! 🔄  \n",
    "\n",
    "### **5️⃣ When to Use a BiRNN? 🤷**  \n",
    "\n",
    "| ✅ Use BiRNN When | ❌ Avoid BiRNN When |  \n",
    "|------------------|------------------|  \n",
    "| You need **full context** from past & future 🔄 | Your dataset is too large, as BiRNNs require **double computation** 💾 |  \n",
    "| Tasks involve **NLP**, **speech recognition**, or **translation** 🗣️📖 | You're working with **real-time applications** where only past info is available ⏳ |  \n",
    "| You need better performance on **long sequences** 🧠 | The problem is **too simple**, and a unidirectional RNN is enough ⚡ |  \n",
    "\n",
    "\n",
    "### **🌟 Conclusion - Why BiRNN is a Game-Changer? 🎮**  \n",
    "\n",
    "🚀 BiRNNs are like **time travelers** in the world of neural networks. Instead of just relying on the past, they **peek into the future** and learn from both sides! This makes them **exceptionally powerful** for tasks like:  \n",
    "\n",
    "✔️ Speech Recognition 🎤  \n",
    "✔️ Text Summarization 📄  \n",
    "✔️ Sentiment Analysis 😊😡  \n",
    "✔️ Named Entity Recognition (NER) 📍  \n",
    "\n",
    "But remember! BiRNNs require **more computation** and are not always the best choice for real-time applications. **Choose wisely!** 🧐  \n",
    "\n",
    "![](bid-rnn.jpg)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔥 Full Architecture of a Bidirectional Recurrent Neural Network (BiRNN) 🔥**  \n",
    "\n",
    "A **Bidirectional Recurrent Neural Network (BiRNN)** is an advanced type of **Recurrent Neural Network (RNN)** that processes sequences in **both forward and backward directions** to capture **past and future context**.  \n",
    "\n",
    "Let’s dive **deep into the architecture** step by step! 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 1. Basic Components of a BiRNN**  \n",
    "\n",
    "A **standard RNN** has the following components:  \n",
    "- **Input layer (X)**: The sequence of data (e.g., words in a sentence, frames in speech).  \n",
    "- **Hidden layer (h)**: Stores information from previous time steps.  \n",
    "- **Output layer (Y)**: Produces predictions at each time step.  \n",
    "\n",
    "A **Bidirectional RNN** consists of **two separate RNNs**:  \n",
    "- **Forward RNN** → Processes input from **left to right** (past to future).  \n",
    "- **Backward RNN** → Processes input from **right to left** (future to past).  \n",
    "\n",
    "At each time step $ t $, both RNNs produce hidden states, which are combined to form the final output.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 2. Step-by-Step Working of a BiRNN**  \n",
    "\n",
    "### **Step 1: Input Representation**  \n",
    "Let’s assume we have a sequence of length $ T $, where each input vector is $ X_t $ (a feature vector at time step $ t $).  \n",
    "\n",
    "$$\n",
    "X = [X_1, X_2, X_3, ..., X_T]\n",
    "$$\n",
    "\n",
    "Each input passes through **two RNNs**:  \n",
    "1. **Forward RNN** → Generates hidden states from past to future.  \n",
    "2. **Backward RNN** → Generates hidden states from future to past.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Forward and Backward Hidden States Computation**  \n",
    "\n",
    "- **Forward Hidden State ($ h_t^{(fwd)} $)**  \n",
    "  The forward RNN computes the hidden state at each time step using:  \n",
    "  $$\n",
    "  h_t^{(fwd)} = f(W_f X_t + U_f h_{t-1}^{(fwd)} + b_f)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_f $ = Input weight matrix for forward RNN  \n",
    "  - $ U_f $ = Hidden weight matrix for forward RNN  \n",
    "  - $ b_f $ = Bias  \n",
    "  - $ f $ = Activation function (usually tanh or ReLU)  \n",
    "\n",
    "- **Backward Hidden State ($ h_t^{(bwd)} $)**  \n",
    "  The backward RNN computes the hidden state moving from **$ T $ to $ 1 $**:  \n",
    "  $$\n",
    "  h_t^{(bwd)} = f(W_b X_t + U_b h_{t+1}^{(bwd)} + b_b)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_b $ = Input weight matrix for backward RNN  \n",
    "  - $ U_b $ = Hidden weight matrix for backward RNN  \n",
    "  - $ b_b $ = Bias  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Combining Forward and Backward States**  \n",
    "\n",
    "At each time step $ t $, the two hidden states **($ h_t^{(fwd)} $ and $ h_t^{(bwd)} $)** are combined into a single hidden state $ h_t $. This can be done in different ways:  \n",
    "- **Concatenation** (most common):  \n",
    "  $$\n",
    "  h_t = [h_t^{(fwd)}; h_t^{(bwd)}]\n",
    "  $$\n",
    "- **Sum**:  \n",
    "  $$\n",
    "  h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Output Layer**  \n",
    "\n",
    "The final output $ Y_t $ at each time step is computed as:  \n",
    "$$\n",
    "Y_t = g(W_o h_t + b_o)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_o $ = Output weight matrix  \n",
    "- $ b_o $ = Bias  \n",
    "- $ g $ = Activation function (e.g., softmax for classification)  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 3. Full Architecture Diagram of BiRNN**  \n",
    "\n",
    "```\n",
    "      Input Sequence: [ X1,  X2,  X3,  X4,  X5]\n",
    "                        ↓    ↓    ↓    ↓    ↓    \n",
    "      Forward RNN:   → h1 → h2 → h3 → h4 → h5 →  \n",
    "                         ↓    ↓    ↓    ↓    ↓    \n",
    "      Backward RNN:  ← h1 ← h2 ← h3 ← h4 ← h5 ←  \n",
    "                        ↓    ↓    ↓    ↓    ↓    \n",
    "      Final Output:  [ Y1,  Y2,  Y3,  Y4,  Y5]\n",
    "```\n",
    "\n",
    "- The **forward hidden states** move **left to right**.  \n",
    "- The **backward hidden states** move **right to left**.  \n",
    "- The **final hidden state at each time step** is a combination of both.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 4. Advantages of BiRNN 🚀**  \n",
    "\n",
    "✅ **Uses full context** (both past & future).  \n",
    "✅ **Improves accuracy** in NLP, speech recognition, and time series tasks.  \n",
    "✅ **Works well with LSTM & GRU for long-term dependencies.**  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 5. Implementing BiRNN in Python (TensorFlow/Keras) 🐍**  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define a Bidirectional RNN Model\n",
    "model = Sequential([\n",
    "    Bidirectional(SimpleRNN(64, return_sequences=True), input_shape=(100, 10)),  # BiRNN Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## **📌 6. When to Use BiRNN vs. Unidirectional RNN?**  \n",
    "\n",
    "| Feature  | Unidirectional RNN  | Bidirectional RNN  |\n",
    "|----------|--------------------|--------------------|\n",
    "| **Direction** | Forward only ➡️  | Forward + Backward 🔄 |\n",
    "| **Context** | Only past context 📜 | Both past & future context 🏆 |\n",
    "| **Computational Cost** | Lower 💰 | Higher ⚡ |\n",
    "| **Use Case** | Real-time tasks (e.g., online chatbots) 💬 | NLP, speech, translation 🌍 |\n",
    "\n",
    "\n",
    "## **🔥 Conclusion: Why BiRNN is a Game-Changer?**  \n",
    "\n",
    "🚀 **Bidirectional RNNs are like superheroes** in sequential tasks! Unlike normal RNNs that only see the past, BiRNNs **see both past and future at the same time**, making them extremely powerful for **speech recognition**, **text processing**, **machine translation**, and more! 💡  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's take a simple sentence and manually work through how a **Bidirectional Recurrent Neural Network (BiRNN)** processes it. This will involve:  \n",
    "\n",
    "1️⃣ **Choosing a sentence**  \n",
    "2️⃣ **Assigning word embeddings**  \n",
    "3️⃣ **Forward pass calculations**  \n",
    "4️⃣ **Backward pass calculations**  \n",
    "5️⃣ **Combining hidden states**  \n",
    "6️⃣ **Generating output**  \n",
    "\n",
    "## **📌 Sentence: \"I love AI\"**\n",
    "We’ll assume this is a 3-word sequence:  \n",
    "\n",
    "$$\n",
    "X = [\"I\", \"love\", \"AI\"]\n",
    "$$\n",
    "\n",
    "Each word will be represented as a **3D embedding vector** (to keep it simple).  \n",
    "\n",
    "| Word  | Embedding (3D Vector) |\n",
    "|--------|----------------|\n",
    "| \"I\"      | [0.1, 0.3, 0.5] |\n",
    "| \"love\"   | [0.2, 0.6, 0.8] |\n",
    "| \"AI\"     | [0.3, 0.7, 0.9] |\n",
    "\n",
    "\n",
    "## **🛠 Step 1: Initialize Parameters**\n",
    "BiRNN consists of **two RNNs**, one running **forward** and one **backward**. Each has:  \n",
    "\n",
    "- **Weight Matrices (Input → Hidden State)**\n",
    "  - $ W_f $ (Forward)\n",
    "  - $ W_b $ (Backward)  \n",
    "\n",
    "- **Weight Matrices (Hidden State → Next Hidden State)**\n",
    "  - $ U_f $ (Forward)\n",
    "  - $ U_b $ (Backward)  \n",
    "\n",
    "- **Bias Vectors**\n",
    "  - $ b_f $ (Forward)\n",
    "  - $ b_b $ (Backward)  \n",
    "\n",
    "For simplicity, let’s assume:  \n",
    "\n",
    "$$\n",
    "W_f = W_b =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_f = U_b =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_f = b_b =\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 2: Forward Pass (Processing left to right)**  \n",
    "\n",
    "### **🔹 Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh(W_f X_1 + U_f h_0 + b_f)\n",
    "$$\n",
    "\n",
    "Since initial **hidden state** is **0**,  \n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.3 \\\\ 0.5\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "(0.5 \\times 0.1) + (0.3 \\times 0.3) + (0.2 \\times 0.5) \\\\ \n",
    "(0.4 \\times 0.1) + (0.7 \\times 0.3) + (0.6 \\times 0.5)\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.05 + 0.09 + 0.1 \\\\ \n",
    "0.04 + 0.21 + 0.3\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.34 \\\\ 0.75\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Approximating **tanh function**:  \n",
    "$$\n",
    "\\tanh(0.34) \\approx 0.327, \\quad \\tanh(0.75) \\approx 0.635\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh(W_f X_2 + U_f h_1^{(fwd)} + b_f)\n",
    "$$\n",
    "\n",
    "Using **previous hidden state**:\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\ 0.6 \\\\ 0.8\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "(Similarly, calculating matrix multiplications and applying **tanh**, we get:)\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\begin{bmatrix} 0.765 \\\\ 0.851 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Time Step 3: \"AI\"**\n",
    "Following the same process:\n",
    "\n",
    "$$\n",
    "h_3^{(fwd)} = \\begin{bmatrix} 0.88 \\\\ 0.92 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 3: Backward Pass (Processing right to left)**\n",
    "We now process in **reverse order**:\n",
    "\n",
    "### **🔹 Time Step 3: \"AI\"**\n",
    "$$\n",
    "h_3^{(bwd)} = \\tanh(W_b X_3 + U_b h_0 + b_b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(bwd)} = \\begin{bmatrix} 0.805 \\\\ 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **🔹 Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(bwd)} = \\begin{bmatrix} 0.742 \\\\ 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **🔹 Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(bwd)} = \\begin{bmatrix} 0.658 \\\\ 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠 Step 4: Combining Forward & Backward States**\n",
    "For each word, we concatenate both hidden states:\n",
    "\n",
    "$$\n",
    "h_1 = [h_1^{(fwd)}; h_1^{(bwd)}] = \\begin{bmatrix} 0.327 & 0.635 & 0.658 & 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = [h_2^{(fwd)}; h_2^{(bwd)}] = \\begin{bmatrix} 0.765 & 0.851 & 0.742 & 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = [h_3^{(fwd)}; h_3^{(bwd)}] = \\begin{bmatrix} 0.88 & 0.92 & 0.805 & 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🔮 Step 5: Output Layer**\n",
    "If this is for **classification**, we would pass the final **concatenated hidden states** through a softmax layer.\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Conclusion**\n",
    "- BiRNN processes **both past & future context**.\n",
    "- Each word has **two hidden states** (forward + backward).\n",
    "- The **final hidden state** is a combination of **both directions**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🔍 What Do These Calculations Signify?**  \n",
    "\n",
    "The calculations we performed help us **understand how Bi-directional RNN (BiRNN) processes text step by step**. Let’s break it down into **key insights**:\n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ BiRNN Captures Both Past & Future Context**  \n",
    "Unlike a normal **unidirectional RNN**, which processes the sequence **left to right** (or right to left), BiRNN does **both simultaneously**.  \n",
    "\n",
    "- **Forward RNN:** Moves from **left to right** (normal reading order).  \n",
    "- **Backward RNN:** Moves from **right to left** (reverse reading order).  \n",
    "- The **final hidden state** for each word is a **combination of both directions**, giving the model **fuller context**.  \n",
    "\n",
    "**Example:**\n",
    "For the word `\"love\"` in `\"I love AI\"`,  \n",
    "- The **forward RNN** only sees `\"I love ...\"`,  \n",
    "- The **backward RNN** sees `\"... love AI\"`.  \n",
    "\n",
    "So, `\"love\"` gets influenced by **both \"I\" (past) and \"AI\" (future)**, giving it **richer meaning**.\n",
    "\n",
    "\n",
    "\n",
    "## **2️⃣ Word Meaning Depends on Full Context**  \n",
    "Consider this sentence:\n",
    "\n",
    "> **\"He plays the bass.\"**  \n",
    "> **\"He caught a bass.\"**\n",
    "\n",
    "The word **\"bass\"** has **two meanings** (musical instrument vs. fish).  \n",
    "\n",
    "- A **unidirectional RNN** (left-to-right) would process `\"He caught a ...\"` before seeing `\"bass\"`, which is **not enough to disambiguate** the meaning.  \n",
    "- A **BiRNN** processes both `\"caught a\"` and the words **after** `\"bass\"` at the same time, giving it more information to determine the meaning.\n",
    "\n",
    "**This is crucial for NLP tasks like Named Entity Recognition, Sentiment Analysis, and Speech Recognition!** 🎯\n",
    "\n",
    "\n",
    "\n",
    "## **3️⃣ Why Do We Combine Forward & Backward States?**  \n",
    "At each time step, we computed **two hidden states**:\n",
    "- $ h_t^{(fwd)} $ → Capturing the meaning from the **left context**  \n",
    "- $ h_t^{(bwd)} $ → Capturing the meaning from the **right context**  \n",
    "- **Final representation** → **Concatenation** of both  \n",
    "\n",
    "**Example:**  \n",
    "For **\"love\"** in `\"I love AI\"`, we got:\n",
    "\n",
    "$$\n",
    "h_2 = [0.765, 0.851, 0.742, 0.831]\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- $ 0.765, 0.851 $ capture **past information** (from \"I\")  \n",
    "- $ 0.742, 0.831 $ capture **future information** (from \"AI\")  \n",
    "\n",
    "Thus, `\"love\"` is **better understood** with the full sentence in mind. 💡  \n",
    "\n",
    "\n",
    "\n",
    "## **4️⃣ BiRNN Is More Powerful Than Simple RNN**\n",
    "Regular RNNs have a **vanishing gradient problem**, making them struggle to capture **long-range dependencies**.  \n",
    "\n",
    "- **BiRNN helps solve this** because it gets **two different perspectives**, making it **better at learning complex relationships** between words.  \n",
    "- This is why BiRNN is often used in **speech recognition, machine translation, and question-answering systems**.\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Summary: What Our Calculations Showed**\n",
    "✅ **BiRNN processes both past and future** at the same time.  \n",
    "✅ **Each word's meaning is enhanced by its surrounding words**.  \n",
    "✅ **Final representation is a fusion of two different contexts**, making the model more powerful than a standard RNN.  \n",
    "✅ **Works great for NLP tasks like sentiment analysis, speech recognition, and machine translation.**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
