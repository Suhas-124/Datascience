{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü **Bidirectional Recurrent Neural Networks (BiRNN) - A Full and Colorful Guide!** üöÄ  \n",
    "\n",
    "### **1Ô∏è‚É£ What is a Bidirectional RNN?**  \n",
    "Imagine you're watching a movie üé¨, but instead of seeing the whole scene, you only see frames one by one in a forward sequence. You might **miss important context** from future events. Wouldn‚Äôt it be amazing if you could **see both past and future** at the same time? ü§Ø  \n",
    "\n",
    "That‚Äôs exactly what **Bidirectional Recurrent Neural Networks (BiRNNs)** do! Instead of processing sequences in just one direction (like a regular RNN), **BiRNNs process them in both forward and backward directions** at the same time. üîÑ This makes them super powerful for **context-heavy** tasks like speech recognition üé§, text processing üìñ, and language translation üåç.  \n",
    "\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ How Does a BiRNN Work? üõ†Ô∏è**  \n",
    "A BiRNN consists of **two RNNs running in parallel:**  \n",
    "\n",
    "1. **Forward RNN**: Reads the sequence from left to right ‚û°Ô∏è  \n",
    "2. **Backward RNN**: Reads the sequence from right to left ‚¨ÖÔ∏è  \n",
    "\n",
    "At each time step **t**, both RNNs process the input and produce two hidden states:  \n",
    "- One from the forward RNN: **$ h_t^{(fwd)} $**  \n",
    "- One from the backward RNN: **$ h_t^{(bwd)} $**  \n",
    "\n",
    "The final output at each time step is a combination (concatenation or sum) of these two hidden states:  \n",
    "$$\n",
    "h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "$$  \n",
    "\n",
    "### **üéØ Key Takeaway:**  \n",
    "üîπ Unlike a regular RNN, a BiRNN can use **both past and future information** at any given time step. This makes it way better for **understanding full context** in sequential data.  \n",
    "\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ Why is BiRNN Better? ü§î**  \n",
    "\n",
    "‚úÖ **More Context = More Accuracy**  \n",
    "   - A normal RNN only considers past words when predicting the next word, which can lead to **misinterpretations**.  \n",
    "   - BiRNNs can **consider both past and future words**, leading to **better predictions**! üéØ  \n",
    "\n",
    "‚úÖ **Great for Speech & NLP Tasks**  \n",
    "   - **Speech Recognition**: The meaning of a word can change based on future words. A BiRNN helps capture that nuance! üéôÔ∏è  \n",
    "   - **Machine Translation**: Words in different languages may have different orders. Understanding the full sentence structure helps a lot! üåç  \n",
    "   - **Named Entity Recognition (NER)**: Knowing the full sentence helps distinguish between similar words used in different contexts.  \n",
    "\n",
    "‚úÖ **Works with LSTMs & GRUs**  \n",
    "   - BiRNNs can use **LSTM (Long Short-Term Memory) cells** or **GRUs (Gated Recurrent Units)** to handle long sequences better. üß†  \n",
    "\n",
    "\n",
    "\n",
    "### **4Ô∏è‚É£ BiRNN in Action - Example with Python üêç**  \n",
    "\n",
    "Let‚Äôs see how a **Bidirectional LSTM** can be implemented in TensorFlow/Keras:  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Define a BiLSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(64, return_sequences=True), input_shape=(100, 10)),  # BiLSTM Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "üîπ Here, the **Bidirectional()** wrapper makes the LSTM layer process input in both directions! üîÑ  \n",
    "\n",
    "### **5Ô∏è‚É£ When to Use a BiRNN? ü§∑**  \n",
    "\n",
    "| ‚úÖ Use BiRNN When | ‚ùå Avoid BiRNN When |  \n",
    "|------------------|------------------|  \n",
    "| You need **full context** from past & future üîÑ | Your dataset is too large, as BiRNNs require **double computation** üíæ |  \n",
    "| Tasks involve **NLP**, **speech recognition**, or **translation** üó£Ô∏èüìñ | You're working with **real-time applications** where only past info is available ‚è≥ |  \n",
    "| You need better performance on **long sequences** üß† | The problem is **too simple**, and a unidirectional RNN is enough ‚ö° |  \n",
    "\n",
    "\n",
    "### **üåü Conclusion - Why BiRNN is a Game-Changer? üéÆ**  \n",
    "\n",
    "üöÄ BiRNNs are like **time travelers** in the world of neural networks. Instead of just relying on the past, they **peek into the future** and learn from both sides! This makes them **exceptionally powerful** for tasks like:  \n",
    "\n",
    "‚úîÔ∏è Speech Recognition üé§  \n",
    "‚úîÔ∏è Text Summarization üìÑ  \n",
    "‚úîÔ∏è Sentiment Analysis üòäüò°  \n",
    "‚úîÔ∏è Named Entity Recognition (NER) üìç  \n",
    "\n",
    "But remember! BiRNNs require **more computation** and are not always the best choice for real-time applications. **Choose wisely!** üßê  \n",
    "\n",
    "![](bid-rnn.jpg)\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üî• Full Architecture of a Bidirectional Recurrent Neural Network (BiRNN) üî•**  \n",
    "\n",
    "A **Bidirectional Recurrent Neural Network (BiRNN)** is an advanced type of **Recurrent Neural Network (RNN)** that processes sequences in **both forward and backward directions** to capture **past and future context**.  \n",
    "\n",
    "Let‚Äôs dive **deep into the architecture** step by step! üöÄ  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå 1. Basic Components of a BiRNN**  \n",
    "\n",
    "A **standard RNN** has the following components:  \n",
    "- **Input layer (X)**: The sequence of data (e.g., words in a sentence, frames in speech).  \n",
    "- **Hidden layer (h)**: Stores information from previous time steps.  \n",
    "- **Output layer (Y)**: Produces predictions at each time step.  \n",
    "\n",
    "A **Bidirectional RNN** consists of **two separate RNNs**:  \n",
    "- **Forward RNN** ‚Üí Processes input from **left to right** (past to future).  \n",
    "- **Backward RNN** ‚Üí Processes input from **right to left** (future to past).  \n",
    "\n",
    "At each time step $ t $, both RNNs produce hidden states, which are combined to form the final output.  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå 2. Step-by-Step Working of a BiRNN**  \n",
    "\n",
    "### **Step 1: Input Representation**  \n",
    "Let‚Äôs assume we have a sequence of length $ T $, where each input vector is $ X_t $ (a feature vector at time step $ t $).  \n",
    "\n",
    "$$\n",
    "X = [X_1, X_2, X_3, ..., X_T]\n",
    "$$\n",
    "\n",
    "Each input passes through **two RNNs**:  \n",
    "1. **Forward RNN** ‚Üí Generates hidden states from past to future.  \n",
    "2. **Backward RNN** ‚Üí Generates hidden states from future to past.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Forward and Backward Hidden States Computation**  \n",
    "\n",
    "- **Forward Hidden State ($ h_t^{(fwd)} $)**  \n",
    "  The forward RNN computes the hidden state at each time step using:  \n",
    "  $$\n",
    "  h_t^{(fwd)} = f(W_f X_t + U_f h_{t-1}^{(fwd)} + b_f)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_f $ = Input weight matrix for forward RNN  \n",
    "  - $ U_f $ = Hidden weight matrix for forward RNN  \n",
    "  - $ b_f $ = Bias  \n",
    "  - $ f $ = Activation function (usually tanh or ReLU)  \n",
    "\n",
    "- **Backward Hidden State ($ h_t^{(bwd)} $)**  \n",
    "  The backward RNN computes the hidden state moving from **$ T $ to $ 1 $**:  \n",
    "  $$\n",
    "  h_t^{(bwd)} = f(W_b X_t + U_b h_{t+1}^{(bwd)} + b_b)\n",
    "  $$  \n",
    "  where:  \n",
    "  - $ W_b $ = Input weight matrix for backward RNN  \n",
    "  - $ U_b $ = Hidden weight matrix for backward RNN  \n",
    "  - $ b_b $ = Bias  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Combining Forward and Backward States**  \n",
    "\n",
    "At each time step $ t $, the two hidden states **($ h_t^{(fwd)} $ and $ h_t^{(bwd)} $)** are combined into a single hidden state $ h_t $. This can be done in different ways:  \n",
    "- **Concatenation** (most common):  \n",
    "  $$\n",
    "  h_t = [h_t^{(fwd)}; h_t^{(bwd)}]\n",
    "  $$\n",
    "- **Sum**:  \n",
    "  $$\n",
    "  h_t = h_t^{(fwd)} + h_t^{(bwd)}\n",
    "  $$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Output Layer**  \n",
    "\n",
    "The final output $ Y_t $ at each time step is computed as:  \n",
    "$$\n",
    "Y_t = g(W_o h_t + b_o)\n",
    "$$  \n",
    "where:  \n",
    "- $ W_o $ = Output weight matrix  \n",
    "- $ b_o $ = Bias  \n",
    "- $ g $ = Activation function (e.g., softmax for classification)  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå 3. Full Architecture Diagram of BiRNN**  \n",
    "\n",
    "```\n",
    "      Input Sequence: [ X1,  X2,  X3,  X4,  X5]\n",
    "                        ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì    \n",
    "      Forward RNN:   ‚Üí h1 ‚Üí h2 ‚Üí h3 ‚Üí h4 ‚Üí h5 ‚Üí  \n",
    "                         ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì    \n",
    "      Backward RNN:  ‚Üê h1 ‚Üê h2 ‚Üê h3 ‚Üê h4 ‚Üê h5 ‚Üê  \n",
    "                        ‚Üì    ‚Üì    ‚Üì    ‚Üì    ‚Üì    \n",
    "      Final Output:  [ Y1,  Y2,  Y3,  Y4,  Y5]\n",
    "```\n",
    "\n",
    "- The **forward hidden states** move **left to right**.  \n",
    "- The **backward hidden states** move **right to left**.  \n",
    "- The **final hidden state at each time step** is a combination of both.  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå 4. Advantages of BiRNN üöÄ**  \n",
    "\n",
    "‚úÖ **Uses full context** (both past & future).  \n",
    "‚úÖ **Improves accuracy** in NLP, speech recognition, and time series tasks.  \n",
    "‚úÖ **Works well with LSTM & GRU for long-term dependencies.**  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå 5. Implementing BiRNN in Python (TensorFlow/Keras) üêç**  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Bidirectional, SimpleRNN, Dense\n",
    "\n",
    "# Define a Bidirectional RNN Model\n",
    "model = Sequential([\n",
    "    Bidirectional(SimpleRNN(64, return_sequences=True), input_shape=(100, 10)),  # BiRNN Layer\n",
    "    Dense(1, activation='sigmoid')  # Output Layer\n",
    "])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "## **üìå 6. When to Use BiRNN vs. Unidirectional RNN?**  \n",
    "\n",
    "| Feature  | Unidirectional RNN  | Bidirectional RNN  |\n",
    "|----------|--------------------|--------------------|\n",
    "| **Direction** | Forward only ‚û°Ô∏è  | Forward + Backward üîÑ |\n",
    "| **Context** | Only past context üìú | Both past & future context üèÜ |\n",
    "| **Computational Cost** | Lower üí∞ | Higher ‚ö° |\n",
    "| **Use Case** | Real-time tasks (e.g., online chatbots) üí¨ | NLP, speech, translation üåç |\n",
    "\n",
    "\n",
    "## **üî• Conclusion: Why BiRNN is a Game-Changer?**  \n",
    "\n",
    "üöÄ **Bidirectional RNNs are like superheroes** in sequential tasks! Unlike normal RNNs that only see the past, BiRNNs **see both past and future at the same time**, making them extremely powerful for **speech recognition**, **text processing**, **machine translation**, and more! üí°  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's take a simple sentence and manually work through how a **Bidirectional Recurrent Neural Network (BiRNN)** processes it. This will involve:  \n",
    "\n",
    "1Ô∏è‚É£ **Choosing a sentence**  \n",
    "2Ô∏è‚É£ **Assigning word embeddings**  \n",
    "3Ô∏è‚É£ **Forward pass calculations**  \n",
    "4Ô∏è‚É£ **Backward pass calculations**  \n",
    "5Ô∏è‚É£ **Combining hidden states**  \n",
    "6Ô∏è‚É£ **Generating output**  \n",
    "\n",
    "## **üìå Sentence: \"I love AI\"**\n",
    "We‚Äôll assume this is a 3-word sequence:  \n",
    "\n",
    "$$\n",
    "X = [\"I\", \"love\", \"AI\"]\n",
    "$$\n",
    "\n",
    "Each word will be represented as a **3D embedding vector** (to keep it simple).  \n",
    "\n",
    "| Word  | Embedding (3D Vector) |\n",
    "|--------|----------------|\n",
    "| \"I\"      | [0.1, 0.3, 0.5] |\n",
    "| \"love\"   | [0.2, 0.6, 0.8] |\n",
    "| \"AI\"     | [0.3, 0.7, 0.9] |\n",
    "\n",
    "\n",
    "## **üõ† Step 1: Initialize Parameters**\n",
    "BiRNN consists of **two RNNs**, one running **forward** and one **backward**. Each has:  \n",
    "\n",
    "- **Weight Matrices (Input ‚Üí Hidden State)**\n",
    "  - $ W_f $ (Forward)\n",
    "  - $ W_b $ (Backward)  \n",
    "\n",
    "- **Weight Matrices (Hidden State ‚Üí Next Hidden State)**\n",
    "  - $ U_f $ (Forward)\n",
    "  - $ U_b $ (Backward)  \n",
    "\n",
    "- **Bias Vectors**\n",
    "  - $ b_f $ (Forward)\n",
    "  - $ b_b $ (Backward)  \n",
    "\n",
    "For simplicity, let‚Äôs assume:  \n",
    "\n",
    "$$\n",
    "W_f = W_b =\n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_f = U_b =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_f = b_b =\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üõ† Step 2: Forward Pass (Processing left to right)**  \n",
    "\n",
    "### **üîπ Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh(W_f X_1 + U_f h_0 + b_f)\n",
    "$$\n",
    "\n",
    "Since initial **hidden state** is **0**,  \n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.3 \\\\ 0.5\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0 \\\\ 0\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "(0.5 \\times 0.1) + (0.3 \\times 0.3) + (0.2 \\times 0.5) \\\\ \n",
    "(0.4 \\times 0.1) + (0.7 \\times 0.3) + (0.6 \\times 0.5)\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.05 + 0.09 + 0.1 \\\\ \n",
    "0.04 + 0.21 + 0.3\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.34 \\\\ 0.75\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Approximating **tanh function**:  \n",
    "$$\n",
    "\\tanh(0.34) \\approx 0.327, \\quad \\tanh(0.75) \\approx 0.635\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1^{(fwd)} = \n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh(W_f X_2 + U_f h_1^{(fwd)} + b_f)\n",
    "$$\n",
    "\n",
    "Using **previous hidden state**:\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\tanh \\left(\n",
    "\\begin{bmatrix} \n",
    "0.5 & 0.3 & 0.2 \\\\\n",
    "0.4 & 0.7 & 0.6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\ 0.6 \\\\ 0.8\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.5 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "0.327 \\\\ 0.635\n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\ 0.2\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "(Similarly, calculating matrix multiplications and applying **tanh**, we get:)\n",
    "\n",
    "$$\n",
    "h_2^{(fwd)} = \\begin{bmatrix} 0.765 \\\\ 0.851 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Time Step 3: \"AI\"**\n",
    "Following the same process:\n",
    "\n",
    "$$\n",
    "h_3^{(fwd)} = \\begin{bmatrix} 0.88 \\\\ 0.92 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üõ† Step 3: Backward Pass (Processing right to left)**\n",
    "We now process in **reverse order**:\n",
    "\n",
    "### **üîπ Time Step 3: \"AI\"**\n",
    "$$\n",
    "h_3^{(bwd)} = \\tanh(W_b X_3 + U_b h_0 + b_b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3^{(bwd)} = \\begin{bmatrix} 0.805 \\\\ 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **üîπ Time Step 2: \"love\"**\n",
    "$$\n",
    "h_2^{(bwd)} = \\begin{bmatrix} 0.742 \\\\ 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### **üîπ Time Step 1: \"I\"**\n",
    "$$\n",
    "h_1^{(bwd)} = \\begin{bmatrix} 0.658 \\\\ 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üõ† Step 4: Combining Forward & Backward States**\n",
    "For each word, we concatenate both hidden states:\n",
    "\n",
    "$$\n",
    "h_1 = [h_1^{(fwd)}; h_1^{(bwd)}] = \\begin{bmatrix} 0.327 & 0.635 & 0.658 & 0.789 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = [h_2^{(fwd)}; h_2^{(bwd)}] = \\begin{bmatrix} 0.765 & 0.851 & 0.742 & 0.831 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = [h_3^{(fwd)}; h_3^{(bwd)}] = \\begin{bmatrix} 0.88 & 0.92 & 0.805 & 0.921 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üîÆ Step 5: Output Layer**\n",
    "If this is for **classification**, we would pass the final **concatenated hidden states** through a softmax layer.\n",
    "\n",
    "\n",
    "\n",
    "## **üéØ Conclusion**\n",
    "- BiRNN processes **both past & future context**.\n",
    "- Each word has **two hidden states** (forward + backward).\n",
    "- The **final hidden state** is a combination of **both directions**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üîç What Do These Calculations Signify?**  \n",
    "\n",
    "The calculations we performed help us **understand how Bi-directional RNN (BiRNN) processes text step by step**. Let‚Äôs break it down into **key insights**:\n",
    "\n",
    "\n",
    "\n",
    "## **1Ô∏è‚É£ BiRNN Captures Both Past & Future Context**  \n",
    "Unlike a normal **unidirectional RNN**, which processes the sequence **left to right** (or right to left), BiRNN does **both simultaneously**.  \n",
    "\n",
    "- **Forward RNN:** Moves from **left to right** (normal reading order).  \n",
    "- **Backward RNN:** Moves from **right to left** (reverse reading order).  \n",
    "- The **final hidden state** for each word is a **combination of both directions**, giving the model **fuller context**.  \n",
    "\n",
    "**Example:**\n",
    "For the word `\"love\"` in `\"I love AI\"`,  \n",
    "- The **forward RNN** only sees `\"I love ...\"`,  \n",
    "- The **backward RNN** sees `\"... love AI\"`.  \n",
    "\n",
    "So, `\"love\"` gets influenced by **both \"I\" (past) and \"AI\" (future)**, giving it **richer meaning**.\n",
    "\n",
    "\n",
    "\n",
    "## **2Ô∏è‚É£ Word Meaning Depends on Full Context**  \n",
    "Consider this sentence:\n",
    "\n",
    "> **\"He plays the bass.\"**  \n",
    "> **\"He caught a bass.\"**\n",
    "\n",
    "The word **\"bass\"** has **two meanings** (musical instrument vs. fish).  \n",
    "\n",
    "- A **unidirectional RNN** (left-to-right) would process `\"He caught a ...\"` before seeing `\"bass\"`, which is **not enough to disambiguate** the meaning.  \n",
    "- A **BiRNN** processes both `\"caught a\"` and the words **after** `\"bass\"` at the same time, giving it more information to determine the meaning.\n",
    "\n",
    "**This is crucial for NLP tasks like Named Entity Recognition, Sentiment Analysis, and Speech Recognition!** üéØ\n",
    "\n",
    "\n",
    "\n",
    "## **3Ô∏è‚É£ Why Do We Combine Forward & Backward States?**  \n",
    "At each time step, we computed **two hidden states**:\n",
    "- $ h_t^{(fwd)} $ ‚Üí Capturing the meaning from the **left context**  \n",
    "- $ h_t^{(bwd)} $ ‚Üí Capturing the meaning from the **right context**  \n",
    "- **Final representation** ‚Üí **Concatenation** of both  \n",
    "\n",
    "**Example:**  \n",
    "For **\"love\"** in `\"I love AI\"`, we got:\n",
    "\n",
    "$$\n",
    "h_2 = [0.765, 0.851, 0.742, 0.831]\n",
    "$$\n",
    "\n",
    "This means:\n",
    "- $ 0.765, 0.851 $ capture **past information** (from \"I\")  \n",
    "- $ 0.742, 0.831 $ capture **future information** (from \"AI\")  \n",
    "\n",
    "Thus, `\"love\"` is **better understood** with the full sentence in mind. üí°  \n",
    "\n",
    "\n",
    "\n",
    "## **4Ô∏è‚É£ BiRNN Is More Powerful Than Simple RNN**\n",
    "Regular RNNs have a **vanishing gradient problem**, making them struggle to capture **long-range dependencies**.  \n",
    "\n",
    "- **BiRNN helps solve this** because it gets **two different perspectives**, making it **better at learning complex relationships** between words.  \n",
    "- This is why BiRNN is often used in **speech recognition, machine translation, and question-answering systems**.\n",
    "\n",
    "\n",
    "\n",
    "## **üéØ Summary: What Our Calculations Showed**\n",
    "‚úÖ **BiRNN processes both past and future** at the same time.  \n",
    "‚úÖ **Each word's meaning is enhanced by its surrounding words**.  \n",
    "‚úÖ **Final representation is a fusion of two different contexts**, making the model more powerful than a standard RNN.  \n",
    "‚úÖ **Works great for NLP tasks like sentiment analysis, speech recognition, and machine translation.**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
