{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåü What is GRU?  \n",
    "Imagine you‚Äôre reading a long novel üìñ, and you need to remember key points from previous chapters to understand the current one. That‚Äôs exactly what GRUs do in **sequence-based deep learning tasks**‚Äîthey **remember important information** and **forget unimportant details**, making them ideal for tasks like speech recognition üé§, machine translation üåé, and time series forecasting üìà.  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)**, but it's an **improved version** that solves the problem of *vanishing gradients* (which makes traditional RNNs forget long-term dependencies). It‚Äôs also a **lighter** alternative to LSTMs (Long Short-Term Memory) while maintaining **high accuracy**.\n",
    "\n",
    "\n",
    "\n",
    "## üèóÔ∏è GRU Architecture: The Magic Inside ‚ú®  \n",
    "\n",
    "A **GRU cell** has **two main gates** that control the flow of information:  \n",
    "\n",
    "### üîµ **1. Update Gate (Zt) ‚Äì \"Should I Remember?\"**  \n",
    "- Think of this as your **memory filter**. üß† It decides **how much of the past information to keep** and **how much of the new information to add**.  \n",
    "- If **Zt is close to 1**, the old memory stays. If it‚Äôs **close to 0**, it gets replaced with fresh new data.  \n",
    "\n",
    "### üî¥ **2. Reset Gate (Rt) ‚Äì \"Should I Forget?\"**  \n",
    "- This gate determines how much of the **past information to erase**. üöÆ  \n",
    "- If Rt is **0**, the old memory is completely reset (like starting a fresh page üìÑ). If Rt is **1**, it keeps the entire past context.  \n",
    "\n",
    "\n",
    "\n",
    "## üî• How GRU Works (Step-by-Step)  \n",
    "\n",
    "Let‚Äôs say you‚Äôre watching a TV series üé¨, and GRU is helping you remember the **important plot points** while forgetting unnecessary side details.  \n",
    "\n",
    "1Ô∏è‚É£ **Reset Gate (Rt) acts first**: It decides how much of the previous memory is relevant for the current moment.  \n",
    "2Ô∏è‚É£ **New candidate memory is created**: It mixes the past with the present input to generate a fresh **contextual memory**.  \n",
    "3Ô∏è‚É£ **Update Gate (Zt) kicks in**: It blends the old memory with the new one, deciding what to **carry forward** and what to **discard**.  \n",
    "4Ô∏è‚É£ **Final memory is updated**: The result is a **refined memory state** that is carried to the next time step.  \n",
    "\n",
    "### üß† Formula Representation:  \n",
    "#### 1Ô∏è‚É£ Reset Gate:  \n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "#### 2Ô∏è‚É£ Update Gate:  \n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "#### 3Ô∏è‚É£ Candidate Hidden State (New Memory Proposal):  \n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "#### 4Ô∏è‚É£ Final Hidden State (Final Memory for the Next Step):  \n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "- Here, **œÉ (sigma) is the sigmoid activation function** üåÄ, which ensures the values are between 0 and 1.  \n",
    "- **tanh is used** to maintain values between -1 and 1, keeping the balance between **positive and negative information**.  \n",
    "\n",
    "## üöÄ Why GRU? (Compared to LSTM & RNN)  \n",
    "\n",
    "| Feature        | RNN üèõÔ∏è | LSTM üèãÔ∏è | GRU ‚ö° |\n",
    "|--------------|--------|--------|------|\n",
    "| Handles Long Sequences? | ‚ùå No (Vanishing Gradient) | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| Number of Gates | ‚ùå None | üü¢ 3 (Forget, Input, Output) | üîµ 2 (Reset, Update) |\n",
    "| Training Time | ‚è≥ Slow | ‚è≥ Slower | ‚ö° Faster |\n",
    "| Memory Efficiency | ‚úÖ Low | ‚ùå High | ‚úÖ Moderate |\n",
    "| Performance | ü§î Decent | ‚úÖ Best for Long Texts | ‚ö° Fast & Effective |\n",
    "\n",
    "**Why choose GRU?**  \n",
    "- **Faster than LSTMs** because it has **fewer gates** and computations.  \n",
    "- **Better than vanilla RNNs** because it **remembers long-term dependencies**.  \n",
    "- **Great for real-time NLP applications** like **speech recognition**, **chatbots**, and **predictive text**.  \n",
    "\n",
    "\n",
    "\n",
    "## üéØ Where is GRU Used?  \n",
    "\n",
    "üîπ **Speech-to-Text** (e.g., Google Assistant, Siri) üó£Ô∏è  \n",
    "üîπ **Machine Translation** (e.g., Google Translate) üåé  \n",
    "üîπ **Stock Price Prediction** üìä  \n",
    "üîπ **Music Generation** üéµ  \n",
    "üîπ **Chatbots & Virtual Assistants** ü§ñ  \n",
    "\n",
    "\n",
    "\n",
    "## üé® Fun Analogy: GRU as a Smart Diary üìì  \n",
    "\n",
    "Imagine you‚Äôre keeping a **daily journal**.  \n",
    "- **Reset Gate (Rt)**: Decides **whether to remove old notes** or keep them.  \n",
    "- **Update Gate (Zt)**: Decides **if a new event should overwrite an old one**.  \n",
    "- **Final Memory (ht)**: The polished diary entry that **carries forward** into the next day!  \n",
    "\n",
    "That‚Äôs how GRU **efficiently maintains and updates memory** while keeping only the **important parts**! üéØ\n",
    "\n",
    "\n",
    "\n",
    "## üî• Summary  \n",
    "\n",
    "üéØ **GRU is a powerful, lightweight RNN variant** that efficiently processes sequential data.  \n",
    "‚ö° **It has two gates (Reset & Update) instead of three like LSTM**, making it faster and simpler.  \n",
    "üß† **It solves the vanishing gradient problem**, making it ideal for handling **long-term dependencies**.  \n",
    "üöÄ **Used in NLP, speech recognition, finance, and more!**  \n",
    "\n",
    "Hope that made GRU fun and colorful for you! üé®‚ú® Let me know if you need a deeper dive into any part! üöÄüí°\n",
    "\n",
    "![](gru.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let‚Äôs break down the **full architecture of a GRU (Gated Recurrent Unit)** in detail. We'll explore:  \n",
    "\n",
    "‚úÖ **High-Level Overview**  \n",
    "‚úÖ **Step-by-Step Working of GRU Cell**  \n",
    "‚úÖ **Mathematical Formulation**  \n",
    "‚úÖ **Computation Flow**  \n",
    "‚úÖ **Comparison with LSTM**  \n",
    "‚úÖ **Advantages & Use Cases**  \n",
    "\n",
    "Let‚Äôs dive in! üöÄüéØ  \n",
    "\n",
    "\n",
    "\n",
    "# **üåü High-Level Overview of GRU**  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)** designed to handle sequential data (e.g., time series, speech, language).  \n",
    "\n",
    "üîπ **Why GRU?**  \n",
    "- Standard RNNs suffer from the **vanishing gradient problem**, making it hard to learn **long-term dependencies**.  \n",
    "- GRUs, like LSTMs, use **gates to control information flow** but are computationally more efficient.  \n",
    "- They have **fewer parameters** than LSTMs, making them **faster to train** while retaining strong performance.  \n",
    "\n",
    "### **üîß GRU Components:**  \n",
    "A **GRU cell** consists of:  \n",
    "1Ô∏è‚É£ **Update Gate ($Z_t$)** ‚Üí Decides **how much past information to keep**.  \n",
    "2Ô∏è‚É£ **Reset Gate ($R_t$)** ‚Üí Decides **how much past information to forget**.  \n",
    "3Ô∏è‚É£ **Candidate Hidden State ($\\tilde{h}_t$)** ‚Üí A new potential memory update.  \n",
    "4Ô∏è‚É£ **Final Hidden State ($h_t$)** ‚Üí The actual memory that carries forward.  \n",
    "\n",
    "\n",
    "\n",
    "# **üèóÔ∏è GRU Architecture (Step-by-Step)**\n",
    "The **GRU cell** takes two inputs at time step $ t $:  \n",
    "üîπ **$ x_t $ (Current input)** ‚Äì This is the new data point (word, feature, etc.).  \n",
    "üîπ **$ h_{t-1} $ (Previous hidden state)** ‚Äì This stores past information.  \n",
    "\n",
    "### **üîµ Step 1: Compute the Reset Gate $ R_t $**\n",
    "- The **reset gate** decides whether to erase part of the past memory.  \n",
    "- Uses a **sigmoid activation** ($ \\sigma $) to squash values between 0 and 1.  \n",
    "\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "üëâ If $ R_t $ is **0**, it forgets the past.  \n",
    "üëâ If $ R_t $ is **1**, it keeps the full past memory.  \n",
    "\n",
    "### **üî¥ Step 2: Compute the Update Gate $ Z_t $**\n",
    "- The **update gate** decides how much of the **past hidden state** to retain versus **how much to update**.  \n",
    "- Also uses **sigmoid activation** to control memory update.  \n",
    "\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "üëâ If $ Z_t $ is **0**, it replaces the old memory entirely.  \n",
    "üëâ If $ Z_t $ is **1**, it keeps the old memory.  \n",
    "\n",
    "### **üü¢ Step 3: Compute the Candidate Hidden State $ \\tilde{h}_t $**\n",
    "- A **new candidate memory** is computed using the reset gate.  \n",
    "- Uses **tanh activation** to balance positive/negative values.  \n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "üëâ If **reset gate is 0**, it ignores past information.  \n",
    "üëâ If **reset gate is 1**, it uses both past and current input.  \n",
    "\n",
    "### **üü† Step 4: Compute the Final Hidden State $ h_t $**\n",
    "- The final output is a **blend of the old memory ($ h_{t-1} $) and new candidate memory ($ \\tilde{h}_t $)** controlled by the update gate.  \n",
    "\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "üëâ If $ Z_t $ is **0**, it fully updates with new memory.  \n",
    "üëâ If $ Z_t $ is **1**, it keeps old memory.  \n",
    "\n",
    "\n",
    "\n",
    "# **üìä Computation Flow in a GRU Cell**  \n",
    "\n",
    "### **üõ†Ô∏è Forward Pass**  \n",
    "\n",
    "1Ô∏è‚É£ **Compute Reset Gate:**  \n",
    "   - $ R_t = \\sigma(W_r [h_{t-1}, x_t] + b_r) $  \n",
    "\n",
    "2Ô∏è‚É£ **Compute Update Gate:**  \n",
    "   - $ Z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z) $  \n",
    "\n",
    "3Ô∏è‚É£ **Compute Candidate Hidden State:**  \n",
    "   - $ \\tilde{h}_t = \\tanh(W_h [R_t \\ast h_{t-1}, x_t] + b_h) $  \n",
    "\n",
    "4Ô∏è‚É£ **Compute Final Hidden State:**  \n",
    "   - $ h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t $  \n",
    "\n",
    "### **üîÑ Backpropagation (Training GRU)**\n",
    "GRUs are trained using **Backpropagation Through Time (BPTT)**, where:  \n",
    "- **Gradients of loss are computed** using **chain rule**.  \n",
    "- **Weights are updated** using **gradient descent**.  \n",
    "- **Gates regulate gradient flow**, preventing vanishing gradients.  \n",
    "\n",
    "# **üî¨ GRU vs. LSTM: Key Differences**\n",
    "| Feature | GRU ‚ö° | LSTM üèãÔ∏è |\n",
    "|---------|------|------|\n",
    "| Number of Gates | 2 (Update, Reset) | 3 (Input, Forget, Output) |\n",
    "| Complexity | ‚úÖ Less | ‚ùå More |\n",
    "| Performance | ‚ö° Fast | üèÜ Better for long texts |\n",
    "| Memory Requirement | ‚úÖ Less | ‚ùå More |\n",
    "| Suitable for | Speech, NLP, real-time apps | Long documents, text generation |\n",
    "\n",
    "\n",
    "\n",
    "# **üî• Advantages of GRU**\n",
    "‚úÖ **Faster Training** ‚Äì Fewer gates than LSTM = Faster updates.  \n",
    "‚úÖ **Solves Vanishing Gradient Problem** ‚Äì Retains long-term dependencies.  \n",
    "‚úÖ **Computationally Efficient** ‚Äì Great for real-time applications.  \n",
    "‚úÖ **Performs Well on Small Datasets** ‚Äì Fewer parameters make it ideal for small-scale problems.  \n",
    "\n",
    "\n",
    "\n",
    "# **üöÄ Where is GRU Used?**\n",
    "üìå **Speech Recognition** (Google Assistant, Alexa) üó£Ô∏è  \n",
    "üìå **Machine Translation** (Google Translate) üåç  \n",
    "üìå **Stock Market Prediction** üìà  \n",
    "üìå **Chatbots & AI Assistants** ü§ñ  \n",
    "üìå **Music Generation** üéµ  \n",
    "\n",
    "\n",
    "\n",
    "# **üéØ Summary**\n",
    "‚úî **GRU is a simplified LSTM** with **fewer gates** and **faster computations**.  \n",
    "‚úî **It solves vanishing gradient issues** and **remembers long-term dependencies**.  \n",
    "‚úî **Uses Reset & Update Gates** to control memory updates.  \n",
    "‚úî **Faster than LSTM** but still **performs well in sequence-based tasks**.  \n",
    "‚úî **Ideal for speech, NLP, real-time applications**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let‚Äôs manually walk through the GRU computations using a simple example. This will give you a **step-by-step breakdown of how a GRU cell processes a sentence**, calculating each gate and hidden state update.  \n",
    "\n",
    "\n",
    "\n",
    "### **üìù Example Sentence:**  \n",
    "üëâ **\"AI is amazing\"**  \n",
    "We will process it word by word using a GRU with a **hidden size of 2** (to keep calculations manageable).  \n",
    "\n",
    "## **üîß Step 1: Define Inputs & Initial Parameters**\n",
    "### **Word Encoding (Input Vectors)**\n",
    "We assume each word is converted into a 3-dimensional vector (using Word Embeddings). Let‚Äôs define:  \n",
    "\n",
    "| Word | Input Vector (\\( x_t \\)) |\n",
    "|-------|----------------|\n",
    "| **AI** | \\([0.5, 0.1, 0.4]\\) |\n",
    "| **is** | \\([0.2, 0.7, 0.3]\\) |\n",
    "| **amazing** | \\([0.6, 0.9, 0.5]\\) |\n",
    "\n",
    "### **Initial Hidden State \\( h_0 \\)**\n",
    "Since it's the first step, we initialize:  \n",
    "$$\n",
    "h_0 = [0, 0] \\quad \\text{(2-dimensional hidden state)}\n",
    "$$\n",
    "\n",
    "\n",
    "## **üõ†Ô∏è Step 2: Define GRU Parameters**\n",
    "We need **weight matrices** and **biases** for reset and update gates. We assume:  \n",
    "\n",
    "**Reset Gate (\\( R_t \\)):**  \n",
    "$$\n",
    "W_r =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad U_r =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix},\n",
    "\\quad b_r = [0.1, 0.2]\n",
    "$$\n",
    "\n",
    "**Update Gate (\\( Z_t \\)):**  \n",
    "$$\n",
    "W_z =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.7 \\\\\n",
    "0.5 & 0.2 & 0.6\n",
    "\\end{bmatrix},\n",
    "\\quad U_z =\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.5 \\\\\n",
    "0.3 & 0.8\n",
    "\\end{bmatrix},\n",
    "\\quad b_z = [0.05, 0.1]\n",
    "$$\n",
    "\n",
    "**Candidate Hidden State (\\( \\tilde{h}_t \\)):**  \n",
    "$$\n",
    "W_h =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix},\n",
    "\\quad U_h =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix},\n",
    "\\quad b_h = [0.2, 0.3]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **‚ö° Step 3: Compute for First Word (\"AI\")**  \n",
    "### **üî¥ Reset Gate \\( R_1 \\)**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\cdot 0.5) + (0.5 \\cdot 0.1) + (0.1 \\cdot 0.4) + 0.1 \\\\\n",
    "(0.3 \\cdot 0.5) + (0.7 \\cdot 0.1) + (0.2 \\cdot 0.4) + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.1 + 0.05 + 0.04 + 0.1 \\\\\n",
    "0.15 + 0.07 + 0.08 + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.29 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Applying **sigmoid** (\\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)):  \n",
    "\n",
    "$$\n",
    "R_1 =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(0.29) \\\\\n",
    "\\sigma(0.5)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.572 \\\\\n",
    "0.622\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üü° Update Gate \\( Z_1 \\)**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "\n",
    "Using similar calculations, we get:  \n",
    "\n",
    "$$\n",
    "Z_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üü¢ Candidate Hidden State \\( \\tilde{h}_1 \\)**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot (R_1 \\ast h_0) + U_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "\n",
    "Since \\( h_0 = 0 \\), the term \\( R_1 \\ast h_0 \\) vanishes, and we compute:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.29 + 0.2 \\\\\n",
    "0.49 + 0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "=\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.49 \\\\\n",
    "0.79\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Approximating \\( \\tanh(x) \\), we get:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üîµ Final Hidden State \\( h_1 \\)**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "(1 - 0.655) \\\\\n",
    "(1 - 0.710)\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "(0.345) \\times 0.45 \\\\\n",
    "(0.290) \\times 0.66\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.155 \\\\\n",
    "0.191\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üìå Repeat for \"is\" and \"amazing\"**\n",
    "Now, \\( h_1 \\) is used for the next step, and the process repeats.\n",
    "\n",
    "This shows **how a GRU cell updates memory word-by-word!** üöÄ Let me know if you want more manual calculations or insights! üéØ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate how a **GRU (Gated Recurrent Unit)** processes a sentence. We'll analyze how it **keeps important information** and **forgets unimportant details** using an actual example.  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Example Sentence:**\n",
    "Let's take a simple sentence:\n",
    "> **\"I love deep learning.\"**  \n",
    "\n",
    "We'll process it **word by word** through a GRU and observe how it decides what to keep and what to forget.\n",
    "\n",
    "## **üîπ Step 1: Define Initial Setup**\n",
    "Each word is represented as a **word vector** $ x_t $. Assume we have:  \n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|------|---------------------|\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "We also assume that the **hidden state** $ h_t $ has two units, so it‚Äôs a 2D vector.  \n",
    "\n",
    "\n",
    "The **GRU parameters** (randomly chosen for simplicity):  \n",
    "\n",
    "- **Update Gate Weights** $ W_z, U_z $  \n",
    "- **Reset Gate Weights** $ W_r, U_r $  \n",
    "- **Candidate State Weights** $ W_h, U_h $  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Step 2: How GRU Decides What to Keep or Forget?**  \n",
    "GRU works with **three key equations** at every time step $ t $:  \n",
    "\n",
    "### **1Ô∏è‚É£ Reset Gate $ R_t $** (Decides whether to erase past memory)\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot x_t + U_r \\cdot h_{t-1} + b_r)\n",
    "$$\n",
    "- If $ R_t $ is **close to 0**, it forgets old information.\n",
    "- If $ R_t $ is **close to 1**, it keeps old memory.  \n",
    "\n",
    "### **2Ô∏è‚É£ Update Gate $ Z_t $** (Decides whether to update hidden state)\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot x_t + U_z \\cdot h_{t-1} + b_z)\n",
    "$$\n",
    "- If $ Z_t $ is **close to 0**, it **replaces** the old state with new info.  \n",
    "- If $ Z_t $ is **close to 1**, it **keeps** the old memory.  \n",
    "\n",
    "### **3Ô∏è‚É£ Candidate Hidden State $ \\tilde{h}_t $**\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot x_t + U_h \\cdot (R_t \\ast h_{t-1}) + b_h)\n",
    "$$\n",
    "This is the new hidden state, considering **reset gate influence**.  \n",
    "\n",
    "### **4Ô∏è‚É£ Final Hidden State**\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$\n",
    "The final hidden state is a combination of **past and new** information.  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Step 3: Manual Calculation for Each Word**\n",
    "Let‚Äôs assume:\n",
    "\n",
    "- $ h_0 = [0, 0] $ (initial hidden state)  \n",
    "- We calculate for each word step by step.\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"I\"**  \n",
    "#### **1Ô∏è‚É£ Reset Gate Calculation**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "Since $ h_0 = [0, 0] $, this simplifies to:\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot [0.5, 0.1, 0.3] + b_r)\n",
    "$$\n",
    "Let‚Äôs say:\n",
    "$$\n",
    "R_1 = [0.8, 0.6]\n",
    "$$\n",
    "Since values are **close to 1**, we **keep past memory**.\n",
    "\n",
    "#### **2Ô∏è‚É£ Update Gate Calculation**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "Again, since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + b_z)\n",
    "$$\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "Z_1 = [0.9, 0.7]\n",
    "$$\n",
    "Since $ Z_1 $ is **close to 1**, GRU **keeps most of the old hidden state** (which is zero for now).\n",
    "\n",
    "#### **3Ô∏è‚É£ Compute Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + U_h \\cdot (R_1 \\ast h_0) + b_h)\n",
    "$$\n",
    "Since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "\\tilde{h}_1 = [0.3, 0.4]\n",
    "$$\n",
    "\n",
    "#### **4Ô∏è‚É£ Compute Final Hidden State**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "$$\n",
    "= [0.9, 0.7] \\ast [0, 0] + [0.1, 0.3] \\ast [0.3, 0.4]\n",
    "$$\n",
    "$$\n",
    "= [0.03, 0.12]\n",
    "$$\n",
    "üöÄ **Hidden state at time step 1**: $ h_1 = [0.03, 0.12] $\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"love\"**  \n",
    "Now, we use $ h_1 = [0.03, 0.12] $.\n",
    "\n",
    "#### **1Ô∏è‚É£ Reset Gate**\n",
    "$$\n",
    "R_2 = \\sigma(W_r \\cdot x_2 + U_r \\cdot h_1 + b_r)\n",
    "$$\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "R_2 = [0.4, 0.2]\n",
    "$$\n",
    "Since $ R_2 $ is **low**, it **forgets some past memory**.\n",
    "\n",
    "#### **2Ô∏è‚É£ Update Gate**\n",
    "$$\n",
    "Z_2 = \\sigma(W_z \\cdot x_2 + U_z \\cdot h_1 + b_z)\n",
    "$$\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "Z_2 = [0.2, 0.6]\n",
    "$$\n",
    "Since $ Z_2 $ is **low for the first unit**, it **updates memory**.\n",
    "\n",
    "#### **3Ô∏è‚É£ Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_2 = \\tanh(W_h \\cdot x_2 + U_h \\cdot (R_2 \\ast h_1) + b_h)\n",
    "$$\n",
    "Let‚Äôs assume:\n",
    "$$\n",
    "\\tilde{h}_2 = [0.6, 0.5]\n",
    "$$\n",
    "\n",
    "#### **4Ô∏è‚É£ Final Hidden State**\n",
    "$$\n",
    "h_2 = Z_2 \\ast h_1 + (1 - Z_2) \\ast \\tilde{h}_2\n",
    "$$\n",
    "$$\n",
    "= [0.2, 0.6] \\ast [0.03, 0.12] + [0.8, 0.4] \\ast [0.6, 0.5]\n",
    "$$\n",
    "$$\n",
    "= [0.006, 0.072] + [0.48, 0.2]\n",
    "$$\n",
    "$$\n",
    "= [0.486, 0.272]\n",
    "$$\n",
    "\n",
    "üöÄ **Hidden state at time step 2**: $ h_2 = [0.486, 0.272] $  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Conclusion**\n",
    "- **\"I\"** ‚Üí Small memory update, since it‚Äôs a common word.  \n",
    "- **\"love\"** ‚Üí Memory updates more because it‚Äôs a strong emotional word.  \n",
    "- **GRU selectively keeps or forgets** based on context.  \n",
    "\n",
    "Would you like me to compute for \"deep\" and \"learning\" too? üöÄ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
