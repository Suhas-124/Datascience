{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 What is GRU?  \n",
    "Imagine you’re reading a long novel 📖, and you need to remember key points from previous chapters to understand the current one. That’s exactly what GRUs do in **sequence-based deep learning tasks**—they **remember important information** and **forget unimportant details**, making them ideal for tasks like speech recognition 🎤, machine translation 🌎, and time series forecasting 📈.  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)**, but it's an **improved version** that solves the problem of *vanishing gradients* (which makes traditional RNNs forget long-term dependencies). It’s also a **lighter** alternative to LSTMs (Long Short-Term Memory) while maintaining **high accuracy**.\n",
    "\n",
    "\n",
    "\n",
    "## 🏗️ GRU Architecture: The Magic Inside ✨  \n",
    "\n",
    "A **GRU cell** has **two main gates** that control the flow of information:  \n",
    "\n",
    "### 🔵 **1. Update Gate (Zt) – \"Should I Remember?\"**  \n",
    "- Think of this as your **memory filter**. 🧠 It decides **how much of the past information to keep** and **how much of the new information to add**.  \n",
    "- If **Zt is close to 1**, the old memory stays. If it’s **close to 0**, it gets replaced with fresh new data.  \n",
    "\n",
    "### 🔴 **2. Reset Gate (Rt) – \"Should I Forget?\"**  \n",
    "- This gate determines how much of the **past information to erase**. 🚮  \n",
    "- If Rt is **0**, the old memory is completely reset (like starting a fresh page 📄). If Rt is **1**, it keeps the entire past context.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 How GRU Works (Step-by-Step)  \n",
    "\n",
    "Let’s say you’re watching a TV series 🎬, and GRU is helping you remember the **important plot points** while forgetting unnecessary side details.  \n",
    "\n",
    "1️⃣ **Reset Gate (Rt) acts first**: It decides how much of the previous memory is relevant for the current moment.  \n",
    "2️⃣ **New candidate memory is created**: It mixes the past with the present input to generate a fresh **contextual memory**.  \n",
    "3️⃣ **Update Gate (Zt) kicks in**: It blends the old memory with the new one, deciding what to **carry forward** and what to **discard**.  \n",
    "4️⃣ **Final memory is updated**: The result is a **refined memory state** that is carried to the next time step.  \n",
    "\n",
    "### 🧠 Formula Representation:  \n",
    "#### 1️⃣ Reset Gate:  \n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "#### 2️⃣ Update Gate:  \n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "#### 3️⃣ Candidate Hidden State (New Memory Proposal):  \n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "#### 4️⃣ Final Hidden State (Final Memory for the Next Step):  \n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "- Here, **σ (sigma) is the sigmoid activation function** 🌀, which ensures the values are between 0 and 1.  \n",
    "- **tanh is used** to maintain values between -1 and 1, keeping the balance between **positive and negative information**.  \n",
    "\n",
    "## 🚀 Why GRU? (Compared to LSTM & RNN)  \n",
    "\n",
    "| Feature        | RNN 🏛️ | LSTM 🏋️ | GRU ⚡ |\n",
    "|--------------|--------|--------|------|\n",
    "| Handles Long Sequences? | ❌ No (Vanishing Gradient) | ✅ Yes | ✅ Yes |\n",
    "| Number of Gates | ❌ None | 🟢 3 (Forget, Input, Output) | 🔵 2 (Reset, Update) |\n",
    "| Training Time | ⏳ Slow | ⏳ Slower | ⚡ Faster |\n",
    "| Memory Efficiency | ✅ Low | ❌ High | ✅ Moderate |\n",
    "| Performance | 🤔 Decent | ✅ Best for Long Texts | ⚡ Fast & Effective |\n",
    "\n",
    "**Why choose GRU?**  \n",
    "- **Faster than LSTMs** because it has **fewer gates** and computations.  \n",
    "- **Better than vanilla RNNs** because it **remembers long-term dependencies**.  \n",
    "- **Great for real-time NLP applications** like **speech recognition**, **chatbots**, and **predictive text**.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 Where is GRU Used?  \n",
    "\n",
    "🔹 **Speech-to-Text** (e.g., Google Assistant, Siri) 🗣️  \n",
    "🔹 **Machine Translation** (e.g., Google Translate) 🌎  \n",
    "🔹 **Stock Price Prediction** 📊  \n",
    "🔹 **Music Generation** 🎵  \n",
    "🔹 **Chatbots & Virtual Assistants** 🤖  \n",
    "\n",
    "\n",
    "\n",
    "## 🎨 Fun Analogy: GRU as a Smart Diary 📓  \n",
    "\n",
    "Imagine you’re keeping a **daily journal**.  \n",
    "- **Reset Gate (Rt)**: Decides **whether to remove old notes** or keep them.  \n",
    "- **Update Gate (Zt)**: Decides **if a new event should overwrite an old one**.  \n",
    "- **Final Memory (ht)**: The polished diary entry that **carries forward** into the next day!  \n",
    "\n",
    "That’s how GRU **efficiently maintains and updates memory** while keeping only the **important parts**! 🎯\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 Summary  \n",
    "\n",
    "🎯 **GRU is a powerful, lightweight RNN variant** that efficiently processes sequential data.  \n",
    "⚡ **It has two gates (Reset & Update) instead of three like LSTM**, making it faster and simpler.  \n",
    "🧠 **It solves the vanishing gradient problem**, making it ideal for handling **long-term dependencies**.  \n",
    "🚀 **Used in NLP, speech recognition, finance, and more!**  \n",
    "\n",
    "Hope that made GRU fun and colorful for you! 🎨✨ Let me know if you need a deeper dive into any part! 🚀💡\n",
    "\n",
    "![](gru.jpg)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let’s break down the **full architecture of a GRU (Gated Recurrent Unit)** in detail. We'll explore:  \n",
    "\n",
    "✅ **High-Level Overview**  \n",
    "✅ **Step-by-Step Working of GRU Cell**  \n",
    "✅ **Mathematical Formulation**  \n",
    "✅ **Computation Flow**  \n",
    "✅ **Comparison with LSTM**  \n",
    "✅ **Advantages & Use Cases**  \n",
    "\n",
    "Let’s dive in! 🚀🎯  \n",
    "\n",
    "\n",
    "\n",
    "# **🌟 High-Level Overview of GRU**  \n",
    "\n",
    "GRU is a type of **Recurrent Neural Network (RNN)** designed to handle sequential data (e.g., time series, speech, language).  \n",
    "\n",
    "🔹 **Why GRU?**  \n",
    "- Standard RNNs suffer from the **vanishing gradient problem**, making it hard to learn **long-term dependencies**.  \n",
    "- GRUs, like LSTMs, use **gates to control information flow** but are computationally more efficient.  \n",
    "- They have **fewer parameters** than LSTMs, making them **faster to train** while retaining strong performance.  \n",
    "\n",
    "### **🔧 GRU Components:**  \n",
    "A **GRU cell** consists of:  \n",
    "1️⃣ **Update Gate ($Z_t$)** → Decides **how much past information to keep**.  \n",
    "2️⃣ **Reset Gate ($R_t$)** → Decides **how much past information to forget**.  \n",
    "3️⃣ **Candidate Hidden State ($\\tilde{h}_t$)** → A new potential memory update.  \n",
    "4️⃣ **Final Hidden State ($h_t$)** → The actual memory that carries forward.  \n",
    "\n",
    "\n",
    "\n",
    "# **🏗️ GRU Architecture (Step-by-Step)**\n",
    "The **GRU cell** takes two inputs at time step $ t $:  \n",
    "🔹 **$ x_t $ (Current input)** – This is the new data point (word, feature, etc.).  \n",
    "🔹 **$ h_{t-1} $ (Previous hidden state)** – This stores past information.  \n",
    "\n",
    "### **🔵 Step 1: Compute the Reset Gate $ R_t $**\n",
    "- The **reset gate** decides whether to erase part of the past memory.  \n",
    "- Uses a **sigmoid activation** ($ \\sigma $) to squash values between 0 and 1.  \n",
    "\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot [h_{t-1}, x_t] + b_r)\n",
    "$$  \n",
    "\n",
    "👉 If $ R_t $ is **0**, it forgets the past.  \n",
    "👉 If $ R_t $ is **1**, it keeps the full past memory.  \n",
    "\n",
    "### **🔴 Step 2: Compute the Update Gate $ Z_t $**\n",
    "- The **update gate** decides how much of the **past hidden state** to retain versus **how much to update**.  \n",
    "- Also uses **sigmoid activation** to control memory update.  \n",
    "\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t] + b_z)\n",
    "$$  \n",
    "\n",
    "👉 If $ Z_t $ is **0**, it replaces the old memory entirely.  \n",
    "👉 If $ Z_t $ is **1**, it keeps the old memory.  \n",
    "\n",
    "### **🟢 Step 3: Compute the Candidate Hidden State $ \\tilde{h}_t $**\n",
    "- A **new candidate memory** is computed using the reset gate.  \n",
    "- Uses **tanh activation** to balance positive/negative values.  \n",
    "\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot [R_t \\ast h_{t-1}, x_t] + b_h)\n",
    "$$  \n",
    "\n",
    "👉 If **reset gate is 0**, it ignores past information.  \n",
    "👉 If **reset gate is 1**, it uses both past and current input.  \n",
    "\n",
    "### **🟠 Step 4: Compute the Final Hidden State $ h_t $**\n",
    "- The final output is a **blend of the old memory ($ h_{t-1} $) and new candidate memory ($ \\tilde{h}_t $)** controlled by the update gate.  \n",
    "\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$  \n",
    "\n",
    "👉 If $ Z_t $ is **0**, it fully updates with new memory.  \n",
    "👉 If $ Z_t $ is **1**, it keeps old memory.  \n",
    "\n",
    "\n",
    "\n",
    "# **📊 Computation Flow in a GRU Cell**  \n",
    "\n",
    "### **🛠️ Forward Pass**  \n",
    "\n",
    "1️⃣ **Compute Reset Gate:**  \n",
    "   - $ R_t = \\sigma(W_r [h_{t-1}, x_t] + b_r) $  \n",
    "\n",
    "2️⃣ **Compute Update Gate:**  \n",
    "   - $ Z_t = \\sigma(W_z [h_{t-1}, x_t] + b_z) $  \n",
    "\n",
    "3️⃣ **Compute Candidate Hidden State:**  \n",
    "   - $ \\tilde{h}_t = \\tanh(W_h [R_t \\ast h_{t-1}, x_t] + b_h) $  \n",
    "\n",
    "4️⃣ **Compute Final Hidden State:**  \n",
    "   - $ h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t $  \n",
    "\n",
    "### **🔄 Backpropagation (Training GRU)**\n",
    "GRUs are trained using **Backpropagation Through Time (BPTT)**, where:  \n",
    "- **Gradients of loss are computed** using **chain rule**.  \n",
    "- **Weights are updated** using **gradient descent**.  \n",
    "- **Gates regulate gradient flow**, preventing vanishing gradients.  \n",
    "\n",
    "# **🔬 GRU vs. LSTM: Key Differences**\n",
    "| Feature | GRU ⚡ | LSTM 🏋️ |\n",
    "|---------|------|------|\n",
    "| Number of Gates | 2 (Update, Reset) | 3 (Input, Forget, Output) |\n",
    "| Complexity | ✅ Less | ❌ More |\n",
    "| Performance | ⚡ Fast | 🏆 Better for long texts |\n",
    "| Memory Requirement | ✅ Less | ❌ More |\n",
    "| Suitable for | Speech, NLP, real-time apps | Long documents, text generation |\n",
    "\n",
    "\n",
    "\n",
    "# **🔥 Advantages of GRU**\n",
    "✅ **Faster Training** – Fewer gates than LSTM = Faster updates.  \n",
    "✅ **Solves Vanishing Gradient Problem** – Retains long-term dependencies.  \n",
    "✅ **Computationally Efficient** – Great for real-time applications.  \n",
    "✅ **Performs Well on Small Datasets** – Fewer parameters make it ideal for small-scale problems.  \n",
    "\n",
    "\n",
    "\n",
    "# **🚀 Where is GRU Used?**\n",
    "📌 **Speech Recognition** (Google Assistant, Alexa) 🗣️  \n",
    "📌 **Machine Translation** (Google Translate) 🌍  \n",
    "📌 **Stock Market Prediction** 📈  \n",
    "📌 **Chatbots & AI Assistants** 🤖  \n",
    "📌 **Music Generation** 🎵  \n",
    "\n",
    "\n",
    "\n",
    "# **🎯 Summary**\n",
    "✔ **GRU is a simplified LSTM** with **fewer gates** and **faster computations**.  \n",
    "✔ **It solves vanishing gradient issues** and **remembers long-term dependencies**.  \n",
    "✔ **Uses Reset & Update Gates** to control memory updates.  \n",
    "✔ **Faster than LSTM** but still **performs well in sequence-based tasks**.  \n",
    "✔ **Ideal for speech, NLP, real-time applications**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let’s manually walk through the GRU computations using a simple example. This will give you a **step-by-step breakdown of how a GRU cell processes a sentence**, calculating each gate and hidden state update.  \n",
    "\n",
    "\n",
    "\n",
    "### **📝 Example Sentence:**  \n",
    "👉 **\"AI is amazing\"**  \n",
    "We will process it word by word using a GRU with a **hidden size of 2** (to keep calculations manageable).  \n",
    "\n",
    "## **🔧 Step 1: Define Inputs & Initial Parameters**\n",
    "### **Word Encoding (Input Vectors)**\n",
    "We assume each word is converted into a 3-dimensional vector (using Word Embeddings). Let’s define:  \n",
    "\n",
    "| Word | Input Vector (\\( x_t \\)) |\n",
    "|-------|----------------|\n",
    "| **AI** | \\([0.5, 0.1, 0.4]\\) |\n",
    "| **is** | \\([0.2, 0.7, 0.3]\\) |\n",
    "| **amazing** | \\([0.6, 0.9, 0.5]\\) |\n",
    "\n",
    "### **Initial Hidden State \\( h_0 \\)**\n",
    "Since it's the first step, we initialize:  \n",
    "$$\n",
    "h_0 = [0, 0] \\quad \\text{(2-dimensional hidden state)}\n",
    "$$\n",
    "\n",
    "\n",
    "## **🛠️ Step 2: Define GRU Parameters**\n",
    "We need **weight matrices** and **biases** for reset and update gates. We assume:  \n",
    "\n",
    "**Reset Gate (\\( R_t \\)):**  \n",
    "$$\n",
    "W_r =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix},\n",
    "\\quad U_r =\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix},\n",
    "\\quad b_r = [0.1, 0.2]\n",
    "$$\n",
    "\n",
    "**Update Gate (\\( Z_t \\)):**  \n",
    "$$\n",
    "W_z =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.3 & 0.7 \\\\\n",
    "0.5 & 0.2 & 0.6\n",
    "\\end{bmatrix},\n",
    "\\quad U_z =\n",
    "\\begin{bmatrix}\n",
    "0.9 & 0.5 \\\\\n",
    "0.3 & 0.8\n",
    "\\end{bmatrix},\n",
    "\\quad b_z = [0.05, 0.1]\n",
    "$$\n",
    "\n",
    "**Candidate Hidden State (\\( \\tilde{h}_t \\)):**  \n",
    "$$\n",
    "W_h =\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix},\n",
    "\\quad U_h =\n",
    "\\begin{bmatrix}\n",
    "0.4 & 0.6 \\\\\n",
    "0.5 & 0.7\n",
    "\\end{bmatrix},\n",
    "\\quad b_h = [0.2, 0.3]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **⚡ Step 3: Compute for First Word (\"AI\")**  \n",
    "### **🔴 Reset Gate \\( R_1 \\)**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.5 & 0.1 \\\\\n",
    "0.3 & 0.7 & 0.2\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.6 & 0.4 \\\\\n",
    "0.8 & 0.9\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.1 \\\\\n",
    "0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\cdot 0.5) + (0.5 \\cdot 0.1) + (0.1 \\cdot 0.4) + 0.1 \\\\\n",
    "(0.3 \\cdot 0.5) + (0.7 \\cdot 0.1) + (0.2 \\cdot 0.4) + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.1 + 0.05 + 0.04 + 0.1 \\\\\n",
    "0.15 + 0.07 + 0.08 + 0.2\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma(\n",
    "\\begin{bmatrix}\n",
    "0.29 \\\\\n",
    "0.5\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Applying **sigmoid** (\\( \\sigma(x) = \\frac{1}{1 + e^{-x}} \\)):  \n",
    "\n",
    "$$\n",
    "R_1 =\n",
    "\\begin{bmatrix}\n",
    "\\sigma(0.29) \\\\\n",
    "\\sigma(0.5)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.572 \\\\\n",
    "0.622\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🟡 Update Gate \\( Z_1 \\)**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "\n",
    "Using similar calculations, we get:  \n",
    "\n",
    "$$\n",
    "Z_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🟢 Candidate Hidden State \\( \\tilde{h}_1 \\)**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot (R_1 \\ast h_0) + U_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "\n",
    "Since \\( h_0 = 0 \\), the term \\( R_1 \\ast h_0 \\) vanishes, and we compute:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.1 \\\\\n",
    "0.4\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "0.2 \\\\\n",
    "0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.29 + 0.2 \\\\\n",
    "0.49 + 0.3\n",
    "\\end{bmatrix}\n",
    ")\n",
    "=\n",
    "\\tanh(\n",
    "\\begin{bmatrix}\n",
    "0.49 \\\\\n",
    "0.79\n",
    "\\end{bmatrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "Approximating \\( \\tanh(x) \\), we get:\n",
    "\n",
    "$$\n",
    "\\tilde{h}_1 =\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔵 Final Hidden State \\( h_1 \\)**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "0.655 \\\\\n",
    "0.710\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "(1 - 0.655) \\\\\n",
    "(1 - 0.710)\n",
    "\\end{bmatrix}\n",
    "\\ast\n",
    "\\begin{bmatrix}\n",
    "0.45 \\\\\n",
    "0.66\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 =\n",
    "\\begin{bmatrix}\n",
    "(0.345) \\times 0.45 \\\\\n",
    "(0.290) \\times 0.66\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "0.155 \\\\\n",
    "0.191\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Repeat for \"is\" and \"amazing\"**\n",
    "Now, \\( h_1 \\) is used for the next step, and the process repeats.\n",
    "\n",
    "This shows **how a GRU cell updates memory word-by-word!** 🚀 Let me know if you want more manual calculations or insights! 🎯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate how a **GRU (Gated Recurrent Unit)** processes a sentence. We'll analyze how it **keeps important information** and **forgets unimportant details** using an actual example.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Example Sentence:**\n",
    "Let's take a simple sentence:\n",
    "> **\"I love deep learning.\"**  \n",
    "\n",
    "We'll process it **word by word** through a GRU and observe how it decides what to keep and what to forget.\n",
    "\n",
    "## **🔹 Step 1: Define Initial Setup**\n",
    "Each word is represented as a **word vector** $ x_t $. Assume we have:  \n",
    "\n",
    "| Word | Input Vector ($ x_t $) |\n",
    "|------|---------------------|\n",
    "| \"I\" | $ [0.5, 0.1, 0.3] $ |\n",
    "| \"love\" | $ [0.7, 0.2, 0.8] $ |\n",
    "| \"deep\" | $ [0.3, 0.9, 0.5] $ |\n",
    "| \"learning\" | $ [0.4, 0.7, 0.6] $ |\n",
    "\n",
    "We also assume that the **hidden state** $ h_t $ has two units, so it’s a 2D vector.  \n",
    "\n",
    "\n",
    "The **GRU parameters** (randomly chosen for simplicity):  \n",
    "\n",
    "- **Update Gate Weights** $ W_z, U_z $  \n",
    "- **Reset Gate Weights** $ W_r, U_r $  \n",
    "- **Candidate State Weights** $ W_h, U_h $  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 2: How GRU Decides What to Keep or Forget?**  \n",
    "GRU works with **three key equations** at every time step $ t $:  \n",
    "\n",
    "### **1️⃣ Reset Gate $ R_t $** (Decides whether to erase past memory)\n",
    "$$\n",
    "R_t = \\sigma(W_r \\cdot x_t + U_r \\cdot h_{t-1} + b_r)\n",
    "$$\n",
    "- If $ R_t $ is **close to 0**, it forgets old information.\n",
    "- If $ R_t $ is **close to 1**, it keeps old memory.  \n",
    "\n",
    "### **2️⃣ Update Gate $ Z_t $** (Decides whether to update hidden state)\n",
    "$$\n",
    "Z_t = \\sigma(W_z \\cdot x_t + U_z \\cdot h_{t-1} + b_z)\n",
    "$$\n",
    "- If $ Z_t $ is **close to 0**, it **replaces** the old state with new info.  \n",
    "- If $ Z_t $ is **close to 1**, it **keeps** the old memory.  \n",
    "\n",
    "### **3️⃣ Candidate Hidden State $ \\tilde{h}_t $**\n",
    "$$\n",
    "\\tilde{h}_t = \\tanh(W_h \\cdot x_t + U_h \\cdot (R_t \\ast h_{t-1}) + b_h)\n",
    "$$\n",
    "This is the new hidden state, considering **reset gate influence**.  \n",
    "\n",
    "### **4️⃣ Final Hidden State**\n",
    "$$\n",
    "h_t = Z_t \\ast h_{t-1} + (1 - Z_t) \\ast \\tilde{h}_t\n",
    "$$\n",
    "The final hidden state is a combination of **past and new** information.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 3: Manual Calculation for Each Word**\n",
    "Let’s assume:\n",
    "\n",
    "- $ h_0 = [0, 0] $ (initial hidden state)  \n",
    "- We calculate for each word step by step.\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"I\"**  \n",
    "#### **1️⃣ Reset Gate Calculation**\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot x_1 + U_r \\cdot h_0 + b_r)\n",
    "$$\n",
    "Since $ h_0 = [0, 0] $, this simplifies to:\n",
    "$$\n",
    "R_1 = \\sigma(W_r \\cdot [0.5, 0.1, 0.3] + b_r)\n",
    "$$\n",
    "Let’s say:\n",
    "$$\n",
    "R_1 = [0.8, 0.6]\n",
    "$$\n",
    "Since values are **close to 1**, we **keep past memory**.\n",
    "\n",
    "#### **2️⃣ Update Gate Calculation**\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + U_z \\cdot h_0 + b_z)\n",
    "$$\n",
    "Again, since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "Z_1 = \\sigma(W_z \\cdot x_1 + b_z)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "Z_1 = [0.9, 0.7]\n",
    "$$\n",
    "Since $ Z_1 $ is **close to 1**, GRU **keeps most of the old hidden state** (which is zero for now).\n",
    "\n",
    "#### **3️⃣ Compute Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + U_h \\cdot (R_1 \\ast h_0) + b_h)\n",
    "$$\n",
    "Since $ h_0 = 0 $, this simplifies to:\n",
    "$$\n",
    "\\tilde{h}_1 = \\tanh(W_h \\cdot x_1 + b_h)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "\\tilde{h}_1 = [0.3, 0.4]\n",
    "$$\n",
    "\n",
    "#### **4️⃣ Compute Final Hidden State**\n",
    "$$\n",
    "h_1 = Z_1 \\ast h_0 + (1 - Z_1) \\ast \\tilde{h}_1\n",
    "$$\n",
    "$$\n",
    "= [0.9, 0.7] \\ast [0, 0] + [0.1, 0.3] \\ast [0.3, 0.4]\n",
    "$$\n",
    "$$\n",
    "= [0.03, 0.12]\n",
    "$$\n",
    "🚀 **Hidden state at time step 1**: $ h_1 = [0.03, 0.12] $\n",
    "\n",
    "\n",
    "\n",
    "### **Processing Word: \"love\"**  \n",
    "Now, we use $ h_1 = [0.03, 0.12] $.\n",
    "\n",
    "#### **1️⃣ Reset Gate**\n",
    "$$\n",
    "R_2 = \\sigma(W_r \\cdot x_2 + U_r \\cdot h_1 + b_r)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "R_2 = [0.4, 0.2]\n",
    "$$\n",
    "Since $ R_2 $ is **low**, it **forgets some past memory**.\n",
    "\n",
    "#### **2️⃣ Update Gate**\n",
    "$$\n",
    "Z_2 = \\sigma(W_z \\cdot x_2 + U_z \\cdot h_1 + b_z)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "Z_2 = [0.2, 0.6]\n",
    "$$\n",
    "Since $ Z_2 $ is **low for the first unit**, it **updates memory**.\n",
    "\n",
    "#### **3️⃣ Candidate Hidden State**\n",
    "$$\n",
    "\\tilde{h}_2 = \\tanh(W_h \\cdot x_2 + U_h \\cdot (R_2 \\ast h_1) + b_h)\n",
    "$$\n",
    "Let’s assume:\n",
    "$$\n",
    "\\tilde{h}_2 = [0.6, 0.5]\n",
    "$$\n",
    "\n",
    "#### **4️⃣ Final Hidden State**\n",
    "$$\n",
    "h_2 = Z_2 \\ast h_1 + (1 - Z_2) \\ast \\tilde{h}_2\n",
    "$$\n",
    "$$\n",
    "= [0.2, 0.6] \\ast [0.03, 0.12] + [0.8, 0.4] \\ast [0.6, 0.5]\n",
    "$$\n",
    "$$\n",
    "= [0.006, 0.072] + [0.48, 0.2]\n",
    "$$\n",
    "$$\n",
    "= [0.486, 0.272]\n",
    "$$\n",
    "\n",
    "🚀 **Hidden state at time step 2**: $ h_2 = [0.486, 0.272] $  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Conclusion**\n",
    "- **\"I\"** → Small memory update, since it’s a common word.  \n",
    "- **\"love\"** → Memory updates more because it’s a strong emotional word.  \n",
    "- **GRU selectively keeps or forgets** based on context.  \n",
    "\n",
    "Would you like me to compute for \"deep\" and \"learning\" too? 🚀\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
