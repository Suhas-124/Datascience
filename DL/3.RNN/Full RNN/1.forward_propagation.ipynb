{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ **Recurrent Neural Network (RNN) Architecture â€“ A Deep Dive!** ğŸ”„  \n",
    "\n",
    "RNNs are a special type of neural network designed to process **sequential data**, such as time-series data, speech, and text. Unlike traditional ANNs, RNNs have a **memory** that allows them to consider past inputs while processing current ones.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ—ï¸ **Basic RNN Architecture**  \n",
    "\n",
    "RNNs are different from standard ANNs because they have a **feedback loop** that allows information to persist over time.\n",
    "\n",
    "### ğŸ”¹ **Structure of a Simple RNN**  \n",
    "The architecture consists of:  \n",
    "1. **Input Layer**: Takes the input sequence.  \n",
    "2. **Hidden Layer (Recurrent Neurons)**: Maintains a memory of previous states and updates at each time step.  \n",
    "3. **Output Layer**: Produces the final prediction.\n",
    "\n",
    "ğŸ’¡ **Key difference from ANN**: The hidden layer is connected to itself! This allows information to flow from previous time steps.\n",
    "\n",
    "### ğŸ“Œ **Mathematical Representation**  \n",
    "At each time step **t**, the RNN updates its hidden state using:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = previous hidden state  \n",
    "- $ W_x $, $ W_h $ = weight matrices  \n",
    "- $ b $ = bias  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "The output is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ g $ = activation function (softmax for classification, linear for regression)  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **Unrolling the RNN (Time Step Representation)**  \n",
    "\n",
    "A simple RNN processes a sequence of inputs **one time step at a time**.  \n",
    "For example, if we have a sequence **X = [xâ‚, xâ‚‚, xâ‚ƒ]**, the RNN unfolds like this:\n",
    "\n",
    "```\n",
    "xâ‚ â†’ [hâ‚] â†’ yâ‚\n",
    "      â†˜\n",
    "xâ‚‚ â†’ [hâ‚‚] â†’ yâ‚‚\n",
    "       â†˜\n",
    "xâ‚ƒ â†’ [hâ‚ƒ] â†’ yâ‚ƒ\n",
    "```\n",
    "  \n",
    "Here:  \n",
    "- The hidden state **h** carries information from previous time steps.\n",
    "- Each output $ y_t $ is computed based on the current hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš§ **Challenges in Basic RNNs**  \n",
    "RNNs are powerful, but they face some problems:\n",
    "\n",
    "### âŒ **Vanishing Gradient Problem**  \n",
    "- When training deep RNNs with many time steps, gradients shrink to near **zero** during backpropagation.  \n",
    "- This makes it **hard to learn long-term dependencies** (i.e., remembering things from many time steps ago).\n",
    "\n",
    "### âŒ **Exploding Gradient Problem**  \n",
    "- If gradients grow **too large**, they can make the training unstable.\n",
    "\n",
    "To solve these, we use **LSTMs (Long Short-Term Memory)** and **GRUs (Gated Recurrent Units)**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Variants of RNNs**\n",
    "There are different types of RNN architectures:\n",
    "\n",
    "1. **One-to-One (Vanilla RNN)**\n",
    "   - Used for simple tasks like image classification.\n",
    "\n",
    "2. **One-to-Many**\n",
    "   - Example: Generating music ğŸµ from a single note.\n",
    "\n",
    "3. **Many-to-One**\n",
    "   - Example: Sentiment analysis (classifying an entire sentence as \"positive\" or \"negative\").\n",
    "\n",
    "4. **Many-to-Many**\n",
    "   - Example: Machine translation (e.g., English â†’ French).\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Key Takeaways**  \n",
    "âœ… RNNs are great for **sequential data** processing.  \n",
    "âœ… They have **memory**, unlike ANNs.  \n",
    "âœ… They suffer from **vanishing/exploding gradients** but can be improved with **LSTMs and GRUs**.  \n",
    "âœ… Used in **speech recognition, time-series forecasting, chatbots, and NLP tasks**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”„ **Forward Propagation in Recurrent Neural Networks (RNNs) â€“ A Complete Breakdown!** ğŸ”„  \n",
    "\n",
    "Forward propagation in an RNN works differently from a standard Artificial Neural Network (ANN) because it processes **sequential data** while maintaining a **hidden state** that carries information from previous time steps.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ— **Basic Structure of RNN Forward Propagation**\n",
    "Unlike traditional feedforward networks, where inputs are independent, an RNN processes inputs **sequentially**, maintaining a memory of past computations.\n",
    "\n",
    "For each time step $ t $, the RNN performs the following computations:\n",
    "\n",
    "1ï¸âƒ£ **Compute the new hidden state $ h_t $ using the current input $ x_t $ and the previous hidden state $ h_{t-1} $.**  \n",
    "2ï¸âƒ£ **Compute the output $ y_t $ using the hidden state $ h_t $.**  \n",
    "3ï¸âƒ£ **Pass the hidden state to the next time step.**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¢ **Mathematical Formulation**\n",
    "At each time step $ t $, forward propagation in an RNN follows these steps:\n",
    "\n",
    "### 1ï¸âƒ£ **Hidden State Update**\n",
    "The hidden state $ h_t $ is calculated using the previous hidden state $ h_{t-1} $ and the current input $ x_t $:\n",
    "\n",
    "$$\n",
    "h_t = f(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h_t $ = hidden state at time step $ t $  \n",
    "- $ x_t $ = input at time step $ t $  \n",
    "- $ h_{t-1} $ = hidden state from the previous time step  \n",
    "- $ W_x $ = weight matrix for input  \n",
    "- $ W_h $ = weight matrix for previous hidden state  \n",
    "- $ b_h $ = bias term  \n",
    "- $ f $ = activation function (commonly **tanh** or **ReLU**)  \n",
    "\n",
    "### 2ï¸âƒ£ **Output Calculation**\n",
    "The output $ y_t $ at time step $ t $ is computed as:\n",
    "\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_t $ = output at time step $ t $  \n",
    "- $ W_y $ = weight matrix for output  \n",
    "- $ b_y $ = bias for output  \n",
    "- $ g $ = activation function (e.g., **softmax** for classification tasks)  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“œ **Step-by-Step Forward Propagation Example**\n",
    "Let's assume we have an RNN processing three time steps with inputs $ x_1, x_2, x_3 $.\n",
    "\n",
    "### ğŸ”„ **Unrolling the RNN**\n",
    "Instead of viewing an RNN as a single network, we **unroll it** across time steps:\n",
    "\n",
    "```\n",
    "xâ‚ â†’ [hâ‚] â†’ yâ‚\n",
    "      â†˜\n",
    "xâ‚‚ â†’ [hâ‚‚] â†’ yâ‚‚\n",
    "       â†˜\n",
    "xâ‚ƒ â†’ [hâ‚ƒ] â†’ yâ‚ƒ\n",
    "```\n",
    "\n",
    "### ğŸ”¢ **Step 1: Compute the first hidden state $ h_1 $**\n",
    "$$\n",
    "h_1 = f(W_x x_1 + W_h h_0 + b_h)\n",
    "$$\n",
    "- $ h_0 $ is typically initialized as a vector of zeros.\n",
    "\n",
    "### ğŸ”¢ **Step 2: Compute the second hidden state $ h_2 $**\n",
    "$$\n",
    "h_2 = f(W_x x_2 + W_h h_1 + b_h)\n",
    "$$\n",
    "- The hidden state $ h_1 $ from the previous time step is used.\n",
    "\n",
    "### ğŸ”¢ **Step 3: Compute the third hidden state $ h_3 $**\n",
    "$$\n",
    "h_3 = f(W_x x_3 + W_h h_2 + b_h)\n",
    "$$\n",
    "\n",
    "### ğŸ”¢ **Step 4: Compute outputs $ y_1, y_2, y_3 $**\n",
    "$$\n",
    "y_t = g(W_y h_t + b_y)\n",
    "$$\n",
    "- The output is calculated at each time step based on the hidden state.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Observations**\n",
    "âœ” **Recurrent Connections**: The hidden state at each time step depends on the previous state.  \n",
    "âœ” **Shared Weights**: The same weight matrices $ W_x, W_h, W_y $ are used across all time steps, reducing complexity.  \n",
    "âœ” **Memory Effect**: The network retains past information, making it suitable for **sequential tasks** like speech recognition, language modeling, and time-series forecasting.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’» **Python Code Example**\n",
    "Hereâ€™s how forward propagation in an RNN can be implemented using NumPy:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Activation function (tanh)\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Define input, weight matrices, and bias\n",
    "x = np.array([[0.5], [0.2], [0.1]])  # Input at three time steps\n",
    "W_x = np.array([[0.8]])  # Input weight\n",
    "W_h = np.array([[0.5]])  # Recurrent weight\n",
    "W_y = np.array([[1.0]])  # Output weight\n",
    "b_h = np.array([[0.1]])  # Bias for hidden state\n",
    "b_y = np.array([[0.2]])  # Bias for output\n",
    "\n",
    "# Initialize hidden state\n",
    "h = np.array([[0]])  # Start with zero hidden state\n",
    "\n",
    "# Forward propagation\n",
    "for t in range(len(x)):\n",
    "    h = tanh(np.dot(W_x, x[t]) + np.dot(W_h, h) + b_h)  # Update hidden state\n",
    "    y = np.dot(W_y, h) + b_y  # Compute output\n",
    "    print(f\"Time Step {t+1}: Hidden State: {h}, Output: {y}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Final Thoughts**\n",
    "âœ… **RNN forward propagation** processes inputs **one at a time** while maintaining memory.  \n",
    "âœ… **Key equations** involve computing the **hidden state** and **output** at each time step.  \n",
    "âœ… **Challenges**: Standard RNNs struggle with long sequences due to the **vanishing gradient problem**.  \n",
    "âœ… **Solution**: Use **LSTMs or GRUs** to improve long-term memory handling.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§® **Manual Calculation of RNN Forward Propagation â€“ Step-by-Step Example** ğŸ”„  \n",
    "\n",
    "Let's take a simple example of an **RNN with one neuron** to manually compute forward propagation for **three time steps**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ Given Parameters**\n",
    "We define a simple RNN where:\n",
    "\n",
    "- **Input size = 1 (one feature per time step)**\n",
    "- **Hidden state size = 1 (one neuron in hidden layer)**\n",
    "- **Output size = 1 (one neuron in output layer)**\n",
    "- **Sequence length = 3 (processing 3 time steps: $ x_1, x_2, x_3 $)**\n",
    "\n",
    "#### ğŸ¯ **Initial Values**\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| $ x_1, x_2, x_3 $ | $ 0.5, 0.2, 0.1 $ (input at each time step) |\n",
    "| $ W_x $ | $ 0.8 $ (weight for input) |\n",
    "| $ W_h $ | $ 0.5 $ (weight for hidden state) |\n",
    "| $ W_y $ | $ 1.0 $ (weight for output) |\n",
    "| $ b_h $ | $ 0.1 $ (bias for hidden state) |\n",
    "| $ b_y $ | $ 0.2 $ (bias for output) |\n",
    "| $ h_0 $ | $ 0 $ (initial hidden state) |\n",
    "\n",
    "ğŸ’¡ **Activation function**: We use the **tanh** function:\n",
    "$$\n",
    "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“ Forward Propagation Steps**\n",
    "At each time step, we compute:\n",
    "\n",
    "1ï¸âƒ£ **Hidden state update**  \n",
    "$$\n",
    "h_t = \\tanh(W_x x_t + W_h h_{t-1} + b_h)\n",
    "$$\n",
    "\n",
    "2ï¸âƒ£ **Output calculation**  \n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š Step-by-Step Computation**\n",
    "### **â³ Time Step 1 ($ t = 1 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_1 $:\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.8 \\times 0.5 + 0.5 \\times 0 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_1 = \\tanh(0.4 + 0 + 0.1) = \\tanh(0.5)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.5) \\approx 0.4621 $:\n",
    "\n",
    "$$\n",
    "h_1 \\approx 0.4621\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_1 $:\n",
    "\n",
    "$$\n",
    "y_1 = 1.0 \\times 0.4621 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_1 \\approx 0.6621\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **â³ Time Step 2 ($ t = 2 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_2 $:\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.8 \\times 0.2 + 0.5 \\times 0.4621 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_2 = \\tanh(0.16 + 0.2311 + 0.1) = \\tanh(0.4911)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4911) \\approx 0.4548 $:\n",
    "\n",
    "$$\n",
    "h_2 \\approx 0.4548\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_2 $:\n",
    "\n",
    "$$\n",
    "y_2 = 1.0 \\times 0.4548 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 \\approx 0.6548\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **â³ Time Step 3 ($ t = 3 $)**\n",
    "#### ğŸ”¹ Compute hidden state $ h_3 $:\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.8 \\times 0.1 + 0.5 \\times 0.4548 + 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_3 = \\tanh(0.08 + 0.2274 + 0.1) = \\tanh(0.4074)\n",
    "$$\n",
    "\n",
    "Using $ \\tanh(0.4074) \\approx 0.3863 $:\n",
    "\n",
    "$$\n",
    "h_3 \\approx 0.3863\n",
    "$$\n",
    "\n",
    "#### ğŸ”¹ Compute output $ y_3 $:\n",
    "\n",
    "$$\n",
    "y_3 = 1.0 \\times 0.3863 + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_3 \\approx 0.5863\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Final Results**\n",
    "| Time Step | $ x_t $ | $ h_t $ (Hidden State) | $ y_t $ (Output) |\n",
    "|-----------|----------|----------------|----------------|\n",
    "| $ t = 1 $ | $ 0.5 $ | $ 0.4621 $ | $ 0.6621 $ |\n",
    "| $ t = 2 $ | $ 0.2 $ | $ 0.4548 $ | $ 0.6548 $ |\n",
    "| $ t = 3 $ | $ 0.1 $ | $ 0.3863 $ | $ 0.5863 $ |\n",
    "\n",
    "ğŸ¯ **Observation**:  \n",
    "- The hidden state **carries information** from previous time steps, updating with each new input.\n",
    "- The outputs are computed at each time step, making the RNN suitable for **sequential data** processing.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Summary**\n",
    "âœ” We **manually computed** RNN forward propagation step by step.  \n",
    "âœ” The **hidden state** maintains memory across time steps.  \n",
    "âœ” The **output at each step** depends on both the current input and previous hidden state.  \n",
    "âœ” **Activation function (tanh)** ensures values remain between $-1$ and $1$.  \n",
    "âœ” **Weights are shared** across all time steps, making the RNN efficient.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
