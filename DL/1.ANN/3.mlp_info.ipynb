{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-Layer Perceptron (MLP) â€“ The Backbone of Deep Learning!**  \n",
    "\n",
    "Imagine a **Multi-Layer Perceptron (MLP)** as a team of **smart neurons** working together to recognize patterns and make predictions. Itâ€™s like a **brain-inspired network** where information flows through multiple layers, each refining the knowledge step by step! ğŸ§ âœ¨  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸŒŸ Structure of an MLP**\n",
    "An MLP consists of three main types of layers:  \n",
    "\n",
    "1ï¸âƒ£ **Input Layer** ğŸ¯  \n",
    "   - The first layer where raw data (features) enters the network.  \n",
    "   - Each neuron represents one feature from the dataset.  \n",
    "\n",
    "2ï¸âƒ£ **Hidden Layers** ğŸ”¥  \n",
    "   - The **magic happens here!**  \n",
    "   - Each hidden layer applies mathematical transformations using **weights, biases, and activation functions** to detect complex relationships in data.  \n",
    "   - The more hidden layers, the deeper the learning!  \n",
    "\n",
    "3ï¸âƒ£ **Output Layer** ğŸ¬  \n",
    "   - Produces the final result based on the transformed data.  \n",
    "   - If itâ€™s a **classification task**, it outputs probabilities for different categories.  \n",
    "   - If itâ€™s a **regression task**, it gives a numerical prediction.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸš€ How Does MLP Work?**\n",
    "1ï¸âƒ£ **Forward Propagation** ğŸ”„  \n",
    "   - Input data flows through the layers.  \n",
    "   - Neurons perform weighted sums and pass the result through activation functions.  \n",
    "\n",
    "2ï¸âƒ£ **Activation Functions** âš¡  \n",
    "   - Introduce non-linearity, helping MLP learn **complex patterns**.  \n",
    "   - Popular choices:  \n",
    "     ğŸ”¹ **ReLU (Rectified Linear Unit)** â€“ Speeds up training.  \n",
    "     ğŸ”¹ **Sigmoid** â€“ Used in binary classification.  \n",
    "     ğŸ”¹ **Softmax** â€“ Converts outputs into probabilities for multi-class classification.  \n",
    "\n",
    "3ï¸âƒ£ **Backpropagation & Learning** ğŸ¯  \n",
    "   - The network **learns** by adjusting weights using **gradient descent** and **backpropagation**.  \n",
    "   - **Loss function** calculates the error, and **optimization algorithms** like Adam or SGD help minimize it.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¯ Why is MLP Powerful?**\n",
    "âœ… Can **learn complex relationships** in data.  \n",
    "âœ… Works for both **classification and regression** tasks.  \n",
    "âœ… Forms the **foundation of deep learning** models like CNNs & RNNs.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¨ Visualizing MLP**\n",
    "Think of an MLP like a **chef preparing a recipe**:  \n",
    "ğŸ”¹ **Input Layer** â€“ Ingredients are gathered.  \n",
    "ğŸ”¹ **Hidden Layers** â€“ Ingredients are mixed, cooked, and transformed.  \n",
    "ğŸ”¹ **Output Layer** â€“ The final dish is served! ğŸ½ï¸ğŸ˜ƒ  \n",
    "\n",
    "\n",
    "\n",
    "### **Final Thoughts**  \n",
    "MLPs may be simple compared to modern deep learning models, but they are **powerful and versatile**. They serve as the **building blocks of neural networks** and help machines **learn and make intelligent decisions**! ğŸš€ğŸ”®  \n",
    "\n",
    "\n",
    "![](images/multi.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ”¹ Understanding Weights, Biases & Input Layers in Artificial Neural Networks (ANN) in Simple Layman Terms**  \n",
    "\n",
    "Think of a **neural network** like a **chef cooking a dish** ğŸ².  \n",
    "\n",
    "Each **ingredient** (like salt, sugar, and spices) represents the **input values** in the network. But different dishes need different amounts of ingredients, right? This is where **weights and biases** come in!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ What is an Input Layer?**\n",
    "The **input layer** is where you provide the **raw ingredients** (data) to the neural network.  \n",
    "- If you're predicting house prices, inputs could be **size, number of rooms, and location**.  \n",
    "- If you're recognizing handwritten digits, inputs could be **pixel values from an image**.  \n",
    "\n",
    "ğŸ‘‰ Itâ€™s just like the **raw materials** a chef uses before cooking!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ What are Weights? (Think of Weights as Recipe Measurements)**\n",
    "Every **input ingredient** has a certain **importance** (weight).  \n",
    "- More important inputs get **higher weights** ğŸ“ˆ  \n",
    "- Less important inputs get **lower weights** ğŸ“‰  \n",
    "\n",
    "**Example:**  \n",
    "If you're making lemonade, the weight of **water** is high, and the weight of **salt** is very low. You wouldn't want salty lemonade!  \n",
    "\n",
    "ğŸ”¹ **Mathematically:**  \n",
    "$$\n",
    "\\text{Weighted Input} = (\\text{Input}_1 \\times \\text{Weight}_1) + (\\text{Input}_2 \\times \\text{Weight}_2) + ...\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ **Weights adjust themselves to make better predictions**â€”just like a chef fine-tunes a recipe based on taste!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ What is Bias? (Think of Bias as Adding Extra Flavor)**\n",
    "Even if all ingredients are perfect, you might need **a little extra sugar or salt** to make the dish just right. That extra adjustment is the **bias**.  \n",
    "\n",
    "ğŸ”¹ **Why do we need bias?**  \n",
    "- If all weights are **zero**, the network wonâ€™t learn properly.  \n",
    "- Bias helps the network **shift the results** in the right direction.  \n",
    "\n",
    "**Example:**  \n",
    "A coffee shop always adds **a little extra sugar** by default. That small **fixed addition** is the **bias**.  \n",
    "\n",
    "ğŸ”¹ **Mathematically:**  \n",
    "$$\n",
    "\\text{Final Output} = (\\text{Weighted Input}) + \\text{Bias}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ How Everything Works Together?**\n",
    "1. **Input Layer** brings in data (ingredients).  \n",
    "2. **Weights** decide the importance of each input (how much of each ingredient to use).  \n",
    "3. **Bias** adds an extra adjustment (fine-tuning the taste).  \n",
    "4. **Activation Function** processes it and passes the result to the next layer (just like a chef tasting and adjusting).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Simple Analogy**\n",
    "Think of a **burger shop ğŸ”**:  \n",
    "- **Input:** Raw ingredients like bun, patty, cheese  \n",
    "- **Weights:** Amount of each ingredient (more cheese = cheesier burger)  \n",
    "- **Bias:** Extra seasoning (customizing flavor)  \n",
    "- **Activation Function:** Final decisionâ€”should we serve the burger or tweak it further?  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Final Takeaway**\n",
    "- **Weights** â†’ Control the importance of inputs (like ingredient amounts)  \n",
    "- **Bias** â†’ Adjusts the final result (like extra seasoning)  \n",
    "- **Input Layer** â†’ Brings in raw data (like ingredients)  \n",
    "\n",
    "ğŸ”¥ **Without weights and biases, a neural network is just a random mess!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ”¹ Who Decides the Weights in a Neural Network?** ğŸ¤”  \n",
    "\n",
    "Great question! The **neural network itself** decides the weights, but it doesnâ€™t start off knowing the right values. It learns them through **training** using a process called **backpropagation**. Let me break it down step by step.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Step 1: Initial Random Weights ğŸ²**\n",
    "At the start, the neural network has **no idea** what the correct weights should be.  \n",
    "So, it just **guesses** by assigning random numbers to the weights.  \n",
    "\n",
    "Example:  \n",
    "Imagine a **robot chef** ğŸ§‘â€ğŸ³ trying to make the perfect burger ğŸ”, but on the first try, it **randomly** picks the amount of salt, cheese, and sauce.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Step 2: Make a Prediction ğŸ¯**\n",
    "The network takes an input (like house size for price prediction) and **calculates the output** using these random weights.  \n",
    "\n",
    "Example:  \n",
    "The robot chef makes a **burger** with the random ingredient amounts and serves it to a customer.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Step 3: Compare with the Actual Answer (Loss Function) ğŸ“‰**\n",
    "The network checks **how wrong** its prediction was by calculating the **error (loss)**.  \n",
    "- If the predicted house price was **$250,000** but the actual price was **$300,000**, the error is **$50,000**.  \n",
    "- If the robot chefâ€™s burger **tastes bad**, thatâ€™s an error too!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Step 4: Adjust Weights Using Backpropagation & Gradient Descent ğŸ”„**\n",
    "Now, the network **corrects the weights** little by little to reduce the error.  \n",
    "This is done using an algorithm called **Gradient Descent**, which tells the network how much to **increase or decrease each weight**.  \n",
    "\n",
    "Example:  \n",
    "The **robot chef** takes customer feedback:  \n",
    "- If the burger was **too salty**, next time it adds **less salt**.  \n",
    "- If it was **too dry**, it adds **more sauce**.  \n",
    "\n",
    "This fine-tuning process continues until the **error is as small as possible**.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Step 5: Repeat Until the Weights are Perfect ğŸ”„**\n",
    "The network keeps adjusting the weights over **many rounds (epochs)** until it learns the best values.  \n",
    "\n",
    "**Final result?**  \n",
    "- A neural network that can make highly accurate predictions! ğŸ¯  \n",
    "- A robot chef that can make the **perfect burger every time! ğŸ”**  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Summary**\n",
    "âœ… **Who decides the weights?** The neural network does, but it learns them by itself using **training and feedback**.  \n",
    "âœ… **How does it learn?** Using **backpropagation and gradient descent** to slowly adjust the weights.  \n",
    "âœ… **Why canâ€™t we set weights manually?** Because the patterns in data are too complex for humans to set them correctly!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ **Forward Propagation in a Multi-Layer Perceptron (MLP) â€“ A Full Breakdown** ğŸ”¥  \n",
    "\n",
    "Forward propagation (also called **forward pass**) is the process where input data moves **layer by layer** through the network until it reaches the output layer, producing a prediction. This is the first step in training a neural network, followed by backpropagation, which adjusts weights to minimize errors.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ— **1. Components Involved in Forward Propagation**  \n",
    "\n",
    "Before diving into the process, letâ€™s define key components:\n",
    "\n",
    "- **Input Layer ($ X $)**: Receives the raw data as feature values.\n",
    "- **Weights ($ W $)**: These are the parameters that adjust how signals pass between neurons.\n",
    "- **Bias ($ b $)**: A bias term allows the activation function to shift.\n",
    "- **Activation Function ($ \\sigma $)**: Introduces non-linearity to the model, allowing it to learn complex patterns.\n",
    "- **Output Layer ($ Y $)**: Produces the final prediction.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”„ **2. Step-by-Step Breakdown of Forward Propagation**  \n",
    "\n",
    "Letâ€™s assume an **MLP with one hidden layer**, meaning the network structure is:\n",
    "\n",
    "- **Input Layer**: 3 neurons\n",
    "- **Hidden Layer**: 2 neurons\n",
    "- **Output Layer**: 1 neuron (Binary classification)\n",
    "\n",
    "### ğŸ”¹ **Step 1: Input Layer to Hidden Layer**  \n",
    "\n",
    "Each neuron in the hidden layer receives input from **all input neurons**, applies weights and bias, and then passes the result through an **activation function**.\n",
    "\n",
    "Mathematically, the operation at each hidden neuron follows:\n",
    "\n",
    "$$\n",
    "Z^{(1)} = W^{(1)} X + b^{(1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W^{(1)} $ = Weight matrix from input to hidden layer  \n",
    "- $ X $ = Input features  \n",
    "- $ b^{(1)} $ = Bias term  \n",
    "- $ Z^{(1)} $ = Weighted sum before activation  \n",
    "\n",
    "Each neuron in the hidden layer applies an **activation function** (e.g., ReLU, Sigmoid):\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\sigma(Z^{(1)})\n",
    "$$\n",
    "\n",
    "Where $ A^{(1)} $ is the **activated output** of the hidden layer.\n",
    "\n",
    "ğŸ“Œ **Example Calculation**:\n",
    "\n",
    "Suppose:\n",
    "- Input $ X = [x_1, x_2, x_3] = [0.5, 0.2, 0.8] $\n",
    "- Weights $ W^{(1)} = \\begin{bmatrix} 0.3 & -0.2 & 0.5 \\\\ 0.7 & 0.1 & -0.6 \\end{bmatrix} $  \n",
    "- Bias $ b^{(1)} = \\begin{bmatrix} 0.1 \\\\ -0.3 \\end{bmatrix} $  \n",
    "- Activation function = **ReLU** $ \\sigma(x) = \\max(0, x) $\n",
    "\n",
    "The weighted sum $ Z^{(1)} $ is calculated as:\n",
    "\n",
    "$$\n",
    "Z^{(1)} = \\begin{bmatrix} 0.3 & -0.2 & 0.5 \\\\ 0.7 & 0.1 & -0.6 \\end{bmatrix} \\times \\begin{bmatrix} 0.5 \\\\ 0.2 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "Z_1 = (0.3 \\times 0.5) + (-0.2 \\times 0.2) + (0.5 \\times 0.8) + 0.1 = 0.47\n",
    "$$\n",
    "$$\n",
    "Z_2 = (0.7 \\times 0.5) + (0.1 \\times 0.2) + (-0.6 \\times 0.8) - 0.3 = -0.23\n",
    "$$\n",
    "\n",
    "Applying **ReLU Activation**:\n",
    "\n",
    "$$\n",
    "A^{(1)}_1 = \\max(0, 0.47) = 0.47\n",
    "$$\n",
    "$$\n",
    "A^{(1)}_2 = \\max(0, -0.23) = 0\n",
    "$$\n",
    "\n",
    "Thus, the activated outputs of the hidden layer are:\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\begin{bmatrix} 0.47 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Step 2: Hidden Layer to Output Layer**  \n",
    "\n",
    "The process is repeated:  \n",
    "1. Compute weighted sum **$ Z^{(2)} $**\n",
    "2. Apply activation function **$ \\sigma $**\n",
    "\n",
    "$$\n",
    "Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\sigma(Z^{(2)})\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Example Calculation**:\n",
    "\n",
    "Suppose:\n",
    "- Weights $ W^{(2)} = \\begin{bmatrix} 0.4 & -0.7 \\end{bmatrix} $\n",
    "- Bias $ b^{(2)} = \\begin{bmatrix} 0.2 \\end{bmatrix} $\n",
    "- Activation function = **Sigmoid** $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "The weighted sum:\n",
    "\n",
    "$$\n",
    "Z^{(2)} = (0.4 \\times 0.47) + (-0.7 \\times 0) + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} = 0.188 + 0 + 0.2 = 0.388\n",
    "$$\n",
    "\n",
    "Applying **Sigmoid Activation**:\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\frac{1}{1 + e^{-0.388}} = 0.595\n",
    "$$\n",
    "\n",
    "This is the **final output (prediction)**. If this is for **binary classification**, we interpret:\n",
    "\n",
    "- $ A^{(2)} > 0.5 $ â†’ Class 1\n",
    "- $ A^{(2)} < 0.5 $ â†’ Class 0\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **3. Key Takeaways on Forward Propagation**\n",
    "âœ… **Weight Matrix Multiplication**: Determines how signals pass through the network.  \n",
    "âœ… **Activation Functions**: Add non-linearity for better learning.  \n",
    "âœ… **Output Layer Interpretation**: Uses Softmax (multi-class) or Sigmoid (binary).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Summary of Forward Propagation Steps**\n",
    "1ï¸âƒ£ Compute weighted sum $ Z = WX + b $ for each layer.  \n",
    "2ï¸âƒ£ Apply **activation function** $ A = \\sigma(Z) $.  \n",
    "3ï¸âƒ£ Repeat until reaching the **output layer**.  \n",
    "4ï¸âƒ£ Output is interpreted based on **activation function** (e.g., Sigmoid for probability).  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Python Code for Forward Propagation in NumPy**\n",
    "Hereâ€™s a simple implementation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Input (3 neurons)\n",
    "X = np.array([[0.5, 0.2, 0.8]])\n",
    "\n",
    "# Weights and Bias for Hidden Layer\n",
    "W1 = np.array([[0.3, -0.2, 0.5], [0.7, 0.1, -0.6]])\n",
    "b1 = np.array([[0.1, -0.3]])\n",
    "\n",
    "# Forward propagation to Hidden Layer\n",
    "Z1 = np.dot(X, W1.T) + b1\n",
    "A1 = relu(Z1)\n",
    "\n",
    "# Weights and Bias for Output Layer\n",
    "W2 = np.array([[0.4, -0.7]])\n",
    "b2 = np.array([[0.2]])\n",
    "\n",
    "# Forward propagation to Output Layer\n",
    "Z2 = np.dot(A1, W2.T) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "print(\"Final Output:\", A2)\n",
    "```\n",
    "\n",
    "This simulates **one forward pass** through an MLP! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸ’¥ Loss Functions in Deep Learning (Full Explanation)**  \n",
    "\n",
    "In **deep learning**, the **loss function** is crucial for training the model, as it defines how far the predicted outputs are from the actual values (or true labels). The optimizer uses the loss to update the model's weights during training, guiding the model toward better performance.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is a Loss Function in Deep Learning?**\n",
    "\n",
    "A **loss function** is a mathematical function that measures the **error** between the model's predictions and the actual results. The goal is to **minimize this error** during training, which is done by **adjusting the weights** of the neural network. The loss function defines **how much the model is wrong** and helps the model learn.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Role of Loss Function in Deep Learning**\n",
    "\n",
    "1. **Guides Training:** It provides the **signal** that guides the **optimization process** during training.\n",
    "2. **Measures Performance:** The loss function is a metric that allows you to track the **performance of the model** over time.\n",
    "3. **Helps with Weight Updates:** It tells the optimizer how to adjust the **weights** to reduce the error.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  Types of Loss Functions in Deep Learning**\n",
    "\n",
    "Different loss functions are used depending on the task (regression, classification, etc.):\n",
    "\n",
    "### **1ï¸âƒ£ Loss Functions for Regression (Continuous Values)**\n",
    "\n",
    "In **regression** problems, we aim to predict a continuous value, such as predicting house prices or stock prices. For this, we use **loss functions** that measure the difference between the predicted value and the actual value.\n",
    "\n",
    "#### **Mean Squared Error (MSE)**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "- **Explanation:** The **MSE** calculates the average squared difference between the actual and predicted values. It's sensitive to outliers, as it **penalizes larger errors** more heavily.\n",
    "\n",
    "#### **Mean Absolute Error (MAE)**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  MAE = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "  $$\n",
    "- **Explanation:** The **MAE** calculates the average of the absolute errors. It's **less sensitive** to outliers compared to MSE.\n",
    "\n",
    "#### **Huber Loss**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  L_{\\delta}(y, \\hat{y}) = \\begin{cases} \n",
    "    \\frac{1}{2} (y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "    \\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta) & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- **Explanation:** The **Huber Loss** is a **combination** of **MSE** and **MAE**. It behaves like **MSE** when the error is small and like **MAE** when the error is large, making it **robust** to outliers.\n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Loss Functions for Classification (Discrete Labels)**\n",
    "\n",
    "In **classification** problems, we are predicting a discrete label or class. For example, classifying an image as a **cat** or **dog**.\n",
    "\n",
    "#### **Binary Cross-Entropy (Log Loss)**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
    "  $$\n",
    "- **Explanation:** Used for **binary classification** problems (e.g., spam detection). The **binary cross-entropy** measures the **difference** between the **true label** and the predicted **probability**.  \n",
    "  - If the prediction is correct (close to 1 or 0), the loss is small.  \n",
    "  - If the prediction is incorrect, the loss is larger.\n",
    "\n",
    "#### **Categorical Cross-Entropy**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  L(y, \\hat{y}) = - \\sum_{i=1}^{C} y_i \\cdot \\log(\\hat{y}_i)\n",
    "  $$\n",
    "- **Explanation:** Used for **multi-class classification** problems (e.g., digit recognition). It computes the loss across multiple classes by comparing the **predicted probabilities** with the **true one-hot encoded labels**.\n",
    "\n",
    "#### **Sparse Categorical Cross-Entropy**\n",
    "- **Formula:**  \n",
    "  Similar to **Categorical Cross-Entropy** but the labels are **integers** instead of one-hot encoded vectors.\n",
    "- **Explanation:** It is efficient for multi-class problems with large numbers of classes when labels are given as integers.\n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Loss Functions for Object Detection and Segmentation**\n",
    "\n",
    "In **object detection** (e.g., detecting objects in images) and **segmentation** tasks (e.g., pixel-wise classification), specialized loss functions are used.\n",
    "\n",
    "#### **Intersection over Union (IoU) Loss**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}\n",
    "  $$\n",
    "- **Explanation:** Used to measure the **overlap** between the predicted and ground truth bounding boxes. This is crucial in object detection tasks.\n",
    "\n",
    "#### **Dice Loss**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  Dice = \\frac{2 \\cdot \\text{Area of Overlap}}{\\text{Total Area}}\n",
    "  $$\n",
    "- **Explanation:** The **Dice coefficient** measures the **similarity** between two sets (predicted and actual). Itâ€™s especially useful in **image segmentation** problems to assess how well the predicted segmentation matches the ground truth.\n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Loss Functions for Generative Models**\n",
    "\n",
    "In models like **Generative Adversarial Networks (GANs)**, the loss functions are more specialized:\n",
    "\n",
    "#### **Adversarial Loss (GANs)**\n",
    "- **Formula:**  \n",
    "  $$\n",
    "  L_{\\text{GAN}} = - \\log(D(x)) \\quad \\text{for real samples}\n",
    "  $$\n",
    "  $$\n",
    "  L_{\\text{GAN}} = - \\log(1 - D(G(z))) \\quad \\text{for fake samples}\n",
    "  $$\n",
    "- **Explanation:** In GANs, two networks (Generator and Discriminator) compete against each other. The **generator** tries to generate fake data, while the **discriminator** tries to distinguish between real and fake data. The loss function drives this adversarial training.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§‘â€ğŸ« When to Use Which Loss Function?**\n",
    "\n",
    "| **Problem Type** | **Loss Function** | **Explanation** |\n",
    "|-------------------|-------------------|-----------------|\n",
    "| Regression        | **MSE**, **MAE**, **Huber Loss** | For continuous output prediction |\n",
    "| Binary Classification | **Binary Cross-Entropy** | For two-class problems (e.g., spam vs. not spam) |\n",
    "| Multi-class Classification | **Categorical Cross-Entropy** | For problems with multiple classes (e.g., digit classification) |\n",
    "| Object Detection   | **IoU Loss** | For measuring the overlap of predicted and actual bounding boxes |\n",
    "| Image Segmentation | **Dice Loss**, **IoU Loss** | For pixel-wise classification tasks |\n",
    "| Generative Models  | **Adversarial Loss** | For GANs, used to train generators and discriminators |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Summary**\n",
    "\n",
    "- **Loss functions** are critical in **deep learning**, as they define how well the model is doing and drive the optimization process.\n",
    "- Different tasks (regression, classification, etc.) require different loss functions.\n",
    "- Loss functions are closely tied to **model architecture** and **task at hand**.\n",
    "- Choosing the **right loss function** can drastically impact your model's performance, speed of convergence, and robustness.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I can explain loss functions for regression in simple layman terms! Think of a loss function as a way to measure how bad or good your modelâ€™s predictions are compared to the actual values. The smaller the loss, the better your model is at making accurate predictions.\n",
    "\n",
    "### ğŸ“Œ Loss Functions for Regression\n",
    "Regression is when we predict **continuous values** (like the price of a house or temperature). Here are the most commonly used loss functions:\n",
    "\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ **Mean Squared Error (MSE)**\n",
    "ğŸ”¹ **Formula:**  \n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (y_{\\text{actual}} - y_{\\text{predicted}})^2\n",
    "$$\n",
    "ğŸ”¹ **Layman Explanation:**  \n",
    "Imagine you're guessing the height of different people. MSE checks how far off your guesses are, **squares the difference** (to remove negatives), and then finds the average error.  \n",
    "\n",
    "âœ… **Good For:**  \n",
    "- When you want to **punish larger errors more** (because squaring makes big mistakes even bigger).  \n",
    "\n",
    "âŒ **Not Good For:**  \n",
    "- If your data has outliers (extreme values), MSE may exaggerate their effect.\n",
    "\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ **Mean Absolute Error (MAE)**\n",
    "ğŸ”¹ **Formula:**  \n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum |y_{\\text{actual}} - y_{\\text{predicted}}|\n",
    "$$\n",
    "ğŸ”¹ **Layman Explanation:**  \n",
    "Instead of squaring the errors like MSE, MAE **takes the absolute difference** between predictions and actual values. Itâ€™s like saying, \"Iâ€™m off by this much on average.\"  \n",
    "\n",
    "âœ… **Good For:**  \n",
    "- When you **want equal weight for all errors** (small and large).  \n",
    "- **More robust to outliers** than MSE.\n",
    "\n",
    "âŒ **Not Good For:**  \n",
    "- The optimization process may not be as smooth as with MSE.\n",
    "\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ **Huber Loss** (MSE + MAE Hybrid)\n",
    "ğŸ”¹ **Formula:**  \n",
    "$$\n",
    "L(y, \\hat{y}) =\n",
    "\\begin{cases} \n",
    "\\frac{1}{2} (y - \\hat{y})^2, & \\text{if } |y - \\hat{y}| \\leq \\delta \\\\\n",
    "\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{if } |y - \\hat{y}| > \\delta\n",
    "\\end{cases}\n",
    "$$\n",
    "ğŸ”¹ **Layman Explanation:**  \n",
    "Huber Loss **combines** MSE (for small errors) and MAE (for big errors). It behaves like MSE when the error is small, and like MAE when the error is large (to reduce outlier impact).  \n",
    "\n",
    "âœ… **Good For:**  \n",
    "- Handling **outliers** better than MSE while keeping a smooth optimization.\n",
    "\n",
    "âŒ **Not Good For:**  \n",
    "- If your dataset has no outliers, MSE or MAE might be enough.\n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ **Log-Cosh Loss** (Smoothed MAE)\n",
    "ğŸ”¹ **Formula:**  \n",
    "$$\n",
    "L(y, \\hat{y}) = \\sum \\log (\\cosh(y - \\hat{y}))\n",
    "$$\n",
    "ğŸ”¹ **Layman Explanation:**  \n",
    "This is similar to MAE, but it **smooths out the error** so that it behaves like MSE for small errors and MAE for large errors.\n",
    "\n",
    "âœ… **Good For:**  \n",
    "- A balance between MAE and MSE, **handling outliers smoothly**.\n",
    "\n",
    "âŒ **Not Good For:**  \n",
    "- Itâ€™s slightly more complex to compute than MAE or MSE.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Which Loss Function Should You Use?**\n",
    "- **MSE** â†’ If you care more about **big errors affecting the loss more**.  \n",
    "- **MAE** â†’ If you want a **simpler, balanced loss** and can handle outliers.  \n",
    "- **Huber Loss** â†’ If you have **some outliers but donâ€™t want them to dominate**.  \n",
    "- **Log-Cosh Loss** â†’ If you want **something smoother than MAE but robust like Huber**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let me explain loss functions for **classification** in the simplest way possible. ğŸ˜Š  \n",
    "\n",
    "\n",
    "### ğŸ”¥ **What is a Loss Function?**  \n",
    "A **loss function** tells us **how wrong** our modelâ€™s predictions are.  \n",
    "For **classification**, our model tries to put things into different **categories** (like \"cat\" vs. \"dog\" or \"spam\" vs. \"not spam\").  \n",
    "The loss function checks **how far off** the predictions are from the actual answers.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Types of Loss Functions for Classification**  \n",
    "\n",
    "### 1ï¸âƒ£ **Binary Cross-Entropy (Log Loss)**\n",
    "ğŸ‘‰ Used when there are **only 2 categories** (e.g., **YES/NO**, **0/1**, **Spam/Not Spam**).  \n",
    "\n",
    "ğŸ”¹ **How It Works (Simple Way)**  \n",
    "- The model gives a probability (e.g., **70% spam, 30% not spam**).  \n",
    "- If the correct answer is **spam (1)**, we want the probability **to be as close to 100% as possible**.  \n",
    "- If it's **not spam (0)**, we want the probability **to be as close to 0% as possible**.  \n",
    "- The loss **increases** if the model is confident **but wrong** (e.g., predicting 99% not spam when it's actually spam).  \n",
    "\n",
    "ğŸ”¹ **Formula (Just for Reference, No Need to Memorize ğŸ˜)**  \n",
    "$$\n",
    "Loss = - \\frac{1}{N} \\sum [ y \\log (\\hat{y}) + (1 - y) \\log (1 - \\hat{y}) ]\n",
    "$$\n",
    "ğŸ“Œ **Key Takeaway**:  \n",
    "- If the model is **very wrong**, the loss is **high**.  \n",
    "- If the model is **very right**, the loss is **low**.  \n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ **Categorical Cross-Entropy**\n",
    "ğŸ‘‰ Used when there are **more than 2 categories** (e.g., **dog, cat, elephant**).  \n",
    "\n",
    "ğŸ”¹ **How It Works (Simple Way)**  \n",
    "- Suppose the model predicts:  \n",
    "  - **Dog: 60%**  \n",
    "  - **Cat: 30%**  \n",
    "  - **Elephant: 10%**  \n",
    "- If the real answer is **Dog**, then the loss function says:  \n",
    "  - \"Oh great! 60% confidence is not bad, but higher is better!\"  \n",
    "  - If the model had said **90% Dog**, the loss would be even smaller.  \n",
    "  - If the model said **90% Elephant**, the loss would be **huge** because it's completely wrong.  \n",
    "\n",
    "ğŸ“Œ **Key Takeaway**:  \n",
    "- The **higher** the probability for the correct class, the **lower the loss**.  \n",
    "- The **more confident but wrong** the model is, the **higher the loss**.  \n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ **Sparse Categorical Cross-Entropy**  \n",
    "ğŸ‘‰ Same as **Categorical Cross-Entropy**, but for when labels are **numbers instead of one-hot vectors**.  \n",
    "(E.g., Instead of `[0, 1, 0]` for \"Cat,\" we just use `1` to represent \"Cat\").  \n",
    "\n",
    "ğŸ“Œ **Use This If:**  \n",
    "- Your labels are **just numbers** (e.g., `0 = Dog, 1 = Cat, 2 = Elephant`).  \n",
    "- It works exactly like categorical cross-entropy but **saves memory**.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ† **Which One Should You Use?**\n",
    "âœ… **Binary Classification (Yes/No, 0/1, Spam/Not Spam)** â†’ **Binary Cross-Entropy**  \n",
    "âœ… **Multi-Class Classification (More than 2 categories)** â†’ **Categorical Cross-Entropy**  \n",
    "âœ… **Multi-Class with Numeric Labels (0,1,2 instead of [0,1,0])** â†’ **Sparse Categorical Cross-Entropy**  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸŒŸ **Summary (Super Simple ğŸ˜ƒ)**  \n",
    "- **Loss function** tells us **how bad** the modelâ€™s prediction is.  \n",
    "- **Smaller loss** = **Better model**  \n",
    "- **Binary Cross-Entropy** = For **2 categories**  \n",
    "- **Categorical Cross-Entropy** = For **3+ categories**  \n",
    "- **Sparse Categorical Cross-Entropy** = Same as above, but for **numeric labels**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Backpropagation in Deep Learning â€“ A Colorful Journey!** ğŸ¯  \n",
    "\n",
    "Imagine you're teaching a young artist how to paint. ğŸ¨ Initially, their strokes are random, and the colors may not blend well. But with guidance (feedback), they refine their skills over time. This is exactly how **backpropagation** works in deep learningâ€”it's a feedback mechanism that helps a neural network improve its predictions by correcting its mistakes! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  What is Backpropagation?**  \n",
    "Backpropagation (short for **Backward Propagation of Errors**) is the learning algorithm used to train deep neural networks. It fine-tunes the weights of neurons so that the model **minimizes its error and improves accuracy**.  \n",
    "\n",
    "ğŸ’¡ **Key Idea:**  \n",
    "- The network **makes a prediction**  \n",
    "- It **compares** the prediction with the actual output using a **loss function**  \n",
    "- The difference (error) is sent **backward** through the network  \n",
    "- The network **adjusts weights** layer by layer to reduce errors  \n",
    "\n",
    "This process repeats for multiple **epochs** until the model is well-trained! ğŸ“Š\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒˆ Step-by-Step Walkthrough of Backpropagation**  \n",
    "\n",
    "### **1ï¸âƒ£ Forward Pass ğŸš€**\n",
    "The input data passes **forward** through the network, layer by layer, until we get an output prediction.  \n",
    "\n",
    "ğŸ¯ **Example:**  \n",
    "Imagine weâ€™re training a deep learning model to classify images of ğŸ¶ dogs and ğŸ± cats.  \n",
    "\n",
    "- Input: An image  \n",
    "- Hidden layers: Apply **weights** and **activations** to extract patterns  \n",
    "- Output: Probability scores â†’ \"70% Dog, 30% Cat\"  \n",
    "\n",
    "At this point, the model makes a guess, but we need to know **how far off** it is from the true answer.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Compute Loss (Error Calculation) âš–ï¸**\n",
    "The error is measured using a **loss function**.  \n",
    "\n",
    "ğŸ”¹ If our model predicted **70% Dog** but the correct answer was **100% Dog**, we need a way to quantify this mistake.  \n",
    "\n",
    "ğŸ’¡ Common loss functions:  \n",
    "- **Mean Squared Error (MSE)** â€“ Used for regression problems ğŸ“‰  \n",
    "- **Cross-Entropy Loss** â€“ Used for classification problems ğŸ“Š  \n",
    "\n",
    "The larger the error, the more **adjustments** the network needs! ğŸ› ï¸  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Backward Pass (Error Propagation) ğŸ”„**\n",
    "Now comes the **magic** of backpropagation! âœ¨  \n",
    "\n",
    "1ï¸âƒ£ The error signal is **propagated backward** from the output layer to the input layer.  \n",
    "2ï¸âƒ£ Each weight in the network is **updated** using the **Gradient Descent algorithm** (or its variants like Adam, RMSprop, etc.).  \n",
    "3ï¸âƒ£ The gradients are computed using **partial derivatives** (this is where **calculus** kicks in! ğŸ§®).  \n",
    "\n",
    "**Mathematically, this uses:**  \n",
    "ğŸ“Œ **Chain Rule of Differentiation** â€“ It helps calculate how much each weight contributed to the error.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Weight Updates (Learning the Right Patterns) ğŸ”„**\n",
    "Each weight in the network is updated using:  \n",
    "\n",
    "ğŸ“Œ **Gradient Descent Formula:**  \n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "where:  \n",
    "- $ W_{\\text{new}} $ = Updated weight  \n",
    "- $ W_{\\text{old}} $ = Current weight  \n",
    "- $ \\eta $ (learning rate) = Small step to avoid overshooting  \n",
    "- $ \\frac{\\partial L}{\\partial W} $ = Gradient (rate of change of loss with respect to weight)  \n",
    "\n",
    "ğŸ’¡ Think of it as **correcting an artistâ€™s brush strokes**â€”small refinements make the painting (model) better over time! ğŸ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒ€ The Process Repeats Until Convergence!**  \n",
    "The steps above repeat **for multiple iterations (epochs)**, and the model gradually **learns better**! ğŸ“ˆ  \n",
    "\n",
    "ğŸ¯ **Final Outcome?**  \n",
    "- The model **reduces its errors**  \n",
    "- The predictions become more **accurate**  \n",
    "- The weights get **optimized** to detect **patterns** effectively  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Why is Backpropagation Important?**  \n",
    "âœ… **Allows deep networks to learn complex patterns**  \n",
    "âœ… **Makes training efficient** using gradient-based optimization  \n",
    "âœ… **Works with any differentiable activation function**  \n",
    "âœ… **Foundation of modern AI** (CNNs, RNNs, Transformersâ€”all rely on it!)  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary in a Nutshell! ğŸŒŸ**\n",
    "ğŸ’¡ Backpropagation is like a **teacher correcting a studentâ€™s mistakes** step by step! ğŸ‘¨â€ğŸ«  \n",
    "\n",
    "ğŸš€ **Forward Pass** â†’ Make a prediction  \n",
    "âš–ï¸ **Compute Loss** â†’ Measure the error  \n",
    "ğŸ”„ **Backward Pass** â†’ Adjust weights using gradients  \n",
    "ğŸ” **Repeat** until the model gets better  \n",
    "\n",
    "Itâ€™s a beautiful cycle of learning and improvement, just like **how humans refine their skills**! ğŸ†  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§© Fun Fact: Why is it Called \"Backpropagation\"?**\n",
    "Because we **propagate** (send) errors **backward** through the network! ğŸ”™ğŸ¯  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the famous **MNIST dataset** ğŸ–¼ï¸ (handwritten digit classification) and see where **backpropagation** is used in a neural network to improve accuracy! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ About the MNIST Dataset**  \n",
    "The **MNIST dataset** contains **70,000 images** (28Ã—28 pixels) of handwritten digits (0-9). The goal is to build a model that can recognize these digits correctly. ğŸ†  \n",
    "\n",
    "ğŸ“Œ **Dataset Details:**  \n",
    "- **60,000** training images ğŸ“š  \n",
    "- **10,000** testing images ğŸ“  \n",
    "- Each image is **grayscale (1 channel)**  \n",
    "- Each pixel value is **0-255**, representing intensity ğŸ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Building a Neural Network with Backpropagation (Step-by-Step)**\n",
    "We will use **TensorFlow & Keras** to build a **feedforward neural network** (MLP) that classifies digits using **backpropagation** for training.  \n",
    "\n",
    "### **ğŸ“Œ Step 1: Import Libraries & Load Data**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (Scale pixel values to 0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "```\n",
    "ğŸ¯ **Why Normalize?**  \n",
    "Scaling ensures that **gradient updates are stable** during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 2: Define the Neural Network**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Convert 2D image to 1D\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "```\n",
    "ğŸ§  **What Happens Here?**  \n",
    "- The **Flatten layer** converts each 28Ã—28 image into a **1D array of 784 pixels**.  \n",
    "- A **Dense layer with 128 neurons** and **ReLU activation** extracts important features.  \n",
    "- The **final Dense layer with 10 neurons** uses **Softmax** to output probabilities for digits 0-9.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 3: Compile the Model**\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "ğŸ¯ **Where is Backpropagation Used Here?**  \n",
    "- The **loss function (categorical crossentropy)** computes how wrong the model is.  \n",
    "- **Adam optimizer** (which uses **gradient descent**) **adjusts weights** using backpropagation.  \n",
    "- The **network updates weights** to minimize loss **in every epoch**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 4: Train the Model (Backpropagation Happens Here!)**\n",
    "```python\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "ğŸ”¥ **Backpropagation in Action!**\n",
    "- **Forward Pass:** The input images pass through the layers to generate predictions.  \n",
    "- **Compute Loss:** The loss function calculates how far off the predictions are.  \n",
    "- **Backward Pass:**  \n",
    "  - **Gradients of the loss w.r.t. each weight** are computed using **chain rule** (calculus ğŸ“–).  \n",
    "  - These gradients **flow backward** through the network.  \n",
    "  - The optimizer **updates the weights** to reduce the error in the next iteration.  \n",
    "- **Repeat** for multiple epochs until the model converges!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 5: Evaluate the Model**\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "```\n",
    "ğŸ¯ **Final Results:**  \n",
    "- The model learns over time using **backpropagation**.  \n",
    "- The accuracy improves as **weights are fine-tuned** with each epoch.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary â€“ Where Did Backpropagation Happen?**\n",
    "1ï¸âƒ£ **Forward Pass:** Input â†’ Layers â†’ Output Predictions ğŸ¯  \n",
    "2ï¸âƒ£ **Compute Loss:** Measures the error âš–ï¸  \n",
    "3ï¸âƒ£ **Backward Pass:**  \n",
    "   - Computes gradients using the **chain rule** ğŸ”„  \n",
    "   - Updates weights using **gradient descent** ğŸ“‰  \n",
    "   - Repeats the process for multiple **epochs**  \n",
    "\n",
    "**This is how backpropagation enables deep learning models to learn from mistakes!** ğŸš€ğŸ”¥  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question! Let's break it down clearly. **Forward propagation** is the process where inputs pass through the network **layer by layer** until we get an output prediction.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”„ Where is Forward Propagation in Code?**  \n",
    "\n",
    "Forward propagation happens **whenever we call the model on data**.  \n",
    "This occurs during:  \n",
    "- **Training (`model.fit()`)** â†’ Forward pass + Backpropagation  \n",
    "- **Prediction (`model.predict()`)** â†’ Only Forward pass  \n",
    "\n",
    "Let's explicitly separate **forward propagation** in the code!\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸš€ Full Code Highlighting Forward Propagation**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (scale pixel values to 0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define the neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Converts 2D image to 1D\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "# Compile the model (choosing optimizer & loss function)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# - ğŸš€ Forward Propagation Happens Here! - #\n",
    "# When calling model.fit(), the input data (x_train) goes through:\n",
    "# 1. Flatten layer â†’ Converts image into 1D array\n",
    "# 2. Dense (128 neurons, ReLU) â†’ Extracts important features\n",
    "# 3. Dense (10 neurons, Softmax) â†’ Outputs probabilities for each digit (0-9)\n",
    "# The model generates predictions, which are compared with y_train.\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "### **ğŸ”¹ What Happens in Forward Propagation?**\n",
    "1ï¸âƒ£ The **Flatten** layer reshapes input images.  \n",
    "2ï¸âƒ£ The first **Dense (128 neurons, ReLU)** transforms data with learned weights.  \n",
    "3ï¸âƒ£ The second **Dense (10 neurons, Softmax)** gives probability scores for each digit.  \n",
    "4ï¸âƒ£ The model **outputs predictions** â†’ Compared with actual labels (`y_train`).  \n",
    "\n",
    "During `model.fit()`, TensorFlow does both **forward propagation** (to get predictions) and **backpropagation** (to adjust weights).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Explicit Forward Propagation for Prediction**\n",
    "If you only want **forward propagation** (without backpropagation), you can use `model.predict()`:\n",
    "\n",
    "```python\n",
    "# Perform forward propagation on test images\n",
    "sample_image = x_test[:5]  # Take 5 sample images\n",
    "predictions = model.predict(sample_image)  # ğŸš€ Only Forward Propagation Here!\n",
    "\n",
    "# Print predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted Labels:\", predicted_classes)\n",
    "```\n",
    "âœ… Here, **only forward propagation** is performed.  \n",
    "âŒ No backpropagation, since we're not updating weights.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary**\n",
    "- **Forward Propagation:** Happens in `model.fit()` (during training) and `model.predict()` (during inference).  \n",
    "- **Backpropagation:** Only happens during `model.fit()` to adjust weights using gradient descent.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸŒŸ Understanding the Concept of Gradient in Backpropagation ğŸŒŸ**  \n",
    "\n",
    "In deep learning, **backpropagation** is the magic behind training a neural network. The **gradient** plays a crucial role in this process by guiding how the model updates its weights.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is a Gradient?**\n",
    "A **gradient** is simply the **slope of a function**.  \n",
    "In deep learning, this function is the **loss function** (which measures how wrong the model is).  \n",
    "The gradient tells us **how much to adjust each weight** to reduce the error.\n",
    "\n",
    "ğŸ”¹ Mathematically, the gradient is the **derivative** of the loss function with respect to the weights:  \n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "$$  \n",
    "This tells us **how a small change in weights (W) affects the loss**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Step-by-Step Explanation of Gradients in Backpropagation**\n",
    "Backpropagation consists of **two main steps**:  \n",
    "1ï¸âƒ£ **Forward Propagation** â†’ Compute predictions ğŸ”®  \n",
    "2ï¸âƒ£ **Backward Propagation (Backprop)** â†’ Compute gradients & update weights ğŸ”„  \n",
    "\n",
    "Letâ€™s go deeper into **step 2 (Backward Propagation)**, where the gradient plays a key role!\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 1: Compute the Loss**\n",
    "First, we calculate how wrong the model is using a **loss function**.  \n",
    "For example, if we use **Mean Squared Error (MSE)** for regression:  \n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{N} \\sum (y_{\\text{true}} - y_{\\text{predicted}})^2\n",
    "$$  \n",
    "Or for classification, we often use **Cross-Entropy Loss**:  \n",
    "$$\n",
    "\\text{Loss} = - \\sum y_{\\text{true}} \\log(y_{\\text{predicted}})\n",
    "$$  \n",
    "**Goal:** Minimize this loss by updating weights using gradients.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 2: Compute Gradients (Partial Derivatives)**\n",
    "Using **calculus (chain rule)**, we compute how much **each weight** contributes to the error.\n",
    "\n",
    "Example for a single neuron:\n",
    "$$\n",
    "z = W \\cdot x + b\n",
    "$$\n",
    "$$\n",
    "a = \\text{activation}(z)\n",
    "$$\n",
    "$$\n",
    "\\text{Loss} = f(a, y)\n",
    "$$\n",
    "\n",
    "To update the weights, we compute:\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial W} = \\frac{\\partial \\text{Loss}}{\\partial a} \\times \\frac{\\partial a}{\\partial z} \\times \\frac{\\partial z}{\\partial W}\n",
    "$$\n",
    "\n",
    "This gives us the **gradient**, which tells us how much to update **W**.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 3: Update Weights Using Gradient Descent**\n",
    "Now that we have the gradients, we use **Gradient Descent** to update the weights.\n",
    "\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ Here, **Î· (eta)** is the **learning rate**, controlling how big the updates are.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Example: Manual Gradient Calculation**\n",
    "Letâ€™s say we have a simple model:\n",
    "\n",
    "$$\n",
    "y = W \\cdot x + b\n",
    "$$\n",
    "\n",
    "Suppose:\n",
    "- **W = 2**, **b = 1**\n",
    "- **x = 3**\n",
    "- True **y = 10**\n",
    "- Our model predicts:  \n",
    "  $$\n",
    "  y_{\\text{pred}} = (2 \\times 3) + 1 = 7\n",
    "  $$\n",
    "\n",
    "Loss (Mean Squared Error):\n",
    "$$\n",
    "\\text{Loss} = (10 - 7)^2 = 9\n",
    "$$\n",
    "\n",
    "### **ğŸ”¹ Compute Gradient**\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial W} = 2 \\times (y_{\\text{pred}} - y_{\\text{true}}) \\times x\n",
    "$$\n",
    "$$\n",
    "= 2 \\times (7 - 10) \\times 3 = -18\n",
    "$$\n",
    "\n",
    "### **ğŸ”¹ Update Weight**\n",
    "Using learning rate **Î· = 0.01**:\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - 0.01 \\times (-18)\n",
    "$$\n",
    "$$\n",
    "= 2 + 0.18 = 2.18\n",
    "$$\n",
    "\n",
    "The weight **W** is updated from **2 to 2.18**, reducing the error in the next step.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary: Why is the Gradient Important?**\n",
    "- The gradient tells **how much** to update weights in backpropagation.\n",
    "- Itâ€™s calculated using **partial derivatives** (chain rule).\n",
    "- We use **gradient descent** to adjust the weights **step by step**.\n",
    "- Small **gradients** â†’ slow learning ğŸ“‰  \n",
    "- Large **gradients** â†’ unstable learning ğŸ“ˆ  \n",
    "- A **proper learning rate** is needed to balance updates âš–ï¸  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸŒŸ Understanding Minima & Convergence in Backpropagation ğŸŒŸ**  \n",
    "\n",
    "Backpropagation works by adjusting the model's weights **step by step** to reduce the **loss function** (error).  \n",
    "The ultimate goal? **Find the best set of weights that minimizes the loss**. This process leads us to the concepts of **Minima** and **Convergence**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is a Minima?**\n",
    "A **minima** is a point where the **loss function is at its lowest** (or at least a local low point).  \n",
    "Since training a deep learning model is like finding the lowest point in a mountain range, we use **gradient descent** to navigate towards the **minima** step by step.\n",
    "\n",
    "ğŸ”¹ **Types of Minima:**  \n",
    "1ï¸âƒ£ **Global Minima** ğŸŒ â†’ The lowest possible loss value  \n",
    "2ï¸âƒ£ **Local Minima** ğŸ”ï¸ â†’ A low point, but not necessarily the lowest  \n",
    "3ï¸âƒ£ **Saddle Point** âš–ï¸ â†’ A flat region where gradients become very small  \n",
    "\n",
    "**Goal:** We want to reach the **global minima** (or at least a good local minima) where our model has the **best accuracy**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is Convergence?**\n",
    "**Convergence** means that the modelâ€™s loss is **not decreasing anymore**, meaning it has reached a stable point.  \n",
    "\n",
    "ğŸ”¹ **How does this happen?**\n",
    "- During backpropagation, we **update weights** using **gradient descent**.\n",
    "- If the steps are too large â†’ We might **overshoot** the minima.  \n",
    "- If the steps are too small â†’ The training **takes forever**.  \n",
    "- If the gradient becomes **almost zero** â†’ The model **converged**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Step-by-Step Explanation of Minima & Convergence in Backpropagation**\n",
    "\n",
    "### **ğŸ“Œ Step 1: Compute Gradient (Direction of Movement)**\n",
    "The **gradient** tells us **which direction to move** to reduce the loss:\n",
    "$$\n",
    "\\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "### **ğŸ“Œ Step 2: Update Weights (Move Toward Minima)**\n",
    "We use **Gradient Descent** to update the weights:\n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "### **ğŸ“Œ Step 3: Check if We Reached Minima (Convergence)**\n",
    "If the **gradient is close to zero**, the model has likely **converged**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Example: Visualizing Minima & Convergence**\n",
    "Imagine a **bowl-shaped** loss function:\n",
    "\n",
    "ğŸ”¹ If we **start at the top**, the **gradient is large**, so we take **big steps** downhill.  \n",
    "ğŸ”¹ As we **get closer to the bottom**, the **gradient gets smaller**, and we take **smaller steps**.  \n",
    "ğŸ”¹ When the **gradient is nearly zero**, we **stop updating weights** â†’ **Convergence!**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Example: Code for Minima & Convergence**\n",
    "Letâ€™s visualize this with **Gradient Descent** in Python! ğŸš€  \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a simple loss function (parabola: y = x^2)\n",
    "def loss_function(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Define the gradient (derivative of loss function)\n",
    "def gradient(x):\n",
    "    return 2 * x\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "x = 5  # Start at x=5 (far from minima)\n",
    "learning_rate = 0.1  # Step size\n",
    "history = [x]  # Store path of x\n",
    "\n",
    "# Run gradient descent for 20 steps\n",
    "for i in range(20):\n",
    "    grad = gradient(x)  # Compute gradient\n",
    "    x = x - learning_rate * grad  # Update x\n",
    "    history.append(x)  # Store new x\n",
    "\n",
    "# Plot the loss function\n",
    "x_vals = np.linspace(-6, 6, 100)\n",
    "y_vals = loss_function(x_vals)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x_vals, y_vals, label=\"Loss Function\")\n",
    "plt.scatter(history, loss_function(np.array(history)), color=\"red\", label=\"Gradient Descent Steps\")\n",
    "plt.xlabel(\"Weight (x)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Finding the Minima Using Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "### **ğŸ”¹ What Happens Here?**\n",
    "âœ… The model **starts at x=5** and gradually moves toward **x=0** (global minima).  \n",
    "âœ… The learning rate controls **how fast we move**.  \n",
    "âœ… The gradient decreases as we approach the **minima**, leading to **convergence**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Factors Affecting Convergence**\n",
    "1ï¸âƒ£ **Learning Rate (Î·)**\n",
    "   - Too **high** â†’ Overshooting ğŸ¹  \n",
    "   - Too **low** â†’ Slow training ğŸŒ  \n",
    "   - **Optimal** â†’ Fast & smooth convergence  \n",
    "\n",
    "2ï¸âƒ£ **Loss Function Shape**\n",
    "   - If the function is **complex**, it may have **multiple local minima**.\n",
    "   - Some optimizers (like Adam) help avoid getting **stuck in bad local minima**.\n",
    "\n",
    "3ï¸âƒ£ **Number of Iterations (Epochs)**\n",
    "   - Too **few** â†’ No convergence ğŸš«  \n",
    "   - Too **many** â†’ Wastes resources ğŸ”‹  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary: Why are Minima & Convergence Important?**\n",
    "- **Minima** â†’ The point where the loss is at its lowest.  \n",
    "- **Convergence** â†’ When the model reaches a stable point with minimal loss.  \n",
    "- **Gradient Descent** helps us move toward the minima by updating weights.  \n",
    "- Choosing the **right learning rate** ensures smooth convergence.  \n",
    "\n",
    "![](images/bkp.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸŒŸ Understanding MLP Memorization in Backpropagation ğŸŒŸ**  \n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is a type of neural network that learns by adjusting weights using **backpropagation**. However, during training, it can sometimes **memorize** the training data instead of generalizing well. This is called **memorization** or **overfitting**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is MLP Memorization?**\n",
    "When training an MLP, the model **should learn patterns** that can be applied to unseen data.  \n",
    "However, if the MLP **memorizes** specific training examples instead of understanding the underlying patterns, it **fails to generalize** to new data.  \n",
    "\n",
    "This happens when:\n",
    "âœ… The model is **too complex** (too many neurons & layers).  \n",
    "âœ… There is **too little training data**.  \n",
    "âœ… The model is **trained for too many epochs**.  \n",
    "\n",
    "ğŸ’¡ **Memorization = Overfitting**  \n",
    "A model that **memorizes** training data has **high accuracy on training data** but **poor accuracy on test data**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ How Does Memorization Happen in Backpropagation?**\n",
    "Letâ€™s break it down into **four steps**:\n",
    "\n",
    "### **ğŸ“Œ Step 1: Forward Propagation (Making Predictions)**\n",
    "- The MLP takes an input **X** and passes it through hidden layers.\n",
    "- Each neuron applies weights **W** and biases **b** to compute **activations** using:  \n",
    "  $$\n",
    "  z = W \\cdot X + b\n",
    "  $$\n",
    "- The activation function (ReLU, Sigmoid, etc.) transforms **z** into **a**:\n",
    "  $$\n",
    "  a = \\text{activation}(z)\n",
    "  $$\n",
    "\n",
    "### **ğŸ“Œ Step 2: Compute Loss (Error Measurement)**\n",
    "- The loss function (e.g., **MSE** for regression, **Cross-Entropy** for classification) measures how wrong the model is:\n",
    "  $$\n",
    "  \\text{Loss} = \\frac{1}{N} \\sum (y_{\\text{true}} - y_{\\text{predicted}})^2\n",
    "  $$\n",
    "\n",
    "### **ğŸ“Œ Step 3: Backpropagation (Gradient Computation)**\n",
    "- The model **computes gradients** of the loss w.r.t. weights:\n",
    "  $$\n",
    "  \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "  $$\n",
    "- The gradients tell **how much to update weights**.\n",
    "\n",
    "### **ğŸ“Œ Step 4: Weight Update (Gradient Descent)**\n",
    "- Weights **W** are updated using gradient descent:\n",
    "  $$\n",
    "  W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "  $$\n",
    "- This process repeats for multiple **epochs**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ When Does Backpropagation Lead to Memorization?**\n",
    "ğŸ”¹ If the model **trains too long**, it **perfectly learns** the training data instead of generalizing.  \n",
    "ğŸ”¹ The loss on **training data goes to zero**, but the model **performs poorly on new data**.  \n",
    "ğŸ”¹ This is because the model **memorizes** noise and small details instead of learning general patterns.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Example: Overfitting in an MLP (Memorization Effect)**\n",
    "Letâ€™s train an **MLP on the MNIST dataset** and see how memorization happens. ğŸš€  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple MLP model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(10, activation='softmax') # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 epochs (causing overfitting)\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **ğŸ”¹ Observations:**\n",
    "- The **training loss keeps decreasing**.\n",
    "- But **validation loss starts increasing after some epochs** â†’ **Memorization detected!**\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ How to Prevent Memorization?**\n",
    "To stop an MLP from **memorizing**, we use **regularization techniques**.\n",
    "\n",
    "### **âœ… 1. Reduce Model Complexity**\n",
    "Use **fewer neurons** and **fewer layers** to prevent excessive memorization.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(128, activation='relu'),  # Fewer neurons\n",
    "    Dense(64, activation='relu'),   # Fewer neurons\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **âœ… 2. Use Dropout (Randomly Remove Neurons)**\n",
    "Dropout **removes random neurons** during training, forcing the network to **learn generalized patterns**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer (50% neurons removed)\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **âœ… 3. Use Early Stopping (Stop Before Overfitting)**\n",
    "Stop training **automatically** when the validation loss starts increasing.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **âœ… 4. Use L2 Regularization (Prevent Large Weights)**\n",
    "This prevents memorization by **penalizing large weight values**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary: Why is MLP Memorization a Problem?**\n",
    "ğŸ”¹ **Good Models** â†’ Learn patterns and generalize well.  \n",
    "ğŸ”¹ **Overfitting Models** â†’ Memorize training data and perform poorly on new data.  \n",
    "ğŸ”¹ **Memorization Happens** when:\n",
    "   - The model is **too complex**.\n",
    "   - The model trains **too long**.\n",
    "   - The dataset is **too small**.\n",
    "\n",
    "âœ… **How to Fix It?**\n",
    "- **Reduce model size** (fewer neurons/layers)\n",
    "- **Use Dropout**\n",
    "- **Apply Early Stopping**\n",
    "- **Use Regularization (L2, L1)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1821 - accuracy: 0.9441 - val_loss: 0.0884 - val_accuracy: 0.9714\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0802 - accuracy: 0.9753 - val_loss: 0.0789 - val_accuracy: 0.9764\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0534 - accuracy: 0.9829 - val_loss: 0.0884 - val_accuracy: 0.9735\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.0720 - val_accuracy: 0.9777\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.0762 - val_accuracy: 0.9798\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.0815 - val_accuracy: 0.9772\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0250 - accuracy: 0.9927 - val_loss: 0.0889 - val_accuracy: 0.9788\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0877 - val_accuracy: 0.9813\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.0922 - val_accuracy: 0.9797\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.1122 - val_accuracy: 0.9779\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.1188 - val_accuracy: 0.9787\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.1155 - val_accuracy: 0.9793\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0188 - accuracy: 0.9944 - val_loss: 0.1234 - val_accuracy: 0.9798\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 0.1176 - val_accuracy: 0.9778\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.1332 - val_accuracy: 0.9794\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.1277 - val_accuracy: 0.9811\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.1115 - val_accuracy: 0.9831\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.1305 - val_accuracy: 0.9814\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0131 - accuracy: 0.9968 - val_loss: 0.1316 - val_accuracy: 0.9834\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.1102 - val_accuracy: 0.9849\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.1475 - val_accuracy: 0.9830\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 0.1412 - val_accuracy: 0.9835\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.1550 - val_accuracy: 0.9803\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.1894 - val_accuracy: 0.9783\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0097 - accuracy: 0.9978 - val_loss: 0.1630 - val_accuracy: 0.9780\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0112 - accuracy: 0.9974 - val_loss: 0.1635 - val_accuracy: 0.9808\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.1836 - val_accuracy: 0.9817\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 0.1901 - val_accuracy: 0.9826\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1982 - val_accuracy: 0.9813\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.2176 - val_accuracy: 0.9794\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.1966 - val_accuracy: 0.9831\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.2060 - val_accuracy: 0.9791\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.1775 - val_accuracy: 0.9818\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.2087 - val_accuracy: 0.9823\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.2556 - val_accuracy: 0.9787\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.2786 - val_accuracy: 0.9792\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0102 - accuracy: 0.9978 - val_loss: 0.2612 - val_accuracy: 0.9813\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.2662 - val_accuracy: 0.9797\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0099 - accuracy: 0.9984 - val_loss: 0.2784 - val_accuracy: 0.9804\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.2454 - val_accuracy: 0.9820\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.2976 - val_accuracy: 0.9801\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 0.2329 - val_accuracy: 0.9831\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0100 - accuracy: 0.9986 - val_loss: 0.2223 - val_accuracy: 0.9822\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.2240 - val_accuracy: 0.9832\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2843 - val_accuracy: 0.9810\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.2370 - val_accuracy: 0.9838\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.2966 - val_accuracy: 0.9827\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.2571 - val_accuracy: 0.9824\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.2825 - val_accuracy: 0.9803\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.2859 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple MLP model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(10, activation='softmax') # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 epochs (causing overfitting)\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let me break it down for you in a simple way.  \n",
    "\n",
    "When training a neural network, these metrics help us understand how well the model is **learning** and **generalizing**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Understanding `loss`, `accuracy`, `val_loss`, and `val_accuracy`**\n",
    "\n",
    "ğŸ’¡ The model is trained in **epochs** (full passes over the dataset). For each epoch, the neural network prints out:  \n",
    "\n",
    "| Metric            | Meaning |\n",
    "|------------------|------------------------------------------------|\n",
    "| `loss`          | The model's error on the **training data**. |\n",
    "| `accuracy`      | The percentage of correct predictions on the **training data**. |\n",
    "| `val_loss`      | The model's error on the **validation data** (unseen data). |\n",
    "| `val_accuracy`  | The percentage of correct predictions on the **validation data**. |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Example Breakdown**\n",
    "Letâ€™s analyze your training log:\n",
    "\n",
    "```\n",
    "Epoch 1/50\n",
    "1875/1875 [==============================] - 11s 5ms/step - loss: 0.1837 - accuracy: 0.9436 - val_loss: 0.0986 - val_accuracy: 0.9694\n",
    "```\n",
    "\n",
    "âœ… **Epoch 1** (First pass over the training data):  \n",
    "- **`loss: 0.1837`** â†’ The error (difference between predicted and actual labels) on **training data**.  \n",
    "- **`accuracy: 0.9436`** â†’ The model correctly classified **94.36%** of training data.  \n",
    "- **`val_loss: 0.0986`** â†’ The error on **validation data** (lower is better).  \n",
    "- **`val_accuracy: 0.9694`** â†’ The model correctly classified **96.94%** of validation data.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Epoch 2/50\n",
    "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0789 - accuracy: 0.9751 - val_loss: 0.0810 - val_accuracy: 0.9744\n",
    "```\n",
    "\n",
    "âœ… **Epoch 2** (Second pass over training data):  \n",
    "- **`loss` decreased** from `0.1837 â†’ 0.0789` (Good! Model is learning).  \n",
    "- **`accuracy` increased** from `94.36% â†’ 97.51%` (More correct predictions).  \n",
    "- **`val_loss` decreased** from `0.0986 â†’ 0.0810` (Model is improving on unseen data).  \n",
    "- **`val_accuracy` increased** from `96.94% â†’ 97.44%` (Generalization is good).\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Key Insights**\n",
    "1. **Lower `loss` is better** (it means the model's predictions are getting closer to the actual labels).  \n",
    "2. **Higher `accuracy` is better** (more correct classifications).  \n",
    "3. **If `loss` keeps decreasing but `val_loss` increases**, it means the model is **overfitting** (memorizing training data but failing on new data).  \n",
    "4. **If `accuracy` is high but `val_accuracy` is much lower**, the model is struggling to generalize.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No worries! Let me simplify it even more. Think of training a neural network like teaching a kid to recognize apples and oranges. ğŸğŸŠ  \n",
    "\n",
    "### **ğŸ”¹ What are these terms?**\n",
    "1. **Loss (`loss`)** â€“ How much the kid is making mistakes while learning.  \n",
    "   - A lower value means fewer mistakes.  \n",
    "\n",
    "2. **Accuracy (`accuracy`)** â€“ How often the kid gets the answer right while learning.  \n",
    "   - A higher value means better learning.  \n",
    "\n",
    "3. **Validation Loss (`val_loss`)** â€“ How many mistakes the kid makes on a new test (unseen fruit).  \n",
    "   - If this goes up, it means the kid is memorizing but not understanding.  \n",
    "\n",
    "4. **Validation Accuracy (`val_accuracy`)** â€“ How often the kid gets new test questions right.  \n",
    "   - A high value means the kid can recognize apples and oranges in new situations.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Example in Simple Terms**\n",
    "ğŸ”¹ **Epoch 1:**  \n",
    "- **Loss: 0.1837** â†’ The kid made **some mistakes** while learning.  \n",
    "- **Accuracy: 94.36%** â†’ The kid correctly identified **94.36% of apples and oranges**.  \n",
    "- **Validation Loss: 0.0986** â†’ On a new test, the kid made **fewer mistakes**.  \n",
    "- **Validation Accuracy: 96.94%** â†’ The kid got **96.94% of new test questions right**.  \n",
    "\n",
    "ğŸ”¹ **Epoch 2:**  \n",
    "- **Loss drops to 0.0789** â†’ The kid is making **even fewer mistakes**.  \n",
    "- **Accuracy rises to 97.51%** â†’ The kid is now **97.51% correct** on training data.  \n",
    "- **Validation Loss drops to 0.0810** â†’ The kid is making even fewer mistakes on new questions.  \n",
    "- **Validation Accuracy rises to 97.44%** â†’ The kid is now **97.44% correct on new tests**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Key Takeaway**\n",
    "âœ… **Lower loss = Fewer mistakes**  \n",
    "âœ… **Higher accuracy = More correct answers**  \n",
    "âœ… **Validation scores show how well the model works on new data**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your graph is a plot of **Training Loss vs. Validation Loss** over 50 epochs.\n",
    "\n",
    "### **What does this graph represent?**\n",
    "- **X-axis (Epochs):** Number of training iterations.\n",
    "- **Y-axis (Loss):** The loss function value (how well or poorly the model is performing).\n",
    "- **Two lines:**\n",
    "  - **Training Loss (Blue Line by default in Matplotlib):** How the model performs on training data.\n",
    "  - **Validation Loss (Orange Line by default in Matplotlib):** How the model generalizes to unseen test data.\n",
    "\n",
    "\n",
    "\n",
    "### **What does your output specify?**\n",
    "1. **Initially, both losses decrease:**  \n",
    "   - This is expected because the model is learning patterns in the data.\n",
    "\n",
    "2. **Training loss keeps decreasing, but validation loss starts increasing after some point:**  \n",
    "   - This indicates **overfitting**. Your model is memorizing the training data but failing to generalize to unseen test data.\n",
    "\n",
    "3. **Overfitting happens because of:**  \n",
    "   - **Large model (512 neurons per layer)**: Too many parameters for a simple dataset like MNIST.  \n",
    "   - **Too many epochs (50):** The model keeps learning even after generalization stops improving.\n",
    "\n",
    "\n",
    "\n",
    "### **How to fix overfitting?**\n",
    "1. **Reduce model complexity:**  \n",
    "   - Try fewer neurons (e.g., `Dense(256, activation='relu')` instead of 512).\n",
    "2. **Use dropout layers:**  \n",
    "   - Add `Dropout(0.2)` between dense layers to randomly deactivate some neurons and prevent reliance on specific patterns.\n",
    "3. **Use early stopping:**  \n",
    "   - Stop training when validation loss starts increasing:\n",
    "     ```python\n",
    "     from tensorflow.keras.callbacks import EarlyStopping\n",
    "     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "     ```\n",
    "     Then modify `model.fit()`:\n",
    "     ```python\n",
    "     model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "     ```\n",
    "4. **Reduce epochs:**  \n",
    "   - Instead of 50, try `epochs=20` and see if validation loss stabilizes.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary of Your Graph:**\n",
    "- Your model is **overfitting** after some epochs.\n",
    "- Training loss is decreasing, but validation loss is increasing.\n",
    "- You should try **dropout, early stopping, or reducing model complexity** to improve generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In neural networks, **Gradient Descent** is an optimization algorithm used to minimize the loss function by updating model weights. There are three main types:\n",
    "\n",
    "### **1. Batch Gradient Descent (BGD)**\n",
    "- Uses **the entire dataset** to compute gradients before updating weights.\n",
    "- **Pros:** More stable convergence.\n",
    "- **Cons:** Slow for large datasets, high memory usage.\n",
    "- **Use case:** When dataset is small and fits in memory.\n",
    "\n",
    "### **2. Stochastic Gradient Descent (SGD)**\n",
    "- Updates weights **after each training sample** (one at a time).\n",
    "- **Pros:** Faster updates, handles large datasets.\n",
    "- **Cons:** Noisy updates, may not converge smoothly.\n",
    "- **Use case:** When dataset is very large (e.g., online learning).\n",
    "\n",
    "### **3. Mini-Batch Gradient Descent (MBGD)**\n",
    "- Uses **a small batch** of data (e.g., 32, 64, 128 samples) for updates.\n",
    "- **Pros:** Balances stability (like BGD) and efficiency (like SGD).\n",
    "- **Cons:** Still needs tuning for batch size.\n",
    "- **Use case:** Most commonly used in deep learning.\n",
    "\n",
    "#### **Variants of Gradient Descent:**\n",
    "To improve performance, gradient descent algorithms use techniques like:\n",
    "- **Momentum:** Helps avoid oscillations by adding a fraction of the previous update.\n",
    "- **RMSprop & Adam:** Adjust learning rates dynamically for better convergence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ **Batch Gradient Descent (BGD) - A Complete Guide** ğŸŒŸ  \n",
    "\n",
    "#### ğŸš€ **What is Batch Gradient Descent?**  \n",
    "Batch Gradient Descent (BGD) is a type of **optimization algorithm** used in machine learning and deep learning to minimize the **loss function** by updating the model's weights. In BGD, the entire dataset is used to compute gradients **before** updating the weights.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§  **How Batch Gradient Descent Works?**\n",
    "1ï¸âƒ£ **Compute the Loss**:  \n",
    "   - Calculate the difference between predicted and actual values using a loss function (e.g., Mean Squared Error, Cross-Entropy).  \n",
    "\n",
    "2ï¸âƒ£ **Calculate Gradients**:  \n",
    "   - Derivatives of the loss function with respect to model parameters (weights & biases) are computed **using the entire dataset**.  \n",
    "\n",
    "3ï¸âƒ£ **Update Parameters**:  \n",
    "   - The weights are updated using the formula:  \n",
    "\n",
    "   $$\n",
    "   W = W - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla L(W, x_i, y_i)\n",
    "   $$\n",
    "\n",
    "   where:  \n",
    "   - **W** â†’ Weight parameters  \n",
    "   - **Î± (alpha)** â†’ Learning rate  \n",
    "   - **m** â†’ Number of training samples  \n",
    "   - **âˆ‡L(W, x_i, y_i)** â†’ Gradient of loss  \n",
    "\n",
    "4ï¸âƒ£ **Repeat Until Convergence**:  \n",
    "   - Steps 1-3 are repeated until the loss stops decreasing or reaches an acceptable level.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Advantages of Batch Gradient Descent**\n",
    "âœ… **Stable Updates** â€“ Since the entire dataset is used, updates are smooth and converge steadily.  \n",
    "âœ… **More Accurate Direction** â€“ Since it considers all data points, it provides a better estimate of the optimal weight updates.  \n",
    "âœ… **Less Noisy** â€“ Unlike **Stochastic Gradient Descent (SGD)**, BGD doesnâ€™t have high variance in updates.  \n",
    "\n",
    "\n",
    "\n",
    "### âŒ **Disadvantages of Batch Gradient Descent**\n",
    "âŒ **Slow for Large Datasets** â€“ Since it requires processing all data before updating, it's computationally expensive.  \n",
    "âŒ **High Memory Usage** â€“ Storing and computing gradients for a large dataset can consume a lot of memory (RAM).  \n",
    "âŒ **May Get Stuck in Local Minima** â€“ If the loss function is non-convex, it might converge to a suboptimal point.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ’¡ **When to Use Batch Gradient Descent?**\n",
    "ğŸ”¹ When the dataset is **small** and fits in memory.  \n",
    "ğŸ”¹ When **stable convergence** is more important than speed.  \n",
    "ğŸ”¹ When training simple models like **linear regression or small neural networks**.  \n",
    "\n",
    "### ğŸ†š **BGD vs. Other Gradient Descent Types**\n",
    "| Type | Update Frequency | Speed | Stability | Best For |\n",
    "|------|----------------|-------|-----------|----------|\n",
    "| **Batch GD** | After **all** samples | â³ Slow | âœ… Very Stable | Small datasets |\n",
    "| **Mini-Batch GD** | After **batch** (e.g., 32, 64 samples) | âš¡ Medium | âœ… Stable | Large datasets |\n",
    "| **Stochastic GD (SGD)** | After **each** sample | ğŸš€ Fast | âŒ Noisy | Online learning |\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Conclusion**\n",
    "Batch Gradient Descent is an effective optimization technique when working with **small datasets** and when **stability in training** is crucial. However, for large datasets, **Mini-Batch GD** is usually preferred as it balances speed and stability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's implement **Batch Gradient Descent (BGD) for the Boston Housing Price Prediction** dataset using TensorFlow/Keras.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Steps:**\n",
    "1. **Load the Boston Housing dataset** (from TensorFlow datasets).\n",
    "2. **Preprocess the data** (normalize features).\n",
    "3. **Create a neural network model** for regression.\n",
    "4. **Use Batch Gradient Descent (BGD)** for optimization.\n",
    "5. **Train the model and plot the loss curve**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Boston Housing Price Prediction using BGD**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1ï¸âƒ£ Load the Boston Housing dataset\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2ï¸âƒ£ Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# 3ï¸âƒ£ Define optimizer using **Batch Gradient Descent**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # SGD acts as BGD here\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4ï¸âƒ£ Train model with **Batch Gradient Descent**\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train), validation_data=(X_test, y_test))\n",
    "\n",
    "# 5ï¸âƒ£ Plot the Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **âœ… Why This is Batch Gradient Descent?**\n",
    "âœ” **Batch Size = Full Dataset** â†’ We set `batch_size=len(X_train)`, meaning the entire dataset is used in **one update per epoch**.  \n",
    "âœ” **SGD Optimizer as BGD** â†’ Using **SGD optimizer** but configured to work like **BGD**.  \n",
    "âœ” **Smooth Convergence** â†’ Updates occur after processing all training data.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Key Observations**\n",
    "- **BGD converges smoothly** but may be **slow for large datasets**.\n",
    "- **Loss decreases steadily** due to full batch updates.\n",
    "- **For larger datasets, Mini-Batch GD is more efficient**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n",
      "65536/57026 [==================================] - 0s 1us/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 602.2394 - mae: 22.8352 - val_loss: 518.9860 - val_mae: 20.9726\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 491.1635 - mae: 20.2860 - val_loss: 300.3114 - val_mae: 15.4371\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 279.1856 - mae: 14.7685 - val_loss: 193.6044 - val_mae: 9.9271\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 217.8533 - mae: 10.6372 - val_loss: 535.4437 - val_mae: 21.1485\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 509.0555 - mae: 20.4958 - val_loss: 437.1647 - val_mae: 18.2467\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 413.2386 - mae: 17.5240 - val_loss: 308.1747 - val_mae: 14.8601\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 286.1104 - mae: 14.0277 - val_loss: 142.6949 - val_mae: 9.7066\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 132.8288 - mae: 8.7276 - val_loss: 96.6198 - val_mae: 8.0664\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 87.3651 - mae: 7.8127 - val_loss: 346.3520 - val_mae: 16.3121\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 328.4189 - mae: 15.8044 - val_loss: 138.6027 - val_mae: 9.8796\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 120.0115 - mae: 8.9871 - val_loss: 51.6047 - val_mae: 5.4848\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 44.9958 - mae: 5.3362 - val_loss: 185.6401 - val_mae: 11.3140\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 171.3701 - mae: 10.5991 - val_loss: 72.1754 - val_mae: 6.0167\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 46.4591 - mae: 5.4089 - val_loss: 98.6893 - val_mae: 8.6964\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 94.4614 - mae: 8.3460 - val_loss: 156.7584 - val_mae: 9.7601\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 159.1293 - mae: 9.6484 - val_loss: 399.4370 - val_mae: 17.6710\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 375.9129 - mae: 17.0070 - val_loss: 261.8440 - val_mae: 13.3443\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 241.0462 - mae: 12.3527 - val_loss: 184.3808 - val_mae: 11.2161\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 164.9553 - mae: 10.5162 - val_loss: 108.0986 - val_mae: 7.8548\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 102.6911 - mae: 7.2870 - val_loss: 76.8441 - val_mae: 6.8287\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 65.7057 - mae: 6.4672 - val_loss: 54.6895 - val_mae: 5.8077\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 54.7945 - mae: 5.5023 - val_loss: 75.8686 - val_mae: 7.1499\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 66.4045 - mae: 6.9141 - val_loss: 186.3156 - val_mae: 11.3954\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 176.2180 - mae: 10.9901 - val_loss: 65.6469 - val_mae: 6.3706\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 47.8477 - mae: 5.2945 - val_loss: 23.9890 - val_mae: 3.3580\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.1632 - mae: 2.7991 - val_loss: 22.7281 - val_mae: 3.2496\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 13.2478 - mae: 2.5005 - val_loss: 24.1463 - val_mae: 3.3036\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.0898 - mae: 2.6223 - val_loss: 24.9305 - val_mae: 3.4773\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.6608 - mae: 2.6116 - val_loss: 31.5631 - val_mae: 3.9172\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 18.1574 - mae: 3.2639 - val_loss: 42.7175 - val_mae: 4.9908\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 33.0621 - mae: 4.2268 - val_loss: 54.8142 - val_mae: 5.4569\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 36.0977 - mae: 4.9586 - val_loss: 94.3353 - val_mae: 7.7878\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.1292 - mae: 7.3635 - val_loss: 45.0650 - val_mae: 4.5814\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 21.9069 - mae: 3.7466 - val_loss: 33.7157 - val_mae: 4.3465\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 27.1270 - mae: 3.8031 - val_loss: 44.6761 - val_mae: 4.7759\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 29.0179 - mae: 4.3307 - val_loss: 70.8895 - val_mae: 6.5000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 61.3261 - mae: 5.9087 - val_loss: 43.1546 - val_mae: 4.4435\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 21.0082 - mae: 3.6989 - val_loss: 37.3839 - val_mae: 4.6129\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 29.1484 - mae: 3.9817 - val_loss: 44.0072 - val_mae: 4.6959\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.0945 - mae: 4.2014 - val_loss: 63.1053 - val_mae: 6.0869\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 53.3262 - mae: 5.4594 - val_loss: 41.1606 - val_mae: 4.2734\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 19.4809 - mae: 3.5452 - val_loss: 35.6590 - val_mae: 4.4547\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.4714 - mae: 3.7569 - val_loss: 40.5680 - val_mae: 4.3874\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 23.1945 - mae: 3.8575 - val_loss: 51.9409 - val_mae: 5.4613\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 41.5094 - mae: 4.7523 - val_loss: 40.0717 - val_mae: 4.2118\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 19.5364 - mae: 3.5529 - val_loss: 38.9034 - val_mae: 4.6568\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 28.8330 - mae: 3.9343 - val_loss: 39.5589 - val_mae: 4.2752\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 21.3106 - mae: 3.6977 - val_loss: 46.1682 - val_mae: 5.1132\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 35.5233 - mae: 4.3722 - val_loss: 38.6567 - val_mae: 4.1127\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 18.9031 - mae: 3.4753 - val_loss: 38.5243 - val_mae: 4.6213\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.9311 - mae: 3.8544 - val_loss: 37.7942 - val_mae: 4.1121\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 19.3735 - mae: 3.5037 - val_loss: 41.2256 - val_mae: 4.7930\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 30.2850 - mae: 4.0074 - val_loss: 37.0970 - val_mae: 4.0166\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.2021 - mae: 3.3891 - val_loss: 37.7737 - val_mae: 4.5604\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.8217 - mae: 3.7589 - val_loss: 36.3149 - val_mae: 3.9721\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.9406 - mae: 3.3524 - val_loss: 37.8042 - val_mae: 4.5575\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 26.6727 - mae: 3.7402 - val_loss: 35.6808 - val_mae: 3.9173\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.3443 - mae: 3.2905 - val_loss: 36.3143 - val_mae: 4.4526\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25.1354 - mae: 3.6245 - val_loss: 35.0499 - val_mae: 3.8722\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.9663 - mae: 3.2467 - val_loss: 35.7177 - val_mae: 4.4074\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 24.4692 - mae: 3.5712 - val_loss: 34.4386 - val_mae: 3.8269\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 16.5737 - mae: 3.2034 - val_loss: 34.8949 - val_mae: 4.3466\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 23.5907 - mae: 3.5019 - val_loss: 33.8742 - val_mae: 3.7815\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 16.1888 - mae: 3.1603 - val_loss: 34.1785 - val_mae: 4.2916\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 22.8490 - mae: 3.4415 - val_loss: 33.3186 - val_mae: 3.7364\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 15.8235 - mae: 3.1193 - val_loss: 33.5064 - val_mae: 4.2392\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 22.0912 - mae: 3.3806 - val_loss: 32.8358 - val_mae: 3.6949\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.4964 - mae: 3.0809 - val_loss: 32.9026 - val_mae: 4.1906\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 21.4683 - mae: 3.3289 - val_loss: 32.3503 - val_mae: 3.6557\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.1956 - mae: 3.0463 - val_loss: 32.3191 - val_mae: 4.1436\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 20.8471 - mae: 3.2779 - val_loss: 31.8946 - val_mae: 3.6195\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.9240 - mae: 3.0143 - val_loss: 31.7841 - val_mae: 4.1000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 20.3331 - mae: 3.2337 - val_loss: 31.4280 - val_mae: 3.5821\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.6568 - mae: 2.9832 - val_loss: 31.3378 - val_mae: 4.0640\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 19.8673 - mae: 3.1945 - val_loss: 30.9893 - val_mae: 3.5499\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.4261 - mae: 2.9564 - val_loss: 30.9517 - val_mae: 4.0309\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 19.4331 - mae: 3.1564 - val_loss: 30.6460 - val_mae: 3.5151\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.1639 - mae: 2.9244 - val_loss: 30.4072 - val_mae: 3.9869\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 18.9290 - mae: 3.1118 - val_loss: 30.2210 - val_mae: 3.4868\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.9398 - mae: 2.8986 - val_loss: 30.0474 - val_mae: 3.9553\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18.5294 - mae: 3.0759 - val_loss: 29.8810 - val_mae: 3.4591\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.7075 - mae: 2.8713 - val_loss: 29.6414 - val_mae: 3.9202\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.0874 - mae: 3.0367 - val_loss: 29.6288 - val_mae: 3.4359\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.4911 - mae: 2.8461 - val_loss: 29.1974 - val_mae: 3.8838\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.7126 - mae: 3.0028 - val_loss: 29.1774 - val_mae: 3.4092\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.3062 - mae: 2.8238 - val_loss: 28.9369 - val_mae: 3.8585\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.4045 - mae: 2.9735 - val_loss: 28.8810 - val_mae: 3.3867\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.1128 - mae: 2.8008 - val_loss: 28.5958 - val_mae: 3.8291\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.0762 - mae: 2.9440 - val_loss: 28.5850 - val_mae: 3.3669\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.9173 - mae: 2.7784 - val_loss: 28.2050 - val_mae: 3.7958\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.7205 - mae: 2.9111 - val_loss: 28.2185 - val_mae: 3.3412\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.7248 - mae: 2.7548 - val_loss: 27.9284 - val_mae: 3.7692\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.4145 - mae: 2.8824 - val_loss: 27.9230 - val_mae: 3.3222\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.5537 - mae: 2.7344 - val_loss: 27.6175 - val_mae: 3.7421\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.1257 - mae: 2.8556 - val_loss: 27.7000 - val_mae: 3.3042\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.3952 - mae: 2.7150 - val_loss: 27.2532 - val_mae: 3.7108\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.8355 - mae: 2.8281 - val_loss: 27.3455 - val_mae: 3.2852\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.2432 - mae: 2.6963 - val_loss: 27.0164 - val_mae: 3.6891\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.5764 - mae: 2.8041 - val_loss: 27.1712 - val_mae: 3.2711\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.0997 - mae: 2.6788 - val_loss: 26.7366 - val_mae: 3.6636\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.3132 - mae: 2.7791 - val_loss: 26.8425 - val_mae: 3.2525\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.9475 - mae: 2.6606 - val_loss: 26.5117 - val_mae: 3.6432\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 15.1181 - mae: 2.7605 - val_loss: 26.5715 - val_mae: 3.2308\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.7535 - mae: 2.6367 - val_loss: 26.1809 - val_mae: 3.6115\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.7447 - mae: 2.7245 - val_loss: 26.2951 - val_mae: 3.2085\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.5679 - mae: 2.6132 - val_loss: 25.8931 - val_mae: 3.5854\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.4663 - mae: 2.6978 - val_loss: 26.0426 - val_mae: 3.1899\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.4176 - mae: 2.5944 - val_loss: 25.6399 - val_mae: 3.5631\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.2354 - mae: 2.6758 - val_loss: 25.7309 - val_mae: 3.1712\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2920 - mae: 2.5789 - val_loss: 25.3967 - val_mae: 3.5417\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.0153 - mae: 2.6547 - val_loss: 25.5664 - val_mae: 3.1588\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.1870 - mae: 2.5660 - val_loss: 25.2509 - val_mae: 3.5288\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.8765 - mae: 2.6416 - val_loss: 25.3123 - val_mae: 3.1428\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.0660 - mae: 2.5513 - val_loss: 25.0206 - val_mae: 3.5086\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.6767 - mae: 2.6219 - val_loss: 25.1580 - val_mae: 3.1324\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.9620 - mae: 2.5388 - val_loss: 24.8197 - val_mae: 3.4919\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.5283 - mae: 2.6073 - val_loss: 24.8860 - val_mae: 3.1162\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.8408 - mae: 2.5243 - val_loss: 24.5854 - val_mae: 3.4716\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.3178 - mae: 2.5867 - val_loss: 24.6715 - val_mae: 3.1012\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.7272 - mae: 2.5100 - val_loss: 24.4725 - val_mae: 3.4608\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.1863 - mae: 2.5737 - val_loss: 24.5302 - val_mae: 3.0928\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.6327 - mae: 2.4997 - val_loss: 24.1959 - val_mae: 3.4390\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.0067 - mae: 2.5556 - val_loss: 24.3149 - val_mae: 3.0786\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.5299 - mae: 2.4873 - val_loss: 24.0890 - val_mae: 3.4288\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.8575 - mae: 2.5407 - val_loss: 24.0782 - val_mae: 3.0642\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.4168 - mae: 2.4728 - val_loss: 23.9308 - val_mae: 3.4151\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.6975 - mae: 2.5248 - val_loss: 23.9157 - val_mae: 3.0533\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.3197 - mae: 2.4616 - val_loss: 23.7103 - val_mae: 3.3990\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.5400 - mae: 2.5096 - val_loss: 23.7154 - val_mae: 3.0426\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.2530 - mae: 2.4541 - val_loss: 23.6760 - val_mae: 3.3967\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.4891 - mae: 2.5051 - val_loss: 23.5329 - val_mae: 3.0319\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1798 - mae: 2.4452 - val_loss: 23.5790 - val_mae: 3.3890\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.4018 - mae: 2.4972 - val_loss: 23.4133 - val_mae: 3.0271\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.1135 - mae: 2.4390 - val_loss: 23.4009 - val_mae: 3.3776\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.3113 - mae: 2.4883 - val_loss: 23.2615 - val_mae: 3.0186\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.0590 - mae: 2.4332 - val_loss: 23.3577 - val_mae: 3.3742\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.2387 - mae: 2.4821 - val_loss: 23.1224 - val_mae: 3.0094\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.9873 - mae: 2.4251 - val_loss: 23.1921 - val_mae: 3.3616\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.1190 - mae: 2.4698 - val_loss: 22.9902 - val_mae: 3.0001\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.9079 - mae: 2.4149 - val_loss: 23.0502 - val_mae: 3.3507\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.0084 - mae: 2.4590 - val_loss: 22.7493 - val_mae: 2.9865\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.8306 - mae: 2.4057 - val_loss: 22.9306 - val_mae: 3.3411\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.9010 - mae: 2.4485 - val_loss: 22.6399 - val_mae: 2.9782\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.7472 - mae: 2.3955 - val_loss: 22.7968 - val_mae: 3.3305\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.7727 - mae: 2.4351 - val_loss: 22.4889 - val_mae: 2.9700\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.6612 - mae: 2.3859 - val_loss: 22.6152 - val_mae: 3.3166\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.6187 - mae: 2.4205 - val_loss: 22.3557 - val_mae: 2.9616\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5940 - mae: 2.3772 - val_loss: 22.5186 - val_mae: 3.3109\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.5370 - mae: 2.4126 - val_loss: 22.1791 - val_mae: 2.9513\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5082 - mae: 2.3664 - val_loss: 22.4198 - val_mae: 3.3019\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.4094 - mae: 2.3996 - val_loss: 22.0149 - val_mae: 2.9414\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.4247 - mae: 2.3562 - val_loss: 22.2343 - val_mae: 3.2881\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2778 - mae: 2.3860 - val_loss: 21.7757 - val_mae: 2.9260\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.3363 - mae: 2.3437 - val_loss: 22.0898 - val_mae: 3.2766\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.1292 - mae: 2.3706 - val_loss: 21.6398 - val_mae: 2.9158\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.2532 - mae: 2.3331 - val_loss: 21.9670 - val_mae: 3.2678\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.0142 - mae: 2.3584 - val_loss: 21.5233 - val_mae: 2.9079\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.1808 - mae: 2.3241 - val_loss: 21.8297 - val_mae: 3.2581\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.9297 - mae: 2.3497 - val_loss: 21.3906 - val_mae: 2.8999\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.1203 - mae: 2.3170 - val_loss: 21.7262 - val_mae: 3.2507\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.8372 - mae: 2.3401 - val_loss: 21.3141 - val_mae: 2.8948\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.0622 - mae: 2.3108 - val_loss: 21.6549 - val_mae: 3.2468\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.7723 - mae: 2.3327 - val_loss: 21.2462 - val_mae: 2.8909\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.0122 - mae: 2.3053 - val_loss: 21.5517 - val_mae: 3.2393\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6863 - mae: 2.3234 - val_loss: 21.1018 - val_mae: 2.8819\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.9530 - mae: 2.2977 - val_loss: 21.4241 - val_mae: 3.2298\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.6125 - mae: 2.3155 - val_loss: 21.0043 - val_mae: 2.8751\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.8933 - mae: 2.2899 - val_loss: 21.3577 - val_mae: 3.2244\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.5244 - mae: 2.3059 - val_loss: 20.8550 - val_mae: 2.8653\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.8238 - mae: 2.2806 - val_loss: 21.2321 - val_mae: 3.2166\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.4348 - mae: 2.2959 - val_loss: 20.7896 - val_mae: 2.8625\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.7775 - mae: 2.2755 - val_loss: 21.1104 - val_mae: 3.2078\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.3613 - mae: 2.2887 - val_loss: 20.7029 - val_mae: 2.8559\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.7249 - mae: 2.2688 - val_loss: 21.0379 - val_mae: 3.2033\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.2930 - mae: 2.2812 - val_loss: 20.6053 - val_mae: 2.8532\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.6717 - mae: 2.2628 - val_loss: 20.9196 - val_mae: 3.1941\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.2065 - mae: 2.2719 - val_loss: 20.5545 - val_mae: 2.8477\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.6256 - mae: 2.2568 - val_loss: 20.8364 - val_mae: 3.1872\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.1425 - mae: 2.2648 - val_loss: 20.4973 - val_mae: 2.8454\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.5744 - mae: 2.2507 - val_loss: 20.7291 - val_mae: 3.1792\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.0430 - mae: 2.2539 - val_loss: 20.3326 - val_mae: 2.8342\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.5133 - mae: 2.2422 - val_loss: 20.6380 - val_mae: 3.1729\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.9706 - mae: 2.2460 - val_loss: 20.2471 - val_mae: 2.8246\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.4429 - mae: 2.2326 - val_loss: 20.5506 - val_mae: 3.1647\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.8712 - mae: 2.2335 - val_loss: 20.1170 - val_mae: 2.8178\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.3800 - mae: 2.2243 - val_loss: 20.4137 - val_mae: 3.1536\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.7738 - mae: 2.2222 - val_loss: 20.0575 - val_mae: 2.8132\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.3289 - mae: 2.2187 - val_loss: 20.3316 - val_mae: 3.1464\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.7132 - mae: 2.2156 - val_loss: 19.9985 - val_mae: 2.8081\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.2861 - mae: 2.2130 - val_loss: 20.2232 - val_mae: 3.1387\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.6360 - mae: 2.2072 - val_loss: 19.9154 - val_mae: 2.8024\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.2360 - mae: 2.2059 - val_loss: 20.1500 - val_mae: 3.1324\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5501 - mae: 2.1978 - val_loss: 19.7596 - val_mae: 2.7914\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.1820 - mae: 2.1979 - val_loss: 20.0339 - val_mae: 3.1256\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.4823 - mae: 2.1901 - val_loss: 19.7171 - val_mae: 2.7864\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1362 - mae: 2.1927 - val_loss: 19.9859 - val_mae: 3.1209\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.4213 - mae: 2.1827 - val_loss: 19.6558 - val_mae: 2.7814\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.1008 - mae: 2.1886 - val_loss: 19.8936 - val_mae: 3.1163\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.3562 - mae: 2.1758 - val_loss: 19.4645 - val_mae: 2.7698\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.0410 - mae: 2.1797 - val_loss: 19.8197 - val_mae: 3.1106\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1ï¸âƒ£ Load the Boston Housing dataset\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2ï¸âƒ£ Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# 3ï¸âƒ£ Define optimizer using **Batch Gradient Descent**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # SGD acts as BGD here\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4ï¸âƒ£ Train model with **Batch Gradient Descent**\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train), validation_data=(X_test, y_test))\n",
    "\n",
    "# 5ï¸âƒ£ Plot the Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸ”¥ Stochastic Gradient Descent (SGD) - Full Explanation ğŸ”¥**  \n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm in **machine learning and deep learning** used to minimize the loss function and update model parameters. **Stochastic Gradient Descent (SGD)** is a popular variant of gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ What is Stochastic Gradient Descent (SGD)?**\n",
    "SGD is an optimization technique where **one random training sample** is used to update the modelâ€™s weights at each iteration.\n",
    "\n",
    "ğŸ“Œ **Key Idea:**  \n",
    "Unlike **Batch Gradient Descent (BGD)**, which computes gradients using the **entire dataset**, SGD updates the model **one sample at a time**.\n",
    "\n",
    "ğŸ’¡ **Formula for Weight Update in SGD**:\n",
    "\\[\n",
    "w = w - \\eta \\cdot \\nabla L(w)\n",
    "\\]\n",
    "Where:  \n",
    "ğŸ”¹ \\( w \\) â†’ Model parameters (weights & biases)  \n",
    "ğŸ”¹ \\( \\eta \\) â†’ Learning rate (step size)  \n",
    "ğŸ”¹ \\( \\nabla L(w) \\) â†’ Gradient of the loss function  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ How Does SGD Work?**\n",
    "1ï¸âƒ£ **Randomly pick a single training sample** from the dataset.  \n",
    "2ï¸âƒ£ **Compute the gradient** of the loss function using only this sample.  \n",
    "3ï¸âƒ£ **Update the model parameters** immediately.  \n",
    "4ï¸âƒ£ **Repeat for every sample**, then move to the next epoch.  \n",
    "\n",
    "\n",
    "### **ğŸš¦ Example of SGD vs BGD**\n",
    "| Feature             | Batch Gradient Descent (BGD) | Stochastic Gradient Descent (SGD) |\n",
    "|---------------------|----------------------------|----------------------------|\n",
    "| Update Frequency   | After **full dataset**     | After **each sample**     |\n",
    "| Convergence Speed  | Slower                      | Faster                     |\n",
    "| Memory Usage       | High                        | Low                         |\n",
    "| Stability          | Stable                      | Noisy updates              |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Implementing SGD in Deep Learning (Boston Housing Example)**  \n",
    "Let's implement SGD in a **neural network** using TensorFlow and the **Boston Housing dataset**.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Stochastic Gradient Descent (SGD)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **SGD** (batch_size = 1 means stochastic updates)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"SGD - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **âœ… Observations**\n",
    "âœ” **SGD updates model parameters more frequently** â†’ Faster convergence.  \n",
    "âœ” **Loss function is noisy** because updates happen after each sample.  \n",
    "âœ” **Good for large datasets** â†’ Less memory usage compared to BGD.  \n",
    "âœ” **Can get stuck in local minima** due to randomness.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ When to Use SGD?**\n",
    "âœ… When the dataset is **large** and full-batch updates are slow.  \n",
    "âœ… When memory is limited, as only **one sample is used** at a time.  \n",
    "âœ… When you want faster updates and **real-time learning**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Conclusion**\n",
    "SGD is a powerful optimization algorithm that helps models learn **faster and efficiently**. However, its **random updates** can cause **fluctuations**. Using techniques like **Momentum, RMSProp, and Adam** can help stabilize learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "404/404 [==============================] - 1s 837us/step - loss: 0.0977 - mae: 0.2313 - val_loss: 0.0473 - val_mae: 0.1715\n",
      "Epoch 2/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0272 - mae: 0.1292 - val_loss: 0.0328 - val_mae: 0.1342\n",
      "Epoch 3/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0184 - mae: 0.1053 - val_loss: 0.0283 - val_mae: 0.1214\n",
      "Epoch 4/100\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 0.0148 - mae: 0.0946 - val_loss: 0.0259 - val_mae: 0.1146\n",
      "Epoch 5/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0128 - mae: 0.0887 - val_loss: 0.0238 - val_mae: 0.1110\n",
      "Epoch 6/100\n",
      "404/404 [==============================] - 0s 979us/step - loss: 0.0113 - mae: 0.0845 - val_loss: 0.0209 - val_mae: 0.1036\n",
      "Epoch 7/100\n",
      "404/404 [==============================] - 0s 854us/step - loss: 0.0105 - mae: 0.0809 - val_loss: 0.0214 - val_mae: 0.1032\n",
      "Epoch 8/100\n",
      "404/404 [==============================] - 0s 906us/step - loss: 0.0097 - mae: 0.0776 - val_loss: 0.0196 - val_mae: 0.0983\n",
      "Epoch 9/100\n",
      "404/404 [==============================] - 0s 865us/step - loss: 0.0091 - mae: 0.0753 - val_loss: 0.0204 - val_mae: 0.1004\n",
      "Epoch 10/100\n",
      "404/404 [==============================] - 0s 771us/step - loss: 0.0086 - mae: 0.0730 - val_loss: 0.0188 - val_mae: 0.0960\n",
      "Epoch 11/100\n",
      "404/404 [==============================] - 0s 860us/step - loss: 0.0081 - mae: 0.0709 - val_loss: 0.0176 - val_mae: 0.0923\n",
      "Epoch 12/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0078 - mae: 0.0691 - val_loss: 0.0176 - val_mae: 0.0920\n",
      "Epoch 13/100\n",
      "404/404 [==============================] - 0s 987us/step - loss: 0.0074 - mae: 0.0669 - val_loss: 0.0181 - val_mae: 0.0928\n",
      "Epoch 14/100\n",
      "404/404 [==============================] - 0s 857us/step - loss: 0.0071 - mae: 0.0657 - val_loss: 0.0175 - val_mae: 0.0915\n",
      "Epoch 15/100\n",
      "404/404 [==============================] - 0s 681us/step - loss: 0.0068 - mae: 0.0641 - val_loss: 0.0161 - val_mae: 0.0871\n",
      "Epoch 16/100\n",
      "404/404 [==============================] - 0s 615us/step - loss: 0.0066 - mae: 0.0628 - val_loss: 0.0164 - val_mae: 0.0876\n",
      "Epoch 17/100\n",
      "404/404 [==============================] - 0s 625us/step - loss: 0.0064 - mae: 0.0616 - val_loss: 0.0165 - val_mae: 0.0877\n",
      "Epoch 18/100\n",
      "404/404 [==============================] - 0s 633us/step - loss: 0.0062 - mae: 0.0605 - val_loss: 0.0161 - val_mae: 0.0865\n",
      "Epoch 19/100\n",
      "404/404 [==============================] - 0s 654us/step - loss: 0.0060 - mae: 0.0594 - val_loss: 0.0159 - val_mae: 0.0855\n",
      "Epoch 20/100\n",
      "404/404 [==============================] - 0s 543us/step - loss: 0.0058 - mae: 0.0584 - val_loss: 0.0154 - val_mae: 0.0843\n",
      "Epoch 21/100\n",
      "404/404 [==============================] - 0s 660us/step - loss: 0.0056 - mae: 0.0575 - val_loss: 0.0154 - val_mae: 0.0841\n",
      "Epoch 22/100\n",
      "404/404 [==============================] - 0s 639us/step - loss: 0.0055 - mae: 0.0564 - val_loss: 0.0147 - val_mae: 0.0823\n",
      "Epoch 23/100\n",
      "404/404 [==============================] - 0s 678us/step - loss: 0.0054 - mae: 0.0556 - val_loss: 0.0150 - val_mae: 0.0829\n",
      "Epoch 24/100\n",
      "404/404 [==============================] - 0s 713us/step - loss: 0.0053 - mae: 0.0551 - val_loss: 0.0147 - val_mae: 0.0815\n",
      "Epoch 25/100\n",
      "404/404 [==============================] - 0s 562us/step - loss: 0.0051 - mae: 0.0542 - val_loss: 0.0142 - val_mae: 0.0798\n",
      "Epoch 26/100\n",
      "404/404 [==============================] - 0s 627us/step - loss: 0.0050 - mae: 0.0536 - val_loss: 0.0142 - val_mae: 0.0795\n",
      "Epoch 27/100\n",
      "404/404 [==============================] - 0s 849us/step - loss: 0.0049 - mae: 0.0528 - val_loss: 0.0141 - val_mae: 0.0794\n",
      "Epoch 28/100\n",
      "404/404 [==============================] - 0s 650us/step - loss: 0.0048 - mae: 0.0524 - val_loss: 0.0140 - val_mae: 0.0790\n",
      "Epoch 29/100\n",
      "404/404 [==============================] - 0s 634us/step - loss: 0.0047 - mae: 0.0514 - val_loss: 0.0145 - val_mae: 0.0810\n",
      "Epoch 30/100\n",
      "404/404 [==============================] - 0s 571us/step - loss: 0.0047 - mae: 0.0514 - val_loss: 0.0138 - val_mae: 0.0784\n",
      "Epoch 31/100\n",
      "404/404 [==============================] - 0s 557us/step - loss: 0.0045 - mae: 0.0510 - val_loss: 0.0139 - val_mae: 0.0786\n",
      "Epoch 32/100\n",
      "404/404 [==============================] - 0s 666us/step - loss: 0.0045 - mae: 0.0500 - val_loss: 0.0132 - val_mae: 0.0762\n",
      "Epoch 33/100\n",
      "404/404 [==============================] - 0s 552us/step - loss: 0.0044 - mae: 0.0497 - val_loss: 0.0130 - val_mae: 0.0753\n",
      "Epoch 34/100\n",
      "404/404 [==============================] - 0s 677us/step - loss: 0.0043 - mae: 0.0489 - val_loss: 0.0130 - val_mae: 0.0754\n",
      "Epoch 35/100\n",
      "404/404 [==============================] - 0s 566us/step - loss: 0.0043 - mae: 0.0489 - val_loss: 0.0132 - val_mae: 0.0762\n",
      "Epoch 36/100\n",
      "404/404 [==============================] - 0s 636us/step - loss: 0.0042 - mae: 0.0480 - val_loss: 0.0132 - val_mae: 0.0763\n",
      "Epoch 37/100\n",
      "404/404 [==============================] - 0s 564us/step - loss: 0.0041 - mae: 0.0476 - val_loss: 0.0131 - val_mae: 0.0761\n",
      "Epoch 38/100\n",
      "404/404 [==============================] - 0s 555us/step - loss: 0.0041 - mae: 0.0474 - val_loss: 0.0133 - val_mae: 0.0770\n",
      "Epoch 39/100\n",
      "404/404 [==============================] - 0s 633us/step - loss: 0.0040 - mae: 0.0471 - val_loss: 0.0129 - val_mae: 0.0754\n",
      "Epoch 40/100\n",
      "404/404 [==============================] - 0s 625us/step - loss: 0.0040 - mae: 0.0464 - val_loss: 0.0127 - val_mae: 0.0745\n",
      "Epoch 41/100\n",
      "404/404 [==============================] - 0s 570us/step - loss: 0.0039 - mae: 0.0462 - val_loss: 0.0127 - val_mae: 0.0749\n",
      "Epoch 42/100\n",
      "404/404 [==============================] - 0s 635us/step - loss: 0.0039 - mae: 0.0457 - val_loss: 0.0123 - val_mae: 0.0734\n",
      "Epoch 43/100\n",
      "404/404 [==============================] - 0s 621us/step - loss: 0.0038 - mae: 0.0456 - val_loss: 0.0120 - val_mae: 0.0720\n",
      "Epoch 44/100\n",
      "404/404 [==============================] - 0s 591us/step - loss: 0.0038 - mae: 0.0451 - val_loss: 0.0125 - val_mae: 0.0738\n",
      "Epoch 45/100\n",
      "404/404 [==============================] - 0s 679us/step - loss: 0.0038 - mae: 0.0450 - val_loss: 0.0123 - val_mae: 0.0733\n",
      "Epoch 46/100\n",
      "404/404 [==============================] - 0s 589us/step - loss: 0.0037 - mae: 0.0446 - val_loss: 0.0122 - val_mae: 0.0729\n",
      "Epoch 47/100\n",
      "404/404 [==============================] - 0s 672us/step - loss: 0.0037 - mae: 0.0440 - val_loss: 0.0122 - val_mae: 0.0735\n",
      "Epoch 48/100\n",
      "404/404 [==============================] - 0s 696us/step - loss: 0.0036 - mae: 0.0440 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 49/100\n",
      "404/404 [==============================] - 0s 688us/step - loss: 0.0036 - mae: 0.0437 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 50/100\n",
      "404/404 [==============================] - 0s 664us/step - loss: 0.0035 - mae: 0.0434 - val_loss: 0.0122 - val_mae: 0.0734\n",
      "Epoch 51/100\n",
      "404/404 [==============================] - 0s 533us/step - loss: 0.0035 - mae: 0.0432 - val_loss: 0.0118 - val_mae: 0.0717\n",
      "Epoch 52/100\n",
      "404/404 [==============================] - 0s 582us/step - loss: 0.0035 - mae: 0.0430 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 53/100\n",
      "404/404 [==============================] - 0s 561us/step - loss: 0.0034 - mae: 0.0433 - val_loss: 0.0116 - val_mae: 0.0711\n",
      "Epoch 54/100\n",
      "404/404 [==============================] - 0s 744us/step - loss: 0.0034 - mae: 0.0423 - val_loss: 0.0118 - val_mae: 0.0718\n",
      "Epoch 55/100\n",
      "404/404 [==============================] - 0s 666us/step - loss: 0.0034 - mae: 0.0422 - val_loss: 0.0115 - val_mae: 0.0704\n",
      "Epoch 56/100\n",
      "404/404 [==============================] - 0s 635us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0117 - val_mae: 0.0710\n",
      "Epoch 57/100\n",
      "404/404 [==============================] - 0s 663us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0111 - val_mae: 0.0688\n",
      "Epoch 58/100\n",
      "404/404 [==============================] - 0s 590us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0112 - val_mae: 0.0699\n",
      "Epoch 59/100\n",
      "404/404 [==============================] - 0s 610us/step - loss: 0.0033 - mae: 0.0414 - val_loss: 0.0116 - val_mae: 0.0707\n",
      "Epoch 60/100\n",
      "404/404 [==============================] - 0s 558us/step - loss: 0.0032 - mae: 0.0416 - val_loss: 0.0112 - val_mae: 0.0699\n",
      "Epoch 61/100\n",
      "404/404 [==============================] - 0s 619us/step - loss: 0.0032 - mae: 0.0412 - val_loss: 0.0112 - val_mae: 0.0691\n",
      "Epoch 62/100\n",
      "404/404 [==============================] - 0s 587us/step - loss: 0.0032 - mae: 0.0414 - val_loss: 0.0109 - val_mae: 0.0684\n",
      "Epoch 63/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0032 - mae: 0.0406 - val_loss: 0.0111 - val_mae: 0.0692\n",
      "Epoch 64/100\n",
      "404/404 [==============================] - 0s 568us/step - loss: 0.0032 - mae: 0.0403 - val_loss: 0.0111 - val_mae: 0.0695\n",
      "Epoch 65/100\n",
      "404/404 [==============================] - 0s 583us/step - loss: 0.0031 - mae: 0.0402 - val_loss: 0.0108 - val_mae: 0.0681\n",
      "Epoch 66/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0031 - mae: 0.0400 - val_loss: 0.0111 - val_mae: 0.0691\n",
      "Epoch 67/100\n",
      "404/404 [==============================] - 0s 616us/step - loss: 0.0031 - mae: 0.0405 - val_loss: 0.0110 - val_mae: 0.0690\n",
      "Epoch 68/100\n",
      "404/404 [==============================] - 0s 665us/step - loss: 0.0030 - mae: 0.0398 - val_loss: 0.0109 - val_mae: 0.0688\n",
      "Epoch 69/100\n",
      "404/404 [==============================] - 0s 651us/step - loss: 0.0030 - mae: 0.0399 - val_loss: 0.0108 - val_mae: 0.0686\n",
      "Epoch 70/100\n",
      "404/404 [==============================] - 0s 536us/step - loss: 0.0030 - mae: 0.0396 - val_loss: 0.0107 - val_mae: 0.0679\n",
      "Epoch 71/100\n",
      "404/404 [==============================] - 0s 592us/step - loss: 0.0030 - mae: 0.0396 - val_loss: 0.0108 - val_mae: 0.0684\n",
      "Epoch 72/100\n",
      "404/404 [==============================] - 0s 542us/step - loss: 0.0030 - mae: 0.0395 - val_loss: 0.0105 - val_mae: 0.0673\n",
      "Epoch 73/100\n",
      "404/404 [==============================] - 0s 608us/step - loss: 0.0030 - mae: 0.0392 - val_loss: 0.0108 - val_mae: 0.0684\n",
      "Epoch 74/100\n",
      "404/404 [==============================] - 0s 563us/step - loss: 0.0030 - mae: 0.0390 - val_loss: 0.0105 - val_mae: 0.0673\n",
      "Epoch 75/100\n",
      "404/404 [==============================] - 0s 573us/step - loss: 0.0029 - mae: 0.0389 - val_loss: 0.0106 - val_mae: 0.0675\n",
      "Epoch 76/100\n",
      "404/404 [==============================] - 0s 551us/step - loss: 0.0029 - mae: 0.0389 - val_loss: 0.0103 - val_mae: 0.0666\n",
      "Epoch 77/100\n",
      "404/404 [==============================] - 0s 566us/step - loss: 0.0029 - mae: 0.0387 - val_loss: 0.0104 - val_mae: 0.0669\n",
      "Epoch 78/100\n",
      "404/404 [==============================] - 0s 571us/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0103 - val_mae: 0.0663\n",
      "Epoch 79/100\n",
      "404/404 [==============================] - 0s 672us/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0106 - val_mae: 0.0676\n",
      "Epoch 80/100\n",
      "404/404 [==============================] - 0s 791us/step - loss: 0.0029 - mae: 0.0381 - val_loss: 0.0104 - val_mae: 0.0668\n",
      "Epoch 81/100\n",
      "404/404 [==============================] - 0s 541us/step - loss: 0.0028 - mae: 0.0381 - val_loss: 0.0105 - val_mae: 0.0675\n",
      "Epoch 82/100\n",
      "404/404 [==============================] - 0s 549us/step - loss: 0.0028 - mae: 0.0379 - val_loss: 0.0105 - val_mae: 0.0677\n",
      "Epoch 83/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0104 - val_mae: 0.0671\n",
      "Epoch 84/100\n",
      "404/404 [==============================] - 0s 648us/step - loss: 0.0028 - mae: 0.0378 - val_loss: 0.0103 - val_mae: 0.0666\n",
      "Epoch 85/100\n",
      "404/404 [==============================] - 0s 586us/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0104 - val_mae: 0.0670\n",
      "Epoch 86/100\n",
      "404/404 [==============================] - 0s 631us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0107 - val_mae: 0.0682\n",
      "Epoch 87/100\n",
      "404/404 [==============================] - 0s 568us/step - loss: 0.0027 - mae: 0.0374 - val_loss: 0.0105 - val_mae: 0.0672\n",
      "Epoch 88/100\n",
      "404/404 [==============================] - 0s 583us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0102 - val_mae: 0.0659\n",
      "Epoch 89/100\n",
      "404/404 [==============================] - 0s 651us/step - loss: 0.0027 - mae: 0.0375 - val_loss: 0.0101 - val_mae: 0.0658\n",
      "Epoch 90/100\n",
      "404/404 [==============================] - 0s 559us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0100 - val_mae: 0.0655\n",
      "Epoch 91/100\n",
      "404/404 [==============================] - 0s 634us/step - loss: 0.0027 - mae: 0.0371 - val_loss: 0.0104 - val_mae: 0.0672\n",
      "Epoch 92/100\n",
      "404/404 [==============================] - 0s 556us/step - loss: 0.0026 - mae: 0.0362 - val_loss: 0.0105 - val_mae: 0.0672\n",
      "Epoch 93/100\n",
      "404/404 [==============================] - 0s 581us/step - loss: 0.0027 - mae: 0.0371 - val_loss: 0.0106 - val_mae: 0.0675\n",
      "Epoch 94/100\n",
      "404/404 [==============================] - 0s 591us/step - loss: 0.0026 - mae: 0.0369 - val_loss: 0.0103 - val_mae: 0.0669\n",
      "Epoch 95/100\n",
      "404/404 [==============================] - 0s 564us/step - loss: 0.0026 - mae: 0.0367 - val_loss: 0.0099 - val_mae: 0.0653\n",
      "Epoch 96/100\n",
      "404/404 [==============================] - 0s 589us/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0101 - val_mae: 0.0661\n",
      "Epoch 97/100\n",
      "404/404 [==============================] - 0s 572us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0101 - val_mae: 0.0663\n",
      "Epoch 98/100\n",
      "404/404 [==============================] - 0s 687us/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0099 - val_mae: 0.0652\n",
      "Epoch 99/100\n",
      "404/404 [==============================] - 0s 614us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0102 - val_mae: 0.0663\n",
      "Epoch 100/100\n",
      "404/404 [==============================] - 0s 639us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0101 - val_mae: 0.0658\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target variable (y_train, y_test)\n",
    "y_train = y_train / 50.0  # Scale to [0, 1] range\n",
    "y_test = y_test / 50.0\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer with gradient clipping\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, clipvalue=1.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using SGD with batch_size=1\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"SGD - Loss Reduction (Fixed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸš€ Mini-Batch Gradient Descent (MBGD) â€“ Full Explanation ğŸš€**  \n",
    "\n",
    "Gradient Descent is the backbone of **neural networks and deep learning**, helping to optimize model weights and minimize loss. **Mini-Batch Gradient Descent (MBGD)** is a compromise between **Batch Gradient Descent (BGD)** and **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ What is Mini-Batch Gradient Descent?**  \n",
    "Mini-Batch Gradient Descent (MBGD) updates model parameters **after computing gradients on a small subset (mini-batch) of the dataset**, instead of using **one sample (SGD)** or the **entire dataset (BGD)**.\n",
    "\n",
    "### **ğŸ“ Formula for Weight Update:**\n",
    "\\[\n",
    "w = w - \\eta \\cdot \\nabla L_{batch}(w)\n",
    "\\]\n",
    "Where:  \n",
    "ğŸ”¹ \\( w \\) â†’ Model parameters (weights & biases)  \n",
    "ğŸ”¹ \\( \\eta \\) â†’ Learning rate  \n",
    "ğŸ”¹ \\( \\nabla L_{batch}(w) \\) â†’ Gradient of the loss function computed over a **mini-batch**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ’¡ How Does Mini-Batch Gradient Descent Work?**\n",
    "1ï¸âƒ£ **Divide the dataset** into smaller chunks (mini-batches).  \n",
    "2ï¸âƒ£ **Compute gradients** for each mini-batch.  \n",
    "3ï¸âƒ£ **Update model parameters** after processing each mini-batch.  \n",
    "4ï¸âƒ£ **Repeat the process** until all mini-batches are processed (one epoch).  \n",
    "5ï¸âƒ£ **Multiple epochs** are used to train the model.  \n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ MBGD vs BGD vs SGD â€“ Key Differences**  \n",
    "\n",
    "| Feature                  | **Batch Gradient Descent (BGD)** | **Stochastic Gradient Descent (SGD)** | **Mini-Batch Gradient Descent (MBGD)** |\n",
    "|--------------------------|--------------------------------|--------------------------------|--------------------------------|\n",
    "| **Update Frequency**     | After processing the full dataset | After **each** sample | After **each mini-batch** |\n",
    "| **Convergence Speed**    | Slow                          | Fast but noisy               | Balanced  |\n",
    "| **Memory Usage**         | High (entire dataset needed)  | Low (one sample at a time)   | Moderate  |\n",
    "| **Stability**            | Stable                        | Noisy updates               | More stable than SGD |\n",
    "| **Suitability**          | Small datasets               | Large datasets               | Large datasets with efficient updates |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” When to Use Mini-Batch Gradient Descent?**\n",
    "âœ… When your dataset is **too large** for BGD but **SGD is too noisy**.  \n",
    "âœ… When you want **faster training** with **better generalization**.  \n",
    "âœ… When you need a **balance** between accuracy and computation time.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Implementing Mini-Batch Gradient Descent in Deep Learning**  \n",
    "Let's implement MBGD using **Boston Housing Price Prediction** in TensorFlow.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Mini-Batch Gradient Descent (SGD with batch_size)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **Mini-Batch Gradient Descent** (batch_size = 32)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Mini-Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **âœ… Observations**\n",
    "âœ” **Faster than BGD** because updates happen after every mini-batch.  \n",
    "âœ” **Less noisy than SGD**, resulting in **smoother loss curves**.  \n",
    "âœ” **Reduces memory consumption**, making it **efficient for large datasets**.  \n",
    "âœ” **Better generalization**, preventing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Conclusion**\n",
    "Mini-Batch Gradient Descent is the **most widely used** gradient descent technique in **deep learning**. It balances **accuracy, speed, and memory efficiency**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 15ms/step - loss: 334.2068 - mae: 15.3474 - val_loss: 302.8398 - val_mae: 13.6637\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 314.8437 - mae: 14.6633 - val_loss: 23.8735 - val_mae: 3.8514\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 197.0880 - mae: 10.9435 - val_loss: 308.9723 - val_mae: 15.1685\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 141.5445 - mae: 8.5866 - val_loss: 143.4266 - val_mae: 8.0101\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 52.6857 - mae: 5.3283 - val_loss: 31.5277 - val_mae: 3.6879\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.9593 - mae: 3.7090 - val_loss: 26.3191 - val_mae: 3.3306\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.4636 - mae: 2.7255 - val_loss: 43.6879 - val_mae: 4.4070\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.8018 - mae: 2.6142 - val_loss: 55.4842 - val_mae: 4.9068\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.7193 - mae: 2.5030 - val_loss: 28.5786 - val_mae: 3.8921\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.3195 - mae: 2.3023 - val_loss: 15.7678 - val_mae: 2.6204\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1525 - mae: 2.4044 - val_loss: 27.4883 - val_mae: 3.4278\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.7442 - mae: 2.9163 - val_loss: 34.0931 - val_mae: 3.9571\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6083 - mae: 3.3806 - val_loss: 22.1819 - val_mae: 3.1188\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.2481 - mae: 2.4404 - val_loss: 38.1127 - val_mae: 4.0361\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.3059 - mae: 2.8989 - val_loss: 22.9152 - val_mae: 3.2803\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.8806 - mae: 2.4096 - val_loss: 63.2002 - val_mae: 5.7660\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.2869 - mae: 2.7748 - val_loss: 15.3323 - val_mae: 2.5761\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.3322 - mae: 2.4995 - val_loss: 29.3436 - val_mae: 3.9645\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.5863 - mae: 2.1768 - val_loss: 20.3694 - val_mae: 2.9156\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.5201 - mae: 2.4667 - val_loss: 21.5760 - val_mae: 3.3033\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.1869 - mae: 1.9602 - val_loss: 18.2293 - val_mae: 2.7332\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6905 - mae: 1.9209 - val_loss: 14.5068 - val_mae: 2.4655\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.4515 - mae: 2.2056 - val_loss: 21.1663 - val_mae: 3.2307\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.8506 - mae: 2.0442 - val_loss: 22.7889 - val_mae: 3.3534\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.1949 - mae: 2.0496 - val_loss: 24.4248 - val_mae: 3.0349\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.5546 - mae: 1.8817 - val_loss: 29.3545 - val_mae: 3.3380\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.0953 - mae: 2.1743 - val_loss: 15.3300 - val_mae: 2.6681\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.7649 - mae: 2.1035 - val_loss: 31.6868 - val_mae: 3.5467\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.8374 - mae: 1.8761 - val_loss: 20.2229 - val_mae: 3.1522\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3396 - mae: 1.9608 - val_loss: 12.5534 - val_mae: 2.3076\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6072 - mae: 1.8230 - val_loss: 18.0670 - val_mae: 2.8732\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.4442 - mae: 2.0702 - val_loss: 15.3011 - val_mae: 2.4880\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.2807 - mae: 1.9476 - val_loss: 13.2858 - val_mae: 2.3360\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1076 - mae: 1.8059 - val_loss: 13.6201 - val_mae: 2.4644\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.5027 - mae: 1.6802 - val_loss: 13.0483 - val_mae: 2.3472\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.8429 - mae: 1.9115 - val_loss: 14.5197 - val_mae: 2.4109\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.7792 - mae: 1.7351 - val_loss: 12.8303 - val_mae: 2.2833\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3184 - mae: 1.8165 - val_loss: 15.5969 - val_mae: 2.5462\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.8085 - mae: 1.7402 - val_loss: 21.7635 - val_mae: 3.5196\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.9110 - mae: 2.1411 - val_loss: 25.9408 - val_mae: 3.8190\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.7741 - mae: 2.3996 - val_loss: 14.2894 - val_mae: 2.6520\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3754 - mae: 1.8260 - val_loss: 12.7468 - val_mae: 2.4209\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.7700 - mae: 1.8659 - val_loss: 25.0139 - val_mae: 3.5640\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3425 - mae: 1.8438 - val_loss: 16.6677 - val_mae: 2.7831\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.7480 - mae: 1.5993 - val_loss: 14.2726 - val_mae: 2.5200\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.0188 - mae: 1.8181 - val_loss: 14.9531 - val_mae: 2.9234\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1683 - mae: 2.0713 - val_loss: 27.6664 - val_mae: 3.8346\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3953 - mae: 1.8118 - val_loss: 16.5493 - val_mae: 2.6697\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.7592 - mae: 1.5626 - val_loss: 17.3265 - val_mae: 3.1081\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.6548 - mae: 1.7631 - val_loss: 19.4868 - val_mae: 2.9304\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3022 - mae: 1.9098 - val_loss: 15.4431 - val_mae: 2.6997\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 7.4340 - mae: 1.9040 - val_loss: 14.9380 - val_mae: 2.7081\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.4394 - mae: 1.6904 - val_loss: 11.6773 - val_mae: 2.4353\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.6310 - mae: 2.0342 - val_loss: 22.5566 - val_mae: 2.9882\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.1908 - mae: 1.8150 - val_loss: 16.3558 - val_mae: 3.0166\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.2360 - mae: 1.6514 - val_loss: 15.1944 - val_mae: 2.8790\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.6490 - mae: 1.6891 - val_loss: 11.4101 - val_mae: 2.3474\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.4905 - mae: 1.5896 - val_loss: 11.2625 - val_mae: 2.3189\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.1158 - mae: 1.4880 - val_loss: 11.8765 - val_mae: 2.4649\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.5447 - mae: 1.5014 - val_loss: 22.4625 - val_mae: 3.5457\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3613 - mae: 1.5224 - val_loss: 15.1707 - val_mae: 2.6411\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.4994 - mae: 1.7079 - val_loss: 17.0004 - val_mae: 3.0115\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0269 - mae: 1.8748 - val_loss: 15.7521 - val_mae: 2.5971\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.0140 - mae: 1.4906 - val_loss: 18.1524 - val_mae: 3.1655\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.3321 - mae: 1.9415 - val_loss: 16.2646 - val_mae: 2.7553\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.9999 - mae: 1.5965 - val_loss: 18.6269 - val_mae: 3.1445\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.0333 - mae: 1.6533 - val_loss: 11.6072 - val_mae: 2.3966\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1891 - mae: 1.5161 - val_loss: 12.7922 - val_mae: 2.5739\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.0529 - mae: 1.6238 - val_loss: 13.1982 - val_mae: 2.5362\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.6364 - mae: 1.7058 - val_loss: 12.6474 - val_mae: 2.6933\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.5043 - mae: 1.5326 - val_loss: 14.9170 - val_mae: 2.8120\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.6426 - mae: 1.3899 - val_loss: 10.9990 - val_mae: 2.3338\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.4395 - mae: 1.3535 - val_loss: 11.4481 - val_mae: 2.4128\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1886 - mae: 1.5107 - val_loss: 18.6719 - val_mae: 3.3525\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3952 - mae: 1.5414 - val_loss: 13.1828 - val_mae: 2.6499\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.7353 - mae: 1.3968 - val_loss: 11.1692 - val_mae: 2.3567\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.2529 - mae: 1.4593 - val_loss: 17.7298 - val_mae: 3.0679\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.8893 - mae: 1.4484 - val_loss: 12.3879 - val_mae: 2.5793\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.7071 - mae: 1.4212 - val_loss: 10.7834 - val_mae: 2.2988\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.1919 - mae: 1.4766 - val_loss: 14.3239 - val_mae: 2.7448\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2947 - mae: 1.4778 - val_loss: 13.9169 - val_mae: 2.7749\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.3064 - mae: 1.7971 - val_loss: 15.0297 - val_mae: 2.9737\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.3559 - mae: 1.5205 - val_loss: 20.3863 - val_mae: 3.3175\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2828 - mae: 1.4760 - val_loss: 17.4069 - val_mae: 3.3044\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.6708 - mae: 1.5656 - val_loss: 11.0047 - val_mae: 2.3882\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.3327 - mae: 1.3399 - val_loss: 11.1444 - val_mae: 2.4112\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1779 - mae: 1.4996 - val_loss: 13.8071 - val_mae: 2.4918\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.9320 - mae: 1.4344 - val_loss: 9.7949 - val_mae: 2.2663\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2310 - mae: 1.4256 - val_loss: 15.8810 - val_mae: 2.9804\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.1631 - mae: 1.6193 - val_loss: 12.5935 - val_mae: 2.5695\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.2921 - mae: 1.3352 - val_loss: 12.9919 - val_mae: 2.4504\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3788 - mae: 1.5483 - val_loss: 14.2261 - val_mae: 2.7442\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.4997 - mae: 1.7972 - val_loss: 12.7425 - val_mae: 2.4979\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.8041 - mae: 1.4061 - val_loss: 11.2824 - val_mae: 2.3451\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.4403 - mae: 1.3538 - val_loss: 11.5712 - val_mae: 2.3725\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.4038 - mae: 1.4733 - val_loss: 11.5143 - val_mae: 2.3239\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.7334 - mae: 1.3778 - val_loss: 11.6916 - val_mae: 2.4940\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1548 - mae: 1.3159 - val_loss: 12.3577 - val_mae: 2.5607\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1602 - mae: 1.3004 - val_loss: 15.6877 - val_mae: 2.9056\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8623 - mae: 1.2612 - val_loss: 14.5451 - val_mae: 2.9005\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Mini-Batch Gradient Descent (SGD with batch_size)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **Mini-Batch Gradient Descent** (batch_size = 32)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Mini-Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŠ **Vanishing Gradient Problem in ANN** ğŸ¢  \n",
    "\n",
    "Imagine you're in a **tall building** ğŸ¢ and want to send a **message** ğŸ“© to your friend on the ground floor. You whisper it to the person next to you, they pass it down, and so on. But, by the time it reaches the bottom, it's so faint that your friend **barely hears anything**! That's exactly what happens in a neural network when gradients **\"vanish\"** while training deep layers.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸš¦ **Whatâ€™s Happening?**  \n",
    "In **Artificial Neural Networks (ANNs)**, we train the model using **backpropagation**, which updates weights by calculating **gradients** (small changes to improve accuracy). These gradients flow **from the output layer to the input layer** like a waterfall. ğŸŒŠ But in deep networks, something strange happens:  \n",
    "\n",
    "ğŸ”¹ **Activation functions like Sigmoid or Tanh** squeeze values into small ranges (0 to 1 for Sigmoid, -1 to 1 for Tanh).  \n",
    "ğŸ”¹ When gradients pass through multiple layers, they get **multiplied by small numbers** (derivatives of these activations).  \n",
    "ğŸ”¹ As a result, the gradients **shrink exponentially** and become **too small to make meaningful updates** in earlier layers.  \n",
    "\n",
    "This means the first layers **hardly learn anything**, making deep networks **slow or even useless** in training. ğŸ˜“  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Example in Action**  \n",
    "Let's say you're training a deep ANN for speech recognition ğŸ¤.  \n",
    "\n",
    "1ï¸âƒ£ The last few layers are learning well and updating fast. âœ…  \n",
    "2ï¸âƒ£ The middle layers are learning, but **a bit slowly**. ğŸ¤”  \n",
    "3ï¸âƒ£ The first few layers (closer to input) **hardly change** because their gradients are too tiny. ğŸš«  \n",
    "\n",
    "Your network **struggles to improve** because early layers, which extract important low-level features (like syllables in speech), donâ€™t learn properly!  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ›  **How to Fix It?**  \n",
    "ğŸ’¡ **1. Use ReLU Instead of Sigmoid/Tanh**  \n",
    "   - ReLU (Rectified Linear Unit) âš¡ doesn't squash values into tiny ranges, so it avoids small gradients.  \n",
    "   - Variants like **Leaky ReLU, ELU, and GELU** help prevent neurons from \"dying\" (outputting zero).  \n",
    "\n",
    "ğŸ’¡ **2. Use Batch Normalization**  \n",
    "   - Normalizing activations prevents them from getting too small, stabilizing training. ğŸ“Š  \n",
    "\n",
    "ğŸ’¡ **3. Use Proper Weight Initialization**  \n",
    "   - **Xavier (Glorot) and He Initialization** ensure weights donâ€™t start too small or too big. ğŸ¯  \n",
    "\n",
    "ğŸ’¡ **4. Use Skip Connections (Residual Networks - ResNets)**  \n",
    "   - These \"shortcuts\" let gradients skip layers, helping earlier layers learn! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Final Takeaway**  \n",
    "The **Vanishing Gradient Problem** is like trying to shout across a football stadium ğŸ¤ğŸŸï¸ but your voice keeps fading away. By choosing the right activation functions, normalization techniques, and architectures, we **boost our signal** ğŸ“¡ and train deep networks **efficiently!**  \n",
    "\n",
    "ğŸ”¥ **TL;DR**: Donâ€™t let your gradients get lost in the deep! Keep them strong, and your ANN will learn like a champ. ğŸ†ğŸ’¡\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **How to Improve the Performance of a Neural Network** ğŸ”¥  \n",
    "\n",
    "A neural network is like a **race car** ğŸï¸â€”tuning it correctly makes it faster, more efficient, and more accurate! If your model is slow, inaccurate, or overfitting, you need to **fine-tune** different aspects. Letâ€™s break it down step by step.  \n",
    "\n",
    "\n",
    "\n",
    "## **1ï¸âƒ£ Data Preprocessing & Feature Engineering ğŸ› ï¸**  \n",
    "\n",
    "### ğŸ”¹ **Clean and Augment Your Data**  \n",
    "- **Remove noise & outliers** ğŸ“‰ to avoid misleading patterns.  \n",
    "- **Handle missing values** with imputation or proper encoding.  \n",
    "- **Data augmentation** (flipping, rotation, noise addition) for images/speech/text to increase dataset diversity.  \n",
    "\n",
    "### ğŸ”¹ **Feature Scaling & Normalization**  \n",
    "- **Use StandardScaler** for algorithms that rely on gradients (e.g., Neural Networks).  \n",
    "- **MinMaxScaler** brings data between [0,1]â€”useful for activations like sigmoid.  \n",
    "- **Batch Normalization** stabilizes training by normalizing activations.  \n",
    "\n",
    "### ğŸ”¹ **Feature Selection**  \n",
    "- **Reduce dimensionality** using PCA or Autoencoders ğŸ§©.  \n",
    "- **Select relevant features** using feature importance (from decision trees or SHAP values).  \n",
    "\n",
    "\n",
    "\n",
    "## **2ï¸âƒ£ Choose the Right Model Architecture ğŸ›ï¸**  \n",
    "\n",
    "### ğŸ”¹ **Increase Model Depth (But Not Too Much!)**  \n",
    "- Deep models **capture complex patterns**, but too deep â†’ **vanishing gradient** ğŸš«.  \n",
    "- Use **Residual Networks (ResNet)** or **Dense Connections (DenseNet)** to avoid information loss.  \n",
    "\n",
    "### ğŸ”¹ **Use the Right Activation Functions**  \n",
    "- **ReLU ğŸ”¥** â†’ Best for deep networks (avoids vanishing gradients).  \n",
    "- **Leaky ReLU / ELU** â†’ Prevents â€œdying neuronsâ€ (ReLU can output 0).  \n",
    "- **Softmax** â†’ Good for multi-class classification.  \n",
    "\n",
    "### ğŸ”¹ **Optimize Layer Sizes**  \n",
    "- Too few neurons â†’ **Underfitting** (model lacks capacity).  \n",
    "- Too many neurons â†’ **Overfitting** (model memorizes instead of generalizing).  \n",
    "- Use **GridSearchCV** or **Random Search** to tune layer sizes.  \n",
    "\n",
    "\n",
    "\n",
    "## **3ï¸âƒ£ Improve Training Process ğŸ‹ï¸â€â™‚ï¸**  \n",
    "\n",
    "### ğŸ”¹ **Optimize Learning Rate**  \n",
    "- Too high ğŸ”º â†’ Overshooting, never converging.  \n",
    "- Too low ğŸ”» â†’ Slow training, gets stuck in local minima.  \n",
    "- Use **Learning Rate Schedulers** like ReduceLROnPlateau or **Cyclic Learning Rates**.  \n",
    "\n",
    "### ğŸ”¹ **Use Adaptive Optimizers**  \n",
    "- **Adam ğŸ”¥** â†’ Most commonly used, balances speed and efficiency.  \n",
    "- **RMSprop** â†’ Good for non-stationary problems.  \n",
    "- **SGD with Momentum** â†’ Helps escape local minima.  \n",
    "\n",
    "### ğŸ”¹ **Use Dropout for Regularization**  \n",
    "- Drop random neurons during training to prevent overfitting. ğŸ²  \n",
    "- **Typical values**: 0.2â€“0.5 depending on network size.  \n",
    "\n",
    "### ğŸ”¹ **Early Stopping**  \n",
    "- **Monitor validation loss** ğŸ“‰ and stop training when performance stops improving.  \n",
    "- Prevents overfitting and reduces training time.  \n",
    "\n",
    "\n",
    "\n",
    "## **4ï¸âƒ£ Improve Model Generalization ğŸ¯**  \n",
    "\n",
    "### ğŸ”¹ **Use More Training Data**  \n",
    "- **More data = better generalization** (if high-quality).  \n",
    "- Use **data augmentation** (image flipping, adding noise to text/audio).  \n",
    "\n",
    "### ğŸ”¹ **Use Transfer Learning**  \n",
    "- **Fine-tune a pretrained model** (e.g., VGG16, ResNet, BERT for NLP).  \n",
    "- Saves training time and improves accuracy.  \n",
    "\n",
    "### ğŸ”¹ **Regularization Techniques**  \n",
    "- **L1 Regularization (Lasso)**: Encourages sparsity (feature selection).  \n",
    "- **L2 Regularization (Ridge)**: Prevents large weights, reducing overfitting.  \n",
    "\n",
    "\n",
    "\n",
    "## **5ï¸âƒ£ Debugging & Fine-Tuning ğŸ”**  \n",
    "\n",
    "### ğŸ”¹ **Check for Overfitting & Underfitting**  \n",
    "- **Overfitting?** Train longer, use dropout, regularization, and data augmentation.  \n",
    "- **Underfitting?** Increase model complexity, remove excessive regularization.  \n",
    "\n",
    "### ğŸ”¹ **Analyze Loss Curves**  \n",
    "- If **training loss >> validation loss** â†’ Overfitting.  \n",
    "- If **both high and similar** â†’ Underfitting.  \n",
    "\n",
    "### ğŸ”¹ **Hyperparameter Tuning**  \n",
    "- Use **Grid Search** or **Bayesian Optimization** for best hyperparameters.  \n",
    "\n",
    "\n",
    "\n",
    "# ğŸ¯ **Final Thoughts**  \n",
    "Improving a neural network is all about **balancing complexity, generalization, and training efficiency**. Try different strategies, analyze results, and **keep optimizing**! ğŸš€  \n",
    "\n",
    "ğŸ”¥ **TL;DR**:  \n",
    "âœ”ï¸ Clean & scale data  \n",
    "âœ”ï¸ Choose the right architecture & activation functions  \n",
    "âœ”ï¸ Use proper training techniques (learning rate tuning, dropout, early stopping)  \n",
    "âœ”ï¸ Avoid overfitting with regularization & more data  \n",
    "âœ”ï¸ Debug using loss curves & hyperparameter tuning  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ›‘ **Early Stopping in Neural Networks** ğŸ¯  \n",
    "\n",
    "### **What is Early Stopping?** ğŸ¤”  \n",
    "Early stopping is a **regularization technique** that helps prevent **overfitting** while training a neural network. Instead of training for a fixed number of epochs, early stopping **monitors the modelâ€™s performance** on validation data and stops training **when performance starts to degrade**.  \n",
    "\n",
    "This technique ensures that your model **learns just enough** without memorizing the training data. ğŸ§ ğŸ’¡  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Why Do We Need Early Stopping?**  \n",
    "Neural networks typically go through **three phases** during training:  \n",
    "\n",
    "1ï¸âƒ£ **Underfitting (Initial Training Stage)**  \n",
    "   - The model hasnâ€™t learned enough yet.  \n",
    "   - Both **training loss** and **validation loss** are high.  \n",
    "\n",
    "2ï¸âƒ£ **Optimal Training (Sweet Spot) âœ…**  \n",
    "   - The model generalizes well to unseen data.  \n",
    "   - **Training loss decreases** ğŸ“‰, and **validation loss is stable**.  \n",
    "\n",
    "3ï¸âƒ£ **Overfitting (Too Much Training) ğŸš¨**  \n",
    "   - The model memorizes training data but fails on unseen data.  \n",
    "   - **Training loss continues decreasing**, but **validation loss starts increasing**.  \n",
    "\n",
    "ğŸ¯ **Early stopping prevents overfitting by stopping training at the optimal point!**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¬ **How Does Early Stopping Work?**  \n",
    "1ï¸âƒ£ **Split Your Data**  \n",
    "   - Training Set ğŸ‹ï¸â€â™‚ï¸ â†’ Used to update model weights.  \n",
    "   - Validation Set ğŸ“Š â†’ Used to monitor model performance.  \n",
    "\n",
    "2ï¸âƒ£ **Monitor Validation Loss**  \n",
    "   - After every epoch, check the **validation loss** (or another metric like accuracy).  \n",
    "\n",
    "3ï¸âƒ£ **Detect Overfitting**  \n",
    "   - If the validation loss **starts increasing**, the model is likely overfitting.  \n",
    "\n",
    "4ï¸âƒ£ **Stop Training**  \n",
    "   - Stop training **after a few more epochs** (patience) to confirm the trend.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ›  **Implementing Early Stopping in Python (Keras Example)**\n",
    "Most deep learning frameworks support early stopping out of the box. Hereâ€™s how you can do it in **Keras**:  \n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitor validation loss\n",
    "    patience=5,           # Wait for 5 epochs before stopping\n",
    "    restore_best_weights=True # Restore the best model\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping] # Apply early stopping\n",
    ")\n",
    "```\n",
    "### **Key Parameters:**\n",
    "ğŸ”¹ `monitor='val_loss'` â†’ Stops training based on validation loss (can also use `val_accuracy`).  \n",
    "ğŸ”¹ `patience=5` â†’ Stops training **only if validation loss doesnâ€™t improve for 5 consecutive epochs**.  \n",
    "ğŸ”¹ `restore_best_weights=True` â†’ Restores the model **to the best weights** before overfitting started.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“ˆ **How to Choose the Right Patience Value?**  \n",
    "ğŸ”¹ **Too low patience (e.g., 1-2 epochs) â†’** May stop training too early. ğŸ˜•  \n",
    "ğŸ”¹ **Too high patience (e.g., 10+ epochs) â†’** May allow overfitting. ğŸ›‘  \n",
    "ğŸ”¹ **Ideal range (3-7 epochs)** â†’ Depends on dataset size & training dynamics.  \n",
    "\n",
    "ğŸ‘‰ **Use visualization** to check where loss starts increasing:  \n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Benefits of Early Stopping**\n",
    "âœ”ï¸ **Prevents Overfitting** ğŸ”¥ â€“ Stops training when validation loss increases.  \n",
    "âœ”ï¸ **Saves Time & Resources** â³ â€“ No need to train unnecessary epochs.  \n",
    "âœ”ï¸ **Automatic Model Selection** ğŸ§  â€“ Keeps the best-performing model.  \n",
    "âœ”ï¸ **Works with Any Model** ğŸ¯ â€“ Usable in CNNs, RNNs, Transformers, etc.  \n",
    "\n",
    "\n",
    "\n",
    "## âš ï¸ **Limitations of Early Stopping**\n",
    "âŒ **Might Stop Too Soon** â€“ If patience is too low, the model may not reach the best performance.  \n",
    "âŒ **Doesnâ€™t Fix Data Issues** â€“ Early stopping **canâ€™t compensate for bad data quality** or poor feature engineering.  \n",
    "âŒ **Not Always Needed** â€“ If your dataset is **very large**, models may generalize well even without early stopping.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Takeaway**\n",
    "Early stopping is like **knowing when to leave a party** ğŸ‰â€”if you stay too long, things get messy (overfitting), but if you leave too soon, you might miss the fun (underfitting). ğŸ†  \n",
    "\n",
    "ğŸš€ **TL;DR:**  \n",
    "âœ”ï¸ **Monitors validation loss to prevent overfitting**  \n",
    "âœ”ï¸ **Stops training when performance degrades**  \n",
    "âœ”ï¸ **Saves time & improves generalization**  \n",
    "âœ”ï¸ **Use patience to avoid stopping too soon**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ **Data Scaling in Neural Networks: Why and How?** ğŸŒŸ  \n",
    "\n",
    "Imagine you're training a neural network, and your dataset contains different types of numerical featuresâ€”one ranging from **0 to 1** (like probability values) and another from **1,000 to 1,000,000** (like annual income). If you feed them **as-is**, your model will struggle, just like trying to compare an ant ğŸœ to an elephant ğŸ˜ in a race. Thatâ€™s where **data scaling** comes in! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Why is Data Scaling Important?**\n",
    "1. **Prevents Larger Values from Dominating** ğŸ­  \n",
    "   Neural networks rely on gradient-based optimization (like **SGD, Adam, RMSprop**). If one feature has much larger values than others, it will dominate the gradient updates, leading to unstable training.  \n",
    "\n",
    "2. **Speeds Up Training** âš¡  \n",
    "   Properly scaled data helps gradients flow smoothly during backpropagation. Otherwise, large differences in scale can cause very slow convergence or even **vanishing/exploding gradients**.  \n",
    "\n",
    "3. **Improves Model Performance** ğŸ¯  \n",
    "   A well-scaled dataset helps the network learn more effectively, leading to better accuracy and faster convergence.  \n",
    "\n",
    "4. **Avoids Bias Toward Certain Features** âš–ï¸  \n",
    "   Without scaling, some features may receive more weight just because they have larger numbers, not because they are more important!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ› ï¸ **Common Data Scaling Techniques**  \n",
    "\n",
    "There are a few popular techniques for scaling data, each with its use case. Letâ€™s break them down:  \n",
    "\n",
    "### 1ï¸âƒ£ **Min-Max Scaling (Normalization) ğŸ“**  \n",
    "   - Formula:  \n",
    "     $$\n",
    "     X' = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "     $$\n",
    "   - Scales values **between 0 and 1** (or another fixed range).  \n",
    "   - Works well when the data distribution is **not normal** (skewed).  \n",
    "   - Example: If income ranges from **$10,000 to $1,000,000**, it will be scaled to **0 to 1**.  \n",
    "   - **Good for:** Neural networks where activations like sigmoid or tanh are used.  \n",
    "\n",
    "### 2ï¸âƒ£ **Standardization (Z-Score Scaling) ğŸ“Š**  \n",
    "   - Formula:  \n",
    "     $$\n",
    "     X' = \\frac{X - \\mu}{\\sigma}\n",
    "     $$\n",
    "   - Centers the data around **mean = 0** and **standard deviation = 1**.  \n",
    "   - Useful when data follows a **normal distribution**.  \n",
    "   - **Good for:** Models that assume Gaussian-like distributions (like logistic regression, SVMs, and deep networks with ReLU).  \n",
    "\n",
    "### 3ï¸âƒ£ **Log Scaling (For Skewed Data) ğŸ”**  \n",
    "   - Formula:  \n",
    "     $$\n",
    "     X' = \\log(X + 1)\n",
    "     $$\n",
    "   - Helps when you have data with extreme **outliers** (like income, population).  \n",
    "   - Transforms highly skewed distributions into more **normal-like ones**.  \n",
    "\n",
    "### 4ï¸âƒ£ **Robust Scaling (For Outliers) ğŸš€**  \n",
    "   - Formula:  \n",
    "     $$\n",
    "     X' = \\frac{X - \\text{median}(X)}{\\text{IQR}(X)}\n",
    "     $$\n",
    "   - Uses **median and interquartile range (IQR)** instead of mean and standard deviation.  \n",
    "   - Works great for **datasets with outliers**, since itâ€™s **not sensitive to extreme values**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ **Which Scaling Method Should You Use?**\n",
    "âœ… If data is **normally distributed** â†’ Use **Standardization (Z-score scaling)**  \n",
    "âœ… If data is **skewed** â†’ Use **Log Scaling**  \n",
    "âœ… If data is in a **fixed range** â†’ Use **Min-Max Scaling**  \n",
    "âœ… If data has **outliers** â†’ Use **Robust Scaling**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Scaling in Action (Code Time!)**\n",
    "Hereâ€™s how you can apply scaling in Python using `scikit-learn`:  \n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([[10], [200], [3000], [40000], [500000]])\n",
    "\n",
    "# Min-Max Scaling\n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled_minmax = min_max_scaler.fit_transform(data)\n",
    "\n",
    "# Standardization\n",
    "standard_scaler = StandardScaler()\n",
    "scaled_standard = standard_scaler.fit_transform(data)\n",
    "\n",
    "# Robust Scaling\n",
    "robust_scaler = RobustScaler()\n",
    "scaled_robust = robust_scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"\\nMin-Max Scaled Data:\\n\", scaled_minmax)\n",
    "print(\"\\nStandardized Data:\\n\", scaled_standard)\n",
    "print(\"\\nRobust Scaled Data:\\n\", scaled_robust)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Final Thoughts**\n",
    "- **Scaling is a MUST for neural networks** to ensure balanced and efficient learning.  \n",
    "- Different techniques work best in different scenarios. Choose wisely!  \n",
    "- Always **apply scaling to both training and test data** using the same scaler instance.  \n",
    "\n",
    "Now youâ€™re ready to **scale like a pro** and make your neural networks train like a **Ferrari on a racetrack!** ğŸï¸ğŸ”¥ Happy coding! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ **Dropout Layers in Neural Networks â€“ A Complete Guide** ğŸ¯  \n",
    "\n",
    "### **ğŸš€ What is Dropout?**  \n",
    "Dropout is a **regularization technique** used in neural networks to prevent **overfitting**. It works by randomly **dropping** (setting to zero) a percentage of neurons during training. This forces the network to **learn more robust and generalizable features**, making it perform better on unseen data.  \n",
    "\n",
    "Think of it like a **sports team** ğŸ½:  \n",
    "- If a team always relies on the same star players ğŸŒŸ, they struggle when those players are missing.  \n",
    "- But if they train by randomly **removing key players**, others improve, making the whole team stronger! ğŸ’ª  \n",
    "\n",
    "In neural networks, dropout **forces the model to learn without relying too much on specific neurons**, leading to better generalization.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  Why Do We Need Dropout?**\n",
    "### âœ… **Prevents Overfitting**\n",
    "Neural networks **memorize** patterns in training data, including noise. Dropout **stops neurons from depending too much on each other**, preventing overfitting.  \n",
    "\n",
    "### âœ… **Improves Generalization**\n",
    "By randomly dropping neurons, the network **learns different pathways** to solve a problem. This improves its ability to handle **new, unseen data**.  \n",
    "\n",
    "### âœ… **Acts as Model Averaging**\n",
    "Since different subsets of neurons are active in each training iteration, dropout acts like training **many smaller networks** and averaging their outputs!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ How Dropout Works (Step-by-Step)**\n",
    "1ï¸âƒ£ **Choose a dropout rate** (e.g., `p = 0.5` means 50% of neurons are randomly dropped).  \n",
    "2ï¸âƒ£ **During training**, neurons are randomly turned **off** (set to 0).  \n",
    "3ï¸âƒ£ **During inference (testing)**, dropout is **turned off**, and the weights are scaled to match training conditions.  \n",
    "\n",
    "ğŸ” **Example: Before and After Dropout (Dropout Rate = 50%)**\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5]\n",
    "Before Dropout: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "After Dropout:  [0, 0.2, 0, 0.4, 0]  <-- 50% of neurons dropped!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Dropout in Neural Network Layers**\n",
    "Dropout can be applied to **fully connected layers (Dense layers) and convolutional layers (CNNs)**.\n",
    "\n",
    "### ğŸ—ï¸ **1. Dropout in Fully Connected (Dense) Layers**\n",
    "Used in deep networks like **MLPs (Multi-Layer Perceptrons)** and deep CNNs.\n",
    "\n",
    "ğŸ”¹ **Example (Keras with TensorFlow backend):**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.5),  # Drop 50% of neurons randomly\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),  # Drop 30% of neurons randomly\n",
    "    Dense(10, activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "ğŸ“Œ **Key Points:**  \n",
    "- The first `Dropout(0.5)` drops **50% of neurons** in the first hidden layer.  \n",
    "- The second `Dropout(0.3)` drops **30% of neurons** in the next layer.  \n",
    "- **Dropout is NOT applied to the output layer.**  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ—ï¸ **2. Dropout in Convolutional Neural Networks (CNNs)**\n",
    "CNNs already use techniques like **max pooling and batch normalization**, so dropout is typically **lower (5% - 25%)**.\n",
    "\n",
    "ğŸ”¹ **Example (Dropout in a CNN):**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),  # Drop 25% of neurons randomly\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),   # Drop 50% in fully connected layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "ğŸ“Œ **Key Points:**  \n",
    "- **Lower dropout** is used in CNN layers (since max pooling already helps generalization).  \n",
    "- **Higher dropout** is used in fully connected layers to prevent overfitting.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¬ Best Practices for Dropout**\n",
    "ğŸ“Œ **1. Don't Use Dropout in the Output Layer**  \n",
    "   - Dropout **only helps hidden layers**. Applying it to the output layer makes learning unstable.  \n",
    "\n",
    "ğŸ“Œ **2. Start with 20-50% Dropout in Dense Layers**  \n",
    "   - **Too much dropout (e.g., 80%)** â†’ Might cause **underfitting** (not learning enough).  \n",
    "   - **Too little dropout (e.g., 10%)** â†’ Might not prevent overfitting.  \n",
    "\n",
    "ğŸ“Œ **3. Use Lower Dropout (5-25%) in CNNs**  \n",
    "   - CNNs already generalize well, so dropout should be minimal.  \n",
    "\n",
    "ğŸ“Œ **4. Use Dropout Only in Training, Not Testing**  \n",
    "   - During inference (testing), dropout is **disabled** and the weights are scaled properly.  \n",
    "\n",
    "ğŸ“Œ **5. Combine Dropout with Other Regularization Methods**  \n",
    "   - **L1/L2 regularization (weight decay)** + **Batch Normalization** + **Dropout** = Powerful combo! ğŸ’ª  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š Experiment: Training a Neural Network With and Without Dropout**\n",
    "Let's train a simple **MLP classifier on the MNIST dataset** (handwritten digits) with and without dropout and compare accuracy.  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Flatten images (28x28 to 784)\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "### ğŸ† Model WITHOUT Dropout\n",
    "model_no_dropout = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(784,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_no_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_no_dropout = model_no_dropout.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "### ğŸ”¥ Model WITH Dropout\n",
    "model_with_dropout = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.5),  # Drop 50% of neurons\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  # Drop 30% of neurons\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Compare test accuracy\n",
    "acc_no_dropout = model_no_dropout.evaluate(X_test, y_test)[1]\n",
    "acc_with_dropout = model_with_dropout.evaluate(X_test, y_test)[1]\n",
    "\n",
    "print(f\"ğŸ¯ Accuracy WITHOUT Dropout: {acc_no_dropout:.4f}\")\n",
    "print(f\"ğŸ”¥ Accuracy WITH Dropout: {acc_with_dropout:.4f}\")\n",
    "```\n",
    "\n",
    "## **ğŸš€ Conclusion**\n",
    "- **Without dropout**, the model may overfit and perform poorly on test data.  \n",
    "- **With dropout**, generalization improves, and test accuracy is often higher.  \n",
    "- **Dropout is a simple yet powerful technique** that should be included in deep neural networks, especially in fully connected layers!  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ† **Final Takeaways**\n",
    "âœ… Dropout helps prevent overfitting by randomly dropping neurons.  \n",
    "âœ… It acts as **model averaging**, improving generalization.  \n",
    "âœ… Works best in **fully connected layers**; use lower values in CNNs.  \n",
    "âœ… Combining dropout with **L2 regularization and batch normalization** can further improve results.  \n",
    "\n",
    "Now you're ready to **drop overfitting and boost model performance!** ğŸš€ğŸ”¥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Why Do We Use Regularization in Neural Networks?**  \n",
    "\n",
    "Regularization is used in neural networks to **prevent overfitting**â€”when a model memorizes the training data instead of generalizing well to new data. Overfitting happens when the network learns noise and random fluctuations in the dataset rather than the actual underlying patterns.  \n",
    "\n",
    "ğŸ›  **Solution?** **Regularization techniques** help by adding constraints to the model, ensuring it remains simple and avoids excessive complexity.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Types of Regularization: L1 & L2 Regularization**  \n",
    "\n",
    "### ğŸŸ  **L1 Regularization (Lasso Regularization)**  \n",
    "L1 regularization adds the **absolute** value of the weights as a penalty to the loss function. This leads to **sparse weights**, meaning some weights become exactly **zero**â€”effectively removing less important features from the model.\n",
    "\n",
    "ğŸ’¡ **Mathematical Formula:**  \n",
    "The L1 regularized loss function is:\n",
    "\n",
    "\\[\n",
    "L = \\text{Loss} + \\lambda \\sum |w_i|\n",
    "\\]\n",
    "\n",
    "where:  \n",
    "âœ… \\(L\\) = Total loss  \n",
    "âœ… \\(\\text{Loss}\\) = Original loss (e.g., cross-entropy, MSE)  \n",
    "âœ… \\(\\lambda\\) = Regularization parameter (controls the penalty strength)  \n",
    "âœ… \\(w_i\\) = Model weights  \n",
    "\n",
    "ğŸ”¹ **Effect of L1 Regularization:**  \n",
    "âœ” Encourages **sparsity** (some weights become exactly **zero**)  \n",
    "âœ” Selects only the most important features  \n",
    "âœ” Helps in feature selection  \n",
    "\n",
    "ğŸ“Œ **Analogy:** Imagine you're packing for a trip but can only carry essentials. L1 regularization helps \"pack\" only the most important features and discards the rest!\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”µ **L2 Regularization (Ridge Regularization)**  \n",
    "L2 regularization adds the **squared** value of the weights as a penalty to the loss function. This discourages large weight values but does **not** make them zero, making the model more stable.\n",
    "\n",
    "ğŸ’¡ **Mathematical Formula:**  \n",
    "The L2 regularized loss function is:\n",
    "\n",
    "\\[\n",
    "L = \\text{Loss} + \\lambda \\sum w_i^2\n",
    "\\]\n",
    "\n",
    "ğŸ”¹ **Effect of L2 Regularization:**  \n",
    "âœ” **Prevents overfitting** by reducing large weight values  \n",
    "âœ” Does **not** set weights to zero but makes them small  \n",
    "âœ” Helps maintain all features but ensures they contribute reasonably  \n",
    "\n",
    "ğŸ“Œ **Analogy:** Think of L2 regularization like a rubber band pulling weights towards **zero**, ensuring the network is not too sensitive to small changes in input.\n",
    "\n",
    "## ğŸ›  **L1 vs. L2 Regularization â€“ Key Differences**  \n",
    "\n",
    "| Feature          | L1 Regularization (Lasso) ğŸ”¥ | L2 Regularization (Ridge) ğŸ”µ |\n",
    "|-----------------|----------------------------|----------------------------|\n",
    "| **Penalty Term** | \\( \\sum |w_i| \\)           | \\( \\sum w_i^2 \\)           |\n",
    "| **Effect on Weights** | Some weights become **exactly zero** | Shrinks weights but **not to zero** |\n",
    "| **Feature Selection?** | Yes âœ… (Sparse Model) | No âŒ (Retains all features) |\n",
    "| **Computational Cost** | Lower â³ | Higher â³â³ |\n",
    "| **Best Used When?** | Feature selection is needed ğŸ† | Overfitting is a concern ğŸ¯ |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **When to Use L1 or L2 Regularization?**  \n",
    "\n",
    "âœ… **Use L1 Regularization** when you want to remove irrelevant features and get a sparse model.  \n",
    "âœ… **Use L2 Regularization** when you want to prevent overfitting but still keep all features.  \n",
    "âœ… **Use Both Together (Elastic Net)** if you want a balance between feature selection and weight regularization.\n",
    "\n",
    "\n",
    "ğŸš€ **Conclusion:** Regularization is a powerful tool to improve the generalization of neural networks. **L1** helps with feature selection, while **L2** keeps weights small and stable. Choosing the right one depends on the nature of your dataset and model needs.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  **Think of a Neural Network Like a Student Studying for an Exam** ğŸ“  \n",
    "\n",
    "Imagine a student (your neural network) is preparing for a big exam. There are two possible ways they can study:  \n",
    "\n",
    "1ï¸âƒ£ **They memorize every single question from past papers** ğŸ“ (Overfitting)  \n",
    "2ï¸âƒ£ **They understand the concepts so they can answer new questions** âœ… (Good Generalization)  \n",
    "\n",
    "ğŸ’¡ **What's the problem with memorization?**  \n",
    "If the exam questions are exactly the same as the ones they memorized, they will do great! But if the questions are different, they will struggle because they donâ€™t actually understand the subjectâ€”they just memorized answers.  \n",
    "\n",
    "âŒ **This is overfitting** in neural networks. The model learns too much detail from the training data, including noise, instead of learning general patterns that work on new data.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸš€ **How Does Regularization Help?**  \n",
    "\n",
    "Regularization acts like a **good teacher** who prevents the student from blindly memorizing answers. Instead, they **encourage the student to focus on key concepts** so they can answer new questions confidently.  \n",
    "\n",
    "ğŸ”¹ **L1 Regularization (Forcing Simplicity)**  \n",
    "ğŸ‘‰ Like a teacher telling the student: **\"Only focus on the most important topics. Forget unnecessary details!\"**  \n",
    "ğŸ‘‰ Some details (weights) are completely ignored (set to zero).  \n",
    "ğŸ‘‰ Helps in picking only the most useful information.  \n",
    "\n",
    "ğŸ”¹ **L2 Regularization (Avoiding Extreme Confidence)**  \n",
    "ğŸ‘‰ Like a teacher saying: **\"Don't rely too much on just one or two topics. Spread your understanding evenly!\"**  \n",
    "ğŸ‘‰ Instead of removing details, it ensures the student doesnâ€™t overly depend on specific topics.  \n",
    "ğŸ‘‰ Reduces extreme reliance on any single piece of information.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **Final Summary**  \n",
    "\n",
    "ğŸ”¸ Without regularization: The neural network **memorizes too much** (overfits).  \n",
    "ğŸ”¸ With L1 regularization: It **picks only the most important information** (sparse learning).  \n",
    "ğŸ”¸ With L2 regularization: It **balances knowledge to avoid overconfidence** (smooth learning).  \n",
    "\n",
    "ğŸ›  **Regularization helps your model be like a smart studentâ€”one who understands concepts rather than just memorizing answers!** ğŸ˜ƒ  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ“Œ Manual Calculation of L1 and L2 Regularization**\n",
    "Let's take a simple example and compute L1 and L2 regularization manually.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¯ Example Setup**\n",
    "#### **Given Data**\n",
    "- **Loss function (without regularization)**: $ L = 5 $ (assume some loss value)\n",
    "- **Weights ($ w_1, w_2, w_3 $)**:  \n",
    "  $$\n",
    "  w_1 = 2, \\quad w_2 = -3, \\quad w_3 = 4\n",
    "  $$\n",
    "- **Regularization strength** ($ \\lambda $):  \n",
    "  $$\n",
    "  \\lambda = 0.1\n",
    "  $$\n",
    "\n",
    "Now, let's compute **L1 and L2 regularization separately**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”· Step 1: L1 Regularization (Lasso)**\n",
    "L1 regularization adds the **absolute values** of the weights to the loss:\n",
    "\n",
    "$$\n",
    "L_{L1} = L + \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "**Substituting the values:**\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times (|2| + |-3| + |4|)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times (2 + 3 + 4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times 9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5.9\n",
    "$$\n",
    "\n",
    "âœ… **Final L1 Regularized Loss** = **5.9**\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Step 2: L2 Regularization (Ridge)**\n",
    "L2 regularization adds the **squared values** of the weights to the loss:\n",
    "\n",
    "$$\n",
    "L_{L2} = L + \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "**Substituting the values:**\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times (2^2 + (-3)^2 + 4^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times (4 + 9 + 16)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times 29\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 2.9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 7.9\n",
    "$$\n",
    "\n",
    "âœ… **Final L2 Regularized Loss** = **7.9**\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Step 3: Weight Update with L1 and L2**\n",
    "### **Gradient Descent Updates**\n",
    "For **L1 Regularization**, the weight update formula is:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\cdot \\left(\\frac{\\partial \\text{Loss}}{\\partial w_i} + \\lambda \\cdot \\text{sign}(w_i)\\right)\n",
    "$$\n",
    "\n",
    "For **L2 Regularization**, the weight update formula is:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\cdot \\left(\\frac{\\partial \\text{Loss}}{\\partial w_i} + 2\\lambda w_i\\right)\n",
    "$$\n",
    "\n",
    "Assuming:\n",
    "- **Learning rate** $ \\eta = 0.01 $\n",
    "- **Gradient of loss** $ \\frac{\\partial \\text{Loss}}{\\partial w_i} = 0.5 $ (assumed for each weight)\n",
    "\n",
    "### **ğŸ”¶ L1 Weight Updates**\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(2))\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.1 \\times 1)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times 0.6\n",
    "$$\n",
    "$$\n",
    "w_1 = 1.994\n",
    "$$\n",
    "\n",
    "Similarly, for $ w_2 = -3 $:\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(-3))\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 - 0.1)\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times 0.4\n",
    "$$\n",
    "$$\n",
    "w_2 = -3.004\n",
    "$$\n",
    "\n",
    "For $ w_3 = 4 $:\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(4))\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.1)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times 0.6\n",
    "$$\n",
    "$$\n",
    "w_3 = 3.994\n",
    "$$\n",
    "\n",
    "**Updated Weights (L1 Regularization)**\n",
    "$$\n",
    "w_1 = 1.994, \\quad w_2 = -3.004, \\quad w_3 = 3.994\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”µ L2 Weight Updates**\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times 2)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.4)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times 0.9\n",
    "$$\n",
    "$$\n",
    "w_1 = 1.991\n",
    "$$\n",
    "\n",
    "Similarly, for $ w_2 = -3 $:\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times (-3))\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 - 0.6)\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times -0.1\n",
    "$$\n",
    "$$\n",
    "w_2 = -2.999\n",
    "$$\n",
    "\n",
    "For $ w_3 = 4 $:\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times 4)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.8)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times 1.3\n",
    "$$\n",
    "$$\n",
    "w_3 = 3.987\n",
    "$$\n",
    "\n",
    "**Updated Weights (L2 Regularization)**\n",
    "$$\n",
    "w_1 = 1.991, \\quad w_2 = -2.999, \\quad w_3 = 3.987\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Final Comparison of Updates**\n",
    "| Weight | Initial Value | After L1 Regularization | After L2 Regularization |\n",
    "|--------|--------------|------------------------|------------------------|\n",
    "| $ w_1 $ | 2 | **1.994** (shrinks, might become 0) | **1.991** (smooth decay) |\n",
    "| $ w_2 $ | -3 | **-3.004** (shrinks) | **-2.999** (shrinks smoothly) |\n",
    "| $ w_3 $ | 4 | **3.994** (shrinks) | **3.987** (shrinks smoothly) |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Key Takeaways**\n",
    "âœ” **L1 Regularization** shrinks weights **more aggressively** and pushes some to **exactly zero** (feature selection).  \n",
    "âœ” **L2 Regularization** reduces weights **smoothly** but **does not make them exactly zero**.  \n",
    "âœ” **L1 is useful** for sparse models where some features can be ignored.  \n",
    "âœ” **L2 is useful** when all features contribute but should have controlled importance.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ğŸ”µ Activation Functions in Neural Networks**\n",
    "Activation functions are a crucial part of neural networks. They determine whether a neuron should be activated or not by introducing **non-linearity** into the model.\n",
    "\n",
    "### **ğŸ”¹ Why Do We Need Activation Functions?**\n",
    "A neural network without activation functions is just a **linear model** (like logistic regression). Activation functions help the network **learn complex patterns and relationships** in data.\n",
    "\n",
    "1. **Introduces Non-Linearity** ğŸŒ€  \n",
    "   - Real-world problems (like image recognition, speech processing) are non-linear.  \n",
    "   - Without activation functions, neural networks would behave **like a simple linear function**, limiting their power.\n",
    "   \n",
    "2. **Helps Backpropagation** ğŸ”„  \n",
    "   - Activation functions introduce gradients, which help in optimizing the model using **gradient descent**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”µ Types of Activation Functions**\n",
    "There are **three major types** of activation functions:\n",
    "\n",
    "1. **Linear Activation**\n",
    "2. **Non-Linear Activations**\n",
    "   - **Sigmoid**\n",
    "   - **Tanh**\n",
    "   - **ReLU (Rectified Linear Unit)**\n",
    "   - **Leaky ReLU & Parametric ReLU**\n",
    "   - **ELU (Exponential Linear Unit)**\n",
    "3. **Softmax Activation (for classification tasks)**\n",
    "\n",
    "\n",
    "\n",
    "## **1ï¸âƒ£ Linear Activation Function**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) = ax + b\n",
    "$$\n",
    "ğŸ”¸ **Graph**: A straight line  \n",
    "ğŸ”¸ **Problem**: Cannot capture complex patterns  \n",
    "\n",
    "ğŸ“Œ **Used in:** Output layers for regression problems.\n",
    "\n",
    "\n",
    "\n",
    "## **2ï¸âƒ£ Non-Linear Activation Functions**\n",
    "### **ğŸ“Œ Sigmoid Activation Function**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:\n",
    "âœ” Smooth, differentiable.  \n",
    "âœ” Output is **always between 0 and 1** (great for probability predictions).\n",
    "\n",
    "ğŸ”¹ **Cons**:\n",
    "âŒ Causes **vanishing gradient** problem (small gradients slow down learning).  \n",
    "âŒ Not zero-centered (output is always positive).  \n",
    "\n",
    "ğŸ“Œ **Used in:** Binary classification (output layer).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Tanh (Hyperbolic Tangent) Activation Function**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:  \n",
    "âœ” Outputs range between **-1 and 1** (zero-centered).  \n",
    "âœ” Works better than sigmoid for deep networks.\n",
    "\n",
    "ğŸ”¹ **Cons**:  \n",
    "âŒ Also suffers from **vanishing gradient problem**.\n",
    "\n",
    "ğŸ“Œ **Used in:** Hidden layers in some RNNs.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ ReLU (Rectified Linear Unit)**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:  \n",
    "âœ” Does **not** suffer from vanishing gradients.  \n",
    "âœ” Computationally efficient.\n",
    "\n",
    "ğŸ”¹ **Cons**:  \n",
    "âŒ Can cause **dying ReLU problem** (neurons stuck at 0).  \n",
    "\n",
    "ğŸ“Œ **Used in:** Most modern deep learning architectures (CNNs, RNNs, Transformers).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Leaky ReLU (Fixes Dying ReLU)**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "0.01x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:  \n",
    "âœ” Solves **dying ReLU problem** by allowing small negative gradients.  \n",
    "\n",
    "ğŸ“Œ **Used in:** Deep neural networks.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ ELU (Exponential Linear Unit)**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:  \n",
    "âœ” Solves **dying ReLU problem**  \n",
    "âœ” More stable training  \n",
    "\n",
    "ğŸ“Œ **Used in:** Deep networks with **complex architectures**.\n",
    "\n",
    "\n",
    "\n",
    "## **3ï¸âƒ£ Softmax Activation Function (For Multi-Class Classification)**\n",
    "ğŸ”¸ **Equation**:  \n",
    "$$\n",
    "\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Pros**:  \n",
    "âœ” Converts outputs into **probabilities** (sum = 1).  \n",
    "âœ” Helps in multi-class classification.\n",
    "\n",
    "ğŸ“Œ **Used in:** Output layer of **multi-class classification** models.\n",
    "\n",
    "## **ğŸ¯ Summary Table**\n",
    "| Activation | Used In | Pros | Cons |\n",
    "|------------|--------|------|------|\n",
    "| **Linear** | Regression | Simple | Cannot capture complexity |\n",
    "| **Sigmoid** | Binary Classification | Probability output | Vanishing gradient |\n",
    "| **Tanh** | RNNs | Zero-centered | Vanishing gradient |\n",
    "| **ReLU** | CNNs, Deep Learning | No vanishing gradient | Dying neurons |\n",
    "| **Leaky ReLU** | Deep Learning | Solves dying ReLU | Small overhead |\n",
    "| **ELU** | Complex Networks | Better than ReLU | More computation |\n",
    "| **Softmax** | Multi-class Classification | Probability output | Can be slow |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Final Thoughts**\n",
    "- **ReLU** is the most widely used for hidden layers.  \n",
    "- **Softmax** is used in **classification** problems.  \n",
    "- **Tanh** and **Sigmoid** are rarely used in deep networks today.  \n",
    "\n",
    "![](images/sigmoid.png)\n",
    "\n",
    "![](images/tanh.png)\n",
    "\n",
    "![](images/relu.png)\n",
    "\n",
    "![](images/elu.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ”¹ What Are Activation Functions? (Super Simple Explanation)**  \n",
    "Think of a **neural network** as a factory processing raw materials (inputs) into useful products (outputs). But before making the final product, we need a **decision-making system** to decide whether a particular part should be **used or discarded**.  \n",
    "\n",
    "That **decision-making system** is the **activation function**! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Why Do We Need Activation Functions?**  \n",
    "Without activation functions, a neural network is just **a boring calculator doing linear math** ğŸ“‰. It won't be able to recognize complex patterns like **faces, voices, or handwritten digits**.  \n",
    "\n",
    "Activation functions help the network **learn and make smart decisions** by introducing **non-linearity** ğŸ”„.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Types of Activation Functions (Like Different Switches)**\n",
    "1. **Sigmoid â†’ Soft Switch** ğŸ”˜   \n",
    "   - Example: A robot deciding whether a glass is half-full (probabilities).  \n",
    "   - Output is between **0 and 1** (good for probability-based decisions).  \n",
    "   - **Problem**: It reacts very **slowly** to big changes (vanishing gradient issue).\n",
    "\n",
    "2. **Tanh â†’ Stronger Soft Switch** ğŸŒ—  \n",
    "   - Example: A sensor measuring **temperature changes** from cold (-1) to hot (+1).  \n",
    "   - Works better than sigmoid but still has **slow learning issues**.\n",
    "\n",
    "3. **ReLU â†’ Simple On/Off Switch** âš¡  \n",
    "   - Example: If a signal is positive, **turn it ON**; otherwise, keep it OFF.  \n",
    "   - Used **most commonly** in deep learning because itâ€™s fast! ğŸš€  \n",
    "   - **Problem**: Some neurons get permanently stuck OFF (called **dying ReLU**).\n",
    "\n",
    "4. **Leaky ReLU â†’ Improved On/Off Switch** âš¡ğŸ’¡  \n",
    "   - Example: If a signal is **negative**, donâ€™t completely turn it offâ€”just let a **small current flow**.  \n",
    "   - Fixes **dying ReLU problem**.\n",
    "\n",
    "5. **Softmax â†’ Decision Maker for Many Options** ğŸ¯  \n",
    "   - Example: If you need to **choose one out of 10 items**, it helps pick the most probable one.  \n",
    "   - Used in the **last layer of classification models**.\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Simple Summary in Layman Terms**\n",
    "| Activation Function | Like a... | Used For | Problem Fixed |\n",
    "|----------------------|-----------|----------|---------------|\n",
    "| **Sigmoid** | Dimmer Switch (0 to 1) | Probability Predictions | Slow learning |\n",
    "| **Tanh** | Thermometer (-1 to +1) | Sensor-based predictions | Slow learning |\n",
    "| **ReLU** | Light Switch (ON/OFF) | Hidden layers in deep learning | Some neurons die |\n",
    "| **Leaky ReLU** | Improved Light Switch | Same as ReLU but better | No dead neurons |\n",
    "| **Softmax** | Voting System | Choosing **one** from **many** | Works only at output |\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Takeaway**  \n",
    "- **Use ReLU** for hidden layers (fast & efficient).  \n",
    "- **Use Softmax** in the output layer for multi-class problems.  \n",
    "- **Use Sigmoid/Tanh** for simpler problems (but not deep networks).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate each activation function for a given input. We'll take $ x = -2, -1, 0, 1, 2$ and compute the values manually.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Sigmoid Function**\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "For each $ x$:\n",
    "\n",
    "| $ x$  | $ e^{-x}$ | $ 1 + e^{-x}$ | $ \\sigma(x)$ |\n",
    "|----------|-------------|----------------|-------------|\n",
    "| -2       | $ e^2 \\approx 7.389$ | $ 8.389$ | $ \\frac{1}{8.389} \\approx 0.119$ |\n",
    "| -1       | $ e^1 \\approx 2.718$ | $ 3.718$ | $ \\frac{1}{3.718} \\approx 0.269$ |\n",
    "| 0        | $ e^0 = 1$ | $ 2$ | $ \\frac{1}{2} = 0.5$ |\n",
    "| 1        | $ e^{-1} \\approx 0.368$ | $ 1.368$ | $ \\frac{1}{1.368} \\approx 0.731$ |\n",
    "| 2        | $ e^{-2} \\approx 0.135$ | $ 1.135$ | $ \\frac{1}{1.135} \\approx 0.881$ |\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Tanh Function**\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "For each $ x$:\n",
    "\n",
    "| $ x$  | $ e^x$ | $ e^{-x}$ | $ e^x - e^{-x}$ | $ e^x + e^{-x}$ | $ \\tanh(x)$ |\n",
    "|----------|-------------|-------------|-------------|-------------|-------------|\n",
    "| -2       | $ e^{-2} \\approx 0.135$ | $ e^2 \\approx 7.389$ | $ -7.254$ | $ 7.524$ | $ \\frac{-7.254}{7.524} \\approx -0.964$ |\n",
    "| -1       | $ e^{-1} \\approx 0.368$ | $ e^1 \\approx 2.718$ | $ -2.350$ | $ 3.086$ | $ \\frac{-2.350}{3.086} \\approx -0.761$ |\n",
    "| 0        | $ e^0 = 1$ | $ e^0 = 1$ | $ 0$ | $ 2$ | $ 0$ |\n",
    "| 1        | $ e^1 \\approx 2.718$ | $ e^{-1} \\approx 0.368$ | $ 2.350$ | $ 3.086$ | $ \\frac{2.350}{3.086} \\approx 0.761$ |\n",
    "| 2        | $ e^2 \\approx 7.389$ | $ e^{-2} \\approx 0.135$ | $ 7.254$ | $ 7.524$ | $ \\frac{7.254}{7.524} \\approx 0.964$ |\n",
    "\n",
    "\n",
    "\n",
    "### 3. **ReLU Function**\n",
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "| $ x$  | $ ReLU(x)$ |\n",
    "|----------|-------------|\n",
    "| -2       | 0 |\n",
    "| -1       | 0 |\n",
    "| 0        | 0 |\n",
    "| 1        | 1 |\n",
    "| 2        | 2 |\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Leaky ReLU Function** ($ \\alpha = 0.01$)\n",
    "$$\n",
    "LeakyReLU(x) = \\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| $ x$  | $ LeakyReLU(x)$ |\n",
    "|----------|----------------|\n",
    "| -2       | $ -2 \\times 0.01 = -0.02$ |\n",
    "| -1       | $ -1 \\times 0.01 = -0.01$ |\n",
    "| 0        | $ 0$ |\n",
    "| 1        | $ 1$ |\n",
    "| 2        | $ 2$ |\n",
    "\n",
    "\n",
    "\n",
    "### 5. **ELU Function** ($ \\alpha = 1$)\n",
    "$$\n",
    "ELU(x) = \\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| $ x$  | $ e^x - 1$ | $ ELU(x)$ |\n",
    "|----------|-------------|-------------|\n",
    "| -2       | $ e^{-2} - 1 \\approx -0.865$ | $ -0.865$ |\n",
    "| -1       | $ e^{-1} - 1 \\approx -0.632$ | $ -0.632$ |\n",
    "| 0        | $ e^0 - 1 = 0$ | $ 0$ |\n",
    "| 1        | $ 1$ | $ 1$ |\n",
    "| 2        | $ 2$ | $ 2$ |\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Softmax Function** (for $ x = [-2, -1, 0, 1, 2]$)\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i}}{\\sum e^{x}}\n",
    "$$\n",
    "\n",
    "#### Step 1: Compute $ e^x$\n",
    "\n",
    "$$\n",
    "e^{-2} \\approx 0.135, \\quad e^{-1} \\approx 0.368, \\quad e^0 = 1, \\quad e^1 \\approx 2.718, \\quad e^2 \\approx 7.389\n",
    "$$\n",
    "\n",
    "#### Step 2: Compute sum of exponentials\n",
    "\n",
    "$$\n",
    "0.135 + 0.368 + 1 + 2.718 + 7.389 = 11.61\n",
    "$$\n",
    "\n",
    "#### Step 3: Compute Softmax values\n",
    "\n",
    "| $ x$  | $ e^x$ | Softmax(x) |\n",
    "|----------|-------------|-------------|\n",
    "| -2       | 0.135 | $ \\frac{0.135}{11.61} \\approx 0.012$ |\n",
    "| -1       | 0.368 | $ \\frac{0.368}{11.61} \\approx 0.032$ |\n",
    "| 0        | 1 | $ \\frac{1}{11.61} \\approx 0.086$ |\n",
    "| 1        | 2.718 | $ \\frac{2.718}{11.61} \\approx 0.234$ |\n",
    "| 2        | 7.389 | $ \\frac{7.389}{11.61} \\approx 0.636$ |\n",
    "\n",
    "\n",
    "\n",
    "### Summary of Manual Calculations\n",
    "\n",
    "- **Sigmoid** smoothly maps values between (0,1).\n",
    "- **Tanh** maps values between (-1,1).\n",
    "- **ReLU** sets negative values to 0.\n",
    "- **Leaky ReLU** allows small negative values.\n",
    "- **ELU** is similar to Leaky ReLU but smooth.\n",
    "- **Softmax** normalizes values into probabilities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ Weight Initialization in Neural Networks â€“ A Colorful Breakdown! ğŸŒˆ  \n",
    "\n",
    "Weight initialization is **crucial** in deep learning! If we start with bad weights, our network might learn too slowly, get stuck, or even explode with massive gradients. Letâ€™s explore some **cool techniques** to set weights smartly! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## 1ï¸âƒ£ **Zero Initialization (ğŸš« Bad Idea!)**  \n",
    "ğŸ’¡ **What is it?**  \n",
    "Set all weights to **zero**.  \n",
    "\n",
    "âš ï¸ **Why is it bad?**  \n",
    "- Every neuron learns the **same thing** (symmetry problem).  \n",
    "- No unique weight updates = network doesnâ€™t learn anything useful! ğŸ¤¯  \n",
    "\n",
    "ğŸ“Œ **Used for biases, but never for weights!**  \n",
    "\n",
    "\n",
    "\n",
    "## 2ï¸âƒ£ **Random Initialization (ğŸ² Basic but Risky!)**  \n",
    "ğŸ’¡ **What is it?**  \n",
    "Set weights randomly from a **uniform or normal distribution**.  \n",
    "\n",
    "âš ï¸ **Why it can go wrong?**  \n",
    "- If values are **too small**, gradients vanish (slow learning ğŸš¶).  \n",
    "- If values are **too large**, gradients explode (unstable training ğŸ’¥).  \n",
    "\n",
    "ğŸ“Œ **Not ideal for deep networks!**  \n",
    "\n",
    "\n",
    "\n",
    "## 3ï¸âƒ£ **Xavier (Glorot) Initialization (ğŸ¯ Balanced Approach!)**  \n",
    "ğŸ’¡ **What is it?**  \n",
    "Designed for **sigmoid** & **tanh** activations to keep activations balanced.  \n",
    "\n",
    "ğŸ¨ **Formula:**  \n",
    "For a layer with **n_in** inputs and **n_out** outputs:  \n",
    "- Draw weights from:  \n",
    "  $$\n",
    "  W \\sim \\mathcal{N}(0, \\frac{1}{n_{in} + n_{out}})\n",
    "  $$\n",
    "  (Normal distribution with mean = 0, variance = 1 / (fan-in + fan-out))  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "- Prevents activations from **dying out** or **exploding**!  \n",
    "- Good for **shallow** networks.  \n",
    "\n",
    "âš ï¸ **Cons:**  \n",
    "- Doesnâ€™t work well for **ReLU** activations!  \n",
    "\n",
    "\n",
    "\n",
    "## 4ï¸âƒ£ **He Initialization (ğŸ”¥ Best for ReLU & Variants!)**  \n",
    "ğŸ’¡ **What is it?**  \n",
    "Designed for **ReLU** and **Leaky ReLU** activations. Since ReLU kills negative values, we need a slightly higher variance.  \n",
    "\n",
    "ğŸ¨ **Formula:**  \n",
    "For a layer with **n_in** inputs:  \n",
    "- Draw weights from:  \n",
    "  $$\n",
    "  W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})\n",
    "  $$\n",
    "  (Normal distribution with mean = 0, variance = 2 / fan-in)  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "- Works great for **deep** networks!  \n",
    "- Handles the **dying ReLU problem** better.  \n",
    "\n",
    "âš ï¸ **Cons:**  \n",
    "- Not great for **sigmoid/tanh** activations.  \n",
    "\n",
    "\n",
    "\n",
    "## 5ï¸âƒ£ **Lecun Initialization (ğŸ§‘â€ğŸ”¬ Best for Sigmoid & Tanh!)**  \n",
    "ğŸ’¡ **What is it?**  \n",
    "Optimized for **sigmoid & tanh** activations to prevent saturation.  \n",
    "\n",
    "ğŸ¨ **Formula:**  \n",
    "- Similar to Xavier, but with **fan-in** only:  \n",
    "  $$\n",
    "  W \\sim \\mathcal{N}(0, \\frac{1}{n_{in}})\n",
    "  $$  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "- Keeps activations in **useful ranges** for sigmoid/tanh.  \n",
    "\n",
    "âš ï¸ **Cons:**  \n",
    "- Not suited for ReLU networks.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Final Thoughts â€“ Which One to Use?**  \n",
    "ğŸ”¹ **ReLU & Variants (Leaky ReLU, GELU, etc.) â†’ Use He Initialization**  \n",
    "ğŸ”¹ **Sigmoid/Tanh â†’ Use Xavier or Lecun**  \n",
    "ğŸ”¹ **If unsure, go with He for deep networks!**  \n",
    "\n",
    "ğŸš€ The right initialization can make training **faster, stabler, and better!** So, choose wisely and happy coding! ğŸ˜ƒğŸ‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No worries! Let me explain weight initialization in the **simplest way possible**.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ—ï¸ What is Weight Initialization?  \n",
    "Think of training a neural network like **teaching a student**.  \n",
    "- If the student starts with **no knowledge (zero weights)**, they canâ€™t learn properly.  \n",
    "- If the student starts with **random, chaotic knowledge (random weights)**, they might get confused.  \n",
    "- If we **give them a good starting point**, they learn faster and better.  \n",
    "\n",
    "Thatâ€™s exactly what weight initialization does â€“ it **sets the starting knowledge** of a neural network so it can learn efficiently!  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¨ Different Ways to Initialize Weights  \n",
    "\n",
    "#### ğŸš« 1. **Zero Initialization (Bad Idea!)**  \n",
    "Imagine a teacher giving **the same book** to every student. Everyone learns the same thing â†’ No diversity â†’ **No learning happens!**  \n",
    "ğŸ‘‰ Thatâ€™s why we **never** initialize all weights to zero.  \n",
    "\n",
    "#### ğŸ² 2. **Random Initialization (Better, but Risky!)**  \n",
    "This is like throwing books randomly at students without checking if they are too easy or too hard.  \n",
    "- **Too small weights?** The student learns **too slowly** (vanishing gradients ğŸ˜´).  \n",
    "- **Too big weights?** The student gets **confused** (exploding gradients ğŸ¤¯).  \n",
    "\n",
    "ğŸ‘‰ We need a **better balance** than just random values!  \n",
    "\n",
    "#### ğŸ¯ 3. **Xavier (Glorot) Initialization (Balanced Learning!)**  \n",
    "Imagine a teacher who gives students books that are **neither too easy nor too hard**.  \n",
    "ğŸ‘‰ **Works well for sigmoid/tanh activations** but **not for ReLU**.  \n",
    "\n",
    "#### ğŸ”¥ 4. **He Initialization (Best for ReLU!)**  \n",
    "If students are learning **only from positive examples** (like ReLU), they need **more challenging books** to learn properly.  \n",
    "ğŸ‘‰ He Initialization gives weights slightly **higher values** so the network doesnâ€™t get stuck.  \n",
    "\n",
    "#### ğŸ§‘â€ğŸ”¬ 5. **Lecun Initialization (Best for Sigmoid/Tanh!)**  \n",
    "This is like giving **special books** to students who need a **gentler learning curve** (like sigmoid/tanh networks).  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ† Which One to Use?  \n",
    "- **If using ReLU â†’ Use He Initialization** (ğŸ”¥ Works best!)  \n",
    "- **If using Sigmoid/Tanh â†’ Use Xavier or Lecun**  \n",
    "- **Never set weights to zero!** ğŸš«  \n",
    "\n",
    "So, **good weight initialization** is like giving students the right books â€“ not too hard, not too easy, just perfect for learning! ğŸ“–âœ¨  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's manually calculate weight initialization for a simple **neural network** layer.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ§® Example: One Layer Neural Network**  \n",
    "Consider a **fully connected layer** with:  \n",
    "- **Inputs (neurons in previous layer) = 3**  \n",
    "- **Outputs (neurons in current layer) = 2**  \n",
    "\n",
    "Each neuron has a weight **W** connecting it to the next layer.  \n",
    "\n",
    "\n",
    "\n",
    "### **1ï¸âƒ£ Xavier (Glorot) Initialization**  \n",
    "ğŸ”¹ Formula:  \n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{in} + n_{out}})\n",
    "$$  \n",
    "where:  \n",
    "- $ n_{in} = 3 $ (number of input neurons)  \n",
    "- $ n_{out} = 2 $ (number of output neurons)  \n",
    "\n",
    "ğŸ“Œ **Calculate Variance:**  \n",
    "$$\n",
    "\\text{Variance} = \\frac{1}{3 + 2} = \\frac{1}{5} = 0.2\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Generate Weights (Random from Normal Distribution)**  \n",
    "Letâ€™s assume some random values (mean = 0, variance = 0.2):  \n",
    "$$\n",
    "W = \\begin{bmatrix} 0.45 & -0.30 \\\\ 0.12 & 0.25 \\\\ -0.50 & 0.33 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "âœ… **This prevents exploding/vanishing gradients for sigmoid/tanh activations.**  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ He Initialization (Best for ReLU!)**  \n",
    "ğŸ”¹ Formula:  \n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{2}{n_{in}})\n",
    "$$  \n",
    "where $ n_{in} = 3 $.  \n",
    "\n",
    "ğŸ“Œ **Calculate Variance:**  \n",
    "$$\n",
    "\\text{Variance} = \\frac{2}{3} = 0.67\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Generate Weights (Random from Normal Distribution)**  \n",
    "Assuming some random values with mean = 0 and variance = 0.67:  \n",
    "$$\n",
    "W = \\begin{bmatrix} 0.80 & -0.60 \\\\ 0.50 & 0.75 \\\\ -0.40 & 0.90 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "âœ… **This helps ReLU neurons get the right scale of activation!**  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Lecun Initialization (For Sigmoid/Tanh)**  \n",
    "ğŸ”¹ Formula:  \n",
    "$$\n",
    "W \\sim \\mathcal{N}(0, \\frac{1}{n_{in}})\n",
    "$$  \n",
    "where $ n_{in} = 3 $.  \n",
    "\n",
    "ğŸ“Œ **Calculate Variance:**  \n",
    "$$\n",
    "\\text{Variance} = \\frac{1}{3} = 0.33\n",
    "$$  \n",
    "\n",
    "ğŸ“Œ **Generate Weights:**  \n",
    "Assuming random values from a normal distribution with variance 0.33:  \n",
    "$$\n",
    "W = \\begin{bmatrix} 0.55 & -0.45 \\\\ 0.30 & 0.20 \\\\ -0.60 & 0.40 \\end{bmatrix}\n",
    "$$  \n",
    "\n",
    "âœ… **Keeps activations in a stable range for sigmoid/tanh!**  \n",
    "\n",
    "\n",
    "### **ğŸš€ Summary of Manual Calculation Results:**  \n",
    "| Method  | Variance Formula | Example Weights |\n",
    "|---------|----------------|----------------|\n",
    "| **Xavier**  | $ 1 / (n_{in} + n_{out}) $ | $ W = \\begin{bmatrix} 0.45 & -0.30 \\\\ 0.12 & 0.25 \\\\ -0.50 & 0.33 \\end{bmatrix} $ |\n",
    "| **He**  | $ 2 / n_{in} $ | $ W = \\begin{bmatrix} 0.80 & -0.60 \\\\ 0.50 & 0.75 \\\\ -0.40 & 0.90 \\end{bmatrix} $ |\n",
    "| **Lecun**  | $ 1 / n_{in} $ | $ W = \\begin{bmatrix} 0.55 & -0.45 \\\\ 0.30 & 0.20 \\\\ -0.60 & 0.40 \\end{bmatrix} $ |\n",
    "\n",
    "\n",
    "### **ğŸ”¥ Conclusion:**  \n",
    "1. **Xavier** works for **sigmoid/tanh** (keeps values balanced).  \n",
    "2. **He** is best for **ReLU** (prevents neurons from dying).  \n",
    "3. **Lecun** is specialized for **sigmoid/tanh** (better for stability).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Normalization in Simple Terms ğŸˆ**  \n",
    "\n",
    "Imagine you're baking a cake ğŸ‚, and every ingredient (flour, sugar, milk) has to be measured properly. If the measurements keep changing every time you bake, the cake will taste different each time. **Batch Normalization** is like a kitchen scale that ensures all ingredients are measured consistently, so every cake turns out perfect!  \n",
    "\n",
    "Now, letâ€™s break it down:  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¤” The Problem: Why Do We Need Batch Normalization?**  \n",
    "\n",
    "1. **Neural Networks Learn from Layer to Layer** ğŸ—ï¸  \n",
    "   - Each layer in a neural network transforms data and passes it forward.  \n",
    "   - If the inputs to each layer vary too much, learning becomes unstable.  \n",
    "\n",
    "2. **Internal Covariate Shift** ğŸ¢  \n",
    "   - Think of a student solving math problems. If the difficulty of problems keeps changing wildly, they struggle.  \n",
    "   - Similarly, if the inputs to a neural network change unpredictably, it struggles to learn efficiently.  \n",
    "\n",
    "3. **Gradients Become Too Big or Too Small** ğŸ“‰ğŸ“ˆ  \n",
    "   - If values explode (too big) or vanish (too small), training becomes slow or even stuck.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ› ï¸ What Does Batch Normalization Do?**  \n",
    "\n",
    "Batch Normalization **fixes these issues** by making sure the activations (outputs of each layer) are well-behaved. It does two things:  \n",
    "\n",
    "1. **Makes Data More Predictable ğŸ“Š**  \n",
    "   - It **normalizes** (adjusts) the output of each layer so that values have a **mean of 0 and variance of 1**.  \n",
    "   - This means the activations wonâ€™t be too large or too small, keeping learning smooth.  \n",
    "\n",
    "2. **Lets the Network Adjust the Scale âš–ï¸**  \n",
    "   - Instead of forcing the activations to always stay zero-centered, it allows some flexibility using two **trainable parameters**:  \n",
    "     - **Gamma (Î³) ğŸ“ˆ** â€“ Controls the scale (how stretched the values are).  \n",
    "     - **Beta (Î²) ğŸ“** â€“ Controls the shift (where the values center around).  \n",
    "   - This lets the network decide the best way to normalize values.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ How Does Batch Normalization Work? (Step-by-Step Example)**  \n",
    "\n",
    "Letâ€™s say a layer in a neural network produces these outputs for a batch of 5 samples:  \n",
    "\n",
    "| Sample | Activation (Before BatchNorm) |\n",
    "|--------|------------------------------|\n",
    "| 1      | **10**                        |\n",
    "| 2      | **20**                        |\n",
    "| 3      | **30**                        |\n",
    "| 4      | **40**                        |\n",
    "| 5      | **50**                        |\n",
    "\n",
    "#### **Step 1: Calculate Mean and Variance**\n",
    "- **Mean (Average):**  \n",
    "  $$\n",
    "  \\mu = \\frac{10 + 20 + 30 + 40 + 50}{5} = 30\n",
    "  $$  \n",
    "- **Variance (Spread of values):**  \n",
    "  $$\n",
    "  \\sigma^2 = \\frac{(10-30)^2 + (20-30)^2 + (30-30)^2 + (40-30)^2 + (50-30)^2}{5} = 200\n",
    "  $$\n",
    "\n",
    "#### **Step 2: Normalize the Values (Make Mean = 0, Variance = 1)**\n",
    "- Subtract the mean and divide by standard deviation:  \n",
    "  $$\n",
    "  \\hat{X}_i = \\frac{X_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "  $$  \n",
    "  - (Using a small **Îµ** to avoid division by zero)  \n",
    "\n",
    "| Sample | Activation (After Normalization) |\n",
    "|--------|-------------------------------|\n",
    "| 1      | **-1.41**                     |\n",
    "| 2      | **-0.71**                     |\n",
    "| 3      | **0.00**                       |\n",
    "| 4      | **0.71**                       |\n",
    "| 5      | **1.41**                       |\n",
    "\n",
    "#### **Step 3: Scale and Shift (Using Î³ & Î²)**\n",
    "- Multiply by **Î³** and add **Î²**:  \n",
    "  $$\n",
    "  Y_i = \\gamma \\hat{X}_i + \\beta\n",
    "  $$\n",
    "  - If Î³ = 2 and Î² = 3, we get:  \n",
    "\n",
    "| Sample | Final Output (After BatchNorm) |\n",
    "|--------|-------------------------------|\n",
    "| 1      | **0.18**                      |\n",
    "| 2      | **1.58**                      |\n",
    "| 3      | **3.00**                       |\n",
    "| 4      | **4.42**                       |\n",
    "| 5      | **5.82**                       |\n",
    "\n",
    "Now, the values are **stable**, and the network can **learn efficiently! ğŸ¯**  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¯ Why Is Batch Normalization Helpful?**\n",
    "âœ… **Speeds up Training ğŸš€** â€“ The network converges faster because activations are well-scaled.  \n",
    "âœ… **Prevents Vanishing/Exploding Gradients ğŸ’¥** â€“ Keeps values balanced, avoiding training issues.  \n",
    "âœ… **Reduces Dependence on Careful Initialization ğŸ›ï¸** â€“ The model works well even if weights are not perfectly set at the start.  \n",
    "âœ… **Acts as a Regularizer ğŸ›¡ï¸** â€“ Adds a slight randomness that reduces overfitting, like dropout.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“ Where Do We Use Batch Normalization?**\n",
    "ğŸ’¡ Typically, BatchNorm is added **after fully connected (Dense) or convolutional layers** and **before activation functions** (like ReLU).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”§ Example Code**\n",
    "#### **ğŸ“ In TensorFlow/Keras**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add BatchNorm after dense layer\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "#### **ğŸ“ In PyTorch**\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # BatchNorm applied after Linear layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(nn.ReLU()(self.fc1(x)))\n",
    "        x = self.bn2(nn.ReLU()(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¨ Final Thoughts (Super Simplified ğŸŒˆ)**\n",
    "Think of **Batch Normalization** as a **temperature control system for your neural network**:  \n",
    "ğŸŒ¡ï¸ **Without BatchNorm** â€“ Some layers get too hot (high activations) or too cold (low activations), making training unstable.  \n",
    "â„ï¸ğŸ”¥ **With BatchNorm** â€“ Keeps everything at a nice, stable temperature so the network can learn efficiently.  \n",
    "\n",
    "So, next time your deep learning model is struggling with slow training or inconsistent results, just **sprinkle some BatchNorm magic!** ğŸª„âœ¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Batch Normalization (BatchNorm) ğŸ­ â€“ The Secret Weapon of Deep Learning**  \n",
    "\n",
    "Imagine you're training a neural network, and after every layer, the distribution of activations keeps changing. This \"internal covariate shift\" makes training slow and unstable. Enter **Batch Normalization (BatchNorm) ğŸŒŸ**, a powerful technique that helps stabilize and accelerate training by normalizing activations!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Why Do We Need Batch Normalization?**\n",
    "1. **Tames Internal Covariate Shift ğŸŒªï¸**  \n",
    "   - During training, the distribution of activations in each layer keeps shifting, making learning chaotic. BatchNorm normalizes them to stay consistent.  \n",
    "\n",
    "2. **Faster Training âš¡**  \n",
    "   - Since activations are well-behaved, the model learns efficiently, requiring a higher learning rate without risk of instability.  \n",
    "\n",
    "3. **Prevents Vanishing/Exploding Gradients ğŸ’¥**  \n",
    "   - Normalized inputs keep gradients in check, ensuring smooth backpropagation.  \n",
    "\n",
    "4. **Reduces Dependence on Careful Weight Initialization ğŸ¯**  \n",
    "   - Normally, weight initialization is critical, but BatchNorm makes the network more robust to bad initialization.  \n",
    "\n",
    "5. **Acts as a Regularizer ğŸ›¡ï¸**  \n",
    "   - It introduces some noise (due to batch-wise statistics), acting like a form of dropout and reducing overfitting.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ How Batch Normalization Works (Step by Step)**\n",
    "Let's say we have an activation output **X** from some layer in the network:  \n",
    "\n",
    "1. **Compute the Mean and Variance ğŸ§®**  \n",
    "   - For a mini-batch of size `m`, calculate:  \n",
    "     $$\n",
    "     \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} X_i\n",
    "     $$\n",
    "     $$\n",
    "     \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (X_i - \\mu_B)^2\n",
    "     $$  \n",
    "   - These represent the mean and variance across the batch.\n",
    "\n",
    "2. **Normalize the Activations ğŸ‹ï¸â€â™‚ï¸**  \n",
    "   - Subtract the mean and divide by the standard deviation:\n",
    "     $$\n",
    "     \\hat{X}_i = \\frac{X_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
    "     $$\n",
    "   - The small **Îµ** (epsilon) prevents division by zero.\n",
    "\n",
    "3. **Scale and Shift (Learnable Parameters) ğŸ›ï¸**  \n",
    "   - Instead of forcing activations to have zero mean and unit variance, BatchNorm introduces two **trainable** parameters:\n",
    "     $$\n",
    "     Y_i = \\gamma \\hat{X}_i + \\beta\n",
    "     $$\n",
    "   - **Î³ (gamma) ğŸ“ˆ**: Controls the spread (scaling factor).  \n",
    "   - **Î² (beta) ğŸ“**: Controls the shift (bias term).  \n",
    "   - This lets the network learn an **optimal distribution** instead of being locked into strict normalization.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ—ï¸ Where Do We Use Batch Normalization?**\n",
    "âœ… **Between Linear Transformations & Activation Functions**  \n",
    "   - Applied **before or after** activation functions like ReLU, Sigmoid, or Tanh.  \n",
    "   - Typically inserted **after a fully connected (Dense) or convolutional layer**.  \n",
    "\n",
    "âœ… **Before or After Dropout?** ğŸ¤”  \n",
    "   - Usually, **before dropout** to ensure stable activations before randomly dropping neurons.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š How Does BatchNorm Improve Performance?**\n",
    "âœ… **Faster Convergence ğŸš€** â€“ Reduces training time significantly.  \n",
    "âœ… **Allows Higher Learning Rates ğŸ¯** â€“ No need to be cautious about small steps.  \n",
    "âœ… **Helps Deep Networks ğŸ—ï¸** â€“ Works well even in very deep architectures.  \n",
    "âœ… **Better Generalization ğŸ”** â€“ Reduces overfitting, especially when dataset size is small.  \n",
    "\n",
    "\n",
    "\n",
    "## **âš ï¸ Potential Downsides of BatchNorm**\n",
    "âŒ **Batch Size Sensitivity ğŸ“**  \n",
    "   - Very small batch sizes can produce unreliable statistics, leading to unstable training.  \n",
    "\n",
    "âŒ **Extra Computation ğŸ–¥ï¸**  \n",
    "   - Slight overhead, but usually worth the benefits.  \n",
    "\n",
    "âŒ **Doesnâ€™t Always Work Best ğŸ”„**  \n",
    "   - In some cases, **LayerNorm, GroupNorm, or InstanceNorm** may be better (especially for non-batch-dependent settings).  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Implementing Batch Normalization in Python (TensorFlow & PyTorch)**  \n",
    "\n",
    "### **ğŸ“Œ In TensorFlow/Keras**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add BatchNorm after dense layer\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "### **ğŸ“Œ In PyTorch**\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)  # BatchNorm applied after Linear layer\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn1(nn.ReLU()(self.fc1(x)))\n",
    "        x = self.bn2(nn.ReLU()(self.fc2(x)))\n",
    "        return self.fc3(x)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¨ Final Thoughts**\n",
    "Batch Normalization is like giving your neural network a **smooth ride on a roller coaster ğŸ¢**â€”keeping the activations well-behaved and preventing extreme fluctuations. It's a **game-changer** for deep networks, making them train faster, perform better, and generalize well.  \n",
    "\n",
    "So next time your neural network struggles with training instability, just sprinkle in some **BatchNorm magic**! âœ¨ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimizers in Deep Learning ğŸš€**\n",
    "  \n",
    "Optimizers are like **coaches** for a neural network. They adjust the weights and biases of the model during training to **reduce the loss and improve accuracy**. Without optimizers, the model wouldn't know how to improve itself!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Why Do We Need Optimizers?**  \n",
    "When training a deep learning model, we want to **minimize the loss function** (which measures how far our predictions are from the actual values). Optimizers **adjust model parameters** (weights & biases) using **gradients** to make better predictions.\n",
    "\n",
    "ğŸ”„ **Think of it like this:**  \n",
    "- The model is trying to find the lowest point in a loss landscape (like a valley).  \n",
    "- The optimizer is the **guide** that helps navigate downhill efficiently.  \n",
    "- Gradients (slopes) tell the model which direction to move.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Types of Optimizers in Deep Learning**  \n",
    "\n",
    "Optimizers fall into two broad categories:  \n",
    "\n",
    "### **1ï¸âƒ£ First-Order Optimizers (Gradient-Based)**\n",
    "- These rely on **gradients (first derivatives) of the loss function**.  \n",
    "- Examples: **Gradient Descent, Momentum, RMSprop, Adam**  \n",
    "\n",
    "### **2ï¸âƒ£ Second-Order Optimizers**\n",
    "- These use **second derivatives (Hessian matrix)** for better curvature information but are computationally expensive.  \n",
    "- Example: **Newton's Method (rarely used in deep learning)**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Common Optimizers Explained Simply**  \n",
    "\n",
    "### **1ï¸âƒ£ Gradient Descent (GD)**\n",
    "**ğŸŒ± The most basic optimizer!**  \n",
    "It updates weights in the direction of the negative gradient to minimize loss.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "W = W - \\eta \\cdot \\nabla L(W)\n",
    "$$\n",
    "- **$ W $** â†’ Model weights  \n",
    "- **$ \\eta $ (Learning Rate)** â†’ Step size  \n",
    "- **$ \\nabla L(W) $** â†’ Gradient of loss  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Simple and effective for convex functions.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Very slow for large datasets (since it updates after seeing the entire dataset).  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Stochastic Gradient Descent (SGD)**\n",
    "**ğŸš€ A faster version of GD!**  \n",
    "Instead of using the entire dataset, **SGD updates weights using one data sample at a time**.  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Much faster than normal Gradient Descent.  \n",
    "âœ” Works well for large datasets.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Noisy updates (weight updates fluctuate a lot).  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Mini-Batch Gradient Descent**\n",
    "**ğŸ“¦ Best of both worlds!**  \n",
    "It updates weights after processing a **small batch of samples** instead of one sample or the whole dataset.  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Balances efficiency and stability.  \n",
    "âœ” Used in almost all deep learning models.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Choosing the right batch size is tricky.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Momentum Optimizer**\n",
    "**ğŸƒ Boosts speed by adding inertia!**  \n",
    "Instead of just using gradients, it **remembers past updates** to smooth out weight updates.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla L(W)\n",
    "$$\n",
    "$$\n",
    "W = W - \\eta v_t\n",
    "$$\n",
    "- **$ v_t $** is the velocity (moving average of past gradients).  \n",
    "- **$ \\beta $** is the momentum factor (common choice: 0.9).  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Reduces zigzag motion and speeds up training.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Can overshoot the minimum if the momentum is too high.  \n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ RMSprop (Root Mean Square Propagation)**\n",
    "**ğŸ“‰ Handles learning rate adaptively!**  \n",
    "Instead of a fixed learning rate, RMSprop **adapts the learning rate** based on recent gradients.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "S_t = \\beta S_{t-1} + (1 - \\beta) \\nabla L(W)^2\n",
    "$$\n",
    "$$\n",
    "W = W - \\frac{\\eta}{\\sqrt{S_t + \\epsilon}} \\nabla L(W)\n",
    "$$\n",
    "- **$ S_t $** keeps track of past squared gradients.  \n",
    "- **$ \\epsilon $** prevents division by zero.  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Works well with non-stationary data.  \n",
    "âœ” Used in RNNs and NLP tasks.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Can get stuck in local minima.  \n",
    "\n",
    "\n",
    "\n",
    "### **6ï¸âƒ£ Adam (Adaptive Moment Estimation)**\n",
    "**ğŸ’¡ The most popular optimizer today!**  \n",
    "Adam combines **Momentum and RMSprop** for the best of both worlds!  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(W)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla L(W)^2\n",
    "$$\n",
    "$$\n",
    "W = W - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} m_t\n",
    "$$\n",
    "- **$ m_t $** â†’ Moving average of past gradients (Momentum).  \n",
    "- **$ v_t $** â†’ Moving average of squared gradients (RMSprop).  \n",
    "\n",
    "âœ… **Pros:**  \n",
    "âœ” Adaptive learning rates.  \n",
    "âœ” Faster convergence than SGD.  \n",
    "âœ” Works well for most deep learning problems.  \n",
    "\n",
    "âŒ **Cons:**  \n",
    "âœ– Can lead to overfitting.  \n",
    "âœ– Sometimes gets stuck in **sharp minima**.  \n",
    "\n",
    "## **ğŸ”¹ Choosing the Right Optimizer ğŸš¦**\n",
    "| **Optimizer**  | **When to Use?**  |\n",
    "|--------------|----------------|\n",
    "| **SGD** | Works well for small, simple datasets. |\n",
    "| **Mini-Batch GD** | Preferred for deep learning tasks. |\n",
    "| **Momentum** | Helps with faster convergence in deep networks. |\n",
    "| **RMSprop** | Good for RNNs and NLP models. |\n",
    "| **Adam** | Best for most deep learning applications. |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Summary**\n",
    "âœ… Optimizers adjust weights to reduce loss.  \n",
    "âœ… SGD, Momentum, RMSprop, and Adam are commonly used.  \n",
    "âœ… **Adam is the most preferred optimizer** in deep learning.  \n",
    "âœ… Choosing the right optimizer depends on the task.  \n",
    "\n",
    "Would you like a **code example** to compare these optimizers? ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizers in Deep Learning â€“ Super Simple Explanation** ğŸš€  \n",
    "\n",
    "Imagine you're a **blindfolded person** trying to find the lowest point in a hilly area (the **lowest loss** in deep learning). You take small steps in different directions, feeling the slope to figure out where to go. This is exactly what an **optimizer** does for a neural networkâ€”it **adjusts the modelâ€™s weights to minimize the loss** and improve accuracy!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ›£ How Optimizers Work (A Simple Story)**\n",
    "Let's say youâ€™re trying to find your way down a hill:  \n",
    "\n",
    "1ï¸âƒ£ **You feel the ground** to check which direction slopes downward (this is like calculating the **gradient**).  \n",
    "2ï¸âƒ£ **You take a step** in the direction that goes downhill (this is **updating the weights**).  \n",
    "3ï¸âƒ£ **If the hill is steep, you take bigger steps**; if it's flat, you take smaller steps (**learning rate** decides step size).  \n",
    "4ï¸âƒ£ **You keep repeating this** until you reach the lowest point (**minimized loss**).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¹ Different Types of Optimizers (Explained with Examples)**\n",
    "Just like different walking strategies help you reach the bottom of the hill **faster or more efficiently**, different optimizers improve neural network training.  \n",
    "\n",
    "\n",
    "\n",
    "### **1ï¸âƒ£ Gradient Descent â€“ Walking Slowly Down the Hill ğŸš¶**\n",
    "- You check the slope of the hill and take **one step at a time** based on the entire landscape.  \n",
    "- **Problem?** Itâ€™s **slow** because it looks at the whole area before deciding where to step.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Stochastic Gradient Descent (SGD) â€“ Running Down Randomly ğŸƒâ€â™‚ï¸**\n",
    "- Instead of analyzing the whole landscape, you **take quick steps based on a small part of the area**.  \n",
    "- **Good?** Faster than regular Gradient Descent.  \n",
    "- **Problem?** You might take **zigzag steps** and miss the exact lowest point.  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Mini-Batch Gradient Descent â€“ Group Walks ğŸ‘¬**\n",
    "- Instead of stepping alone (SGD) or checking the whole area (GD), you **walk in small groups** and decide based on **average direction**.  \n",
    "- **Good?** Balances speed and accuracy.  \n",
    "- **Used in?** Almost all deep learning models today.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Momentum â€“ Running with a Push ğŸƒâ€â™‚ï¸ğŸ’¨**\n",
    "- Imagine you're on a bicycle. Instead of stopping after each step, **you use past speed to keep moving forward smoothly**.  \n",
    "- Helps prevent sudden stops and makes progress **faster and smoother**.  \n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ RMSprop â€“ Smart Steps to Avoid Slipping ğŸ¤–**\n",
    "- If you see **slippery areas** (steep parts), you **slow down automatically**.  \n",
    "- Helps **avoid overshooting the lowest point** and works well for unpredictable landscapes (like speech or text data).  \n",
    "\n",
    "\n",
    "\n",
    "### **6ï¸âƒ£ Adam â€“ The Smartest Guide ğŸ§­**\n",
    "- **Combines Momentum and RMSprop** for the best of both worlds.  \n",
    "- It **remembers past steps** (Momentum) and **adjusts speed** based on terrain steepness (RMSprop).  \n",
    "- **Why do people love Adam?**  \n",
    "  âœ… Fast  \n",
    "  âœ… Works for almost any deep learning problem  \n",
    "  âœ… Smartly adjusts learning rate  \n",
    "\n",
    "### **ğŸ”¹ Choosing the Right Optimizer**\n",
    "| **Optimizer**  | **Best For?**  |\n",
    "|--------------|----------------|\n",
    "| **Gradient Descent** | Simple models, small datasets. |\n",
    "| **SGD** | Faster training, but less stable. |\n",
    "| **Mini-Batch GD** | Used in almost all deep learning models. |\n",
    "| **Momentum** | Prevents sudden stops, smooth training. |\n",
    "| **RMSprop** | Good for speech, NLP, and RNNs. |\n",
    "| **Adam** | Best for most deep learning applications! |\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“ Final Takeaway â€“ Which Optimizer is Best?**\n",
    "- If **you donâ€™t know what to choose** â†’ **Adam** is the safest choice.  \n",
    "- If **you want something simple and stable** â†’ **Mini-Batch GD** is great.  \n",
    "- If **you work with sequential data like text or speech** â†’ **RMSprop** is better.  \n",
    "\n",
    "Would you like a simple **Python example** to see these optimizers in action? ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! We can manually calculate how different optimizers update weights step by step. Below, I'll walk through the manual calculations for **Gradient Descent, SGD, Momentum, RMSprop, and Adam** using a simple loss function.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Our Setup:**\n",
    "Let's consider a simple quadratic loss function:  \n",
    "$$\n",
    "L(w) = w^2\n",
    "$$\n",
    "where $ L $ is the loss, and $ w $ is the weight.  \n",
    "Our goal is to minimize this function by adjusting $ w $.  \n",
    "\n",
    "### **ğŸ”¢ Given Values:**\n",
    "- Initial weight: $ w = 4 $  \n",
    "- Learning rate: $ \\eta = 0.1 $  \n",
    "- Gradient: $ \\frac{dL}{dw} = 2w $  \n",
    "- Momentum: $ \\beta = 0.9 $  \n",
    "- RMSprop & Adam decay rates: $ \\beta_1 = 0.9 $, $ \\beta_2 = 0.999 $  \n",
    "- Small constant: $ \\epsilon = 10^{-8} $  \n",
    "\n",
    "\n",
    "\n",
    "## **1ï¸âƒ£ Gradient Descent (GD) â€“ Basic Update Rule**\n",
    "GD updates weights using:  \n",
    "$$\n",
    "w_{\\text{new}} = w - \\eta \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "### **ğŸ”¢ Manual Calculation**\n",
    "1st Iteration:  \n",
    "$$\n",
    "\\frac{dL}{dw} = 2(4) = 8\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - (0.1 \\times 8) = 4 - 0.8 = 3.2\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "\\frac{dL}{dw} = 2(3.2) = 6.4\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.2 - (0.1 \\times 6.4) = 3.2 - 0.64 = 2.56\n",
    "$$\n",
    "\n",
    "**GD keeps updating weights until convergence.** ğŸš¶\n",
    "\n",
    "\n",
    "\n",
    "## **2ï¸âƒ£ Stochastic Gradient Descent (SGD) â€“ Random Updates**\n",
    "SGD follows the same formula as GD but updates using **random samples instead of full batch**. The process is the same as GD, but each update is based on a random small dataset rather than all data.\n",
    "\n",
    "For this example, SGD and GD will behave similarly, but with noisy updates in real-world cases.\n",
    "\n",
    "\n",
    "\n",
    "## **3ï¸âƒ£ Momentum â€“ Adds Speed Boost ğŸš€**\n",
    "Momentum helps **accelerate learning** by considering previous updates:  \n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + \\eta \\frac{dL}{dw}\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - v_t\n",
    "$$\n",
    "\n",
    "### **ğŸ”¢ Manual Calculation**\n",
    "Letâ€™s initialize $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "v_1 = (0.9 \\times 0) + (0.1 \\times 8) = 0.8\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - 0.8 = 3.2\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "v_2 = (0.9 \\times 0.8) + (0.1 \\times 6.4) = 0.72 + 0.64 = 1.36\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.2 - 1.36 = 1.84\n",
    "$$\n",
    "\n",
    "Momentum **smoothens updates** and avoids oscillations! ğŸš²\n",
    "\n",
    "\n",
    "\n",
    "## **4ï¸âƒ£ RMSprop â€“ Adjusts Learning Rate Dynamically**\n",
    "RMSprop scales the learning rate based on the squared gradient:  \n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) (\\frac{dL}{dw})^2\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "### **ğŸ”¢ Manual Calculation**\n",
    "Letâ€™s initialize $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "v_1 = (0.9 \\times 0) + (0.1 \\times 8^2) = 6.4\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - \\frac{0.1}{\\sqrt{6.4} + 10^{-8}} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - \\frac{0.1}{2.53} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - 0.32 = 3.68\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "v_2 = (0.9 \\times 6.4) + (0.1 \\times 6.4^2) = 10.24\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.68 - \\frac{0.1}{\\sqrt{10.24} + 10^{-8}} \\times 6.4\n",
    "$$\n",
    "$$\n",
    "= 3.68 - \\frac{0.1}{3.2} \\times 6.4\n",
    "$$\n",
    "$$\n",
    "= 3.68 - 0.2 = 3.48\n",
    "$$\n",
    "\n",
    "RMSprop **adapts learning rates to each step** ğŸ“‰.\n",
    "\n",
    "\n",
    "\n",
    "## **5ï¸âƒ£ Adam â€“ The Best of Momentum + RMSprop**\n",
    "Adam uses **two moving averages**:  \n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot (\\frac{dL}{dw})^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "$$\n",
    "\n",
    "### **ğŸ”¢ Manual Calculation**\n",
    "Letâ€™s initialize $ m_0 = 0 $, $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "m_1 = (0.9 \\times 0) + (0.1 \\times 8) = 0.8\n",
    "$$\n",
    "$$\n",
    "v_1 = (0.999 \\times 0) + (0.001 \\times 8^2) = 0.064\n",
    "$$\n",
    "Bias correction:\n",
    "$$\n",
    "\\hat{m}_1 = \\frac{0.8}{1 - 0.9} = 8, \\quad \\hat{v}_1 = \\frac{0.064}{1 - 0.999} = 64\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - \\frac{0.1}{\\sqrt{64} + 10^{-8}} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - \\frac{0.1}{8} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - 0.1 = 3.9\n",
    "$$\n",
    "\n",
    "Adam **smooths learning and adapts step sizes** ğŸ¤–.\n",
    "\n",
    "## **Final Summary**\n",
    "| Optimizer  | Manual Calculation Process | Benefit |\n",
    "|------------|-----------------------------|----------|\n",
    "| **GD** | $ w = w - \\eta \\cdot \\text{grad} $ | Simple but slow |\n",
    "| **SGD** | Same as GD but on **random** data | Faster but noisy |\n",
    "| **Momentum** | Uses velocity to **accelerate** learning | Smooth updates |\n",
    "| **RMSprop** | Uses squared gradients for adaptive learning | Avoids overshooting |\n",
    "| **Adam** | Combines Momentum + RMSprop | Best for most cases |\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¯ Conclusion**\n",
    "- You **can** manually calculate how each optimizer updates weights!  \n",
    "- **Gradient Descent** is simple but slow.  \n",
    "- **Momentum** speeds things up.  \n",
    "- **RMSprop & Adam** adapt learning rates and work better in complex cases.  \n",
    "- **Adam is usually the best default choice!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
