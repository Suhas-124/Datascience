{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåà **Gradient Descent: The Key to Learning in Neural Networks! üöÄ**  \n",
    "\n",
    "Gradient Descent is the **GPS** üß≠ for neural networks, guiding them to minimize errors and improve predictions by updating model weights. Let‚Äôs explore its **3 main types** in a colorful way! üé®‚ú®  \n",
    "\n",
    "\n",
    "\n",
    "## üî• **1. Batch Gradient Descent (BGD) ‚Äì The Careful Planner üìù**  \n",
    "üîπ Uses **the entire dataset** at once to compute gradients before updating weights.  \n",
    "üîπ Think of it as **a marathon runner** who plans every step carefully before moving! üèÉ‚Äç‚ôÇÔ∏èüèÖ  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úîÔ∏è More **stable** convergence (fewer fluctuations).  \n",
    "‚úîÔ∏è Moves **directly** towards the optimal solution.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚ùå **Slow** for large datasets üö∂‚Äç‚ôÇÔ∏èüí§  \n",
    "‚ùå **High memory usage** (needs the entire dataset at once).  \n",
    "\n",
    "üéØ **Use case:** When dataset is **small and fits in memory**.  \n",
    "\n",
    "\n",
    "\n",
    "## ‚ö° **2. Stochastic Gradient Descent (SGD) ‚Äì The Fast & Furious Racer üèéÔ∏è**  \n",
    "üîπ Updates weights **after each training sample** (one at a time).  \n",
    "üîπ Like a **Formula 1 driver**‚Äîsuper fast but a little shaky! üèÅüî•  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úîÔ∏è **Faster** updates (great for huge datasets).  \n",
    "‚úîÔ∏è Works well in **online learning** (real-time updates).  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚ùå Noisy updates, making it **jump around** instead of smoothly converging.  \n",
    "‚ùå Might **overshoot** the best solution.  \n",
    "\n",
    "üéØ **Use case:** When dealing with **very large datasets** (e.g., real-time applications).  \n",
    "\n",
    "\n",
    "\n",
    "## üåü **3. Mini-Batch Gradient Descent (MBGD) ‚Äì The Balanced Middle Ground ‚öñÔ∏è**  \n",
    "üîπ Uses a **small batch** (e.g., 32, 64, 128 samples) for each update.  \n",
    "üîπ Like **a relay race**‚Äîfaster than BGD, steadier than SGD! üèÉ‚Äç‚ôÄÔ∏èüèÉ‚Äç‚ôÇÔ∏èüèÉ  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úîÔ∏è **Balances** stability (like BGD) and efficiency (like SGD).  \n",
    "‚úîÔ∏è Works well for **deep learning**.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚ùå Still needs tuning for **batch size** (trial & error).  \n",
    "\n",
    "üéØ **Use case:** **Most common** in deep learning (e.g., training neural networks on GPUs).  \n",
    "\n",
    "\n",
    "\n",
    "### üé© **Magic Tricks to Improve Gradient Descent! üéØ**  \n",
    "\n",
    "üöÄ **Momentum** ‚Äì Adds a **memory effect** to avoid zig-zagging and reach the goal faster! üèÄ‚õπÔ∏è  \n",
    "üöÄ **RMSprop & Adam** ‚Äì Adjust learning rates **automatically** to speed up convergence! üìàüîù  \n",
    "\n",
    "Gradient Descent is like training for a **race**‚Äîchoosing the right strategy makes all the difference! üèÜ  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåü **Batch Gradient Descent (BGD) - A Complete Guide** üåü  \n",
    "\n",
    "#### üöÄ **What is Batch Gradient Descent?**  \n",
    "Batch Gradient Descent (BGD) is a type of **optimization algorithm** used in machine learning and deep learning to minimize the **loss function** by updating the model's weights. In BGD, the entire dataset is used to compute gradients **before** updating the weights.\n",
    "\n",
    "\n",
    "\n",
    "### üß† **How Batch Gradient Descent Works?**\n",
    "1Ô∏è‚É£ **Compute the Loss**:  \n",
    "   - Calculate the difference between predicted and actual values using a loss function (e.g., Mean Squared Error, Cross-Entropy).  \n",
    "\n",
    "2Ô∏è‚É£ **Calculate Gradients**:  \n",
    "   - Derivatives of the loss function with respect to model parameters (weights & biases) are computed **using the entire dataset**.  \n",
    "\n",
    "3Ô∏è‚É£ **Update Parameters**:  \n",
    "   - The weights are updated using the formula:  \n",
    "\n",
    "   $$\n",
    "   W = W - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla L(W, x_i, y_i)\n",
    "   $$\n",
    "\n",
    "   where:  \n",
    "   - **W** ‚Üí Weight parameters  \n",
    "   - **Œ± (alpha)** ‚Üí Learning rate  \n",
    "   - **m** ‚Üí Number of training samples  \n",
    "   - **‚àáL(W, x_i, y_i)** ‚Üí Gradient of loss  \n",
    "\n",
    "4Ô∏è‚É£ **Repeat Until Convergence**:  \n",
    "   - Steps 1-3 are repeated until the loss stops decreasing or reaches an acceptable level.  \n",
    "\n",
    "\n",
    "\n",
    "### üéØ **Advantages of Batch Gradient Descent**\n",
    "‚úÖ **Stable Updates** ‚Äì Since the entire dataset is used, updates are smooth and converge steadily.  \n",
    "‚úÖ **More Accurate Direction** ‚Äì Since it considers all data points, it provides a better estimate of the optimal weight updates.  \n",
    "‚úÖ **Less Noisy** ‚Äì Unlike **Stochastic Gradient Descent (SGD)**, BGD doesn‚Äôt have high variance in updates.  \n",
    "\n",
    "\n",
    "\n",
    "### ‚ùå **Disadvantages of Batch Gradient Descent**\n",
    "‚ùå **Slow for Large Datasets** ‚Äì Since it requires processing all data before updating, it's computationally expensive.  \n",
    "‚ùå **High Memory Usage** ‚Äì Storing and computing gradients for a large dataset can consume a lot of memory (RAM).  \n",
    "‚ùå **May Get Stuck in Local Minima** ‚Äì If the loss function is non-convex, it might converge to a suboptimal point.  \n",
    "\n",
    "\n",
    "\n",
    "### üí° **When to Use Batch Gradient Descent?**\n",
    "üîπ When the dataset is **small** and fits in memory.  \n",
    "üîπ When **stable convergence** is more important than speed.  \n",
    "üîπ When training simple models like **linear regression or small neural networks**.  \n",
    "\n",
    "### üÜö **BGD vs. Other Gradient Descent Types**\n",
    "| Type | Update Frequency | Speed | Stability | Best For |\n",
    "|------|----------------|-------|-----------|----------|\n",
    "| **Batch GD** | After **all** samples | ‚è≥ Slow | ‚úÖ Very Stable | Small datasets |\n",
    "| **Mini-Batch GD** | After **batch** (e.g., 32, 64 samples) | ‚ö° Medium | ‚úÖ Stable | Large datasets |\n",
    "| **Stochastic GD (SGD)** | After **each** sample | üöÄ Fast | ‚ùå Noisy | Online learning |\n",
    "\n",
    "\n",
    "\n",
    "### üî• **Conclusion**\n",
    "Batch Gradient Descent is an effective optimization technique when working with **small datasets** and when **stability in training** is crucial. However, for large datasets, **Mini-Batch GD** is usually preferred as it balances speed and stability.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's implement **Batch Gradient Descent (BGD) for the Boston Housing Price Prediction** dataset using TensorFlow/Keras.  \n",
    "\n",
    "\n",
    "\n",
    "### **üìå Steps:**\n",
    "1. **Load the Boston Housing dataset** (from TensorFlow datasets).\n",
    "2. **Preprocess the data** (normalize features).\n",
    "3. **Create a neural network model** for regression.\n",
    "4. **Use Batch Gradient Descent (BGD)** for optimization.\n",
    "5. **Train the model and plot the loss curve**.\n",
    "\n",
    "\n",
    "\n",
    "## **üî• Boston Housing Price Prediction using BGD**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1Ô∏è‚É£ Load the Boston Housing dataset\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2Ô∏è‚É£ Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# 3Ô∏è‚É£ Define optimizer using **Batch Gradient Descent**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # SGD acts as BGD here\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4Ô∏è‚É£ Train model with **Batch Gradient Descent**\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train), validation_data=(X_test, y_test))\n",
    "\n",
    "# 5Ô∏è‚É£ Plot the Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **‚úÖ Why This is Batch Gradient Descent?**\n",
    "‚úî **Batch Size = Full Dataset** ‚Üí We set `batch_size=len(X_train)`, meaning the entire dataset is used in **one update per epoch**.  \n",
    "‚úî **SGD Optimizer as BGD** ‚Üí Using **SGD optimizer** but configured to work like **BGD**.  \n",
    "‚úî **Smooth Convergence** ‚Üí Updates occur after processing all training data.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Key Observations**\n",
    "- **BGD converges smoothly** but may be **slow for large datasets**.\n",
    "- **Loss decreases steadily** due to full batch updates.\n",
    "- **For larger datasets, Mini-Batch GD is more efficient**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57344/57026 [==============================] - 0s 1us/step\n",
      "65536/57026 [==================================] - 0s 1us/step\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 602.2394 - mae: 22.8352 - val_loss: 518.9860 - val_mae: 20.9726\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 491.1635 - mae: 20.2860 - val_loss: 300.3114 - val_mae: 15.4371\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 279.1856 - mae: 14.7685 - val_loss: 193.6044 - val_mae: 9.9271\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 217.8533 - mae: 10.6372 - val_loss: 535.4437 - val_mae: 21.1485\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 509.0555 - mae: 20.4958 - val_loss: 437.1647 - val_mae: 18.2467\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 413.2386 - mae: 17.5240 - val_loss: 308.1747 - val_mae: 14.8601\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 286.1104 - mae: 14.0277 - val_loss: 142.6949 - val_mae: 9.7066\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 132.8288 - mae: 8.7276 - val_loss: 96.6198 - val_mae: 8.0664\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 87.3651 - mae: 7.8127 - val_loss: 346.3520 - val_mae: 16.3121\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 328.4189 - mae: 15.8044 - val_loss: 138.6027 - val_mae: 9.8796\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 120.0115 - mae: 8.9871 - val_loss: 51.6047 - val_mae: 5.4848\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 44.9958 - mae: 5.3362 - val_loss: 185.6401 - val_mae: 11.3140\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 171.3701 - mae: 10.5991 - val_loss: 72.1754 - val_mae: 6.0167\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 46.4591 - mae: 5.4089 - val_loss: 98.6893 - val_mae: 8.6964\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 94.4614 - mae: 8.3460 - val_loss: 156.7584 - val_mae: 9.7601\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 159.1293 - mae: 9.6484 - val_loss: 399.4370 - val_mae: 17.6710\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 375.9129 - mae: 17.0070 - val_loss: 261.8440 - val_mae: 13.3443\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 241.0462 - mae: 12.3527 - val_loss: 184.3808 - val_mae: 11.2161\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 164.9553 - mae: 10.5162 - val_loss: 108.0986 - val_mae: 7.8548\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 102.6911 - mae: 7.2870 - val_loss: 76.8441 - val_mae: 6.8287\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 65.7057 - mae: 6.4672 - val_loss: 54.6895 - val_mae: 5.8077\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 54.7945 - mae: 5.5023 - val_loss: 75.8686 - val_mae: 7.1499\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 66.4045 - mae: 6.9141 - val_loss: 186.3156 - val_mae: 11.3954\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 176.2180 - mae: 10.9901 - val_loss: 65.6469 - val_mae: 6.3706\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 47.8477 - mae: 5.2945 - val_loss: 23.9890 - val_mae: 3.3580\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.1632 - mae: 2.7991 - val_loss: 22.7281 - val_mae: 3.2496\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 13.2478 - mae: 2.5005 - val_loss: 24.1463 - val_mae: 3.3036\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.0898 - mae: 2.6223 - val_loss: 24.9305 - val_mae: 3.4773\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.6608 - mae: 2.6116 - val_loss: 31.5631 - val_mae: 3.9172\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 18.1574 - mae: 3.2639 - val_loss: 42.7175 - val_mae: 4.9908\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 33.0621 - mae: 4.2268 - val_loss: 54.8142 - val_mae: 5.4569\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 36.0977 - mae: 4.9586 - val_loss: 94.3353 - val_mae: 7.7878\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 86.1292 - mae: 7.3635 - val_loss: 45.0650 - val_mae: 4.5814\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 21.9069 - mae: 3.7466 - val_loss: 33.7157 - val_mae: 4.3465\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 27.1270 - mae: 3.8031 - val_loss: 44.6761 - val_mae: 4.7759\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 29.0179 - mae: 4.3307 - val_loss: 70.8895 - val_mae: 6.5000\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 61.3261 - mae: 5.9087 - val_loss: 43.1546 - val_mae: 4.4435\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 21.0082 - mae: 3.6989 - val_loss: 37.3839 - val_mae: 4.6129\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 29.1484 - mae: 3.9817 - val_loss: 44.0072 - val_mae: 4.6959\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.0945 - mae: 4.2014 - val_loss: 63.1053 - val_mae: 6.0869\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 53.3262 - mae: 5.4594 - val_loss: 41.1606 - val_mae: 4.2734\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 19.4809 - mae: 3.5452 - val_loss: 35.6590 - val_mae: 4.4547\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.4714 - mae: 3.7569 - val_loss: 40.5680 - val_mae: 4.3874\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 23.1945 - mae: 3.8575 - val_loss: 51.9409 - val_mae: 5.4613\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 41.5094 - mae: 4.7523 - val_loss: 40.0717 - val_mae: 4.2118\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 19.5364 - mae: 3.5529 - val_loss: 38.9034 - val_mae: 4.6568\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 28.8330 - mae: 3.9343 - val_loss: 39.5589 - val_mae: 4.2752\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 21.3106 - mae: 3.6977 - val_loss: 46.1682 - val_mae: 5.1132\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 35.5233 - mae: 4.3722 - val_loss: 38.6567 - val_mae: 4.1127\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 18.9031 - mae: 3.4753 - val_loss: 38.5243 - val_mae: 4.6213\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.9311 - mae: 3.8544 - val_loss: 37.7942 - val_mae: 4.1121\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 19.3735 - mae: 3.5037 - val_loss: 41.2256 - val_mae: 4.7930\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 30.2850 - mae: 4.0074 - val_loss: 37.0970 - val_mae: 4.0166\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.2021 - mae: 3.3891 - val_loss: 37.7737 - val_mae: 4.5604\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.8217 - mae: 3.7589 - val_loss: 36.3149 - val_mae: 3.9721\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.9406 - mae: 3.3524 - val_loss: 37.8042 - val_mae: 4.5575\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 26.6727 - mae: 3.7402 - val_loss: 35.6808 - val_mae: 3.9173\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.3443 - mae: 3.2905 - val_loss: 36.3143 - val_mae: 4.4526\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 25.1354 - mae: 3.6245 - val_loss: 35.0499 - val_mae: 3.8722\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.9663 - mae: 3.2467 - val_loss: 35.7177 - val_mae: 4.4074\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 24.4692 - mae: 3.5712 - val_loss: 34.4386 - val_mae: 3.8269\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 16.5737 - mae: 3.2034 - val_loss: 34.8949 - val_mae: 4.3466\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 23.5907 - mae: 3.5019 - val_loss: 33.8742 - val_mae: 3.7815\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 16.1888 - mae: 3.1603 - val_loss: 34.1785 - val_mae: 4.2916\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 22.8490 - mae: 3.4415 - val_loss: 33.3186 - val_mae: 3.7364\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 15.8235 - mae: 3.1193 - val_loss: 33.5064 - val_mae: 4.2392\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 22.0912 - mae: 3.3806 - val_loss: 32.8358 - val_mae: 3.6949\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.4964 - mae: 3.0809 - val_loss: 32.9026 - val_mae: 4.1906\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 21.4683 - mae: 3.3289 - val_loss: 32.3503 - val_mae: 3.6557\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.1956 - mae: 3.0463 - val_loss: 32.3191 - val_mae: 4.1436\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 20.8471 - mae: 3.2779 - val_loss: 31.8946 - val_mae: 3.6195\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.9240 - mae: 3.0143 - val_loss: 31.7841 - val_mae: 4.1000\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 20.3331 - mae: 3.2337 - val_loss: 31.4280 - val_mae: 3.5821\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.6568 - mae: 2.9832 - val_loss: 31.3378 - val_mae: 4.0640\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 19.8673 - mae: 3.1945 - val_loss: 30.9893 - val_mae: 3.5499\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.4261 - mae: 2.9564 - val_loss: 30.9517 - val_mae: 4.0309\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 19.4331 - mae: 3.1564 - val_loss: 30.6460 - val_mae: 3.5151\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.1639 - mae: 2.9244 - val_loss: 30.4072 - val_mae: 3.9869\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 18.9290 - mae: 3.1118 - val_loss: 30.2210 - val_mae: 3.4868\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.9398 - mae: 2.8986 - val_loss: 30.0474 - val_mae: 3.9553\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18.5294 - mae: 3.0759 - val_loss: 29.8810 - val_mae: 3.4591\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.7075 - mae: 2.8713 - val_loss: 29.6414 - val_mae: 3.9202\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.0874 - mae: 3.0367 - val_loss: 29.6288 - val_mae: 3.4359\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.4911 - mae: 2.8461 - val_loss: 29.1974 - val_mae: 3.8838\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.7126 - mae: 3.0028 - val_loss: 29.1774 - val_mae: 3.4092\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.3062 - mae: 2.8238 - val_loss: 28.9369 - val_mae: 3.8585\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.4045 - mae: 2.9735 - val_loss: 28.8810 - val_mae: 3.3867\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.1128 - mae: 2.8008 - val_loss: 28.5958 - val_mae: 3.8291\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.0762 - mae: 2.9440 - val_loss: 28.5850 - val_mae: 3.3669\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.9173 - mae: 2.7784 - val_loss: 28.2050 - val_mae: 3.7958\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.7205 - mae: 2.9111 - val_loss: 28.2185 - val_mae: 3.3412\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.7248 - mae: 2.7548 - val_loss: 27.9284 - val_mae: 3.7692\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.4145 - mae: 2.8824 - val_loss: 27.9230 - val_mae: 3.3222\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.5537 - mae: 2.7344 - val_loss: 27.6175 - val_mae: 3.7421\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.1257 - mae: 2.8556 - val_loss: 27.7000 - val_mae: 3.3042\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.3952 - mae: 2.7150 - val_loss: 27.2532 - val_mae: 3.7108\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.8355 - mae: 2.8281 - val_loss: 27.3455 - val_mae: 3.2852\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.2432 - mae: 2.6963 - val_loss: 27.0164 - val_mae: 3.6891\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.5764 - mae: 2.8041 - val_loss: 27.1712 - val_mae: 3.2711\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.0997 - mae: 2.6788 - val_loss: 26.7366 - val_mae: 3.6636\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.3132 - mae: 2.7791 - val_loss: 26.8425 - val_mae: 3.2525\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.9475 - mae: 2.6606 - val_loss: 26.5117 - val_mae: 3.6432\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 15.1181 - mae: 2.7605 - val_loss: 26.5715 - val_mae: 3.2308\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.7535 - mae: 2.6367 - val_loss: 26.1809 - val_mae: 3.6115\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.7447 - mae: 2.7245 - val_loss: 26.2951 - val_mae: 3.2085\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.5679 - mae: 2.6132 - val_loss: 25.8931 - val_mae: 3.5854\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.4663 - mae: 2.6978 - val_loss: 26.0426 - val_mae: 3.1899\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.4176 - mae: 2.5944 - val_loss: 25.6399 - val_mae: 3.5631\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.2354 - mae: 2.6758 - val_loss: 25.7309 - val_mae: 3.1712\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.2920 - mae: 2.5789 - val_loss: 25.3967 - val_mae: 3.5417\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.0153 - mae: 2.6547 - val_loss: 25.5664 - val_mae: 3.1588\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.1870 - mae: 2.5660 - val_loss: 25.2509 - val_mae: 3.5288\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.8765 - mae: 2.6416 - val_loss: 25.3123 - val_mae: 3.1428\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.0660 - mae: 2.5513 - val_loss: 25.0206 - val_mae: 3.5086\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.6767 - mae: 2.6219 - val_loss: 25.1580 - val_mae: 3.1324\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.9620 - mae: 2.5388 - val_loss: 24.8197 - val_mae: 3.4919\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.5283 - mae: 2.6073 - val_loss: 24.8860 - val_mae: 3.1162\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.8408 - mae: 2.5243 - val_loss: 24.5854 - val_mae: 3.4716\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.3178 - mae: 2.5867 - val_loss: 24.6715 - val_mae: 3.1012\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.7272 - mae: 2.5100 - val_loss: 24.4725 - val_mae: 3.4608\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.1863 - mae: 2.5737 - val_loss: 24.5302 - val_mae: 3.0928\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.6327 - mae: 2.4997 - val_loss: 24.1959 - val_mae: 3.4390\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.0067 - mae: 2.5556 - val_loss: 24.3149 - val_mae: 3.0786\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.5299 - mae: 2.4873 - val_loss: 24.0890 - val_mae: 3.4288\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.8575 - mae: 2.5407 - val_loss: 24.0782 - val_mae: 3.0642\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.4168 - mae: 2.4728 - val_loss: 23.9308 - val_mae: 3.4151\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.6975 - mae: 2.5248 - val_loss: 23.9157 - val_mae: 3.0533\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.3197 - mae: 2.4616 - val_loss: 23.7103 - val_mae: 3.3990\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.5400 - mae: 2.5096 - val_loss: 23.7154 - val_mae: 3.0426\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.2530 - mae: 2.4541 - val_loss: 23.6760 - val_mae: 3.3967\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.4891 - mae: 2.5051 - val_loss: 23.5329 - val_mae: 3.0319\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1798 - mae: 2.4452 - val_loss: 23.5790 - val_mae: 3.3890\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.4018 - mae: 2.4972 - val_loss: 23.4133 - val_mae: 3.0271\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.1135 - mae: 2.4390 - val_loss: 23.4009 - val_mae: 3.3776\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.3113 - mae: 2.4883 - val_loss: 23.2615 - val_mae: 3.0186\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.0590 - mae: 2.4332 - val_loss: 23.3577 - val_mae: 3.3742\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.2387 - mae: 2.4821 - val_loss: 23.1224 - val_mae: 3.0094\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.9873 - mae: 2.4251 - val_loss: 23.1921 - val_mae: 3.3616\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.1190 - mae: 2.4698 - val_loss: 22.9902 - val_mae: 3.0001\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.9079 - mae: 2.4149 - val_loss: 23.0502 - val_mae: 3.3507\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.0084 - mae: 2.4590 - val_loss: 22.7493 - val_mae: 2.9865\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.8306 - mae: 2.4057 - val_loss: 22.9306 - val_mae: 3.3411\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.9010 - mae: 2.4485 - val_loss: 22.6399 - val_mae: 2.9782\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.7472 - mae: 2.3955 - val_loss: 22.7968 - val_mae: 3.3305\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.7727 - mae: 2.4351 - val_loss: 22.4889 - val_mae: 2.9700\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.6612 - mae: 2.3859 - val_loss: 22.6152 - val_mae: 3.3166\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.6187 - mae: 2.4205 - val_loss: 22.3557 - val_mae: 2.9616\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5940 - mae: 2.3772 - val_loss: 22.5186 - val_mae: 3.3109\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.5370 - mae: 2.4126 - val_loss: 22.1791 - val_mae: 2.9513\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5082 - mae: 2.3664 - val_loss: 22.4198 - val_mae: 3.3019\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.4094 - mae: 2.3996 - val_loss: 22.0149 - val_mae: 2.9414\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.4247 - mae: 2.3562 - val_loss: 22.2343 - val_mae: 3.2881\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.2778 - mae: 2.3860 - val_loss: 21.7757 - val_mae: 2.9260\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.3363 - mae: 2.3437 - val_loss: 22.0898 - val_mae: 3.2766\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.1292 - mae: 2.3706 - val_loss: 21.6398 - val_mae: 2.9158\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.2532 - mae: 2.3331 - val_loss: 21.9670 - val_mae: 3.2678\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.0142 - mae: 2.3584 - val_loss: 21.5233 - val_mae: 2.9079\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.1808 - mae: 2.3241 - val_loss: 21.8297 - val_mae: 3.2581\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.9297 - mae: 2.3497 - val_loss: 21.3906 - val_mae: 2.8999\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.1203 - mae: 2.3170 - val_loss: 21.7262 - val_mae: 3.2507\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.8372 - mae: 2.3401 - val_loss: 21.3141 - val_mae: 2.8948\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.0622 - mae: 2.3108 - val_loss: 21.6549 - val_mae: 3.2468\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.7723 - mae: 2.3327 - val_loss: 21.2462 - val_mae: 2.8909\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.0122 - mae: 2.3053 - val_loss: 21.5517 - val_mae: 3.2393\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6863 - mae: 2.3234 - val_loss: 21.1018 - val_mae: 2.8819\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.9530 - mae: 2.2977 - val_loss: 21.4241 - val_mae: 3.2298\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.6125 - mae: 2.3155 - val_loss: 21.0043 - val_mae: 2.8751\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.8933 - mae: 2.2899 - val_loss: 21.3577 - val_mae: 3.2244\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.5244 - mae: 2.3059 - val_loss: 20.8550 - val_mae: 2.8653\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.8238 - mae: 2.2806 - val_loss: 21.2321 - val_mae: 3.2166\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.4348 - mae: 2.2959 - val_loss: 20.7896 - val_mae: 2.8625\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.7775 - mae: 2.2755 - val_loss: 21.1104 - val_mae: 3.2078\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.3613 - mae: 2.2887 - val_loss: 20.7029 - val_mae: 2.8559\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.7249 - mae: 2.2688 - val_loss: 21.0379 - val_mae: 3.2033\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.2930 - mae: 2.2812 - val_loss: 20.6053 - val_mae: 2.8532\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.6717 - mae: 2.2628 - val_loss: 20.9196 - val_mae: 3.1941\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.2065 - mae: 2.2719 - val_loss: 20.5545 - val_mae: 2.8477\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.6256 - mae: 2.2568 - val_loss: 20.8364 - val_mae: 3.1872\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.1425 - mae: 2.2648 - val_loss: 20.4973 - val_mae: 2.8454\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.5744 - mae: 2.2507 - val_loss: 20.7291 - val_mae: 3.1792\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.0430 - mae: 2.2539 - val_loss: 20.3326 - val_mae: 2.8342\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.5133 - mae: 2.2422 - val_loss: 20.6380 - val_mae: 3.1729\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.9706 - mae: 2.2460 - val_loss: 20.2471 - val_mae: 2.8246\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.4429 - mae: 2.2326 - val_loss: 20.5506 - val_mae: 3.1647\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.8712 - mae: 2.2335 - val_loss: 20.1170 - val_mae: 2.8178\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.3800 - mae: 2.2243 - val_loss: 20.4137 - val_mae: 3.1536\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.7738 - mae: 2.2222 - val_loss: 20.0575 - val_mae: 2.8132\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.3289 - mae: 2.2187 - val_loss: 20.3316 - val_mae: 3.1464\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.7132 - mae: 2.2156 - val_loss: 19.9985 - val_mae: 2.8081\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.2861 - mae: 2.2130 - val_loss: 20.2232 - val_mae: 3.1387\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.6360 - mae: 2.2072 - val_loss: 19.9154 - val_mae: 2.8024\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.2360 - mae: 2.2059 - val_loss: 20.1500 - val_mae: 3.1324\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.5501 - mae: 2.1978 - val_loss: 19.7596 - val_mae: 2.7914\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.1820 - mae: 2.1979 - val_loss: 20.0339 - val_mae: 3.1256\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.4823 - mae: 2.1901 - val_loss: 19.7171 - val_mae: 2.7864\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1362 - mae: 2.1927 - val_loss: 19.9859 - val_mae: 3.1209\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.4213 - mae: 2.1827 - val_loss: 19.6558 - val_mae: 2.7814\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.1008 - mae: 2.1886 - val_loss: 19.8936 - val_mae: 3.1163\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.3562 - mae: 2.1758 - val_loss: 19.4645 - val_mae: 2.7698\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.0410 - mae: 2.1797 - val_loss: 19.8197 - val_mae: 3.1106\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1Ô∏è‚É£ Load the Boston Housing dataset\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 2Ô∏è‚É£ Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# 3Ô∏è‚É£ Define optimizer using **Batch Gradient Descent**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)  # SGD acts as BGD here\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4Ô∏è‚É£ Train model with **Batch Gradient Descent**\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=len(X_train), validation_data=(X_test, y_test))\n",
    "\n",
    "# 5Ô∏è‚É£ Plot the Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üî• Stochastic Gradient Descent (SGD) - Full Explanation üî•**  \n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm in **machine learning and deep learning** used to minimize the loss function and update model parameters. **Stochastic Gradient Descent (SGD)** is a popular variant of gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ What is Stochastic Gradient Descent (SGD)?**\n",
    "SGD is an optimization technique where **one random training sample** is used to update the model‚Äôs weights at each iteration.\n",
    "\n",
    "üìå **Key Idea:**  \n",
    "Unlike **Batch Gradient Descent (BGD)**, which computes gradients using the **entire dataset**, SGD updates the model **one sample at a time**.\n",
    "\n",
    "üí° **Formula for Weight Update in SGD**:\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla L(w)\n",
    "$$\n",
    "Where:  \n",
    "üîπ $ w $ ‚Üí Model parameters (weights & biases)  \n",
    "üîπ $ \\eta $ ‚Üí Learning rate (step size)  \n",
    "üîπ $ \\nabla L(w) $ ‚Üí Gradient of the loss function  \n",
    "\n",
    "\n",
    "\n",
    "## **üõ†Ô∏è How Does SGD Work?**\n",
    "1Ô∏è‚É£ **Randomly pick a single training sample** from the dataset.  \n",
    "2Ô∏è‚É£ **Compute the gradient** of the loss function using only this sample.  \n",
    "3Ô∏è‚É£ **Update the model parameters** immediately.  \n",
    "4Ô∏è‚É£ **Repeat for every sample**, then move to the next epoch.  \n",
    "\n",
    "\n",
    "### **üö¶ Example of SGD vs BGD**\n",
    "| Feature             | Batch Gradient Descent (BGD) | Stochastic Gradient Descent (SGD) |\n",
    "|---------------------|----------------------------|----------------------------|\n",
    "| Update Frequency   | After **full dataset**     | After **each sample**     |\n",
    "| Convergence Speed  | Slower                      | Faster                     |\n",
    "| Memory Usage       | High                        | Low                         |\n",
    "| Stability          | Stable                      | Noisy updates              |\n",
    "\n",
    "\n",
    "\n",
    "## **üõ†Ô∏è Implementing SGD in Deep Learning (Boston Housing Example)**  \n",
    "Let's implement SGD in a **neural network** using TensorFlow and the **Boston Housing dataset**.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Stochastic Gradient Descent (SGD)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **SGD** (batch_size = 1 means stochastic updates)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"SGD - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **‚úÖ Observations**\n",
    "‚úî **SGD updates model parameters more frequently** ‚Üí Faster convergence.  \n",
    "‚úî **Loss function is noisy** because updates happen after each sample.  \n",
    "‚úî **Good for large datasets** ‚Üí Less memory usage compared to BGD.  \n",
    "‚úî **Can get stuck in local minima** due to randomness.\n",
    "\n",
    "\n",
    "\n",
    "## **üéØ When to Use SGD?**\n",
    "‚úÖ When the dataset is **large** and full-batch updates are slow.  \n",
    "‚úÖ When memory is limited, as only **one sample is used** at a time.  \n",
    "‚úÖ When you want faster updates and **real-time learning**.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Conclusion**\n",
    "SGD is a powerful optimization algorithm that helps models learn **faster and efficiently**. However, its **random updates** can cause **fluctuations**. Using techniques like **Momentum, RMSProp, and Adam** can help stabilize learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "404/404 [==============================] - 1s 837us/step - loss: 0.0977 - mae: 0.2313 - val_loss: 0.0473 - val_mae: 0.1715\n",
      "Epoch 2/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0272 - mae: 0.1292 - val_loss: 0.0328 - val_mae: 0.1342\n",
      "Epoch 3/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0184 - mae: 0.1053 - val_loss: 0.0283 - val_mae: 0.1214\n",
      "Epoch 4/100\n",
      "404/404 [==============================] - 1s 1ms/step - loss: 0.0148 - mae: 0.0946 - val_loss: 0.0259 - val_mae: 0.1146\n",
      "Epoch 5/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0128 - mae: 0.0887 - val_loss: 0.0238 - val_mae: 0.1110\n",
      "Epoch 6/100\n",
      "404/404 [==============================] - 0s 979us/step - loss: 0.0113 - mae: 0.0845 - val_loss: 0.0209 - val_mae: 0.1036\n",
      "Epoch 7/100\n",
      "404/404 [==============================] - 0s 854us/step - loss: 0.0105 - mae: 0.0809 - val_loss: 0.0214 - val_mae: 0.1032\n",
      "Epoch 8/100\n",
      "404/404 [==============================] - 0s 906us/step - loss: 0.0097 - mae: 0.0776 - val_loss: 0.0196 - val_mae: 0.0983\n",
      "Epoch 9/100\n",
      "404/404 [==============================] - 0s 865us/step - loss: 0.0091 - mae: 0.0753 - val_loss: 0.0204 - val_mae: 0.1004\n",
      "Epoch 10/100\n",
      "404/404 [==============================] - 0s 771us/step - loss: 0.0086 - mae: 0.0730 - val_loss: 0.0188 - val_mae: 0.0960\n",
      "Epoch 11/100\n",
      "404/404 [==============================] - 0s 860us/step - loss: 0.0081 - mae: 0.0709 - val_loss: 0.0176 - val_mae: 0.0923\n",
      "Epoch 12/100\n",
      "404/404 [==============================] - 0s 1ms/step - loss: 0.0078 - mae: 0.0691 - val_loss: 0.0176 - val_mae: 0.0920\n",
      "Epoch 13/100\n",
      "404/404 [==============================] - 0s 987us/step - loss: 0.0074 - mae: 0.0669 - val_loss: 0.0181 - val_mae: 0.0928\n",
      "Epoch 14/100\n",
      "404/404 [==============================] - 0s 857us/step - loss: 0.0071 - mae: 0.0657 - val_loss: 0.0175 - val_mae: 0.0915\n",
      "Epoch 15/100\n",
      "404/404 [==============================] - 0s 681us/step - loss: 0.0068 - mae: 0.0641 - val_loss: 0.0161 - val_mae: 0.0871\n",
      "Epoch 16/100\n",
      "404/404 [==============================] - 0s 615us/step - loss: 0.0066 - mae: 0.0628 - val_loss: 0.0164 - val_mae: 0.0876\n",
      "Epoch 17/100\n",
      "404/404 [==============================] - 0s 625us/step - loss: 0.0064 - mae: 0.0616 - val_loss: 0.0165 - val_mae: 0.0877\n",
      "Epoch 18/100\n",
      "404/404 [==============================] - 0s 633us/step - loss: 0.0062 - mae: 0.0605 - val_loss: 0.0161 - val_mae: 0.0865\n",
      "Epoch 19/100\n",
      "404/404 [==============================] - 0s 654us/step - loss: 0.0060 - mae: 0.0594 - val_loss: 0.0159 - val_mae: 0.0855\n",
      "Epoch 20/100\n",
      "404/404 [==============================] - 0s 543us/step - loss: 0.0058 - mae: 0.0584 - val_loss: 0.0154 - val_mae: 0.0843\n",
      "Epoch 21/100\n",
      "404/404 [==============================] - 0s 660us/step - loss: 0.0056 - mae: 0.0575 - val_loss: 0.0154 - val_mae: 0.0841\n",
      "Epoch 22/100\n",
      "404/404 [==============================] - 0s 639us/step - loss: 0.0055 - mae: 0.0564 - val_loss: 0.0147 - val_mae: 0.0823\n",
      "Epoch 23/100\n",
      "404/404 [==============================] - 0s 678us/step - loss: 0.0054 - mae: 0.0556 - val_loss: 0.0150 - val_mae: 0.0829\n",
      "Epoch 24/100\n",
      "404/404 [==============================] - 0s 713us/step - loss: 0.0053 - mae: 0.0551 - val_loss: 0.0147 - val_mae: 0.0815\n",
      "Epoch 25/100\n",
      "404/404 [==============================] - 0s 562us/step - loss: 0.0051 - mae: 0.0542 - val_loss: 0.0142 - val_mae: 0.0798\n",
      "Epoch 26/100\n",
      "404/404 [==============================] - 0s 627us/step - loss: 0.0050 - mae: 0.0536 - val_loss: 0.0142 - val_mae: 0.0795\n",
      "Epoch 27/100\n",
      "404/404 [==============================] - 0s 849us/step - loss: 0.0049 - mae: 0.0528 - val_loss: 0.0141 - val_mae: 0.0794\n",
      "Epoch 28/100\n",
      "404/404 [==============================] - 0s 650us/step - loss: 0.0048 - mae: 0.0524 - val_loss: 0.0140 - val_mae: 0.0790\n",
      "Epoch 29/100\n",
      "404/404 [==============================] - 0s 634us/step - loss: 0.0047 - mae: 0.0514 - val_loss: 0.0145 - val_mae: 0.0810\n",
      "Epoch 30/100\n",
      "404/404 [==============================] - 0s 571us/step - loss: 0.0047 - mae: 0.0514 - val_loss: 0.0138 - val_mae: 0.0784\n",
      "Epoch 31/100\n",
      "404/404 [==============================] - 0s 557us/step - loss: 0.0045 - mae: 0.0510 - val_loss: 0.0139 - val_mae: 0.0786\n",
      "Epoch 32/100\n",
      "404/404 [==============================] - 0s 666us/step - loss: 0.0045 - mae: 0.0500 - val_loss: 0.0132 - val_mae: 0.0762\n",
      "Epoch 33/100\n",
      "404/404 [==============================] - 0s 552us/step - loss: 0.0044 - mae: 0.0497 - val_loss: 0.0130 - val_mae: 0.0753\n",
      "Epoch 34/100\n",
      "404/404 [==============================] - 0s 677us/step - loss: 0.0043 - mae: 0.0489 - val_loss: 0.0130 - val_mae: 0.0754\n",
      "Epoch 35/100\n",
      "404/404 [==============================] - 0s 566us/step - loss: 0.0043 - mae: 0.0489 - val_loss: 0.0132 - val_mae: 0.0762\n",
      "Epoch 36/100\n",
      "404/404 [==============================] - 0s 636us/step - loss: 0.0042 - mae: 0.0480 - val_loss: 0.0132 - val_mae: 0.0763\n",
      "Epoch 37/100\n",
      "404/404 [==============================] - 0s 564us/step - loss: 0.0041 - mae: 0.0476 - val_loss: 0.0131 - val_mae: 0.0761\n",
      "Epoch 38/100\n",
      "404/404 [==============================] - 0s 555us/step - loss: 0.0041 - mae: 0.0474 - val_loss: 0.0133 - val_mae: 0.0770\n",
      "Epoch 39/100\n",
      "404/404 [==============================] - 0s 633us/step - loss: 0.0040 - mae: 0.0471 - val_loss: 0.0129 - val_mae: 0.0754\n",
      "Epoch 40/100\n",
      "404/404 [==============================] - 0s 625us/step - loss: 0.0040 - mae: 0.0464 - val_loss: 0.0127 - val_mae: 0.0745\n",
      "Epoch 41/100\n",
      "404/404 [==============================] - 0s 570us/step - loss: 0.0039 - mae: 0.0462 - val_loss: 0.0127 - val_mae: 0.0749\n",
      "Epoch 42/100\n",
      "404/404 [==============================] - 0s 635us/step - loss: 0.0039 - mae: 0.0457 - val_loss: 0.0123 - val_mae: 0.0734\n",
      "Epoch 43/100\n",
      "404/404 [==============================] - 0s 621us/step - loss: 0.0038 - mae: 0.0456 - val_loss: 0.0120 - val_mae: 0.0720\n",
      "Epoch 44/100\n",
      "404/404 [==============================] - 0s 591us/step - loss: 0.0038 - mae: 0.0451 - val_loss: 0.0125 - val_mae: 0.0738\n",
      "Epoch 45/100\n",
      "404/404 [==============================] - 0s 679us/step - loss: 0.0038 - mae: 0.0450 - val_loss: 0.0123 - val_mae: 0.0733\n",
      "Epoch 46/100\n",
      "404/404 [==============================] - 0s 589us/step - loss: 0.0037 - mae: 0.0446 - val_loss: 0.0122 - val_mae: 0.0729\n",
      "Epoch 47/100\n",
      "404/404 [==============================] - 0s 672us/step - loss: 0.0037 - mae: 0.0440 - val_loss: 0.0122 - val_mae: 0.0735\n",
      "Epoch 48/100\n",
      "404/404 [==============================] - 0s 696us/step - loss: 0.0036 - mae: 0.0440 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 49/100\n",
      "404/404 [==============================] - 0s 688us/step - loss: 0.0036 - mae: 0.0437 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 50/100\n",
      "404/404 [==============================] - 0s 664us/step - loss: 0.0035 - mae: 0.0434 - val_loss: 0.0122 - val_mae: 0.0734\n",
      "Epoch 51/100\n",
      "404/404 [==============================] - 0s 533us/step - loss: 0.0035 - mae: 0.0432 - val_loss: 0.0118 - val_mae: 0.0717\n",
      "Epoch 52/100\n",
      "404/404 [==============================] - 0s 582us/step - loss: 0.0035 - mae: 0.0430 - val_loss: 0.0119 - val_mae: 0.0720\n",
      "Epoch 53/100\n",
      "404/404 [==============================] - 0s 561us/step - loss: 0.0034 - mae: 0.0433 - val_loss: 0.0116 - val_mae: 0.0711\n",
      "Epoch 54/100\n",
      "404/404 [==============================] - 0s 744us/step - loss: 0.0034 - mae: 0.0423 - val_loss: 0.0118 - val_mae: 0.0718\n",
      "Epoch 55/100\n",
      "404/404 [==============================] - 0s 666us/step - loss: 0.0034 - mae: 0.0422 - val_loss: 0.0115 - val_mae: 0.0704\n",
      "Epoch 56/100\n",
      "404/404 [==============================] - 0s 635us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0117 - val_mae: 0.0710\n",
      "Epoch 57/100\n",
      "404/404 [==============================] - 0s 663us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0111 - val_mae: 0.0688\n",
      "Epoch 58/100\n",
      "404/404 [==============================] - 0s 590us/step - loss: 0.0033 - mae: 0.0417 - val_loss: 0.0112 - val_mae: 0.0699\n",
      "Epoch 59/100\n",
      "404/404 [==============================] - 0s 610us/step - loss: 0.0033 - mae: 0.0414 - val_loss: 0.0116 - val_mae: 0.0707\n",
      "Epoch 60/100\n",
      "404/404 [==============================] - 0s 558us/step - loss: 0.0032 - mae: 0.0416 - val_loss: 0.0112 - val_mae: 0.0699\n",
      "Epoch 61/100\n",
      "404/404 [==============================] - 0s 619us/step - loss: 0.0032 - mae: 0.0412 - val_loss: 0.0112 - val_mae: 0.0691\n",
      "Epoch 62/100\n",
      "404/404 [==============================] - 0s 587us/step - loss: 0.0032 - mae: 0.0414 - val_loss: 0.0109 - val_mae: 0.0684\n",
      "Epoch 63/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0032 - mae: 0.0406 - val_loss: 0.0111 - val_mae: 0.0692\n",
      "Epoch 64/100\n",
      "404/404 [==============================] - 0s 568us/step - loss: 0.0032 - mae: 0.0403 - val_loss: 0.0111 - val_mae: 0.0695\n",
      "Epoch 65/100\n",
      "404/404 [==============================] - 0s 583us/step - loss: 0.0031 - mae: 0.0402 - val_loss: 0.0108 - val_mae: 0.0681\n",
      "Epoch 66/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0031 - mae: 0.0400 - val_loss: 0.0111 - val_mae: 0.0691\n",
      "Epoch 67/100\n",
      "404/404 [==============================] - 0s 616us/step - loss: 0.0031 - mae: 0.0405 - val_loss: 0.0110 - val_mae: 0.0690\n",
      "Epoch 68/100\n",
      "404/404 [==============================] - 0s 665us/step - loss: 0.0030 - mae: 0.0398 - val_loss: 0.0109 - val_mae: 0.0688\n",
      "Epoch 69/100\n",
      "404/404 [==============================] - 0s 651us/step - loss: 0.0030 - mae: 0.0399 - val_loss: 0.0108 - val_mae: 0.0686\n",
      "Epoch 70/100\n",
      "404/404 [==============================] - 0s 536us/step - loss: 0.0030 - mae: 0.0396 - val_loss: 0.0107 - val_mae: 0.0679\n",
      "Epoch 71/100\n",
      "404/404 [==============================] - 0s 592us/step - loss: 0.0030 - mae: 0.0396 - val_loss: 0.0108 - val_mae: 0.0684\n",
      "Epoch 72/100\n",
      "404/404 [==============================] - 0s 542us/step - loss: 0.0030 - mae: 0.0395 - val_loss: 0.0105 - val_mae: 0.0673\n",
      "Epoch 73/100\n",
      "404/404 [==============================] - 0s 608us/step - loss: 0.0030 - mae: 0.0392 - val_loss: 0.0108 - val_mae: 0.0684\n",
      "Epoch 74/100\n",
      "404/404 [==============================] - 0s 563us/step - loss: 0.0030 - mae: 0.0390 - val_loss: 0.0105 - val_mae: 0.0673\n",
      "Epoch 75/100\n",
      "404/404 [==============================] - 0s 573us/step - loss: 0.0029 - mae: 0.0389 - val_loss: 0.0106 - val_mae: 0.0675\n",
      "Epoch 76/100\n",
      "404/404 [==============================] - 0s 551us/step - loss: 0.0029 - mae: 0.0389 - val_loss: 0.0103 - val_mae: 0.0666\n",
      "Epoch 77/100\n",
      "404/404 [==============================] - 0s 566us/step - loss: 0.0029 - mae: 0.0387 - val_loss: 0.0104 - val_mae: 0.0669\n",
      "Epoch 78/100\n",
      "404/404 [==============================] - 0s 571us/step - loss: 0.0029 - mae: 0.0384 - val_loss: 0.0103 - val_mae: 0.0663\n",
      "Epoch 79/100\n",
      "404/404 [==============================] - 0s 672us/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0106 - val_mae: 0.0676\n",
      "Epoch 80/100\n",
      "404/404 [==============================] - 0s 791us/step - loss: 0.0029 - mae: 0.0381 - val_loss: 0.0104 - val_mae: 0.0668\n",
      "Epoch 81/100\n",
      "404/404 [==============================] - 0s 541us/step - loss: 0.0028 - mae: 0.0381 - val_loss: 0.0105 - val_mae: 0.0675\n",
      "Epoch 82/100\n",
      "404/404 [==============================] - 0s 549us/step - loss: 0.0028 - mae: 0.0379 - val_loss: 0.0105 - val_mae: 0.0677\n",
      "Epoch 83/100\n",
      "404/404 [==============================] - 0s 656us/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0104 - val_mae: 0.0671\n",
      "Epoch 84/100\n",
      "404/404 [==============================] - 0s 648us/step - loss: 0.0028 - mae: 0.0378 - val_loss: 0.0103 - val_mae: 0.0666\n",
      "Epoch 85/100\n",
      "404/404 [==============================] - 0s 586us/step - loss: 0.0028 - mae: 0.0376 - val_loss: 0.0104 - val_mae: 0.0670\n",
      "Epoch 86/100\n",
      "404/404 [==============================] - 0s 631us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0107 - val_mae: 0.0682\n",
      "Epoch 87/100\n",
      "404/404 [==============================] - 0s 568us/step - loss: 0.0027 - mae: 0.0374 - val_loss: 0.0105 - val_mae: 0.0672\n",
      "Epoch 88/100\n",
      "404/404 [==============================] - 0s 583us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0102 - val_mae: 0.0659\n",
      "Epoch 89/100\n",
      "404/404 [==============================] - 0s 651us/step - loss: 0.0027 - mae: 0.0375 - val_loss: 0.0101 - val_mae: 0.0658\n",
      "Epoch 90/100\n",
      "404/404 [==============================] - 0s 559us/step - loss: 0.0027 - mae: 0.0372 - val_loss: 0.0100 - val_mae: 0.0655\n",
      "Epoch 91/100\n",
      "404/404 [==============================] - 0s 634us/step - loss: 0.0027 - mae: 0.0371 - val_loss: 0.0104 - val_mae: 0.0672\n",
      "Epoch 92/100\n",
      "404/404 [==============================] - 0s 556us/step - loss: 0.0026 - mae: 0.0362 - val_loss: 0.0105 - val_mae: 0.0672\n",
      "Epoch 93/100\n",
      "404/404 [==============================] - 0s 581us/step - loss: 0.0027 - mae: 0.0371 - val_loss: 0.0106 - val_mae: 0.0675\n",
      "Epoch 94/100\n",
      "404/404 [==============================] - 0s 591us/step - loss: 0.0026 - mae: 0.0369 - val_loss: 0.0103 - val_mae: 0.0669\n",
      "Epoch 95/100\n",
      "404/404 [==============================] - 0s 564us/step - loss: 0.0026 - mae: 0.0367 - val_loss: 0.0099 - val_mae: 0.0653\n",
      "Epoch 96/100\n",
      "404/404 [==============================] - 0s 589us/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0101 - val_mae: 0.0661\n",
      "Epoch 97/100\n",
      "404/404 [==============================] - 0s 572us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0101 - val_mae: 0.0663\n",
      "Epoch 98/100\n",
      "404/404 [==============================] - 0s 687us/step - loss: 0.0026 - mae: 0.0365 - val_loss: 0.0099 - val_mae: 0.0652\n",
      "Epoch 99/100\n",
      "404/404 [==============================] - 0s 614us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0102 - val_mae: 0.0663\n",
      "Epoch 100/100\n",
      "404/404 [==============================] - 0s 639us/step - loss: 0.0026 - mae: 0.0361 - val_loss: 0.0101 - val_mae: 0.0658\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA3hElEQVR4nO3deXyU5b3//9dnluwhQBLWgCyyCLIawbVCtVarlVZB5dBWrNXqr62t57Rq++1RavW09uepltYuWrejVrS25YsVpXX3aKuAIopAZVPCmgQSspBkls/3j+ueZDJMIEAmQ5LP8+E8MnOv1z2D857ruu77ukVVMcYYYxL50l0AY4wxxyYLCGOMMUlZQBhjjEnKAsIYY0xSFhDGGGOSsoAwxhiTlAWEMZ1IRIaJiIpIIAXbXiMiMzp6u962vy4i9xximTNFZH0K9r1ARB7znvcXkbUiktnR+zEHsoAwBxCRM0TkTRGpFpE9IvKGiJwcN3+giNwvIttFpFZENonIwyIy1psf+xKs9R67ROSvIvKZoyzXKyLytaM9viPY7wIRCXnHUuW9N6d2djkSyvSwiNweP01Vx6vqKynYVwbwQ+D/914nfr61IvKeqr6uqmM6ev/xVHUX8DJwTSr3YxwLCNOKiPQC/gr8EugLDAZ+BDR68wuBN4Ec4EwgH5gKvAokBkBvVc0DJgF/B/4iIvNTfxQp8aR3LEW4L6g/prk8nWkWsE5VtyVM762qed5jUieW53Hg6524vx7LAsIkGg2gqk+oakRV96vq31R1tTf/BmAf8GVV3ahOlao+pKq/TLZBVd2pqr8AFgB3ikiH/rsTEZ+I/FBEPhaR3SLyPyJS4M3LEpHHRKTS+/W/XET6e/Pme7WfGhHZLCLzDrUvVQ3jvqAGi0ixt50CEXlARHaIyDYRuV1E/N48v4jcJSIVIrIJuCCh7FtE5Jy4183NKd7rWG2uSkS2emW+BpgH3Oj9en8mcVsikiki93i1vO3e80xv3gwRKROR//Derx0icuVBDvt83A+AQ30OM0SkzHs+0qt9TvVeDxKR8lgTmIicEndc78U3jYnIcBF51ftc/o4L5XhvASNE5LhDlckcHQsIk+hfQEREHhGR80WkT8L8c4C/qGr0CLb9Z6Af0NHNEPO9x0xgBJAH/MqbdwVQAAwBCoFrgf0ikgssBM5X1XzgNGDVoXbkNbd8BagE9nqTHwbCwPHAFOBcINYUdjVwoTe9FJjd3oPyvgCfw9XmioHJwCpVvQ8XUj/zfr1/Psnq/wc4xVtnEjAN10wUMwD3vgwGrgLuTfJZx0wADqtvQVU3AjcBj4lIDvAQ8IiqviIig4FngdtxtdTvAn+KBS7wB2AlLhh+jPsM47cdBjZ4x2VSyALCtKKq+4AzAAXuB8pFZEnsVzfuf9qdseVF5CLvV2CNiPztEJvf7v3t28HFngf8XFU3qWot8H3gcnEdwSFcMBzv1YhWescIEAVOFJFsVd2hqmsOso9LRaQK2I/70p+tqmHvffkc8B1VrVPV3cDdwOWx9YB7VHWrqu4BfnIYx/VvwAtebS6kqpWquqqd684DblPV3apajmsm/HLc/JA3P6SqS4Fa2g7u3kBNkukV3mdfJSLfTZypqvfjvsjfAgbiQgvgS8BSVV2qqlFV/TuwAviciAwFTgb+U1UbVfU14Jkk+67xymVSyALCHEBV16rqfFUtAU4EBgH3eLMrcf+zx5Zdoqq9cU1PGYfY9GDv757EGSLyg7gOz98eZpEHAR/Hvf4YCAD9gUeBZcAir6nlZyISVNU64DJcjWKHiDwrXid7G57yjrM/8AFwkjf9OCDobaPKC5Hf4WpKsbJtTShbew0BNh7G8vGSvSeD4l5Xer/EY+pxNa9k9uL6mhIVqWpv73FXG+vej/s39EtVbfSmHQfMiQuXKtyPkoFeGfd6n0982RPlA1Vt7NN0EAsIc1Cqug7XhHKiN+lF4AtH2I/wRWA3SZorVPW/4jo8rz3M7W7HfenEDMU1+ezyfiH/SFXH4ZqRLsQ1EaGqy1T1M7gvpnW4L7ODUtUK3Bk0C0RkIO7Lv5HWX5a9VHW8t8oO3Bd9fNni1eE6/GMGxD3fCoxsqyiHKGqy92R7G8seymq8vqnDISJ5uB8WD+Der1jNcSvwaNz71VtVc1X1p7j3q4/XBBhf9vjtBnDNee8d/qGYw2EBYVoRkbFe52WJ93oIMBf4p7fIz4E+wKNeR6SISD6urbutbfYXkW8CtwLfP8L+i5iA1/EcewSBJ4AbvM7NPOC/cGcdhUVkpohM8DqN9+GaVqJemWZ5X0SNuCaWdpVLVdfjaiU3quoO4G/Af4tIL3Ed5iNF5Cxv8aeA60WkxGvjvzlhc6twzWFBEUnso3gcOEdELhWRgIgUishkb94uXH9LW54AfigixSJSBNwCPHaQ5Q9mKXDWIZc60C+AFar6NVyfQ6xm+BjweRH5rLhO/Cyvg7tEVT/GNTf9SEQyROQMILGPZRqwxVvWpJKq2sMezQ9cM9BTwDbcr9ttuCaTXnHLDML9KtyB+2LdCDwCnODNH4b7hVvrbWM37kvmvKMs2yveduMfj+F+6NyC+2Va7k3r460zF1djqcN9qS7ENT8NxJ2ZU41rqngFGNfGfhcAjyVMm+5tsx+us/c3QJm3vXeBy73lArg+iUpgM/ANr9wBb/4IXBt9Le5LdGH8vnCnEr+FC7etwBXe9FG4cKkCFnvTtgDneM+zvG3t8B4LgSxv3gygLOF4mtdNcvxB4BNgUMLnG0hYrnm7uFNjtwF9vdd5uP6IeXHv36u45sZy79iHxr0nr3vvyd9xJxzEvyf3Aten+/+VnvAQ7w03xpg2iTu1dpyqfifN5eiHC5YpqtqQzrL0BBYQxhhjkrI+CGOMMUlZQBhjjEnKAsIYY0xSHT7kcLoUFRXpsGHD0l0MY4zpUlauXFmhqsXJ5nWbgBg2bBgrVqxIdzGMMaZLEZE2rydJaROTiJwnIutFZIOIJF4ghIh8SkTeEZGwiMxOmHeFiHzkPa5IXNcYY0xqpSwgvCtX78UNFTwOmCsi4xIW+wQ3CucfEtbti7vqdjruqslbDzLSpDHGmBRIZQ1iGrBB3QibTcAi3NWVzVR1i7r7DCQOcfBZ4O+qukdV9+KupjwvhWU1xhiTIJV9EINpPYplGa5GcKTrDk5cyLu68xqAoUMTx0AzxqRKKBSirKyMhga7mLmryMrKoqSkhGAw2O51unQntbobp9wHUFpaapeEG9NJysrKyM/PZ9iwYYhIuotjDkFVqayspKysjOHDh7d7vVQ2MW2j9TDHJd60VK9rjEmxhoYGCgsLLRy6CBGhsLDwsGt8qQyI5cAobwjmDNwdtpa0c91lwLki0sfrnD7Xm2aMOUZYOHQtR/J5pSwg1N2t6pu4L/a1uDtyrRGR20TkIgAROVncTc7nAL8TkTXeuntw96Jd7j1u86Z1uB3V+/nvv61nU3ltKjZvjDFdVkqvg1B3z9nRqjpSVe/wpt2iqku858tVtUTd3aQKteUuXKjqg6p6vPd4KFVlLK9p5JcvbWBzRd2hFzbGpF1lZSWTJ09m8uTJDBgwgMGDBze/bmpqOui6K1as4Prrrz/kPk477bQOKesrr7zChRde2CHbSocu3UndEQI+l5GhyNHc5MwY01kKCwtZtWoVAAsWLCAvL4/vfve7zfPD4TCBQPKvttLSUkpLSw+5jzfffLNDytrV9fjB+oJ+1y4XithJUMZ0VfPnz+faa69l+vTp3Hjjjbz99tuceuqpTJkyhdNOO431691t0ON/0S9YsICvfvWrzJgxgxEjRrBw4cLm7eXl5TUvP2PGDGbPns3YsWOZN29e7K52LF26lLFjx3LSSSdx/fXXH1ZN4YknnmDChAmceOKJ3HTTTQBEIhHmz5/PiSeeyIQJE7j77rsBWLhwIePGjWPixIlcfvnlR/9mHYYeX4MI+l1GhqNWgzDmSPzomTV8uH1fh25z3KBe3Pr58YdeME5ZWRlvvvkmfr+fffv28frrrxMIBHjhhRf4wQ9+wJ/+9KcD1lm3bh0vv/wyNTU1jBkzhuuuu+6A6wTeffdd1qxZw6BBgzj99NN54403KC0t5etf/zqvvfYaw4cPZ+7cue0u5/bt27nppptYuXIlffr04dxzz2Xx4sUMGTKEbdu28cEHHwBQVVUFwE9/+lM2b95MZmZm87TO0uNrEIFYDSJsNQhjurI5c+bg9/sBqK6uZs6cOZx44onccMMNrFmzJuk6F1xwAZmZmRQVFdGvXz927dp1wDLTpk2jpKQEn8/H5MmT2bJlC+vWrWPEiBHN1xQcTkAsX76cGTNmUFxcTCAQYN68ebz22muMGDGCTZs28a1vfYvnn3+eXr16ATBx4kTmzZvHY4891mbTWapYDcKrQYSsBmHMETncX/qpkpub2/z8P//zP5k5cyZ/+ctf2LJlCzNmzEi6TmZmZvNzv99POBw+omU6Qp8+fXjvvfdYtmwZv/3tb3nqqad48MEHefbZZ3nttdd45plnuOOOO3j//fc7LSh6fA2iOSDCFhDGdBfV1dUMHuxG53n44Yc7fPtjxoxh06ZNbNmyBYAnn3yy3etOmzaNV199lYqKCiKRCE888QRnnXUWFRUVRKNRLrnkEm6//XbeeecdotEoW7duZebMmdx5551UV1dTW9t5p+T3+BpErIkpHLUmJmO6ixtvvJErrriC22+/nQsuuKDDt5+dnc2vf/1rzjvvPHJzczn55JPbXPbFF1+kpKSk+fUf//hHfvrTnzJz5kxUlQsuuIBZs2bx3nvvceWVVxL1WjN+8pOfEIlE+NKXvkR1dTWqyvXXX0/v3r07/HjaIrEe+a6utLRUj+SGQQ2hCGP/83luOm8s180YmYKSGdP9rF27lhNOOCHdxUir2tpa8vLyUFW+8Y1vMGrUKG644YZ0F+ugkn1uIrJSVZOe+9vjm5gCvthprtbEZIxpv/vvv5/Jkyczfvx4qqur+frXv57uInW4Ht/E5PcCImwBYYw5DDfccMMxX2M4Wj2+BiEiZPh9hKwPwhhjWunxAQGuo9rOYjLGmNYsIHD9EHYWkzHGtGYBAWQEfDRZH4QxxrRiAYEb0dU6qY3pOmbOnMmyZa3vIXbPPfdw3XXXtbnOjBkziJ0K/7nPfS7puEYLFizgrrvuOui+Fy9ezIcfftj8+pZbbuGFF144jNIndywODW4BgeuDCNtorsZ0GXPnzmXRokWtpi1atKjdYyItXbr0iC84SwyI2267jXPOOeeItnWss4AAMvzWxGRMVzJ79myeffbZ5hsEbdmyhe3bt3PmmWdy3XXXUVpayvjx47n11luTrj9s2DAqKioAuOOOOxg9ejRnnHFG87Dg4K5zOPnkk5k0aRKXXHIJ9fX1vPnmmyxZsoTvfe97TJ48mY0bNzJ//nyefvppwF01PWXKFCZMmMBXv/pVGhsbm/d36623MnXqVCZMmMC6devafazpHBq8x18HAVaDMOaoPHcz7Hy/Y7c5YAKc/9M2Z/ft25dp06bx3HPPMWvWLBYtWsSll16KiHDHHXfQt29fIpEIZ599NqtXr2bixIlJt7Ny5UoWLVrEqlWrCIfDTJ06lZNOOgmAiy++mKuvvhqAH/7whzzwwAN861vf4qKLLuLCCy9k9uzZrbbV0NDA/PnzefHFFxk9ejRf+cpX+M1vfsN3vvMdAIqKinjnnXf49a9/zV133cXvf//7Q74N6R4a3GoQuAH77H4QxnQt8c1M8c1LTz31FFOnTmXKlCmsWbOmVXNQotdff50vfvGL5OTk0KtXLy666KLmeR988AFnnnkmEyZM4PHHH29zyPCY9evXM3z4cEaPHg3AFVdcwWuvvdY8/+KLLwbgpJNOah7k71DSPTS41SCAgN9Hk9UgjDkyB/mln0qzZs3ihhtu4J133qG+vp6TTjqJzZs3c9ddd7F8+XL69OnD/PnzaWhoOKLtz58/n8WLFzNp0iQefvhhXnnllaMqb2zY8I4YMryzhga3GgQQ9ImdxWRMF5OXl8fMmTP56le/2lx72LdvH7m5uRQUFLBr1y6ee+65g27jU5/6FIsXL2b//v3U1NTwzDPPNM+rqalh4MCBhEIhHn/88ebp+fn51NTUHLCtMWPGsGXLFjZs2ADAo48+yllnnXVUx5juocGtBoFrYrLB+ozpeubOncsXv/jF5qamSZMmMWXKFMaOHcuQIUM4/fTTD7r+1KlTueyyy5g0aRL9+vVrNWz3j3/8Y6ZPn05xcTHTp09vDoXLL7+cq6++moULFzZ3TgNkZWXx0EMPMWfOHMLhMCeffDLXXnvtYR3PsTY0eI8f7hvgyw+8RU1DmMXfOPg/JmOMY8N9d0023PcRsE5qY4w5kAUEEPQLoXD3qEkZY0xHsYDAncUUshqEMYeluzRP9xRH8nlZQOCupLYL5Yxpv6ysLCorKy0kughVpbKykqysrMNaz85iwg33bWcxGdN+JSUllJWVUV5enu6imHbKyspqdYZUe1hA4DUxWQ3CmHYLBoMMHz483cUwKWZNTECG32oQxhiTyAICV4OwK6mNMaY1Cwi8e1LbLUeNMaYVCwjcWUzWxGSMMa2lNCBE5DwRWS8iG0Tk5iTzM0XkSW/+WyIyzJseFJFHROR9EVkrIt9PZTkDPh+qELFahDHGNEtZQIiIH7gXOB8YB8wVkXEJi10F7FXV44G7gTu96XOATFWdAJwEfD0WHqkQDAiA1SKMMSZOKmsQ04ANqrpJVZuARcCshGVmAY94z58GzhYRARTIFZEAkA00AftSVdCgz70NFhDGGNMilQExGNga97rMm5Z0GVUNA9VAIS4s6oAdwCfAXaq6J3EHInKNiKwQkRVHc8FOwO9qEHY1tTHGtDhWO6mnARFgEDAc+A8RGZG4kKrep6qlqlpaXFx8xDsL+q0GYYwxiVIZENuAIXGvS7xpSZfxmpMKgErg34DnVTWkqruBN4Ck45V3hKBXg7BTXY0xpkUqA2I5MEpEhotIBnA5sCRhmSXAFd7z2cBL6kb/+gT4NICI5AKnAOtSVdCA1wdhF8sZY0yLlAWE16fwTWAZsBZ4SlXXiMhtInKRt9gDQKGIbAD+HYidCnsvkCcia3BB85Cqrk5VWYMBa2IyxphEKR2sT1WXAksTpt0S97wBd0pr4nq1yaanStAXO83VmpiMMSbmWO2k7lSxTmo7i8kYY1pYQNBymmuTNTEZY0wzCwjiaxAWEMYYE2MBQfx1ENbEZIwxMRYQtDQxhaJWgzDGmBgLCFrGYrJOamOMaWEBgY3maowxyVhA0HIltQWEMca0sIDA3VEOrInJGGPiWUAQ10ltNQhjjGlmAUH8WUxWgzDGmBgLCFqamEJhq0EYY0yMBQQQiPVB2HUQxhjTzAICCNhorsYYcwALCOyWo8YYk4wFBOD3CT6x01yNMSaeBYQn6PfZWEzGGBPHAsIT9PsIha0GYYwxMRYQnoBf7CwmY4yJYwHhCfp91kltjDFxLCA8QZ/Yaa7GGBPHAsIT8PvslqPGGBPHAsIT9FsNwhhj4llAeKwPwhhjWrOA8AT9PsI2mqsxxjSzgPAE/GI1CGOMiWMB4Qn6rInJGGPiWUB4ggHrpDbGmHgWEJ6Az05zNcaYeBYQHjvN1RhjWrOA8NhprsYY05oFhCdgp7kaY0wrKQ0IETlPRNaLyAYRuTnJ/EwRedKb/5aIDIubN1FE/iEia0TkfRHJSmVZ3VhMVoMwxpiYlAWEiPiBe4HzgXHAXBEZl7DYVcBeVT0euBu401s3ADwGXKuq44EZQChVZQVrYjLGmESprEFMAzao6iZVbQIWAbMSlpkFPOI9fxo4W0QEOBdYrarvAahqpapGUlhWdz8I66Q2xphmqQyIwcDWuNdl3rSky6hqGKgGCoHRgIrIMhF5R0RuTGE5AVeDaLIahDHGNAukuwBtCABnACcD9cCLIrJSVV+MX0hErgGuARg6dOhR7TBoNQhjjGkllTWIbcCQuNcl3rSky3j9DgVAJa628ZqqVqhqPbAUmJq4A1W9T1VLVbW0uLj4qArrzmKyGoQxxsSkMiCWA6NEZLiIZACXA0sSllkCXOE9nw28pKoKLAMmiEiOFxxnAR+msKxeJ7Xidm+MMSZlTUyqGhaRb+K+7P3Ag6q6RkRuA1ao6hLgAeBREdkA7MGFCKq6V0R+jgsZBZaq6rOpKiu401wBwlEl6JdU7soYY7qElPZBqOpSXPNQ/LRb4p43AHPaWPcx3KmunSLgd5WpcEQJ+jtrr8YYc+yyK6k9sVqDnclkjDGOBYQn2FyDsIAwxhiwgGgWCwgb0dUYYxwLCE/Aa2Ky4TaMMcaxgPDE+iBsRFdjjHEsIDwtTUxWgzDGGLCAaBbwWUAYY0y8dgWEiOSKiM97PlpELhKRYGqL1rmam5isk9oYY4D21yBeA7JEZDDwN+DLwMOpKlQ6WBOTMca01t6AEG/QvIuBX6vqHGB86orV+VrOYrIahDHGwGEEhIicCswDYmMidasBKTKsBmGMMa20NyC+A3wf+Is34N4I4OWUlSoNmsdisiG/jTEGaOdgfar6KvAqgNdZXaGq16eyYJ0t4LMmJmOMidfes5j+ICK9RCQX+AD4UES+l9qida6MgDUxGWNMvPY2MY1T1X3AF4DngOG4M5m6jVgNwk5zNcYYp70BEfSue/gCsERVQ7gb+XQbdpqrMca01t6A+B2wBcgFXhOR44B9qSpUOthorsYY01p7O6kXAgvjJn0sIjNTU6T0CDQP1mc1CGOMgfZ3UheIyM9FZIX3+G9cbaLbiNUgmsIWEMYYA+1vYnoQqAEu9R77gIdSVah0sOG+jTGmtfYGxEhVvVVVN3mPHwEjUlmwTlO+Hh6/lOCu9wG75agxxsS0NyD2i8gZsRcicjqwPzVF6mSq8NEyAlUbAWiyTmpjjAHa2UkNXAv8j4gUeK/3AlekpkidLH8AAFKzk4BvhNUgjDHG064ahKq+p6qTgInARFWdAnw6pSXrLFkFEMiGmh0E/GJ9EMYY4zmsO8qp6j7vimqAf09BeTqfiKtF1Owk6PfZWUzGGOM5mluOSoeVIt3yBzYHhF0HYYwxztEERPdpi8kfADU7CPqFULj7HJYxxhyNg3ZSi0gNyYNAgOyUlCgd8gfCv5YR8Akhq0EYYwxwiIBQ1fzOKkha5Q+AUB0FOQ02mqsxxniOpomp+8gfCMBA2WujuRpjjMcCAqCXC4h+stdGczXGGI8FBDTXIIrZa2cxGWOMxwICIK8/AMXssSYmY4zxpDQgROQ8EVkvIhtE5OYk8zNF5Elv/lsiMixh/lARqRWR76aynGTmQWYvCqN7rInJGGM8KQsIEfED9wLnA+OAuSIyLmGxq4C9qno8cDdwZ8L8n+PugZ16+QMoUqtBGGNMTCprENOADd7w4E3AImBWwjKzgEe8508DZ4uIAIjIF4DNwJoUlrFF/gD6RCvtNFdjjPGkMiAGA1vjXpd505Iuo6phoBooFJE84CbgRwfbgYhcE7vLXXl5+dGVNn8gfSJWgzDGmJhjtZN6AXC3qtYebCFVvU9VS1W1tLi4+Oj2mD+AgkgloXDk6LZjjDHdRHvvB3EktgFD4l6XeNOSLVMmIgGgAKgEpgOzReRnQG8gKiINqvqrlJU2fyBBDZET3XfoZY0xpgdIZUAsB0aJyHBcEFwO/FvCMktwNx76BzAbeElVFTgztoCILABqUxoO0HzjoN7hPSndjTHGdBUpCwhVDYvIN4FlgB94UFXXiMhtwApVXQI8ADwqIhuAPbgQSQ/vYrk+kYq0FcEYY44lqaxBoKpLgaUJ026Je94AzDnENhakpHCJvBpE32hlp+zOGGOOdcdqJ3Xn82oQfaPWxGSMMWAB0SKQSX2ggCK1gDDGGLCAaKUuo5hi9qa7GMYYc0ywgIhTl1FEMXuIRu1qamOMsYCIsz+rH/2lym47aowxWEC0sj+zmGKqCIfC6S6KMcaknQVEnIasfgQkSrhmd7qLYowxaWcBEacpux8A0X0701wSY4xJPwuIOI057s5y0X3b01wSY4xJPwuIOI35xxFRIbDj3XQXxRhj0s4CIo5m9+Gt6Alkb3gG1E51Ncb0bBYQcYJ+H0uj08nYuwF2r013cYwxJq0sIOIEfMLzkWmo+ODDxekujjHGpJUFRJyg30cFBdT2nwZrFlszkzGmR7OAiBP0u7ej8rjPQcV6a2YyxvRoFhBxAn4BoLzkXECsmckY06NZQMQJegFRl1EIw85wzUzGGNNDWUDEiTUxhSMK42ZZM5MxpkezgIgT8HkBEY3CCReBLwD/uDfNpTLGmPSwgIiTEXBNTE0Rhfz+cMp18O6j8MlbaS6ZMcZ0PguIOLEmpsZQxE0462boVQJ/vQEioTSWzBhjOp8FRJwBBVn4BD7ZU+8mZObB+XfC7jXw1m/TWzhjjOlkFhBxMgN+hhXm8q9dNS0Tx14Ao8+Dl38C1WXpK5wxxnQyC4gEo/rn8dHu2pYJInD+z0CjsOR6sNuRGmN6CAuIBKP75/NxZT2N4UjLxD7HwWdvh40vwtu/S1/hjDGmE1lAJBjVP59IVNlUXtd6RulVMPp8+PstsPOD9BTOGGM6kQVEgtH98wBa90OAa2qa9SvI6g1/+hqE9nd+4YwxphNZQCQYXpSL3yd8tKv2wJm5RfCF30D5Wnjlp51fOGOM6UQWEAkyA36OK8w5sAYRM+ocmHApvPU7qC3v3MIZY0wnsoBIYnS/fDbsTlKDiDnrRog0wpu/6LxCGWNMJ7OASGJ0/zy2VNbREIokX6BoFEyYA2//3moRxphuywIiiVH984kqB57JFO9T37NahDGmW7OASGJ0/3wAPtrdRj8EtK5FbF8F/3s3/PpUWDQPIuHOKagxxqRQSgNCRM4TkfUiskFEbk4yP1NEnvTmvyUiw7zpnxGRlSLyvvf306ksZ6LhRbkEfNJ2R3VMrBZx31nwwgIQP6z7K/zt/3RKOY0xJpUCqdqwiPiBe4HPAGXAchFZoqofxi12FbBXVY8XkcuBO4HLgArg86q6XUROBJYBg1NV1kQZAR/DinL5V7JTXeMVjXLDcNRVwKTLoO8IeP4H8M97oWg0nHwV7FoDL90BDdUw9w+QVdA5B2GMMUcpZQEBTAM2qOomABFZBMwC4gNiFrDAe/408CsREVV9N26ZNUC2iGSqamMKy9vKqH55rN2x79ALTru69etzfwyVH8HS78HGl2Dds5DZC0J1rvlp3tMQzEpNoY0xpgOlsolpMLA17nUZB9YCmpdR1TBQDRQmLHMJ8E6ycBCRa0RkhYisKC/v2LOJRvXP55M99W2fydQWnx8ueQCKx8CGF+C0b8G3V8EXfgtbXoc/fw2ih7lNY4xJg1TWII6aiIzHNTudm2y+qt4H3AdQWlqqHbnv0f3ziCpsLK9l/KDDbBbK6gVX/R3CjZDr5d3EOVBXDsu+725AdOHdLkyMMeYYlcoaxDZgSNzrEm9a0mVEJAAUAJXe6xLgL8BXVHVjCsuZVOxMpkN2VLclM68lHGJO/f/gzP+Adx6Bx2dD/Z6jLKUxxqROKgNiOTBKRIaLSAZwObAkYZklwBXe89nAS6qqItIbeBa4WVXfSGEZ2zS8KJe+uRk8/8HOjt3w2bfA538Bm1+H+2dC2Up3sV1tOexeC/97Dzx4HvxXCfzxStjwojVJGWPSImVNTKoaFpFv4s5A8gMPquoaEbkNWKGqS4AHgEdFZAOwBxciAN8EjgduEZFbvGnnquruVJU3UdDv47KTh/C7VzeyrWo/g3tnd9zGT5oP/cbBk1+G3yc5g3fABDjhQlj/HKz5MxQMgYvvh+NO7bgyGGPMIYhqhzbdp01paamuWLGiQ7dZtreeT/3sZa6bMZLvfXZsh24bgJpdsP7ZlhpCMBtGzICCEvc63Ajrl8KLP4Z922DOwzDm/I4vhzGmxxKRlapamnSeBcTBfe2RFbz7yV7e/P6nyQykqVO5rsL1WexY7Tq3+4+Hnath14fQUOXuTRHaD4UjYegpMPRU6DUoPWU1xnQpBwuIY/ospmPBV049jhfW7uK593fyhSmddq1ea7lFcMUz8OSX4JnrW6Zn9oKcQgjmgD8In/wD3r7PzcsbAAMnwoCJcNxpMOxMCGSkp/zGmC7JAuIQzji+iOFFufzPP7akLyAAMvPh3/4I7z/l7mo3YAL0HurudBcTCcHO92HrW258qJ2rXSf363dBZgGMPtf1fWgEVKHweDjh8y5cjDEmgQXEIfh8wpdPOY7b/vohH2yr5sTBaRwqI5ABU77U9nx/EAZPdY+YpnrY/Cqs/avrz3j/j63XKRgCp1wHx53uwmXHKmishZJSGDLdNWfZ9RrG9EjWB9EO1ftDnPaTFzlxcAF/uPoU/D459ErHomgUIk0gPlfz2PAivLkQPo47kzgjHzJyoHaXe51VAKPPg7EXuA70YI63vq917SVxP5+86a4kH3YGjDy77WWNMWllndQd4I8rtvK9p1fz758ZzfVnj0rZftJi20rY+7Hrr+g7wn2ZV33imqo2vgz/eg72701YSVx4ZPdxj/wBkNcfAllu/KnqT1oWHTABTv+O60RvqodwAwyaAjl9O/MojTFJWEB0AFXlhidXseS97Sy65lSmDe9BX26RsOsAL1ve0n8RaXIj1O7fC/WVULsbana4acPPgklz3f271/4V3rgHKje03mYw110Pcto3O+aMq6Y698jrd/TbMqYHsYDoILWNYS5c+DqN4SjPfftMeufYWUEHUD2wOSkacQMVNtW75iuAVX+A9592TVVDpkGf4dB3mOuAVwXUjV1VuRH2bHTDkmjUPXxByO7tai7gltlX5p73HQkjZ8LwT7kaUe/jwGf3xTKmLRYQHej9smou/s0bTB3ah4euPJmcDOvnP2J7t8A/fwvb34W9m1v6PWLE587U6jvSNV/5vL6PcJO7/mP/Xhc+hcdD0fGueWvz67Dlf93w6gAZea7ZLBKCplpX8xk4GYaf6U79HTChdSd8wz7Y/Jrr8C8ocY/MXtaHYrotC4gO9n9XbeOGJ1cxbXhfHpxvIdFhmupcLUMEEHdq75FcuxFu8i4k/MBdTLhnk7sHR4YbgJGyt1uavDIL3MWFg6a4oNr0sguReP4M19+S1duFSbjBXeWeUwiDJsOgqdBnWEu5a3fDthVQtgJC9XD8Oe4K+CHT3bYbawF1oXdAbStqNR7TqSwgUiAWEtOHF/LA/FILia5m33ZX0/j4Dfj4Taj4FxQMddeFjP2cq41Ub4XqMncle0MV7K8C1M3zZ7htbH8nSQc+ro9l0BQXcFv+98DQARc4/U+EPse5kwIqN0LtTnetypDpLnxqd0H5v1xZhp0JpVce2GfTVO/CbdtKN4rw4FK3Db/9mzSHZgGRIovf3ca/P7WKEcV53HTeWM45oR9iTRFdU2ONa4463M9P1TWV1e4G1PWRZPV2N4yKNV011ri7C+78wPXBZOS55XavdbWcqq2uKa3weMgrdkOqlC13TWIAvUogvz9se8c1sY053/W/1OxwIVW+3p08EC+Y4/pfMvPdI38AFI+Ffie4/VeXudDZv8edhBANubHABkx0wZY/0G1352qo+tgFXma+u9dJTpEbyj6n0E0PZEAg267U76IsIFLo5fW7+fEzH7Kpoo6Th/Xh+587galD+3R6OUw3E4244Mnr72oF4F4vfwDeW+SCLH+gq030GwclJ7uLGxtrXNNW2XKo2e5eN+xzgz0m9vFAS23IF3BNfJEkd/UV/4EBlExGHuQWu+AI7Xdnt+3fA70Gu9AZNNk1/1V+BBUfQUYuDJzk+oRy+rgz4Br2uTIVj3H3dY8de+x7Kj7A6/e40Ny72Q1Hkz/QhVc07I4j3OhCtrHWlSczr/Vp2Vm9W28v3Ag73nO1yq1vu36o/ie6i0X7neCd8NDOi0Yba1zwV2+Fmp1u2+EGt7/+4937kX1sfE9YQKRYKBLlyeVbueeFj6iobeTiKYO56fyx9O9l9542x5D6PVC+zvWLFAxxX9yxL2BwNYmK9W6YlprtrsYxYIL7Yox18jdUue3UlbsACO13zWeheje9drebnpHrrnPJKnDX2Gxf1XJtTMEQd01Mwz7YtSZ5KMVk5LvtRxpd7Smrt/tijYZcs9zRCOa6gI00uWbCxrh70Bce74X05pZp/kw3ve9wFzD5A1xZ/JkQyHTHX7bcPao+PvT++46EYae708JLSt17V77ONTVGmryz9hR6DYTCUe5ki/pKV/OsWO+Wych3n+GACa559AhYQHSSusYw9768gd+/vpmAX7j6zBHMmz6UfhYUxrgv0ECmC4+YSMg1ZTXVujDJ7OWel693X4J1la7pyp8JqOsH2r/HfXEOnASDT3Jf2vWV7pd6faXre4l9aWfkuf0Fc9x2Y9ft1OyA6m0uCP2ZLsyy+7qawtBTXVMfuNrH7g/dF3fFv1x/UNUn3jU/VQceY6/B7st+4CR34kLBUBckwRxXnkiTq6Vsf8fV9La8AY3VrbfhC7rmPhFQDpwP7uSKYJZXO6qDCXPgkt8f0cdiAdHJPq6s47+WrmXZml0EfMK54/tzaekQTh1ZmL4hw40xHSu03zWLhRu9X/O5h3/RZzTiBca7bt3iMQc2ZTVUu7Pu9mx2NZZ+J7jmtFjzWDTqalSBzCM6DAuINNlSUccf3v6Ep1Zspao+RE6Gn9OPL+KcE/rx2fED7EI7Y0zaWUCkWUMowpsbK3hp3W5eXlfOtqr9BHzCmaOKOH/CQE4bWUhJn5x0F9MY0wPZDYPSLCvo59Nj+/Ppsf1RVdZs38cz723nr6t38PL61QAM7p3N9OF9OWVEIaeOLGRIXwsMY0x6WQ0ijVSVtTtqeHtzJW9v2cNbm/ZQWecuqBrcO5tJQwqYWNKbiSUFTBhcQH6W3djHGNOxrImpi1BVPtpdyz83VfLW5j2sLqti6579gOuPGlmcx8SSAkYW5zG0bw7HFeYwvCjXgsMYc8QsILqwPXVNvFdWxeqt1awuq2L1tmrKa1qfN94vP5ORxXkMK8phaN9chvbNoaRPNoN6Z1OUl2FXdxtj2mR9EF1Y39wMZo7px8wxLfc5qGsMs3VvPVsq6tlcUcfG8lo2ltfytzW7mpuoYjICPgb3zqakTzYlfXIY3DuLgQXZDCzIYmBv9zcraKfeGmMOZAHRBeVmBhg7oBdjB/Q6YF5NQ4hP9tSzvaqB7VX72eY9yvbUs2z7TvbUHThoXFFeJv17ZVKQHaRXVpCC7CC9c4P0ycmgb24GRXkZFOVlUpSXSWFehl3LYUwPYQHRzeRnBRk/qIDxgwqSzm8IRdhZ3cD26v3NIbK9aj+7axrZtz/EpopaqupDVNWHaIpEk26jV1aAorxMcjL95AQD5GT66Z0dpE9uBn1yMijMy6AwN4O+uZlkBHz4RfD7hIKcIIW5GVZjMaaLsIDoYbKCfoYV5TKsKPegy6kq9U0R9tQ1UVHbSEWt97emkYraRirrmqhvilDfFGZPXRObyuvYW9dETWP4kGXIzfBTkB0kLytAXmaA3MzWf3My/ORmBsgM+Aj6fQT8Qk6Gn97ZGfTOCZKfFSDod/MyAj6yg36yg358PutrMaYjWUCYpESEXO9L+3CuyWgKR9lb78JkT10ToUiUSBTCkSjV+0NU1jVRWdtETUOI2sYwNQ3usbO6gbrGMLWNYeqaIkSih3/yREbAR6YXGpkBHzle+XMz/ORkuODJyfCT4QVP0O+Wywz6yAy46Rl+aZ4f8PkI+ITMoI+soJ/MgPubFfB767RsJ+gXOxnAdDsWEKZDZQR89O+VdVQj2aoqjeEojaEo4WiUcFSpawxTtT9EdX2IfQ0hwhElFInSGI7SEIpQ3xShIRShKRIlFInSEIpS3xSmtjFCXWOYvfX7qW8KU98UIRyJEoooTeFom81oR3rsmX4fmUE/2RmuZhP0+5qHzBGEgF8I+ISg3+cFVoCsoA+/zwWMT8Avgs/nlssO+snK8DdvK+gX/D4fPnGnPse2GatNBX1umYDfLRMT8PnI9gIy6PehqsQyOOgXgt66fp9rDnTbt8Dr6SwgzDFHRNwv9U7oq4hGlaaIC6PGSMSFRtiFUigSJRxpmd8QijQHUkPYLRvywqYxFKEx4tZtCEVpDEXYH3LLNO9LlXBUCUdcAFbVh6hvCrM/FEEVouqWiaoSiSihqAvAdJyJLgIZXuhk+H3EiiC4IIzVqGIhIt46PhFEwO8FnM/rf/J7zwPe86Dfh88LIp/ElqN52Vjtze9384T4eS4k3f5i+xT8Aj5vmVjIxgI14K0XC8KoKhFVolF1y/l8XtBKc63QFxeQPqHVccS/T/74fcaOxS9kBtx+Y+9RLJS7UvhaQJgezecTsnyxMDr2LjiM1ab2N0UIRV1ghSMuRNSbH4621IZiNatQJEp8roQj6sKoKUIoEvW+nAVVCEejXti57YYj2jyt0QvBWG0lqm5fDWEXgq4MAIqqG506qkok6h6xssW+jMPe9FAk6oVi7Isat463bjjiQjocdRuNLdfVLtvyCQT8vub3IybD61sL+Fxtz+8Tot77EguRjIBr5ozPEp8IPh/NYYT7j5lj+vHDC8d1ePktIIw5hnVmbaoriHohEo4o6oVSLDhi8+KXiYVUOBofnupqHt6v/tiy4UiUUFQJhQ8M2GhzcLkQjYntPxYAsVpCfMA2RaLNzXc+ESLRlv3EyhaJanPNye+T5hANJTSBRpXmfcV+JKAwsHd2St5vCwhjTJfh8wk+BMvLzuFL5cZF5DwRWS8iG0Tk5iTzM0XkSW/+WyIyLG7e973p60Xks6kspzHGmAOlLCBExA/cC5wPjAPmikhiI9lVwF5VPR64G7jTW3cccDkwHjgP+LW3PWOMMZ0klTWIacAGVd2kqk3AImBWwjKzgEe8508DZ4vr3p8FLFLVRlXdDGzwtmeMMaaTpDIgBgNb416XedOSLqOqYaAaKGznuojINSKyQkRWlJeXd2DRjTHGpLQPItVU9T5VLVXV0uLi4nQXxxhjupVUBsQ2YEjc6xJvWtJlRCQAFACV7VzXGGNMCqUyIJYDo0RkuIhk4DqdlyQsswS4wns+G3hJ3R2MlgCXe2c5DQdGAW+nsKzGGGMSpOw6CFUNi8g3gWWAH3hQVdeIyG3AClVdAjwAPCoiG4A9uBDBW+4p4EMgDHxDVSOpKqsxxpgDdZtbjopIOfDxUWyiCKjooOJ0FT3xmKFnHrcdc89xuMd9nKom7cTtNgFxtERkRVv3Ze2ueuIxQ888bjvmnqMjj7tLn8VkjDEmdSwgjDHGJGUB0eK+dBcgDXriMUPPPG475p6jw47b+iCMMcYkZTUIY4wxSVlAGGOMSarHB8Sh7lnRHYjIEBF5WUQ+FJE1IvJtb3pfEfm7iHzk/e2T7rKmgoj4ReRdEfmr93q4d/+RDd79SDLSXcaOJCK9ReRpEVknImtF5NSe8FmLyA3ev+8PROQJEcnqjp+1iDwoIrtF5IO4aUk/X3EWese/WkSmHs6+enRAtPOeFd1BGPgPVR0HnAJ8wzvOm4EXVXUU8KL3ujv6NrA27vWdwN3efUj24u5L0p38AnheVccCk3DH3q0/axEZDFwPlKrqibjRGy6ne37WD+PukxOvrc/3fNxQRaOAa4DfHM6OenRA0L57VnR5qrpDVd/xntfgvjAG0/p+HI8AX0hLAVNIREqAC4Dfe68F+DTu/iPQzY5bRAqAT+GGsUFVm1S1ih7wWeOGDsr2Bv7MAXbQDT9rVX0NNzRRvLY+31nA/6jzT6C3iAxs7756ekC0674T3Yl3W9cpwFtAf1Xd4c3aCfRPV7lS6B7gRiB29/dCoMq7/wh0v898OFAOPOQ1q/1eRHLp5p+1qm4D7gI+wQVDNbCS7v1Zx2vr8z2q77ieHhA9iojkAX8CvqOq++LneaPodqtznkXkQmC3qq5Md1k6UQCYCvxGVacAdSQ0J3XTz7oP7tfycGAQkMuBzTA9Qkd+vj09IHrMfSdEJIgLh8dV9c/e5F2x6qb3d3e6ypcipwMXicgWXPPhp3Ht8729Zgjofp95GVCmqm95r5/GBUZ3/6zPATararmqhoA/4z7/7vxZx2vr8z2q77ieHhDtuWdFl+e1uz8ArFXVn8fNir8fxxXA/+3ssqWSqn5fVUtUdRjus31JVecBL+PuPwLd7LhVdSewVUTGeJPOxg2b360/a1zT0ikikuP9e48dd7f9rBO09fkuAb7inc10ClAd1xR1SD3+SmoR+RyunTp2z4o70luijiciZwCvA+/T0hb/A1w/xFPAUNxQ6ZeqamLnV7cgIjOA76rqhSIyAlej6Au8C3xJVRvTWLwOJSKTcZ3yGcAm4Ercj8Fu/VmLyI+Ay3Bn7b0LfA3X3t6tPmsReQKYgRvWexdwK7CYJJ+vF5a/wjW31QNXquqKdu+rpweEMcaY5Hp6E5Mxxpg2WEAYY4xJygLCGGNMUhYQxhhjkrKAMMYYk5QFhDGHICIREVkV9+iwge5EZFj8qJzGHEsCh17EmB5vv6pOTnchjOlsVoMw5giJyBYR+ZmIvC8ib4vI8d70YSLykjf+/osiMtSb3l9E/iIi73mP07xN+UXkfu9eBn8TkWxv+evF3cNjtYgsStNhmh7MAsKYQ8tOaGK6LG5etapOwF2teo837ZfAI6o6EXgcWOhNXwi8qqqTcOMjrfGmjwLuVdXxQBVwiTf9ZmCKt51rU3NoxrTNrqQ25hBEpFZV85JM3wJ8WlU3eYMh7lTVQhGpAAaqasibvkNVi0SkHCiJH+rBG379796NXhCRm4Cgqt4uIs8DtbhhFBaram2KD9WYVqwGYczR0TaeH474sYEitPQNXoC74+FUYHncqKTGdAoLCGOOzmVxf//hPX8TN3oswDzcQIngbgV5HTTfJ7ugrY2KiA8YoqovAzcBBcABtRhjUsl+kRhzaNkisiru9fOqGjvVtY+IrMbVAuZ6076Fu6Pb93B3d7vSm/5t4D4RuQpXU7gOd/ezZPzAY16ICLDQu3WoMZ3G+iCMOUJeH0SpqlakuyzGpII1MRljjEnKahDGGGOSshqEMcaYpCwgjDHGJGUBYYwxJikLCGOMMUlZQBhjjEnq/wHzbttR5yfhAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target variable (y_train, y_test)\n",
    "y_train = y_train / 50.0  # Scale to [0, 1] range\n",
    "y_test = y_test / 50.0\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer with gradient clipping\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, clipvalue=1.0)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using SGD with batch_size=1\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=1, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"SGD - Loss Reduction (Fixed)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üöÄ Mini-Batch Gradient Descent (MBGD) ‚Äì Full Explanation üöÄ**  \n",
    "\n",
    "Gradient Descent is the backbone of **neural networks and deep learning**, helping to optimize model weights and minimize loss. **Mini-Batch Gradient Descent (MBGD)** is a compromise between **Batch Gradient Descent (BGD)** and **Stochastic Gradient Descent (SGD)**.\n",
    "\n",
    "\n",
    "\n",
    "## **üåü What is Mini-Batch Gradient Descent?**  \n",
    "Mini-Batch Gradient Descent (MBGD) updates model parameters **after computing gradients on a small subset (mini-batch) of the dataset**, instead of using **one sample (SGD)** or the **entire dataset (BGD)**.\n",
    "\n",
    "### **üìù Formula for Weight Update:**\n",
    "$$\n",
    "w = w - \\eta \\cdot \\nabla L_{batch}(w)\n",
    "$$\n",
    "Where:  \n",
    "üîπ $ w $ ‚Üí Model parameters (weights & biases)  \n",
    "üîπ $ \\eta $ ‚Üí Learning rate  \n",
    "üîπ $ \\nabla L_{batch}(w) $ ‚Üí Gradient of the loss function computed over a **mini-batch**  \n",
    "\n",
    "\n",
    "\n",
    "## **üí° How Does Mini-Batch Gradient Descent Work?**\n",
    "1Ô∏è‚É£ **Divide the dataset** into smaller chunks (mini-batches).  \n",
    "2Ô∏è‚É£ **Compute gradients** for each mini-batch.  \n",
    "3Ô∏è‚É£ **Update model parameters** after processing each mini-batch.  \n",
    "4Ô∏è‚É£ **Repeat the process** until all mini-batches are processed (one epoch).  \n",
    "5Ô∏è‚É£ **Multiple epochs** are used to train the model.  \n",
    "\n",
    "\n",
    "## **üõ†Ô∏è MBGD vs BGD vs SGD ‚Äì Key Differences**  \n",
    "\n",
    "| Feature                  | **Batch Gradient Descent (BGD)** | **Stochastic Gradient Descent (SGD)** | **Mini-Batch Gradient Descent (MBGD)** |\n",
    "|--------------------------|--------------------------------|--------------------------------|--------------------------------|\n",
    "| **Update Frequency**     | After processing the full dataset | After **each** sample | After **each mini-batch** |\n",
    "| **Convergence Speed**    | Slow                          | Fast but noisy               | Balanced  |\n",
    "| **Memory Usage**         | High (entire dataset needed)  | Low (one sample at a time)   | Moderate  |\n",
    "| **Stability**            | Stable                        | Noisy updates               | More stable than SGD |\n",
    "| **Suitability**          | Small datasets               | Large datasets               | Large datasets with efficient updates |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **üîé When to Use Mini-Batch Gradient Descent?**\n",
    "‚úÖ When your dataset is **too large** for BGD but **SGD is too noisy**.  \n",
    "‚úÖ When you want **faster training** with **better generalization**.  \n",
    "‚úÖ When you need a **balance** between accuracy and computation time.\n",
    "\n",
    "\n",
    "\n",
    "## **üõ†Ô∏è Implementing Mini-Batch Gradient Descent in Deep Learning**  \n",
    "Let's implement MBGD using **Boston Housing Price Prediction** in TensorFlow.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Mini-Batch Gradient Descent (SGD with batch_size)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **Mini-Batch Gradient Descent** (batch_size = 32)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Mini-Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **‚úÖ Observations**\n",
    "‚úî **Faster than BGD** because updates happen after every mini-batch.  \n",
    "‚úî **Less noisy than SGD**, resulting in **smoother loss curves**.  \n",
    "‚úî **Reduces memory consumption**, making it **efficient for large datasets**.  \n",
    "‚úî **Better generalization**, preventing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Conclusion**\n",
    "Mini-Batch Gradient Descent is the **most widely used** gradient descent technique in **deep learning**. It balances **accuracy, speed, and memory efficiency**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 1s 15ms/step - loss: 334.2068 - mae: 15.3474 - val_loss: 302.8398 - val_mae: 13.6637\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 314.8437 - mae: 14.6633 - val_loss: 23.8735 - val_mae: 3.8514\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 197.0880 - mae: 10.9435 - val_loss: 308.9723 - val_mae: 15.1685\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 141.5445 - mae: 8.5866 - val_loss: 143.4266 - val_mae: 8.0101\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 52.6857 - mae: 5.3283 - val_loss: 31.5277 - val_mae: 3.6879\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 27.9593 - mae: 3.7090 - val_loss: 26.3191 - val_mae: 3.3306\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 14.4636 - mae: 2.7255 - val_loss: 43.6879 - val_mae: 4.4070\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 12.8018 - mae: 2.6142 - val_loss: 55.4842 - val_mae: 4.9068\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 12.7193 - mae: 2.5030 - val_loss: 28.5786 - val_mae: 3.8921\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 10.3195 - mae: 2.3023 - val_loss: 15.7678 - val_mae: 2.6204\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 11.1525 - mae: 2.4044 - val_loss: 27.4883 - val_mae: 3.4278\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 16.7442 - mae: 2.9163 - val_loss: 34.0931 - val_mae: 3.9571\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 20.6083 - mae: 3.3806 - val_loss: 22.1819 - val_mae: 3.1188\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.2481 - mae: 2.4404 - val_loss: 38.1127 - val_mae: 4.0361\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 16.3059 - mae: 2.8989 - val_loss: 22.9152 - val_mae: 3.2803\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.8806 - mae: 2.4096 - val_loss: 63.2002 - val_mae: 5.7660\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 15.2869 - mae: 2.7748 - val_loss: 15.3323 - val_mae: 2.5761\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 12.3322 - mae: 2.4995 - val_loss: 29.3436 - val_mae: 3.9645\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.5863 - mae: 2.1768 - val_loss: 20.3694 - val_mae: 2.9156\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.5201 - mae: 2.4667 - val_loss: 21.5760 - val_mae: 3.3033\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.1869 - mae: 1.9602 - val_loss: 18.2293 - val_mae: 2.7332\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6905 - mae: 1.9209 - val_loss: 14.5068 - val_mae: 2.4655\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.4515 - mae: 2.2056 - val_loss: 21.1663 - val_mae: 3.2307\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.8506 - mae: 2.0442 - val_loss: 22.7889 - val_mae: 3.3534\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 8.1949 - mae: 2.0496 - val_loss: 24.4248 - val_mae: 3.0349\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.5546 - mae: 1.8817 - val_loss: 29.3545 - val_mae: 3.3380\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 9.0953 - mae: 2.1743 - val_loss: 15.3300 - val_mae: 2.6681\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.7649 - mae: 2.1035 - val_loss: 31.6868 - val_mae: 3.5467\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.8374 - mae: 1.8761 - val_loss: 20.2229 - val_mae: 3.1522\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3396 - mae: 1.9608 - val_loss: 12.5534 - val_mae: 2.3076\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.6072 - mae: 1.8230 - val_loss: 18.0670 - val_mae: 2.8732\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.4442 - mae: 2.0702 - val_loss: 15.3011 - val_mae: 2.4880\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 7.2807 - mae: 1.9476 - val_loss: 13.2858 - val_mae: 2.3360\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1076 - mae: 1.8059 - val_loss: 13.6201 - val_mae: 2.4644\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.5027 - mae: 1.6802 - val_loss: 13.0483 - val_mae: 2.3472\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.8429 - mae: 1.9115 - val_loss: 14.5197 - val_mae: 2.4109\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.7792 - mae: 1.7351 - val_loss: 12.8303 - val_mae: 2.2833\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3184 - mae: 1.8165 - val_loss: 15.5969 - val_mae: 2.5462\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.8085 - mae: 1.7402 - val_loss: 21.7635 - val_mae: 3.5196\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.9110 - mae: 2.1411 - val_loss: 25.9408 - val_mae: 3.8190\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 11.7741 - mae: 2.3996 - val_loss: 14.2894 - val_mae: 2.6520\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3754 - mae: 1.8260 - val_loss: 12.7468 - val_mae: 2.4209\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.7700 - mae: 1.8659 - val_loss: 25.0139 - val_mae: 3.5640\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3425 - mae: 1.8438 - val_loss: 16.6677 - val_mae: 2.7831\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.7480 - mae: 1.5993 - val_loss: 14.2726 - val_mae: 2.5200\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.0188 - mae: 1.8181 - val_loss: 14.9531 - val_mae: 2.9234\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.1683 - mae: 2.0713 - val_loss: 27.6664 - val_mae: 3.8346\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.3953 - mae: 1.8118 - val_loss: 16.5493 - val_mae: 2.6697\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.7592 - mae: 1.5626 - val_loss: 17.3265 - val_mae: 3.1081\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.6548 - mae: 1.7631 - val_loss: 19.4868 - val_mae: 2.9304\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.3022 - mae: 1.9098 - val_loss: 15.4431 - val_mae: 2.6997\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 7.4340 - mae: 1.9040 - val_loss: 14.9380 - val_mae: 2.7081\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.4394 - mae: 1.6904 - val_loss: 11.6773 - val_mae: 2.4353\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.6310 - mae: 2.0342 - val_loss: 22.5566 - val_mae: 2.9882\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.1908 - mae: 1.8150 - val_loss: 16.3558 - val_mae: 3.0166\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.2360 - mae: 1.6514 - val_loss: 15.1944 - val_mae: 2.8790\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.6490 - mae: 1.6891 - val_loss: 11.4101 - val_mae: 2.3474\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.4905 - mae: 1.5896 - val_loss: 11.2625 - val_mae: 2.3189\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.1158 - mae: 1.4880 - val_loss: 11.8765 - val_mae: 2.4649\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.5447 - mae: 1.5014 - val_loss: 22.4625 - val_mae: 3.5457\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3613 - mae: 1.5224 - val_loss: 15.1707 - val_mae: 2.6411\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.4994 - mae: 1.7079 - val_loss: 17.0004 - val_mae: 3.0115\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.0269 - mae: 1.8748 - val_loss: 15.7521 - val_mae: 2.5971\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.0140 - mae: 1.4906 - val_loss: 18.1524 - val_mae: 3.1655\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 8.3321 - mae: 1.9415 - val_loss: 16.2646 - val_mae: 2.7553\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.9999 - mae: 1.5965 - val_loss: 18.6269 - val_mae: 3.1445\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.0333 - mae: 1.6533 - val_loss: 11.6072 - val_mae: 2.3966\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1891 - mae: 1.5161 - val_loss: 12.7922 - val_mae: 2.5739\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.0529 - mae: 1.6238 - val_loss: 13.1982 - val_mae: 2.5362\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.6364 - mae: 1.7058 - val_loss: 12.6474 - val_mae: 2.6933\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.5043 - mae: 1.5326 - val_loss: 14.9170 - val_mae: 2.8120\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.6426 - mae: 1.3899 - val_loss: 10.9990 - val_mae: 2.3338\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.4395 - mae: 1.3535 - val_loss: 11.4481 - val_mae: 2.4128\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1886 - mae: 1.5107 - val_loss: 18.6719 - val_mae: 3.3525\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3952 - mae: 1.5414 - val_loss: 13.1828 - val_mae: 2.6499\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.7353 - mae: 1.3968 - val_loss: 11.1692 - val_mae: 2.3567\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 4.2529 - mae: 1.4593 - val_loss: 17.7298 - val_mae: 3.0679\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.8893 - mae: 1.4484 - val_loss: 12.3879 - val_mae: 2.5793\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.7071 - mae: 1.4212 - val_loss: 10.7834 - val_mae: 2.2988\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 4.1919 - mae: 1.4766 - val_loss: 14.3239 - val_mae: 2.7448\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2947 - mae: 1.4778 - val_loss: 13.9169 - val_mae: 2.7749\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 6.3064 - mae: 1.7971 - val_loss: 15.0297 - val_mae: 2.9737\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.3559 - mae: 1.5205 - val_loss: 20.3863 - val_mae: 3.3175\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2828 - mae: 1.4760 - val_loss: 17.4069 - val_mae: 3.3044\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.6708 - mae: 1.5656 - val_loss: 11.0047 - val_mae: 2.3882\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.3327 - mae: 1.3399 - val_loss: 11.1444 - val_mae: 2.4112\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.1779 - mae: 1.4996 - val_loss: 13.8071 - val_mae: 2.4918\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.9320 - mae: 1.4344 - val_loss: 9.7949 - val_mae: 2.2663\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 4.2310 - mae: 1.4256 - val_loss: 15.8810 - val_mae: 2.9804\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 5.1631 - mae: 1.6193 - val_loss: 12.5935 - val_mae: 2.5695\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.2921 - mae: 1.3352 - val_loss: 12.9919 - val_mae: 2.4504\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3788 - mae: 1.5483 - val_loss: 14.2261 - val_mae: 2.7442\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.4997 - mae: 1.7972 - val_loss: 12.7425 - val_mae: 2.4979\n",
      "Epoch 94/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.8041 - mae: 1.4061 - val_loss: 11.2824 - val_mae: 2.3451\n",
      "Epoch 95/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.4403 - mae: 1.3538 - val_loss: 11.5712 - val_mae: 2.3725\n",
      "Epoch 96/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.4038 - mae: 1.4733 - val_loss: 11.5143 - val_mae: 2.3239\n",
      "Epoch 97/100\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 3.7334 - mae: 1.3778 - val_loss: 11.6916 - val_mae: 2.4940\n",
      "Epoch 98/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1548 - mae: 1.3159 - val_loss: 12.3577 - val_mae: 2.5607\n",
      "Epoch 99/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1602 - mae: 1.3004 - val_loss: 15.6877 - val_mae: 2.9056\n",
      "Epoch 100/100\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.8623 - mae: 1.2612 - val_loss: 14.5451 - val_mae: 2.9005\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a simple Neural Network model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Define optimizer using **Mini-Batch Gradient Descent (SGD with batch_size)**\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model using **Mini-Batch Gradient Descent** (batch_size = 32)\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot Loss Curve\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Mini-Batch Gradient Descent - Loss Reduction\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
