{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🔵 Activation Functions in Neural Networks**\n",
    "Activation functions are a crucial part of neural networks. They determine whether a neuron should be activated or not by introducing **non-linearity** into the model.\n",
    "\n",
    "### **🔹 Why Do We Need Activation Functions?**\n",
    "A neural network without activation functions is just a **linear model** (like logistic regression). Activation functions help the network **learn complex patterns and relationships** in data.\n",
    "\n",
    "1. **Introduces Non-Linearity** 🌀  \n",
    "   - Real-world problems (like image recognition, speech processing) are non-linear.  \n",
    "   - Without activation functions, neural networks would behave **like a simple linear function**, limiting their power.\n",
    "   \n",
    "2. **Helps Backpropagation** 🔄  \n",
    "   - Activation functions introduce gradients, which help in optimizing the model using **gradient descent**.\n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Types of Activation Functions**\n",
    "There are **three major types** of activation functions:\n",
    "\n",
    "1. **Linear Activation**\n",
    "2. **Non-Linear Activations**\n",
    "   - **Sigmoid**\n",
    "   - **Tanh**\n",
    "   - **ReLU (Rectified Linear Unit)**\n",
    "   - **Leaky ReLU & Parametric ReLU**\n",
    "   - **ELU (Exponential Linear Unit)**\n",
    "3. **Softmax Activation (for classification tasks)**\n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ Linear Activation Function**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) = ax + b\n",
    "$$\n",
    "🔸 **Graph**: A straight line  \n",
    "🔸 **Problem**: Cannot capture complex patterns  \n",
    "\n",
    "📌 **Used in:** Output layers for regression problems.\n",
    "\n",
    "\n",
    "\n",
    "## **2️⃣ Non-Linear Activation Functions**\n",
    "### **📌 Sigmoid Activation Function**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:\n",
    "✔ Smooth, differentiable.  \n",
    "✔ Output is **always between 0 and 1** (great for probability predictions).\n",
    "\n",
    "🔹 **Cons**:\n",
    "❌ Causes **vanishing gradient** problem (small gradients slow down learning).  \n",
    "❌ Not zero-centered (output is always positive).  \n",
    "\n",
    "📌 **Used in:** Binary classification (output layer).  \n",
    "\n",
    "\n",
    "\n",
    "### **📌 Tanh (Hyperbolic Tangent) Activation Function**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:  \n",
    "✔ Outputs range between **-1 and 1** (zero-centered).  \n",
    "✔ Works better than sigmoid for deep networks.\n",
    "\n",
    "🔹 **Cons**:  \n",
    "❌ Also suffers from **vanishing gradient problem**.\n",
    "\n",
    "📌 **Used in:** Hidden layers in some RNNs.\n",
    "\n",
    "\n",
    "\n",
    "### **📌 ReLU (Rectified Linear Unit)**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:  \n",
    "✔ Does **not** suffer from vanishing gradients.  \n",
    "✔ Computationally efficient.\n",
    "\n",
    "🔹 **Cons**:  \n",
    "❌ Can cause **dying ReLU problem** (neurons stuck at 0).  \n",
    "\n",
    "📌 **Used in:** Most modern deep learning architectures (CNNs, RNNs, Transformers).  \n",
    "\n",
    "\n",
    "\n",
    "### **📌 Leaky ReLU (Fixes Dying ReLU)**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "0.01x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:  \n",
    "✔ Solves **dying ReLU problem** by allowing small negative gradients.  \n",
    "\n",
    "📌 **Used in:** Deep neural networks.\n",
    "\n",
    "\n",
    "\n",
    "### **📌 ELU (Exponential Linear Unit)**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "f(x) =\n",
    "\\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:  \n",
    "✔ Solves **dying ReLU problem**  \n",
    "✔ More stable training  \n",
    "\n",
    "📌 **Used in:** Deep networks with **complex architectures**.\n",
    "\n",
    "\n",
    "\n",
    "## **3️⃣ Softmax Activation Function (For Multi-Class Classification)**\n",
    "🔸 **Equation**:  \n",
    "$$\n",
    "\\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "🔹 **Pros**:  \n",
    "✔ Converts outputs into **probabilities** (sum = 1).  \n",
    "✔ Helps in multi-class classification.\n",
    "\n",
    "📌 **Used in:** Output layer of **multi-class classification** models.\n",
    "\n",
    "## **🎯 Summary Table**\n",
    "| Activation | Used In | Pros | Cons |\n",
    "|------------|--------|------|------|\n",
    "| **Linear** | Regression | Simple | Cannot capture complexity |\n",
    "| **Sigmoid** | Binary Classification | Probability output | Vanishing gradient |\n",
    "| **Tanh** | RNNs | Zero-centered | Vanishing gradient |\n",
    "| **ReLU** | CNNs, Deep Learning | No vanishing gradient | Dying neurons |\n",
    "| **Leaky ReLU** | Deep Learning | Solves dying ReLU | Small overhead |\n",
    "| **ELU** | Complex Networks | Better than ReLU | More computation |\n",
    "| **Softmax** | Multi-class Classification | Probability output | Can be slow |\n",
    "\n",
    "\n",
    "\n",
    "## **🔎 Final Thoughts**\n",
    "- **ReLU** is the most widely used for hidden layers.  \n",
    "- **Softmax** is used in **classification** problems.  \n",
    "- **Tanh** and **Sigmoid** are rarely used in deep networks today.  \n",
    "\n",
    "![](images/sigmoid.png)\n",
    "\n",
    "![](images/tanh.png)\n",
    "\n",
    "![](images/relu.png)\n",
    "\n",
    "![](images/elu.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🔹 What Are Activation Functions? (Super Simple Explanation)**  \n",
    "Think of a **neural network** as a factory processing raw materials (inputs) into useful products (outputs). But before making the final product, we need a **decision-making system** to decide whether a particular part should be **used or discarded**.  \n",
    "\n",
    "That **decision-making system** is the **activation function**! 🚀  \n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Why Do We Need Activation Functions?**  \n",
    "Without activation functions, a neural network is just **a boring calculator doing linear math** 📉. It won't be able to recognize complex patterns like **faces, voices, or handwritten digits**.  \n",
    "\n",
    "Activation functions help the network **learn and make smart decisions** by introducing **non-linearity** 🔄.\n",
    "\n",
    "\n",
    "\n",
    "### **🔹 Types of Activation Functions (Like Different Switches)**\n",
    "1. **Sigmoid → Soft Switch** 🔘   \n",
    "   - Example: A robot deciding whether a glass is half-full (probabilities).  \n",
    "   - Output is between **0 and 1** (good for probability-based decisions).  \n",
    "   - **Problem**: It reacts very **slowly** to big changes (vanishing gradient issue).\n",
    "\n",
    "2. **Tanh → Stronger Soft Switch** 🌗  \n",
    "   - Example: A sensor measuring **temperature changes** from cold (-1) to hot (+1).  \n",
    "   - Works better than sigmoid but still has **slow learning issues**.\n",
    "\n",
    "3. **ReLU → Simple On/Off Switch** ⚡  \n",
    "   - Example: If a signal is positive, **turn it ON**; otherwise, keep it OFF.  \n",
    "   - Used **most commonly** in deep learning because it’s fast! 🚀  \n",
    "   - **Problem**: Some neurons get permanently stuck OFF (called **dying ReLU**).\n",
    "\n",
    "4. **Leaky ReLU → Improved On/Off Switch** ⚡💡  \n",
    "   - Example: If a signal is **negative**, don’t completely turn it off—just let a **small current flow**.  \n",
    "   - Fixes **dying ReLU problem**.\n",
    "\n",
    "5. **Softmax → Decision Maker for Many Options** 🎯  \n",
    "   - Example: If you need to **choose one out of 10 items**, it helps pick the most probable one.  \n",
    "   - Used in the **last layer of classification models**.\n",
    "\n",
    "\n",
    "### **🔹 Simple Summary in Layman Terms**\n",
    "| Activation Function | Like a... | Used For | Problem Fixed |\n",
    "|----------------------|-----------|----------|---------------|\n",
    "| **Sigmoid** | Dimmer Switch (0 to 1) | Probability Predictions | Slow learning |\n",
    "| **Tanh** | Thermometer (-1 to +1) | Sensor-based predictions | Slow learning |\n",
    "| **ReLU** | Light Switch (ON/OFF) | Hidden layers in deep learning | Some neurons die |\n",
    "| **Leaky ReLU** | Improved Light Switch | Same as ReLU but better | No dead neurons |\n",
    "| **Softmax** | Voting System | Choosing **one** from **many** | Works only at output |\n",
    "\n",
    "\n",
    "### **🔹 Takeaway**  \n",
    "- **Use ReLU** for hidden layers (fast & efficient).  \n",
    "- **Use Softmax** in the output layer for multi-class problems.  \n",
    "- **Use Sigmoid/Tanh** for simpler problems (but not deep networks).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate each activation function for a given input. We'll take $ x = -2, -1, 0, 1, 2$ and compute the values manually.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Sigmoid Function**\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "For each $ x$:\n",
    "\n",
    "| $ x$  | $ e^{-x}$ | $ 1 + e^{-x}$ | $ \\sigma(x)$ |\n",
    "|----------|-------------|----------------|-------------|\n",
    "| -2       | $ e^2 \\approx 7.389$ | $ 8.389$ | $ \\frac{1}{8.389} \\approx 0.119$ |\n",
    "| -1       | $ e^1 \\approx 2.718$ | $ 3.718$ | $ \\frac{1}{3.718} \\approx 0.269$ |\n",
    "| 0        | $ e^0 = 1$ | $ 2$ | $ \\frac{1}{2} = 0.5$ |\n",
    "| 1        | $ e^{-1} \\approx 0.368$ | $ 1.368$ | $ \\frac{1}{1.368} \\approx 0.731$ |\n",
    "| 2        | $ e^{-2} \\approx 0.135$ | $ 1.135$ | $ \\frac{1}{1.135} \\approx 0.881$ |\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Tanh Function**\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "For each $ x$:\n",
    "\n",
    "| $ x$  | $ e^x$ | $ e^{-x}$ | $ e^x - e^{-x}$ | $ e^x + e^{-x}$ | $ \\tanh(x)$ |\n",
    "|----------|-------------|-------------|-------------|-------------|-------------|\n",
    "| -2       | $ e^{-2} \\approx 0.135$ | $ e^2 \\approx 7.389$ | $ -7.254$ | $ 7.524$ | $ \\frac{-7.254}{7.524} \\approx -0.964$ |\n",
    "| -1       | $ e^{-1} \\approx 0.368$ | $ e^1 \\approx 2.718$ | $ -2.350$ | $ 3.086$ | $ \\frac{-2.350}{3.086} \\approx -0.761$ |\n",
    "| 0        | $ e^0 = 1$ | $ e^0 = 1$ | $ 0$ | $ 2$ | $ 0$ |\n",
    "| 1        | $ e^1 \\approx 2.718$ | $ e^{-1} \\approx 0.368$ | $ 2.350$ | $ 3.086$ | $ \\frac{2.350}{3.086} \\approx 0.761$ |\n",
    "| 2        | $ e^2 \\approx 7.389$ | $ e^{-2} \\approx 0.135$ | $ 7.254$ | $ 7.524$ | $ \\frac{7.254}{7.524} \\approx 0.964$ |\n",
    "\n",
    "\n",
    "\n",
    "### 3. **ReLU Function**\n",
    "$$\n",
    "ReLU(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "| $ x$  | $ ReLU(x)$ |\n",
    "|----------|-------------|\n",
    "| -2       | 0 |\n",
    "| -1       | 0 |\n",
    "| 0        | 0 |\n",
    "| 1        | 1 |\n",
    "| 2        | 2 |\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Leaky ReLU Function** ($ \\alpha = 0.01$)\n",
    "$$\n",
    "LeakyReLU(x) = \\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha x, & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| $ x$  | $ LeakyReLU(x)$ |\n",
    "|----------|----------------|\n",
    "| -2       | $ -2 \\times 0.01 = -0.02$ |\n",
    "| -1       | $ -1 \\times 0.01 = -0.01$ |\n",
    "| 0        | $ 0$ |\n",
    "| 1        | $ 1$ |\n",
    "| 2        | $ 2$ |\n",
    "\n",
    "\n",
    "\n",
    "### 5. **ELU Function** ($ \\alpha = 1$)\n",
    "$$\n",
    "ELU(x) = \\begin{cases} \n",
    "x, & x > 0 \\\\\n",
    "\\alpha (e^x - 1), & x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "| $ x$  | $ e^x - 1$ | $ ELU(x)$ |\n",
    "|----------|-------------|-------------|\n",
    "| -2       | $ e^{-2} - 1 \\approx -0.865$ | $ -0.865$ |\n",
    "| -1       | $ e^{-1} - 1 \\approx -0.632$ | $ -0.632$ |\n",
    "| 0        | $ e^0 - 1 = 0$ | $ 0$ |\n",
    "| 1        | $ 1$ | $ 1$ |\n",
    "| 2        | $ 2$ | $ 2$ |\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Softmax Function** (for $ x = [-2, -1, 0, 1, 2]$)\n",
    "$$\n",
    "Softmax(x_i) = \\frac{e^{x_i}}{\\sum e^{x}}\n",
    "$$\n",
    "\n",
    "#### Step 1: Compute $ e^x$\n",
    "\n",
    "$$\n",
    "e^{-2} \\approx 0.135, \\quad e^{-1} \\approx 0.368, \\quad e^0 = 1, \\quad e^1 \\approx 2.718, \\quad e^2 \\approx 7.389\n",
    "$$\n",
    "\n",
    "#### Step 2: Compute sum of exponentials\n",
    "\n",
    "$$\n",
    "0.135 + 0.368 + 1 + 2.718 + 7.389 = 11.61\n",
    "$$\n",
    "\n",
    "#### Step 3: Compute Softmax values\n",
    "\n",
    "| $ x$  | $ e^x$ | Softmax(x) |\n",
    "|----------|-------------|-------------|\n",
    "| -2       | 0.135 | $ \\frac{0.135}{11.61} \\approx 0.012$ |\n",
    "| -1       | 0.368 | $ \\frac{0.368}{11.61} \\approx 0.032$ |\n",
    "| 0        | 1 | $ \\frac{1}{11.61} \\approx 0.086$ |\n",
    "| 1        | 2.718 | $ \\frac{2.718}{11.61} \\approx 0.234$ |\n",
    "| 2        | 7.389 | $ \\frac{7.389}{11.61} \\approx 0.636$ |\n",
    "\n",
    "\n",
    "\n",
    "### Summary of Manual Calculations\n",
    "\n",
    "- **Sigmoid** smoothly maps values between (0,1).\n",
    "- **Tanh** maps values between (-1,1).\n",
    "- **ReLU** sets negative values to 0.\n",
    "- **Leaky ReLU** allows small negative values.\n",
    "- **ELU** is similar to Leaky ReLU but smooth.\n",
    "- **Softmax** normalizes values into probabilities.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
