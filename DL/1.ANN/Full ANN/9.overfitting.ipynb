{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛑 **Early Stopping in Neural Networks** 🎯  \n",
    "\n",
    "### **What is Early Stopping?** 🤔  \n",
    "Early stopping is a **regularization technique** that helps prevent **overfitting** while training a neural network. Instead of training for a fixed number of epochs, early stopping **monitors the model’s performance** on validation data and stops training **when performance starts to degrade**.  \n",
    "\n",
    "This technique ensures that your model **learns just enough** without memorizing the training data. 🧠💡  \n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Why Do We Need Early Stopping?**  \n",
    "Neural networks typically go through **three phases** during training:  \n",
    "\n",
    "1️⃣ **Underfitting (Initial Training Stage)**  \n",
    "   - The model hasn’t learned enough yet.  \n",
    "   - Both **training loss** and **validation loss** are high.  \n",
    "\n",
    "2️⃣ **Optimal Training (Sweet Spot) ✅**  \n",
    "   - The model generalizes well to unseen data.  \n",
    "   - **Training loss decreases** 📉, and **validation loss is stable**.  \n",
    "\n",
    "3️⃣ **Overfitting (Too Much Training) 🚨**  \n",
    "   - The model memorizes training data but fails on unseen data.  \n",
    "   - **Training loss continues decreasing**, but **validation loss starts increasing**.  \n",
    "\n",
    "🎯 **Early stopping prevents overfitting by stopping training at the optimal point!**  \n",
    "\n",
    "\n",
    "\n",
    "## 🔬 **How Does Early Stopping Work?**  \n",
    "1️⃣ **Split Your Data**  \n",
    "   - Training Set 🏋️‍♂️ → Used to update model weights.  \n",
    "   - Validation Set 📊 → Used to monitor model performance.  \n",
    "\n",
    "2️⃣ **Monitor Validation Loss**  \n",
    "   - After every epoch, check the **validation loss** (or another metric like accuracy).  \n",
    "\n",
    "3️⃣ **Detect Overfitting**  \n",
    "   - If the validation loss **starts increasing**, the model is likely overfitting.  \n",
    "\n",
    "4️⃣ **Stop Training**  \n",
    "   - Stop training **after a few more epochs** (patience) to confirm the trend.  \n",
    "\n",
    "\n",
    "\n",
    "## 🛠 **Implementing Early Stopping in Python (Keras Example)**\n",
    "Most deep learning frameworks support early stopping out of the box. Here’s how you can do it in **Keras**:  \n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Monitor validation loss\n",
    "    patience=5,           # Wait for 5 epochs before stopping\n",
    "    restore_best_weights=True # Restore the best model\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping] # Apply early stopping\n",
    ")\n",
    "```\n",
    "### **Key Parameters:**\n",
    "🔹 `monitor='val_loss'` → Stops training based on validation loss (can also use `val_accuracy`).  \n",
    "🔹 `patience=5` → Stops training **only if validation loss doesn’t improve for 5 consecutive epochs**.  \n",
    "🔹 `restore_best_weights=True` → Restores the model **to the best weights** before overfitting started.  \n",
    "\n",
    "\n",
    "\n",
    "## 📈 **How to Choose the Right Patience Value?**  \n",
    "🔹 **Too low patience (e.g., 1-2 epochs) →** May stop training too early. 😕  \n",
    "🔹 **Too high patience (e.g., 10+ epochs) →** May allow overfitting. 🛑  \n",
    "🔹 **Ideal range (3-7 epochs)** → Depends on dataset size & training dynamics.  \n",
    "\n",
    "👉 **Use visualization** to check where loss starts increasing:  \n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 🏆 **Benefits of Early Stopping**\n",
    "✔️ **Prevents Overfitting** 🔥 – Stops training when validation loss increases.  \n",
    "✔️ **Saves Time & Resources** ⏳ – No need to train unnecessary epochs.  \n",
    "✔️ **Automatic Model Selection** 🧠 – Keeps the best-performing model.  \n",
    "✔️ **Works with Any Model** 🎯 – Usable in CNNs, RNNs, Transformers, etc.  \n",
    "\n",
    "\n",
    "\n",
    "## ⚠️ **Limitations of Early Stopping**\n",
    "❌ **Might Stop Too Soon** – If patience is too low, the model may not reach the best performance.  \n",
    "❌ **Doesn’t Fix Data Issues** – Early stopping **can’t compensate for bad data quality** or poor feature engineering.  \n",
    "❌ **Not Always Needed** – If your dataset is **very large**, models may generalize well even without early stopping.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Final Takeaway**\n",
    "Early stopping is like **knowing when to leave a party** 🎉—if you stay too long, things get messy (overfitting), but if you leave too soon, you might miss the fun (underfitting). 🏆  \n",
    "\n",
    "🚀 **TL;DR:**  \n",
    "✔️ **Monitors validation loss to prevent overfitting**  \n",
    "✔️ **Stops training when performance degrades**  \n",
    "✔️ **Saves time & improves generalization**  \n",
    "✔️ **Use patience to avoid stopping too soon**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 **Dropout Layers in Neural Networks – A Complete Guide** 🎯  \n",
    "\n",
    "### **🚀 What is Dropout?**  \n",
    "Dropout is a **regularization technique** used in neural networks to prevent **overfitting**. It works by randomly **dropping** (setting to zero) a percentage of neurons during training. This forces the network to **learn more robust and generalizable features**, making it perform better on unseen data.  \n",
    "\n",
    "Think of it like a **sports team** 🎽:  \n",
    "- If a team always relies on the same star players 🌟, they struggle when those players are missing.  \n",
    "- But if they train by randomly **removing key players**, others improve, making the whole team stronger! 💪  \n",
    "\n",
    "In neural networks, dropout **forces the model to learn without relying too much on specific neurons**, leading to better generalization.  \n",
    "\n",
    "\n",
    "\n",
    "## **🧠 Why Do We Need Dropout?**\n",
    "### ✅ **Prevents Overfitting**\n",
    "Neural networks **memorize** patterns in training data, including noise. Dropout **stops neurons from depending too much on each other**, preventing overfitting.  \n",
    "\n",
    "### ✅ **Improves Generalization**\n",
    "By randomly dropping neurons, the network **learns different pathways** to solve a problem. This improves its ability to handle **new, unseen data**.  \n",
    "\n",
    "### ✅ **Acts as Model Averaging**\n",
    "Since different subsets of neurons are active in each training iteration, dropout acts like training **many smaller networks** and averaging their outputs!  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ How Dropout Works (Step-by-Step)**\n",
    "1️⃣ **Choose a dropout rate** (e.g., `p = 0.5` means 50% of neurons are randomly dropped).  \n",
    "2️⃣ **During training**, neurons are randomly turned **off** (set to 0).  \n",
    "3️⃣ **During inference (testing)**, dropout is **turned off**, and the weights are scaled to match training conditions.  \n",
    "\n",
    "🔍 **Example: Before and After Dropout (Dropout Rate = 50%)**\n",
    "```\n",
    "Input:  [1, 2, 3, 4, 5]\n",
    "Before Dropout: [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "After Dropout:  [0, 0.2, 0, 0.4, 0]  <-- 50% of neurons dropped!\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Dropout in Neural Network Layers**\n",
    "Dropout can be applied to **fully connected layers (Dense layers) and convolutional layers (CNNs)**.\n",
    "\n",
    "### 🏗️ **1. Dropout in Fully Connected (Dense) Layers**\n",
    "Used in deep networks like **MLPs (Multi-Layer Perceptrons)** and deep CNNs.\n",
    "\n",
    "🔹 **Example (Keras with TensorFlow backend):**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Build a simple neural network\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.5),  # Drop 50% of neurons randomly\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),  # Drop 30% of neurons randomly\n",
    "    Dense(10, activation='softmax')  # Output layer for classification\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "📌 **Key Points:**  \n",
    "- The first `Dropout(0.5)` drops **50% of neurons** in the first hidden layer.  \n",
    "- The second `Dropout(0.3)` drops **30% of neurons** in the next layer.  \n",
    "- **Dropout is NOT applied to the output layer.**  \n",
    "\n",
    "\n",
    "\n",
    "### 🏗️ **2. Dropout in Convolutional Neural Networks (CNNs)**\n",
    "CNNs already use techniques like **max pooling and batch normalization**, so dropout is typically **lower (5% - 25%)**.\n",
    "\n",
    "🔹 **Example (Dropout in a CNN):**\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Dropout(0.25),  # Drop 25% of neurons randomly\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),   # Drop 50% in fully connected layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "```\n",
    "📌 **Key Points:**  \n",
    "- **Lower dropout** is used in CNN layers (since max pooling already helps generalization).  \n",
    "- **Higher dropout** is used in fully connected layers to prevent overfitting.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔬 Best Practices for Dropout**\n",
    "📌 **1. Don't Use Dropout in the Output Layer**  \n",
    "   - Dropout **only helps hidden layers**. Applying it to the output layer makes learning unstable.  \n",
    "\n",
    "📌 **2. Start with 20-50% Dropout in Dense Layers**  \n",
    "   - **Too much dropout (e.g., 80%)** → Might cause **underfitting** (not learning enough).  \n",
    "   - **Too little dropout (e.g., 10%)** → Might not prevent overfitting.  \n",
    "\n",
    "📌 **3. Use Lower Dropout (5-25%) in CNNs**  \n",
    "   - CNNs already generalize well, so dropout should be minimal.  \n",
    "\n",
    "📌 **4. Use Dropout Only in Training, Not Testing**  \n",
    "   - During inference (testing), dropout is **disabled** and the weights are scaled properly.  \n",
    "\n",
    "📌 **5. Combine Dropout with Other Regularization Methods**  \n",
    "   - **L1/L2 regularization (weight decay)** + **Batch Normalization** + **Dropout** = Powerful combo! 💪  \n",
    "\n",
    "\n",
    "\n",
    "## **📊 Experiment: Training a Neural Network With and Without Dropout**\n",
    "Let's train a simple **MLP classifier on the MNIST dataset** (handwritten digits) with and without dropout and compare accuracy.  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Flatten images (28x28 to 784)\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "### 🏆 Model WITHOUT Dropout\n",
    "model_no_dropout = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(784,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_no_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_no_dropout = model_no_dropout.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "### 🔥 Model WITH Dropout\n",
    "model_with_dropout = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(784,)),\n",
    "    Dropout(0.5),  # Drop 50% of neurons\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),  # Drop 30% of neurons\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "model_with_dropout.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_with_dropout = model_with_dropout.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "# Compare test accuracy\n",
    "acc_no_dropout = model_no_dropout.evaluate(X_test, y_test)[1]\n",
    "acc_with_dropout = model_with_dropout.evaluate(X_test, y_test)[1]\n",
    "\n",
    "print(f\"🎯 Accuracy WITHOUT Dropout: {acc_no_dropout:.4f}\")\n",
    "print(f\"🔥 Accuracy WITH Dropout: {acc_with_dropout:.4f}\")\n",
    "```\n",
    "\n",
    "## **🚀 Conclusion**\n",
    "- **Without dropout**, the model may overfit and perform poorly on test data.  \n",
    "- **With dropout**, generalization improves, and test accuracy is often higher.  \n",
    "- **Dropout is a simple yet powerful technique** that should be included in deep neural networks, especially in fully connected layers!  \n",
    "\n",
    "\n",
    "\n",
    "### 🏆 **Final Takeaways**\n",
    "✅ Dropout helps prevent overfitting by randomly dropping neurons.  \n",
    "✅ It acts as **model averaging**, improving generalization.  \n",
    "✅ Works best in **fully connected layers**; use lower values in CNNs.  \n",
    "✅ Combining dropout with **L2 regularization and batch normalization** can further improve results.  \n",
    "\n",
    "Now you're ready to **drop overfitting and boost model performance!** 🚀🔥\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎯 **Why Do We Use Regularization in Neural Networks?**  \n",
    "\n",
    "Regularization is used in neural networks to **prevent overfitting**—when a model memorizes the training data instead of generalizing well to new data. Overfitting happens when the network learns noise and random fluctuations in the dataset rather than the actual underlying patterns.  \n",
    "\n",
    "🛠 **Solution?** **Regularization techniques** help by adding constraints to the model, ensuring it remains simple and avoids excessive complexity.\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Types of Regularization: L1 & L2 Regularization**  \n",
    "\n",
    "### 🟠 **L1 Regularization (Lasso Regularization)**  \n",
    "L1 regularization adds the **absolute** value of the weights as a penalty to the loss function. This leads to **sparse weights**, meaning some weights become exactly **zero**—effectively removing less important features from the model.\n",
    "\n",
    "💡 **Mathematical Formula:**  \n",
    "The L1 regularized loss function is:\n",
    "\n",
    "$$\n",
    "L = \\text{Loss} + \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "where:  \n",
    "✅ $L$ = Total loss  \n",
    "✅ $\\text{Loss}$ = Original loss (e.g., cross-entropy, MSE)  \n",
    "✅ $\\lambda$ = Regularization parameter (controls the penalty strength)  \n",
    "✅ $w_i$ = Model weights  \n",
    "\n",
    "🔹 **Effect of L1 Regularization:**  \n",
    "✔ Encourages **sparsity** (some weights become exactly **zero**)  \n",
    "✔ Selects only the most important features  \n",
    "✔ Helps in feature selection  \n",
    "\n",
    "📌 **Analogy:** Imagine you're packing for a trip but can only carry essentials. L1 regularization helps \"pack\" only the most important features and discards the rest!\n",
    "\n",
    "\n",
    "\n",
    "### 🔵 **L2 Regularization (Ridge Regularization)**  \n",
    "L2 regularization adds the **squared** value of the weights as a penalty to the loss function. This discourages large weight values but does **not** make them zero, making the model more stable.\n",
    "\n",
    "💡 **Mathematical Formula:**  \n",
    "The L2 regularized loss function is:\n",
    "\n",
    "$$\n",
    "L = \\text{Loss} + \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "🔹 **Effect of L2 Regularization:**  \n",
    "✔ **Prevents overfitting** by reducing large weight values  \n",
    "✔ Does **not** set weights to zero but makes them small  \n",
    "✔ Helps maintain all features but ensures they contribute reasonably  \n",
    "\n",
    "📌 **Analogy:** Think of L2 regularization like a rubber band pulling weights towards **zero**, ensuring the network is not too sensitive to small changes in input.\n",
    "\n",
    "## 🛠 **L1 vs. L2 Regularization – Key Differences**  \n",
    "\n",
    "| Feature          | L1 Regularization (Lasso) 🔥 | L2 Regularization (Ridge) 🔵 |\n",
    "|-----------------|----------------------------|----------------------------|\n",
    "| **Penalty Term** | $ \\sum |w_i| $           | $ \\sum w_i^2 $           |\n",
    "| **Effect on Weights** | Some weights become **exactly zero** | Shrinks weights but **not to zero** |\n",
    "| **Feature Selection?** | Yes ✅ (Sparse Model) | No ❌ (Retains all features) |\n",
    "| **Computational Cost** | Lower ⏳ | Higher ⏳⏳ |\n",
    "| **Best Used When?** | Feature selection is needed 🏆 | Overfitting is a concern 🎯 |\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **When to Use L1 or L2 Regularization?**  \n",
    "\n",
    "✅ **Use L1 Regularization** when you want to remove irrelevant features and get a sparse model.  \n",
    "✅ **Use L2 Regularization** when you want to prevent overfitting but still keep all features.  \n",
    "✅ **Use Both Together (Elastic Net)** if you want a balance between feature selection and weight regularization.\n",
    "\n",
    "\n",
    "🚀 **Conclusion:** Regularization is a powerful tool to improve the generalization of neural networks. **L1** helps with feature selection, while **L2** keeps weights small and stable. Choosing the right one depends on the nature of your dataset and model needs.  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 **Think of a Neural Network Like a Student Studying for an Exam** 🎓  \n",
    "\n",
    "Imagine a student (your neural network) is preparing for a big exam. There are two possible ways they can study:  \n",
    "\n",
    "1️⃣ **They memorize every single question from past papers** 📝 (Overfitting)  \n",
    "2️⃣ **They understand the concepts so they can answer new questions** ✅ (Good Generalization)  \n",
    "\n",
    "💡 **What's the problem with memorization?**  \n",
    "If the exam questions are exactly the same as the ones they memorized, they will do great! But if the questions are different, they will struggle because they don’t actually understand the subject—they just memorized answers.  \n",
    "\n",
    "❌ **This is overfitting** in neural networks. The model learns too much detail from the training data, including noise, instead of learning general patterns that work on new data.  \n",
    "\n",
    "\n",
    "\n",
    "### 🚀 **How Does Regularization Help?**  \n",
    "\n",
    "Regularization acts like a **good teacher** who prevents the student from blindly memorizing answers. Instead, they **encourage the student to focus on key concepts** so they can answer new questions confidently.  \n",
    "\n",
    "🔹 **L1 Regularization (Forcing Simplicity)**  \n",
    "👉 Like a teacher telling the student: **\"Only focus on the most important topics. Forget unnecessary details!\"**  \n",
    "👉 Some details (weights) are completely ignored (set to zero).  \n",
    "👉 Helps in picking only the most useful information.  \n",
    "\n",
    "🔹 **L2 Regularization (Avoiding Extreme Confidence)**  \n",
    "👉 Like a teacher saying: **\"Don't rely too much on just one or two topics. Spread your understanding evenly!\"**  \n",
    "👉 Instead of removing details, it ensures the student doesn’t overly depend on specific topics.  \n",
    "👉 Reduces extreme reliance on any single piece of information.  \n",
    "\n",
    "\n",
    "\n",
    "### 🎯 **Final Summary**  \n",
    "\n",
    "🔸 Without regularization: The neural network **memorizes too much** (overfits).  \n",
    "🔸 With L1 regularization: It **picks only the most important information** (sparse learning).  \n",
    "🔸 With L2 regularization: It **balances knowledge to avoid overconfidence** (smooth learning).  \n",
    "\n",
    "🛠 **Regularization helps your model be like a smart student—one who understands concepts rather than just memorizing answers!** 😃  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **📌 Manual Calculation of L1 and L2 Regularization**\n",
    "Let's take a simple example and compute L1 and L2 regularization manually.  \n",
    "\n",
    "\n",
    "\n",
    "### **🎯 Example Setup**\n",
    "#### **Given Data**\n",
    "- **Loss function (without regularization)**: $ L = 5 $ (assume some loss value)\n",
    "- **Weights ($ w_1, w_2, w_3 $)**:  \n",
    "  $$\n",
    "  w_1 = 2, \\quad w_2 = -3, \\quad w_3 = 4\n",
    "  $$\n",
    "- **Regularization strength** ($ \\lambda $):  \n",
    "  $$\n",
    "  \\lambda = 0.1\n",
    "  $$\n",
    "\n",
    "Now, let's compute **L1 and L2 regularization separately**.\n",
    "\n",
    "\n",
    "\n",
    "## **🔷 Step 1: L1 Regularization (Lasso)**\n",
    "L1 regularization adds the **absolute values** of the weights to the loss:\n",
    "\n",
    "$$\n",
    "L_{L1} = L + \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "**Substituting the values:**\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times (|2| + |-3| + |4|)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times (2 + 3 + 4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.1 \\times 9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5 + 0.9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L1} = 5.9\n",
    "$$\n",
    "\n",
    "✅ **Final L1 Regularized Loss** = **5.9**\n",
    "\n",
    "\n",
    "\n",
    "## **🔵 Step 2: L2 Regularization (Ridge)**\n",
    "L2 regularization adds the **squared values** of the weights to the loss:\n",
    "\n",
    "$$\n",
    "L_{L2} = L + \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "**Substituting the values:**\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times (2^2 + (-3)^2 + 4^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times (4 + 9 + 16)\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 0.1 \\times 29\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 5 + 2.9\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{L2} = 7.9\n",
    "$$\n",
    "\n",
    "✅ **Final L2 Regularized Loss** = **7.9**\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Step 3: Weight Update with L1 and L2**\n",
    "### **Gradient Descent Updates**\n",
    "For **L1 Regularization**, the weight update formula is:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\cdot \\left(\\frac{\\partial \\text{Loss}}{\\partial w_i} + \\lambda \\cdot \\text{sign}(w_i)\\right)\n",
    "$$\n",
    "\n",
    "For **L2 Regularization**, the weight update formula is:\n",
    "\n",
    "$$\n",
    "w_i = w_i - \\eta \\cdot \\left(\\frac{\\partial \\text{Loss}}{\\partial w_i} + 2\\lambda w_i\\right)\n",
    "$$\n",
    "\n",
    "Assuming:\n",
    "- **Learning rate** $ \\eta = 0.01 $\n",
    "- **Gradient of loss** $ \\frac{\\partial \\text{Loss}}{\\partial w_i} = 0.5 $ (assumed for each weight)\n",
    "\n",
    "### **🔶 L1 Weight Updates**\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(2))\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.1 \\times 1)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times 0.6\n",
    "$$\n",
    "$$\n",
    "w_1 = 1.994\n",
    "$$\n",
    "\n",
    "Similarly, for $ w_2 = -3 $:\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(-3))\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 - 0.1)\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times 0.4\n",
    "$$\n",
    "$$\n",
    "w_2 = -3.004\n",
    "$$\n",
    "\n",
    "For $ w_3 = 4 $:\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.1 \\times \\text{sign}(4))\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.1)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times 0.6\n",
    "$$\n",
    "$$\n",
    "w_3 = 3.994\n",
    "$$\n",
    "\n",
    "**Updated Weights (L1 Regularization)**\n",
    "$$\n",
    "w_1 = 1.994, \\quad w_2 = -3.004, \\quad w_3 = 3.994\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🔵 L2 Weight Updates**\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times 2)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times (0.5 + 0.4)\n",
    "$$\n",
    "$$\n",
    "w_1 = 2 - 0.01 \\times 0.9\n",
    "$$\n",
    "$$\n",
    "w_1 = 1.991\n",
    "$$\n",
    "\n",
    "Similarly, for $ w_2 = -3 $:\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times (-3))\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times (0.5 - 0.6)\n",
    "$$\n",
    "$$\n",
    "w_2 = -3 - 0.01 \\times -0.1\n",
    "$$\n",
    "$$\n",
    "w_2 = -2.999\n",
    "$$\n",
    "\n",
    "For $ w_3 = 4 $:\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 2 \\times 0.1 \\times 4)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times (0.5 + 0.8)\n",
    "$$\n",
    "$$\n",
    "w_3 = 4 - 0.01 \\times 1.3\n",
    "$$\n",
    "$$\n",
    "w_3 = 3.987\n",
    "$$\n",
    "\n",
    "**Updated Weights (L2 Regularization)**\n",
    "$$\n",
    "w_1 = 1.991, \\quad w_2 = -2.999, \\quad w_3 = 3.987\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Final Comparison of Updates**\n",
    "| Weight | Initial Value | After L1 Regularization | After L2 Regularization |\n",
    "|--------|--------------|------------------------|------------------------|\n",
    "| $ w_1 $ | 2 | **1.994** (shrinks, might become 0) | **1.991** (smooth decay) |\n",
    "| $ w_2 $ | -3 | **-3.004** (shrinks) | **-2.999** (shrinks smoothly) |\n",
    "| $ w_3 $ | 4 | **3.994** (shrinks) | **3.987** (shrinks smoothly) |\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Key Takeaways**\n",
    "✔ **L1 Regularization** shrinks weights **more aggressively** and pushes some to **exactly zero** (feature selection).  \n",
    "✔ **L2 Regularization** reduces weights **smoothly** but **does not make them exactly zero**.  \n",
    "✔ **L1 is useful** for sparse models where some features can be ignored.  \n",
    "✔ **L2 is useful** when all features contribute but should have controlled importance.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
