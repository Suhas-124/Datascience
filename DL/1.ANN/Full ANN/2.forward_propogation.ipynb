{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔥 **Forward Propagation in a Multi-Layer Perceptron (MLP) – A Full Breakdown** 🔥  \n",
    "\n",
    "Forward propagation (also called **forward pass**) is the process where input data moves **layer by layer** through the network until it reaches the output layer, producing a prediction. This is the first step in training a neural network, followed by backpropagation, which adjusts weights to minimize errors.\n",
    "\n",
    "\n",
    "\n",
    "## 🏗 **1. Components Involved in Forward Propagation**  \n",
    "\n",
    "Before diving into the process, let’s define key components:\n",
    "\n",
    "- **Input Layer ($ X $)**: Receives the raw data as feature values.\n",
    "- **Weights ($ W $)**: These are the parameters that adjust how signals pass between neurons.\n",
    "- **Bias ($ b $)**: A bias term allows the activation function to shift.\n",
    "- **Activation Function ($ \\sigma $)**: Introduces non-linearity to the model, allowing it to learn complex patterns.\n",
    "- **Output Layer ($ Y $)**: Produces the final prediction.\n",
    "\n",
    "\n",
    "\n",
    "## 🔄 **2. Step-by-Step Breakdown of Forward Propagation**  \n",
    "\n",
    "Let’s assume an **MLP with one hidden layer**, meaning the network structure is:\n",
    "\n",
    "- **Input Layer**: 3 neurons\n",
    "- **Hidden Layer**: 2 neurons\n",
    "- **Output Layer**: 1 neuron (Binary classification)\n",
    "\n",
    "### 🔹 **Step 1: Input Layer to Hidden Layer**  \n",
    "\n",
    "Each neuron in the hidden layer receives input from **all input neurons**, applies weights and bias, and then passes the result through an **activation function**.\n",
    "\n",
    "Mathematically, the operation at each hidden neuron follows:\n",
    "\n",
    "$$\n",
    "Z^{(1)} = W^{(1)} X + b^{(1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W^{(1)} $ = Weight matrix from input to hidden layer  \n",
    "- $ X $ = Input features  \n",
    "- $ b^{(1)} $ = Bias term  \n",
    "- $ Z^{(1)} $ = Weighted sum before activation  \n",
    "\n",
    "Each neuron in the hidden layer applies an **activation function** (e.g., ReLU, Sigmoid):\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\sigma(Z^{(1)})\n",
    "$$\n",
    "\n",
    "Where $ A^{(1)} $ is the **activated output** of the hidden layer.\n",
    "\n",
    "📌 **Example Calculation**:\n",
    "\n",
    "Suppose:\n",
    "- Input $ X = [x_1, x_2, x_3] = [0.5, 0.2, 0.8] $\n",
    "- Weights $ W^{(1)} = \\begin{bmatrix} 0.3 & -0.2 & 0.5 \\\\ 0.7 & 0.1 & -0.6 \\end{bmatrix} $  \n",
    "- Bias $ b^{(1)} = \\begin{bmatrix} 0.1 \\\\ -0.3 \\end{bmatrix} $  \n",
    "- Activation function = **ReLU** $ \\sigma(x) = \\max(0, x) $\n",
    "\n",
    "The weighted sum $ Z^{(1)} $ is calculated as:\n",
    "\n",
    "$$\n",
    "Z^{(1)} = \\begin{bmatrix} 0.3 & -0.2 & 0.5 \\\\ 0.7 & 0.1 & -0.6 \\end{bmatrix} \\times \\begin{bmatrix} 0.5 \\\\ 0.2 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.1 \\\\ -0.3 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "$$\n",
    "Z_1 = (0.3 \\times 0.5) + (-0.2 \\times 0.2) + (0.5 \\times 0.8) + 0.1 = 0.47\n",
    "$$\n",
    "$$\n",
    "Z_2 = (0.7 \\times 0.5) + (0.1 \\times 0.2) + (-0.6 \\times 0.8) - 0.3 = -0.23\n",
    "$$\n",
    "\n",
    "Applying **ReLU Activation**:\n",
    "\n",
    "$$\n",
    "A^{(1)}_1 = \\max(0, 0.47) = 0.47\n",
    "$$\n",
    "$$\n",
    "A^{(1)}_2 = \\max(0, -0.23) = 0\n",
    "$$\n",
    "\n",
    "Thus, the activated outputs of the hidden layer are:\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\begin{bmatrix} 0.47 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Step 2: Hidden Layer to Output Layer**  \n",
    "\n",
    "The process is repeated:  \n",
    "1. Compute weighted sum **$ Z^{(2)} $**\n",
    "2. Apply activation function **$ \\sigma $**\n",
    "\n",
    "$$\n",
    "Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\sigma(Z^{(2)})\n",
    "$$\n",
    "\n",
    "📌 **Example Calculation**:\n",
    "\n",
    "Suppose:\n",
    "- Weights $ W^{(2)} = \\begin{bmatrix} 0.4 & -0.7 \\end{bmatrix} $\n",
    "- Bias $ b^{(2)} = \\begin{bmatrix} 0.2 \\end{bmatrix} $\n",
    "- Activation function = **Sigmoid** $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "\n",
    "The weighted sum:\n",
    "\n",
    "$$\n",
    "Z^{(2)} = (0.4 \\times 0.47) + (-0.7 \\times 0) + 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} = 0.188 + 0 + 0.2 = 0.388\n",
    "$$\n",
    "\n",
    "Applying **Sigmoid Activation**:\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\frac{1}{1 + e^{-0.388}} = 0.595\n",
    "$$\n",
    "\n",
    "This is the **final output (prediction)**. If this is for **binary classification**, we interpret:\n",
    "\n",
    "- $ A^{(2)} > 0.5 $ → Class 1\n",
    "- $ A^{(2)} < 0.5 $ → Class 0\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **3. Key Takeaways on Forward Propagation**\n",
    "✅ **Weight Matrix Multiplication**: Determines how signals pass through the network.  \n",
    "✅ **Activation Functions**: Add non-linearity for better learning.  \n",
    "✅ **Output Layer Interpretation**: Uses Softmax (multi-class) or Sigmoid (binary).  \n",
    "\n",
    "\n",
    "\n",
    "### **📌 Summary of Forward Propagation Steps**\n",
    "1️⃣ Compute weighted sum $ Z = WX + b $ for each layer.  \n",
    "2️⃣ Apply **activation function** $ A = \\sigma(Z) $.  \n",
    "3️⃣ Repeat until reaching the **output layer**.  \n",
    "4️⃣ Output is interpreted based on **activation function** (e.g., Sigmoid for probability).  \n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Python Code for Forward Propagation in NumPy**\n",
    "Here’s a simple implementation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Input (3 neurons)\n",
    "X = np.array([[0.5, 0.2, 0.8]])\n",
    "\n",
    "# Weights and Bias for Hidden Layer\n",
    "W1 = np.array([[0.3, -0.2, 0.5], [0.7, 0.1, -0.6]])\n",
    "b1 = np.array([[0.1, -0.3]])\n",
    "\n",
    "# Forward propagation to Hidden Layer\n",
    "Z1 = np.dot(X, W1.T) + b1\n",
    "A1 = relu(Z1)\n",
    "\n",
    "# Weights and Bias for Output Layer\n",
    "W2 = np.array([[0.4, -0.7]])\n",
    "b2 = np.array([[0.2]])\n",
    "\n",
    "# Forward propagation to Output Layer\n",
    "Z2 = np.dot(A1, W2.T) + b2\n",
    "A2 = sigmoid(Z2)\n",
    "\n",
    "print(\"Final Output:\", A2)\n",
    "```\n",
    "\n",
    "This simulates **one forward pass** through an MLP! 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great question! Let's break it down clearly. **Forward propagation** is the process where inputs pass through the network **layer by layer** until we get an output prediction.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔄 Where is Forward Propagation in Code?**  \n",
    "\n",
    "Forward propagation happens **whenever we call the model on data**.  \n",
    "This occurs during:  \n",
    "- **Training (`model.fit()`)** → Forward pass + Backpropagation  \n",
    "- **Prediction (`model.predict()`)** → Only Forward pass  \n",
    "\n",
    "Let's explicitly separate **forward propagation** in the code!\n",
    "\n",
    "\n",
    "\n",
    "### **🚀 Full Code Highlighting Forward Propagation**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (scale pixel values to 0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define the neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Converts 2D image to 1D\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "\n",
    "# Compile the model (choosing optimizer & loss function)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# - 🚀 Forward Propagation Happens Here! - #\n",
    "# When calling model.fit(), the input data (x_train) goes through:\n",
    "# 1. Flatten layer → Converts image into 1D array\n",
    "# 2. Dense (128 neurons, ReLU) → Extracts important features\n",
    "# 3. Dense (10 neurons, Softmax) → Outputs probabilities for each digit (0-9)\n",
    "# The model generates predictions, which are compared with y_train.\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "### **🔹 What Happens in Forward Propagation?**\n",
    "1️⃣ The **Flatten** layer reshapes input images.  \n",
    "2️⃣ The first **Dense (128 neurons, ReLU)** transforms data with learned weights.  \n",
    "3️⃣ The second **Dense (10 neurons, Softmax)** gives probability scores for each digit.  \n",
    "4️⃣ The model **outputs predictions** → Compared with actual labels (`y_train`).  \n",
    "\n",
    "During `model.fit()`, TensorFlow does both **forward propagation** (to get predictions) and **backpropagation** (to adjust weights).  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 Explicit Forward Propagation for Prediction**\n",
    "If you only want **forward propagation** (without backpropagation), you can use `model.predict()`:\n",
    "\n",
    "```python\n",
    "# Perform forward propagation on test images\n",
    "sample_image = x_test[:5]  # Take 5 sample images\n",
    "predictions = model.predict(sample_image)  # 🚀 Only Forward Propagation Here!\n",
    "\n",
    "# Print predicted classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(\"Predicted Labels:\", predicted_classes)\n",
    "```\n",
    "✅ Here, **only forward propagation** is performed.  \n",
    "❌ No backpropagation, since we're not updating weights.  \n",
    "\n",
    "\n",
    "\n",
    "## **🌟 Summary**\n",
    "- **Forward Propagation:** Happens in `model.fit()` (during training) and `model.predict()` (during inference).  \n",
    "- **Backpropagation:** Only happens during `model.fit()` to adjust weights using gradient descent.  \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/multi.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
