{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ **Backpropagation in Deep Learning â€“ A Colorful Journey!** ğŸ¯  \n",
    "\n",
    "Imagine you're teaching a young artist how to paint. ğŸ¨ Initially, their strokes are random, and the colors may not blend well. But with guidance (feedback), they refine their skills over time. This is exactly how **backpropagation** works in deep learningâ€”it's a feedback mechanism that helps a neural network improve its predictions by correcting its mistakes! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  What is Backpropagation?**  \n",
    "Backpropagation (short for **Backward Propagation of Errors**) is the learning algorithm used to train deep neural networks. It fine-tunes the weights of neurons so that the model **minimizes its error and improves accuracy**.  \n",
    "\n",
    "ğŸ’¡ **Key Idea:**  \n",
    "- The network **makes a prediction**  \n",
    "- It **compares** the prediction with the actual output using a **loss function**  \n",
    "- The difference (error) is sent **backward** through the network  \n",
    "- The network **adjusts weights** layer by layer to reduce errors  \n",
    "\n",
    "This process repeats for multiple **epochs** until the model is well-trained! ğŸ“Š\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒˆ Step-by-Step Walkthrough of Backpropagation**  \n",
    "\n",
    "### **1ï¸âƒ£ Forward Pass ğŸš€**\n",
    "The input data passes **forward** through the network, layer by layer, until we get an output prediction.  \n",
    "\n",
    "ğŸ¯ **Example:**  \n",
    "Imagine weâ€™re training a deep learning model to classify images of ğŸ¶ dogs and ğŸ± cats.  \n",
    "\n",
    "- Input: An image  \n",
    "- Hidden layers: Apply **weights** and **activations** to extract patterns  \n",
    "- Output: Probability scores â†’ \"70% Dog, 30% Cat\"  \n",
    "\n",
    "At this point, the model makes a guess, but we need to know **how far off** it is from the true answer.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Compute Loss (Error Calculation) âš–ï¸**\n",
    "The error is measured using a **loss function**.  \n",
    "\n",
    "ğŸ”¹ If our model predicted **70% Dog** but the correct answer was **100% Dog**, we need a way to quantify this mistake.  \n",
    "\n",
    "ğŸ’¡ Common loss functions:  \n",
    "- **Mean Squared Error (MSE)** â€“ Used for regression problems ğŸ“‰  \n",
    "- **Cross-Entropy Loss** â€“ Used for classification problems ğŸ“Š  \n",
    "\n",
    "The larger the error, the more **adjustments** the network needs! ğŸ› ï¸  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Backward Pass (Error Propagation) ğŸ”„**\n",
    "Now comes the **magic** of backpropagation! âœ¨  \n",
    "\n",
    "1ï¸âƒ£ The error signal is **propagated backward** from the output layer to the input layer.  \n",
    "2ï¸âƒ£ Each weight in the network is **updated** using the **Gradient Descent algorithm** (or its variants like Adam, RMSprop, etc.).  \n",
    "3ï¸âƒ£ The gradients are computed using **partial derivatives** (this is where **calculus** kicks in! ğŸ§®).  \n",
    "\n",
    "**Mathematically, this uses:**  \n",
    "ğŸ“Œ **Chain Rule of Differentiation** â€“ It helps calculate how much each weight contributed to the error.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Weight Updates (Learning the Right Patterns) ğŸ”„**\n",
    "Each weight in the network is updated using:  \n",
    "\n",
    "ğŸ“Œ **Gradient Descent Formula:**  \n",
    "$$\n",
    "W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial W}\n",
    "$$\n",
    "where:  \n",
    "- $ W_{\\text{new}} $ = Updated weight  \n",
    "- $ W_{\\text{old}} $ = Current weight  \n",
    "- $ \\eta $ (learning rate) = Small step to avoid overshooting  \n",
    "- $ \\frac{\\partial L}{\\partial W} $ = Gradient (rate of change of loss with respect to weight)  \n",
    "\n",
    "ğŸ’¡ Think of it as **correcting an artistâ€™s brush strokes**â€”small refinements make the painting (model) better over time! ğŸ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒ€ The Process Repeats Until Convergence!**  \n",
    "The steps above repeat **for multiple iterations (epochs)**, and the model gradually **learns better**! ğŸ“ˆ  \n",
    "\n",
    "ğŸ¯ **Final Outcome?**  \n",
    "- The model **reduces its errors**  \n",
    "- The predictions become more **accurate**  \n",
    "- The weights get **optimized** to detect **patterns** effectively  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Why is Backpropagation Important?**  \n",
    "âœ… **Allows deep networks to learn complex patterns**  \n",
    "âœ… **Makes training efficient** using gradient-based optimization  \n",
    "âœ… **Works with any differentiable activation function**  \n",
    "âœ… **Foundation of modern AI** (CNNs, RNNs, Transformersâ€”all rely on it!)  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary in a Nutshell! ğŸŒŸ**\n",
    "ğŸ’¡ Backpropagation is like a **teacher correcting a studentâ€™s mistakes** step by step! ğŸ‘¨â€ğŸ«  \n",
    "\n",
    "ğŸš€ **Forward Pass** â†’ Make a prediction  \n",
    "âš–ï¸ **Compute Loss** â†’ Measure the error  \n",
    "ğŸ”„ **Backward Pass** â†’ Adjust weights using gradients  \n",
    "ğŸ” **Repeat** until the model gets better  \n",
    "\n",
    "Itâ€™s a beautiful cycle of learning and improvement, just like **how humans refine their skills**! ğŸ†  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§© Fun Fact: Why is it Called \"Backpropagation\"?**\n",
    "Because we **propagate** (send) errors **backward** through the network! ğŸ”™ğŸ¯  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the famous **MNIST dataset** ğŸ–¼ï¸ (handwritten digit classification) and see where **backpropagation** is used in a neural network to improve accuracy! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ About the MNIST Dataset**  \n",
    "The **MNIST dataset** contains **70,000 images** (28Ã—28 pixels) of handwritten digits (0-9). The goal is to build a model that can recognize these digits correctly. ğŸ†  \n",
    "\n",
    "ğŸ“Œ **Dataset Details:**  \n",
    "- **60,000** training images ğŸ“š  \n",
    "- **10,000** testing images ğŸ“  \n",
    "- Each image is **grayscale (1 channel)**  \n",
    "- Each pixel value is **0-255**, representing intensity ğŸ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Building a Neural Network with Backpropagation (Step-by-Step)**\n",
    "We will use **TensorFlow & Keras** to build a **feedforward neural network** (MLP) that classifies digits using **backpropagation** for training.  \n",
    "\n",
    "### **ğŸ“Œ Step 1: Import Libraries & Load Data**\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images (Scale pixel values to 0-1)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "```\n",
    "ğŸ¯ **Why Normalize?**  \n",
    "Scaling ensures that **gradient updates are stable** during backpropagation.\n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 2: Define the Neural Network**\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),  # Convert 2D image to 1D\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer\n",
    "    keras.layers.Dense(10, activation='softmax')  # Output layer (10 classes)\n",
    "])\n",
    "```\n",
    "ğŸ§  **What Happens Here?**  \n",
    "- The **Flatten layer** converts each 28Ã—28 image into a **1D array of 784 pixels**.  \n",
    "- A **Dense layer with 128 neurons** and **ReLU activation** extracts important features.  \n",
    "- The **final Dense layer with 10 neurons** uses **Softmax** to output probabilities for digits 0-9.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 3: Compile the Model**\n",
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "ğŸ¯ **Where is Backpropagation Used Here?**  \n",
    "- The **loss function (categorical crossentropy)** computes how wrong the model is.  \n",
    "- **Adam optimizer** (which uses **gradient descent**) **adjusts weights** using backpropagation.  \n",
    "- The **network updates weights** to minimize loss **in every epoch**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 4: Train the Model (Backpropagation Happens Here!)**\n",
    "```python\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "```\n",
    "ğŸ”¥ **Backpropagation in Action!**\n",
    "- **Forward Pass:** The input images pass through the layers to generate predictions.  \n",
    "- **Compute Loss:** The loss function calculates how far off the predictions are.  \n",
    "- **Backward Pass:**  \n",
    "  - **Gradients of the loss w.r.t. each weight** are computed using **chain rule** (calculus ğŸ“–).  \n",
    "  - These gradients **flow backward** through the network.  \n",
    "  - The optimizer **updates the weights** to reduce the error in the next iteration.  \n",
    "- **Repeat** for multiple epochs until the model converges!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 5: Evaluate the Model**\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "```\n",
    "ğŸ¯ **Final Results:**  \n",
    "- The model learns over time using **backpropagation**.  \n",
    "- The accuracy improves as **weights are fine-tuned** with each epoch.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ Summary â€“ Where Did Backpropagation Happen?**\n",
    "1ï¸âƒ£ **Forward Pass:** Input â†’ Layers â†’ Output Predictions ğŸ¯  \n",
    "2ï¸âƒ£ **Compute Loss:** Measures the error âš–ï¸  \n",
    "3ï¸âƒ£ **Backward Pass:**  \n",
    "   - Computes gradients using the **chain rule** ğŸ”„  \n",
    "   - Updates weights using **gradient descent** ğŸ“‰  \n",
    "   - Repeats the process for multiple **epochs**  \n",
    "\n",
    "**This is how backpropagation enables deep learning models to learn from mistakes!** ğŸš€ğŸ”¥  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
