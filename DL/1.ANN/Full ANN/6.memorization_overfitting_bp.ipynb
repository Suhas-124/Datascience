{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üåü Understanding MLP Memorization in Backpropagation üåü**  \n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is a type of neural network that learns by adjusting weights using **backpropagation**. However, during training, it can sometimes **memorize** the training data instead of generalizing well. This is called **memorization** or **overfitting**.\n",
    "\n",
    "\n",
    "\n",
    "## **üîπ What is MLP Memorization?**\n",
    "When training an MLP, the model **should learn patterns** that can be applied to unseen data.  \n",
    "However, if the MLP **memorizes** specific training examples instead of understanding the underlying patterns, it **fails to generalize** to new data.  \n",
    "\n",
    "This happens when:\n",
    "‚úÖ The model is **too complex** (too many neurons & layers).  \n",
    "‚úÖ There is **too little training data**.  \n",
    "‚úÖ The model is **trained for too many epochs**.  \n",
    "\n",
    "üí° **Memorization = Overfitting**  \n",
    "A model that **memorizes** training data has **high accuracy on training data** but **poor accuracy on test data**.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ How Does Memorization Happen in Backpropagation?**\n",
    "Let‚Äôs break it down into **four steps**:\n",
    "\n",
    "### **üìå Step 1: Forward Propagation (Making Predictions)**\n",
    "- The MLP takes an input **X** and passes it through hidden layers.\n",
    "- Each neuron applies weights **W** and biases **b** to compute **activations** using:  \n",
    "  $$\n",
    "  z = W \\cdot X + b\n",
    "  $$\n",
    "- The activation function (ReLU, Sigmoid, etc.) transforms **z** into **a**:\n",
    "  $$\n",
    "  a = \\text{activation}(z)\n",
    "  $$\n",
    "\n",
    "### **üìå Step 2: Compute Loss (Error Measurement)**\n",
    "- The loss function (e.g., **MSE** for regression, **Cross-Entropy** for classification) measures how wrong the model is:\n",
    "  $$\n",
    "  \\text{Loss} = \\frac{1}{N} \\sum (y_{\\text{true}} - y_{\\text{predicted}})^2\n",
    "  $$\n",
    "\n",
    "### **üìå Step 3: Backpropagation (Gradient Computation)**\n",
    "- The model **computes gradients** of the loss w.r.t. weights:\n",
    "  $$\n",
    "  \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "  $$\n",
    "- The gradients tell **how much to update weights**.\n",
    "\n",
    "### **üìå Step 4: Weight Update (Gradient Descent)**\n",
    "- Weights **W** are updated using gradient descent:\n",
    "  $$\n",
    "  W_{\\text{new}} = W_{\\text{old}} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial W}\n",
    "  $$\n",
    "- This process repeats for multiple **epochs**.\n",
    "\n",
    "\n",
    "\n",
    "## **üîπ When Does Backpropagation Lead to Memorization?**\n",
    "üîπ If the model **trains too long**, it **perfectly learns** the training data instead of generalizing.  \n",
    "üîπ The loss on **training data goes to zero**, but the model **performs poorly on new data**.  \n",
    "üîπ This is because the model **memorizes** noise and small details instead of learning general patterns.\n",
    "\n",
    "\n",
    "\n",
    "## **üìå Example: Overfitting in an MLP (Memorization Effect)**\n",
    "Let‚Äôs train an **MLP on the MNIST dataset** and see how memorization happens. üöÄ  \n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple MLP model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(10, activation='softmax') # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 epochs (causing overfitting)\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **üîπ Observations:**\n",
    "- The **training loss keeps decreasing**.\n",
    "- But **validation loss starts increasing after some epochs** ‚Üí **Memorization detected!**\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ How to Prevent Memorization?**\n",
    "To stop an MLP from **memorizing**, we use **regularization techniques**.\n",
    "\n",
    "### **‚úÖ 1. Reduce Model Complexity**\n",
    "Use **fewer neurons** and **fewer layers** to prevent excessive memorization.\n",
    "\n",
    "```python\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(128, activation='relu'),  # Fewer neurons\n",
    "    Dense(64, activation='relu'),   # Fewer neurons\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **‚úÖ 2. Use Dropout (Randomly Remove Neurons)**\n",
    "Dropout **removes random neurons** during training, forcing the network to **learn generalized patterns**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),  # Dropout layer (50% neurons removed)\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **‚úÖ 3. Use Early Stopping (Stop Before Overfitting)**\n",
    "Stop training **automatically** when the validation loss starts increasing.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **‚úÖ 4. Use L2 Regularization (Prevent Large Weights)**\n",
    "This prevents memorization by **penalizing large weight values**.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  \n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),  # L2 regularization\n",
    "    Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **üåü Summary: Why is MLP Memorization a Problem?**\n",
    "üîπ **Good Models** ‚Üí Learn patterns and generalize well.  \n",
    "üîπ **Overfitting Models** ‚Üí Memorize training data and perform poorly on new data.  \n",
    "üîπ **Memorization Happens** when:\n",
    "   - The model is **too complex**.\n",
    "   - The model trains **too long**.\n",
    "   - The dataset is **too small**.\n",
    "\n",
    "‚úÖ **How to Fix It?**\n",
    "- **Reduce model size** (fewer neurons/layers)\n",
    "- **Use Dropout**\n",
    "- **Apply Early Stopping**\n",
    "- **Use Regularization (L2, L1)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1821 - accuracy: 0.9441 - val_loss: 0.0884 - val_accuracy: 0.9714\n",
      "Epoch 2/50\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0802 - accuracy: 0.9753 - val_loss: 0.0789 - val_accuracy: 0.9764\n",
      "Epoch 3/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0534 - accuracy: 0.9829 - val_loss: 0.0884 - val_accuracy: 0.9735\n",
      "Epoch 4/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0440 - accuracy: 0.9862 - val_loss: 0.0720 - val_accuracy: 0.9777\n",
      "Epoch 5/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0331 - accuracy: 0.9891 - val_loss: 0.0762 - val_accuracy: 0.9798\n",
      "Epoch 6/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0318 - accuracy: 0.9903 - val_loss: 0.0815 - val_accuracy: 0.9772\n",
      "Epoch 7/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0250 - accuracy: 0.9927 - val_loss: 0.0889 - val_accuracy: 0.9788\n",
      "Epoch 8/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0212 - accuracy: 0.9929 - val_loss: 0.0877 - val_accuracy: 0.9813\n",
      "Epoch 9/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.0922 - val_accuracy: 0.9797\n",
      "Epoch 10/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 0.1122 - val_accuracy: 0.9779\n",
      "Epoch 11/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.1188 - val_accuracy: 0.9787\n",
      "Epoch 12/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.1155 - val_accuracy: 0.9793\n",
      "Epoch 13/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0188 - accuracy: 0.9944 - val_loss: 0.1234 - val_accuracy: 0.9798\n",
      "Epoch 14/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0167 - accuracy: 0.9956 - val_loss: 0.1176 - val_accuracy: 0.9778\n",
      "Epoch 15/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.1332 - val_accuracy: 0.9794\n",
      "Epoch 16/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0178 - accuracy: 0.9952 - val_loss: 0.1277 - val_accuracy: 0.9811\n",
      "Epoch 17/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.1115 - val_accuracy: 0.9831\n",
      "Epoch 18/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.1305 - val_accuracy: 0.9814\n",
      "Epoch 19/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0131 - accuracy: 0.9968 - val_loss: 0.1316 - val_accuracy: 0.9834\n",
      "Epoch 20/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0140 - accuracy: 0.9965 - val_loss: 0.1102 - val_accuracy: 0.9849\n",
      "Epoch 21/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0144 - accuracy: 0.9966 - val_loss: 0.1475 - val_accuracy: 0.9830\n",
      "Epoch 22/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 0.1412 - val_accuracy: 0.9835\n",
      "Epoch 23/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0127 - accuracy: 0.9971 - val_loss: 0.1550 - val_accuracy: 0.9803\n",
      "Epoch 24/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0127 - accuracy: 0.9970 - val_loss: 0.1894 - val_accuracy: 0.9783\n",
      "Epoch 25/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0097 - accuracy: 0.9978 - val_loss: 0.1630 - val_accuracy: 0.9780\n",
      "Epoch 26/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0112 - accuracy: 0.9974 - val_loss: 0.1635 - val_accuracy: 0.9808\n",
      "Epoch 27/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.1836 - val_accuracy: 0.9817\n",
      "Epoch 28/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0102 - accuracy: 0.9976 - val_loss: 0.1901 - val_accuracy: 0.9826\n",
      "Epoch 29/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1982 - val_accuracy: 0.9813\n",
      "Epoch 30/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.2176 - val_accuracy: 0.9794\n",
      "Epoch 31/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.1966 - val_accuracy: 0.9831\n",
      "Epoch 32/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.2060 - val_accuracy: 0.9791\n",
      "Epoch 33/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0102 - accuracy: 0.9980 - val_loss: 0.1775 - val_accuracy: 0.9818\n",
      "Epoch 34/50\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.2087 - val_accuracy: 0.9823\n",
      "Epoch 35/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.2556 - val_accuracy: 0.9787\n",
      "Epoch 36/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0093 - accuracy: 0.9980 - val_loss: 0.2786 - val_accuracy: 0.9792\n",
      "Epoch 37/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0102 - accuracy: 0.9978 - val_loss: 0.2612 - val_accuracy: 0.9813\n",
      "Epoch 38/50\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.2662 - val_accuracy: 0.9797\n",
      "Epoch 39/50\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0099 - accuracy: 0.9984 - val_loss: 0.2784 - val_accuracy: 0.9804\n",
      "Epoch 40/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.2454 - val_accuracy: 0.9820\n",
      "Epoch 41/50\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.0144 - accuracy: 0.9975 - val_loss: 0.2976 - val_accuracy: 0.9801\n",
      "Epoch 42/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0104 - accuracy: 0.9983 - val_loss: 0.2329 - val_accuracy: 0.9831\n",
      "Epoch 43/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0100 - accuracy: 0.9986 - val_loss: 0.2223 - val_accuracy: 0.9822\n",
      "Epoch 44/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.2240 - val_accuracy: 0.9832\n",
      "Epoch 45/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.2843 - val_accuracy: 0.9810\n",
      "Epoch 46/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0154 - accuracy: 0.9972 - val_loss: 0.2370 - val_accuracy: 0.9838\n",
      "Epoch 47/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 0.2966 - val_accuracy: 0.9827\n",
      "Epoch 48/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0098 - accuracy: 0.9986 - val_loss: 0.2571 - val_accuracy: 0.9824\n",
      "Epoch 49/50\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0139 - accuracy: 0.9980 - val_loss: 0.2825 - val_accuracy: 0.9803\n",
      "Epoch 50/50\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0075 - accuracy: 0.9988 - val_loss: 0.2859 - val_accuracy: 0.9814\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a simple MLP model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(512, activation='relu'),  # Large layer\n",
    "    Dense(10, activation='softmax') # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 50 epochs (causing overfitting)\n",
    "history = model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training vs validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let me break it down for you in a simple way.  \n",
    "\n",
    "When training a neural network, these metrics help us understand how well the model is **learning** and **generalizing**.  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Understanding `loss`, `accuracy`, `val_loss`, and `val_accuracy`**\n",
    "\n",
    "üí° The model is trained in **epochs** (full passes over the dataset). For each epoch, the neural network prints out:  \n",
    "\n",
    "| Metric            | Meaning |\n",
    "|------------------|------------------------------------------------|\n",
    "| `loss`          | The model's error on the **training data**. |\n",
    "| `accuracy`      | The percentage of correct predictions on the **training data**. |\n",
    "| `val_loss`      | The model's error on the **validation data** (unseen data). |\n",
    "| `val_accuracy`  | The percentage of correct predictions on the **validation data**. |\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Example Breakdown**\n",
    "Let‚Äôs analyze your training log:\n",
    "\n",
    "```\n",
    "Epoch 1/50\n",
    "1875/1875 [==============================] - 11s 5ms/step - loss: 0.1837 - accuracy: 0.9436 - val_loss: 0.0986 - val_accuracy: 0.9694\n",
    "```\n",
    "\n",
    "‚úÖ **Epoch 1** (First pass over the training data):  \n",
    "- **`loss: 0.1837`** ‚Üí The error (difference between predicted and actual labels) on **training data**.  \n",
    "- **`accuracy: 0.9436`** ‚Üí The model correctly classified **94.36%** of training data.  \n",
    "- **`val_loss: 0.0986`** ‚Üí The error on **validation data** (lower is better).  \n",
    "- **`val_accuracy: 0.9694`** ‚Üí The model correctly classified **96.94%** of validation data.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "Epoch 2/50\n",
    "1875/1875 [==============================] - 10s 5ms/step - loss: 0.0789 - accuracy: 0.9751 - val_loss: 0.0810 - val_accuracy: 0.9744\n",
    "```\n",
    "\n",
    "‚úÖ **Epoch 2** (Second pass over training data):  \n",
    "- **`loss` decreased** from `0.1837 ‚Üí 0.0789` (Good! Model is learning).  \n",
    "- **`accuracy` increased** from `94.36% ‚Üí 97.51%` (More correct predictions).  \n",
    "- **`val_loss` decreased** from `0.0986 ‚Üí 0.0810` (Model is improving on unseen data).  \n",
    "- **`val_accuracy` increased** from `96.94% ‚Üí 97.44%` (Generalization is good).\n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Key Insights**\n",
    "1. **Lower `loss` is better** (it means the model's predictions are getting closer to the actual labels).  \n",
    "2. **Higher `accuracy` is better** (more correct classifications).  \n",
    "3. **If `loss` keeps decreasing but `val_loss` increases**, it means the model is **overfitting** (memorizing training data but failing on new data).  \n",
    "4. **If `accuracy` is high but `val_accuracy` is much lower**, the model is struggling to generalize.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No worries! Let me simplify it even more. Think of training a neural network like teaching a kid to recognize apples and oranges. üçéüçä  \n",
    "\n",
    "### **üîπ What are these terms?**\n",
    "1. **Loss (`loss`)** ‚Äì How much the kid is making mistakes while learning.  \n",
    "   - A lower value means fewer mistakes.  \n",
    "\n",
    "2. **Accuracy (`accuracy`)** ‚Äì How often the kid gets the answer right while learning.  \n",
    "   - A higher value means better learning.  \n",
    "\n",
    "3. **Validation Loss (`val_loss`)** ‚Äì How many mistakes the kid makes on a new test (unseen fruit).  \n",
    "   - If this goes up, it means the kid is memorizing but not understanding.  \n",
    "\n",
    "4. **Validation Accuracy (`val_accuracy`)** ‚Äì How often the kid gets new test questions right.  \n",
    "   - A high value means the kid can recognize apples and oranges in new situations.  \n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Example in Simple Terms**\n",
    "üîπ **Epoch 1:**  \n",
    "- **Loss: 0.1837** ‚Üí The kid made **some mistakes** while learning.  \n",
    "- **Accuracy: 94.36%** ‚Üí The kid correctly identified **94.36% of apples and oranges**.  \n",
    "- **Validation Loss: 0.0986** ‚Üí On a new test, the kid made **fewer mistakes**.  \n",
    "- **Validation Accuracy: 96.94%** ‚Üí The kid got **96.94% of new test questions right**.  \n",
    "\n",
    "üîπ **Epoch 2:**  \n",
    "- **Loss drops to 0.0789** ‚Üí The kid is making **even fewer mistakes**.  \n",
    "- **Accuracy rises to 97.51%** ‚Üí The kid is now **97.51% correct** on training data.  \n",
    "- **Validation Loss drops to 0.0810** ‚Üí The kid is making even fewer mistakes on new questions.  \n",
    "- **Validation Accuracy rises to 97.44%** ‚Üí The kid is now **97.44% correct on new tests**.  \n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Key Takeaway**\n",
    "‚úÖ **Lower loss = Fewer mistakes**  \n",
    "‚úÖ **Higher accuracy = More correct answers**  \n",
    "‚úÖ **Validation scores show how well the model works on new data**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your graph is a plot of **Training Loss vs. Validation Loss** over 50 epochs.\n",
    "\n",
    "### **What does this graph represent?**\n",
    "- **X-axis (Epochs):** Number of training iterations.\n",
    "- **Y-axis (Loss):** The loss function value (how well or poorly the model is performing).\n",
    "- **Two lines:**\n",
    "  - **Training Loss (Blue Line by default in Matplotlib):** How the model performs on training data.\n",
    "  - **Validation Loss (Orange Line by default in Matplotlib):** How the model generalizes to unseen test data.\n",
    "\n",
    "\n",
    "\n",
    "### **What does your output specify?**\n",
    "1. **Initially, both losses decrease:**  \n",
    "   - This is expected because the model is learning patterns in the data.\n",
    "\n",
    "2. **Training loss keeps decreasing, but validation loss starts increasing after some point:**  \n",
    "   - This indicates **overfitting**. Your model is memorizing the training data but failing to generalize to unseen test data.\n",
    "\n",
    "3. **Overfitting happens because of:**  \n",
    "   - **Large model (512 neurons per layer)**: Too many parameters for a simple dataset like MNIST.  \n",
    "   - **Too many epochs (50):** The model keeps learning even after generalization stops improving.\n",
    "\n",
    "\n",
    "\n",
    "### **How to fix overfitting?**\n",
    "1. **Reduce model complexity:**  \n",
    "   - Try fewer neurons (e.g., `Dense(256, activation='relu')` instead of 512).\n",
    "2. **Use dropout layers:**  \n",
    "   - Add `Dropout(0.2)` between dense layers to randomly deactivate some neurons and prevent reliance on specific patterns.\n",
    "3. **Use early stopping:**  \n",
    "   - Stop training when validation loss starts increasing:\n",
    "     ```python\n",
    "     from tensorflow.keras.callbacks import EarlyStopping\n",
    "     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "     ```\n",
    "     Then modify `model.fit()`:\n",
    "     ```python\n",
    "     model.fit(x_train, y_train, epochs=50, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "     ```\n",
    "4. **Reduce epochs:**  \n",
    "   - Instead of 50, try `epochs=20` and see if validation loss stabilizes.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary of Your Graph:**\n",
    "- Your model is **overfitting** after some epochs.\n",
    "- Training loss is decreasing, but validation loss is increasing.\n",
    "- You should try **dropout, early stopping, or reducing model complexity** to improve generalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preparation\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model Definition\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = ANN()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 Regularization\n",
    "\n",
    "# Training Loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "    train_accuracies.append(correct / total)\n",
    "    \n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images = images.view(images.size(0), -1)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    val_losses.append(val_loss / len(test_loader))\n",
    "    val_accuracies.append(correct / total)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, \"\n",
    "          f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "    \n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Plot Results\n",
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(epochs), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(epochs), train_accuracies, label='Train Acc')\n",
    "plt.plot(range(epochs), val_accuracies, label='Val Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curves')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! I'll explain each part of the code so that it's easy to understand. This code is implementing a **fully connected neural network (ANN)** using PyTorch to classify handwritten digits from the **MNIST dataset**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Breakdown of the Code**\n",
    "---\n",
    "\n",
    "### **1Ô∏è‚É£ Importing Required Libraries**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "- **torch** ‚Üí The main PyTorch library.  \n",
    "- **torch.nn** ‚Üí Contains layers and loss functions to build a neural network.  \n",
    "- **torch.optim** ‚Üí Provides optimizers like Adam, SGD for training models.  \n",
    "- **torchvision** ‚Üí Contains popular datasets like MNIST, CIFAR, etc.  \n",
    "- **torchvision.transforms** ‚Üí Used for preprocessing (converting images to tensors & normalizing).  \n",
    "- **torch.utils.data.DataLoader** ‚Üí Helps load data in batches for training/testing.  \n",
    "- **matplotlib.pyplot** ‚Üí Used to plot graphs for loss and accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2Ô∏è‚É£ Loading and Preprocessing Data**\n",
    "```python\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "```\n",
    "- **`transforms.ToTensor()`** ‚Üí Converts image data (0 to 255) into tensors with values between 0 and 1.  \n",
    "- **`transforms.Normalize((0.5,), (0.5,))`** ‚Üí Normalizes pixel values to **[-1, 1]** for faster convergence.  \n",
    "- **`torchvision.datasets.MNIST()`** ‚Üí Loads the **MNIST dataset**.  \n",
    "- **`DataLoader()`** ‚Üí Helps **batch** the data (batch size = 64) and **shuffle** it for better training.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3Ô∏è‚É£ Defining the Neural Network Model**\n",
    "```python\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 128),  # Input layer (28x28 = 784 pixels) ‚Üí Hidden layer (128 neurons)\n",
    "            nn.ReLU(),  # Activation function\n",
    "            nn.Dropout(0.2),  # Dropout to prevent overfitting\n",
    "            nn.Linear(128, 128),  # Second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)  # Output layer (10 classes, one for each digit)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "```\n",
    "- This is a **fully connected feedforward neural network**.  \n",
    "- **`nn.Linear(in_features, out_features)`** ‚Üí Fully connected layer.  \n",
    "- **`ReLU()`** ‚Üí Activation function to introduce non-linearity.  \n",
    "- **`Dropout(0.2)`** ‚Üí Drops 20% of neurons randomly to reduce overfitting.  \n",
    "- **The network structure:**  \n",
    "  - **Input layer** ‚Üí 784 neurons (since images are **28√ó28 pixels**).  \n",
    "  - **2 Hidden layers** ‚Üí Each has **128 neurons** with **ReLU activation**.  \n",
    "  - **Output layer** ‚Üí **10 neurons** (one for each digit **0-9**).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4Ô∏è‚É£ Setting Up Device (CPU or GPU)**\n",
    "```python\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "```\n",
    "- Checks if **GPU (CUDA)** is available and moves the model to GPU if possible.\n",
    "- If no GPU is available, it uses the **CPU**.\n",
    "\n",
    "---\n",
    "\n",
    "### **5Ô∏è‚É£ Defining the Loss Function & Optimizer**\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # L2 Regularization\n",
    "```\n",
    "- **Loss function:**  \n",
    "  - We use **CrossEntropyLoss()** because it's a classification problem with multiple classes.  \n",
    "- **Optimizer:**  \n",
    "  - **Adam Optimizer** ‚Üí Adjusts learning rates automatically to speed up training.  \n",
    "  - **Learning rate = 0.001** (controls how fast the model learns).  \n",
    "  - **Weight decay (1e-4)** ‚Üí Helps prevent overfitting by adding **L2 regularization**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6Ô∏è‚É£ Training Loop**\n",
    "```python\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "```\n",
    "- **Early stopping** ‚Üí Stops training if validation loss doesn't improve for **3 epochs**.  \n",
    "- Lists (`train_losses`, `val_losses`, etc.) store loss & accuracy history for plotting.  \n",
    "\n",
    "#### **Loop through epochs**\n",
    "```python\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)  # Flatten 28x28 images into 1D (784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "```\n",
    "- **Loops over each batch** in `train_loader`.  \n",
    "- **Flattens images** (28√ó28 ‚Üí 784 pixels).  \n",
    "- **Zero gradients** before backpropagation (`optimizer.zero_grad()`).  \n",
    "- **Forward pass** ‚Üí Get predictions (`outputs = model(images)`).  \n",
    "- **Compute loss** (`criterion(outputs, labels)`).  \n",
    "- **Backward pass & optimization** (`loss.backward()`, `optimizer.step()`).  \n",
    "- **Compute accuracy** (compares predictions with actual labels).  \n",
    "\n",
    "---\n",
    "\n",
    "### **7Ô∏è‚É£ Validation Loop**\n",
    "```python\n",
    "model.eval()\n",
    "val_loss, correct, total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images = images.view(images.size(0), -1)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        val_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "```\n",
    "- **Disables gradient calculation** (`torch.no_grad()`) to save memory.  \n",
    "- Computes **validation loss & accuracy** (no training updates).  \n",
    "\n",
    "---\n",
    "\n",
    "### **8Ô∏è‚É£ Early Stopping & Logging**\n",
    "```python\n",
    "print(f\"Epoch {epoch+1}: Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, \"\n",
    "      f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}\")\n",
    "\n",
    "if val_losses[-1] < best_val_loss:\n",
    "    best_val_loss = val_losses[-1]\n",
    "    patience_counter = 0\n",
    "else:\n",
    "    patience_counter += 1\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "```\n",
    "- **Stops training early** if validation loss doesn't improve for **3 consecutive epochs**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9Ô∏è‚É£ Plot Training Results**\n",
    "```python\n",
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(epochs), val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(epochs), train_accuracies, label='Train Acc')\n",
    "plt.plot(range(epochs), val_accuracies, label='Val Acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy Curves')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "- **Plots loss & accuracy curves** to visualize training progress.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üí° Summary**\n",
    "‚úÖ Loads and preprocesses the MNIST dataset.  \n",
    "‚úÖ Defines a simple **ANN** with **2 hidden layers**.  \n",
    "‚úÖ Uses **Adam optimizer + L2 regularization** for training.  \n",
    "‚úÖ Implements **early stopping** to avoid overfitting.  \n",
    "‚úÖ Plots **loss & accuracy curves** for performance analysis.  \n",
    "\n",
    "Let me know if you have any questions! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
