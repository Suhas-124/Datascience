{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimizers in Deep Learning üöÄ**\n",
    "  \n",
    "Optimizers are like **coaches** for a neural network. They adjust the weights and biases of the model during training to **reduce the loss and improve accuracy**. Without optimizers, the model wouldn't know how to improve itself!  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Why Do We Need Optimizers?**  \n",
    "When training a deep learning model, we want to **minimize the loss function** (which measures how far our predictions are from the actual values). Optimizers **adjust model parameters** (weights & biases) using **gradients** to make better predictions.\n",
    "\n",
    "üîÑ **Think of it like this:**  \n",
    "- The model is trying to find the lowest point in a loss landscape (like a valley).  \n",
    "- The optimizer is the **guide** that helps navigate downhill efficiently.  \n",
    "- Gradients (slopes) tell the model which direction to move.  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Types of Optimizers in Deep Learning**  \n",
    "\n",
    "Optimizers fall into two broad categories:  \n",
    "\n",
    "### **1Ô∏è‚É£ First-Order Optimizers (Gradient-Based)**\n",
    "- These rely on **gradients (first derivatives) of the loss function**.  \n",
    "- Examples: **Gradient Descent, Momentum, RMSprop, Adam**  \n",
    "\n",
    "### **2Ô∏è‚É£ Second-Order Optimizers**\n",
    "- These use **second derivatives (Hessian matrix)** for better curvature information but are computationally expensive.  \n",
    "- Example: **Newton's Method (rarely used in deep learning)**  \n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Common Optimizers Explained Simply**  \n",
    "\n",
    "### **1Ô∏è‚É£ Gradient Descent (GD)**\n",
    "**üå± The most basic optimizer!**  \n",
    "It updates weights in the direction of the negative gradient to minimize loss.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "W = W - \\eta \\cdot \\nabla L(W)\n",
    "$$\n",
    "- **$ W $** ‚Üí Model weights  \n",
    "- **$ \\eta $ (Learning Rate)** ‚Üí Step size  \n",
    "- **$ \\nabla L(W) $** ‚Üí Gradient of loss  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Simple and effective for convex functions.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Very slow for large datasets (since it updates after seeing the entire dataset).  \n",
    "\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ Stochastic Gradient Descent (SGD)**\n",
    "**üöÄ A faster version of GD!**  \n",
    "Instead of using the entire dataset, **SGD updates weights using one data sample at a time**.  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Much faster than normal Gradient Descent.  \n",
    "‚úî Works well for large datasets.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Noisy updates (weight updates fluctuate a lot).  \n",
    "\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ Mini-Batch Gradient Descent**\n",
    "**üì¶ Best of both worlds!**  \n",
    "It updates weights after processing a **small batch of samples** instead of one sample or the whole dataset.  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Balances efficiency and stability.  \n",
    "‚úî Used in almost all deep learning models.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Choosing the right batch size is tricky.  \n",
    "\n",
    "\n",
    "\n",
    "### **4Ô∏è‚É£ Momentum Optimizer**\n",
    "**üèÉ Boosts speed by adding inertia!**  \n",
    "Instead of just using gradients, it **remembers past updates** to smooth out weight updates.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla L(W)\n",
    "$$\n",
    "$$\n",
    "W = W - \\eta v_t\n",
    "$$\n",
    "- **$ v_t $** is the velocity (moving average of past gradients).  \n",
    "- **$ \\beta $** is the momentum factor (common choice: 0.9).  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Reduces zigzag motion and speeds up training.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Can overshoot the minimum if the momentum is too high.  \n",
    "\n",
    "\n",
    "\n",
    "### **5Ô∏è‚É£ RMSprop (Root Mean Square Propagation)**\n",
    "**üìâ Handles learning rate adaptively!**  \n",
    "Instead of a fixed learning rate, RMSprop **adapts the learning rate** based on recent gradients.  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "S_t = \\beta S_{t-1} + (1 - \\beta) \\nabla L(W)^2\n",
    "$$\n",
    "$$\n",
    "W = W - \\frac{\\eta}{\\sqrt{S_t + \\epsilon}} \\nabla L(W)\n",
    "$$\n",
    "- **$ S_t $** keeps track of past squared gradients.  \n",
    "- **$ \\epsilon $** prevents division by zero.  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Works well with non-stationary data.  \n",
    "‚úî Used in RNNs and NLP tasks.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Can get stuck in local minima.  \n",
    "\n",
    "\n",
    "\n",
    "### **6Ô∏è‚É£ Adam (Adaptive Moment Estimation)**\n",
    "**üí° The most popular optimizer today!**  \n",
    "Adam combines **Momentum and RMSprop** for the best of both worlds!  \n",
    "\n",
    "**Formula:**  \n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(W)\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla L(W)^2\n",
    "$$\n",
    "$$\n",
    "W = W - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} m_t\n",
    "$$\n",
    "- **$ m_t $** ‚Üí Moving average of past gradients (Momentum).  \n",
    "- **$ v_t $** ‚Üí Moving average of squared gradients (RMSprop).  \n",
    "\n",
    "‚úÖ **Pros:**  \n",
    "‚úî Adaptive learning rates.  \n",
    "‚úî Faster convergence than SGD.  \n",
    "‚úî Works well for most deep learning problems.  \n",
    "\n",
    "‚ùå **Cons:**  \n",
    "‚úñ Can lead to overfitting.  \n",
    "‚úñ Sometimes gets stuck in **sharp minima**.  \n",
    "\n",
    "## **üîπ Choosing the Right Optimizer üö¶**\n",
    "| **Optimizer**  | **When to Use?**  |\n",
    "|--------------|----------------|\n",
    "| **SGD** | Works well for small, simple datasets. |\n",
    "| **Mini-Batch GD** | Preferred for deep learning tasks. |\n",
    "| **Momentum** | Helps with faster convergence in deep networks. |\n",
    "| **RMSprop** | Good for RNNs and NLP models. |\n",
    "| **Adam** | Best for most deep learning applications. |\n",
    "\n",
    "\n",
    "\n",
    "## **üîπ Summary**\n",
    "‚úÖ Optimizers adjust weights to reduce loss.  \n",
    "‚úÖ SGD, Momentum, RMSprop, and Adam are commonly used.  \n",
    "‚úÖ **Adam is the most preferred optimizer** in deep learning.  \n",
    "‚úÖ Choosing the right optimizer depends on the task.  \n",
    "\n",
    "Would you like a **code example** to compare these optimizers? üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Optimizers in Deep Learning ‚Äì Super Simple Explanation** üöÄ  \n",
    "\n",
    "Imagine you're a **blindfolded person** trying to find the lowest point in a hilly area (the **lowest loss** in deep learning). You take small steps in different directions, feeling the slope to figure out where to go. This is exactly what an **optimizer** does for a neural network‚Äîit **adjusts the model‚Äôs weights to minimize the loss** and improve accuracy!  \n",
    "\n",
    "\n",
    "\n",
    "### **üõ£ How Optimizers Work (A Simple Story)**\n",
    "Let's say you‚Äôre trying to find your way down a hill:  \n",
    "\n",
    "1Ô∏è‚É£ **You feel the ground** to check which direction slopes downward (this is like calculating the **gradient**).  \n",
    "2Ô∏è‚É£ **You take a step** in the direction that goes downhill (this is **updating the weights**).  \n",
    "3Ô∏è‚É£ **If the hill is steep, you take bigger steps**; if it's flat, you take smaller steps (**learning rate** decides step size).  \n",
    "4Ô∏è‚É£ **You keep repeating this** until you reach the lowest point (**minimized loss**).  \n",
    "\n",
    "\n",
    "\n",
    "### **üîπ Different Types of Optimizers (Explained with Examples)**\n",
    "Just like different walking strategies help you reach the bottom of the hill **faster or more efficiently**, different optimizers improve neural network training.  \n",
    "\n",
    "\n",
    "\n",
    "### **1Ô∏è‚É£ Gradient Descent ‚Äì Walking Slowly Down the Hill üö∂**\n",
    "- You check the slope of the hill and take **one step at a time** based on the entire landscape.  \n",
    "- **Problem?** It‚Äôs **slow** because it looks at the whole area before deciding where to step.  \n",
    "\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ Stochastic Gradient Descent (SGD) ‚Äì Running Down Randomly üèÉ‚Äç‚ôÇÔ∏è**\n",
    "- Instead of analyzing the whole landscape, you **take quick steps based on a small part of the area**.  \n",
    "- **Good?** Faster than regular Gradient Descent.  \n",
    "- **Problem?** You might take **zigzag steps** and miss the exact lowest point.  \n",
    "\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ Mini-Batch Gradient Descent ‚Äì Group Walks üë¨**\n",
    "- Instead of stepping alone (SGD) or checking the whole area (GD), you **walk in small groups** and decide based on **average direction**.  \n",
    "- **Good?** Balances speed and accuracy.  \n",
    "- **Used in?** Almost all deep learning models today.  \n",
    "\n",
    "\n",
    "\n",
    "### **4Ô∏è‚É£ Momentum ‚Äì Running with a Push üèÉ‚Äç‚ôÇÔ∏èüí®**\n",
    "- Imagine you're on a bicycle. Instead of stopping after each step, **you use past speed to keep moving forward smoothly**.  \n",
    "- Helps prevent sudden stops and makes progress **faster and smoother**.  \n",
    "\n",
    "\n",
    "\n",
    "### **5Ô∏è‚É£ RMSprop ‚Äì Smart Steps to Avoid Slipping ü§ñ**\n",
    "- If you see **slippery areas** (steep parts), you **slow down automatically**.  \n",
    "- Helps **avoid overshooting the lowest point** and works well for unpredictable landscapes (like speech or text data).  \n",
    "\n",
    "\n",
    "\n",
    "### **6Ô∏è‚É£ Adam ‚Äì The Smartest Guide üß≠**\n",
    "- **Combines Momentum and RMSprop** for the best of both worlds.  \n",
    "- It **remembers past steps** (Momentum) and **adjusts speed** based on terrain steepness (RMSprop).  \n",
    "- **Why do people love Adam?**  \n",
    "  ‚úÖ Fast  \n",
    "  ‚úÖ Works for almost any deep learning problem  \n",
    "  ‚úÖ Smartly adjusts learning rate  \n",
    "\n",
    "### **üîπ Choosing the Right Optimizer**\n",
    "| **Optimizer**  | **Best For?**  |\n",
    "|--------------|----------------|\n",
    "| **Gradient Descent** | Simple models, small datasets. |\n",
    "| **SGD** | Faster training, but less stable. |\n",
    "| **Mini-Batch GD** | Used in almost all deep learning models. |\n",
    "| **Momentum** | Prevents sudden stops, smooth training. |\n",
    "| **RMSprop** | Good for speech, NLP, and RNNs. |\n",
    "| **Adam** | Best for most deep learning applications! |\n",
    "\n",
    "\n",
    "\n",
    "### **üìù Final Takeaway ‚Äì Which Optimizer is Best?**\n",
    "- If **you don‚Äôt know what to choose** ‚Üí **Adam** is the safest choice.  \n",
    "- If **you want something simple and stable** ‚Üí **Mini-Batch GD** is great.  \n",
    "- If **you work with sequential data like text or speech** ‚Üí **RMSprop** is better.  \n",
    "\n",
    "Would you like a simple **Python example** to see these optimizers in action? üöÄ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! We can manually calculate how different optimizers update weights step by step. Below, I'll walk through the manual calculations for **Gradient Descent, SGD, Momentum, RMSprop, and Adam** using a simple loss function.  \n",
    "\n",
    "\n",
    "\n",
    "## **üéØ Our Setup:**\n",
    "Let's consider a simple quadratic loss function:  \n",
    "$$\n",
    "L(w) = w^2\n",
    "$$\n",
    "where $ L $ is the loss, and $ w $ is the weight.  \n",
    "Our goal is to minimize this function by adjusting $ w $.  \n",
    "\n",
    "### **üî¢ Given Values:**\n",
    "- Initial weight: $ w = 4 $  \n",
    "- Learning rate: $ \\eta = 0.1 $  \n",
    "- Gradient: $ \\frac{dL}{dw} = 2w $  \n",
    "- Momentum: $ \\beta = 0.9 $  \n",
    "- RMSprop & Adam decay rates: $ \\beta_1 = 0.9 $, $ \\beta_2 = 0.999 $  \n",
    "- Small constant: $ \\epsilon = 10^{-8} $  \n",
    "\n",
    "\n",
    "\n",
    "## **1Ô∏è‚É£ Gradient Descent (GD) ‚Äì Basic Update Rule**\n",
    "GD updates weights using:  \n",
    "$$\n",
    "w_{\\text{new}} = w - \\eta \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "### **üî¢ Manual Calculation**\n",
    "1st Iteration:  \n",
    "$$\n",
    "\\frac{dL}{dw} = 2(4) = 8\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - (0.1 \\times 8) = 4 - 0.8 = 3.2\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "\\frac{dL}{dw} = 2(3.2) = 6.4\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.2 - (0.1 \\times 6.4) = 3.2 - 0.64 = 2.56\n",
    "$$\n",
    "\n",
    "**GD keeps updating weights until convergence.** üö∂\n",
    "\n",
    "\n",
    "\n",
    "## **2Ô∏è‚É£ Stochastic Gradient Descent (SGD) ‚Äì Random Updates**\n",
    "SGD follows the same formula as GD but updates using **random samples instead of full batch**. The process is the same as GD, but each update is based on a random small dataset rather than all data.\n",
    "\n",
    "For this example, SGD and GD will behave similarly, but with noisy updates in real-world cases.\n",
    "\n",
    "\n",
    "\n",
    "## **3Ô∏è‚É£ Momentum ‚Äì Adds Speed Boost üöÄ**\n",
    "Momentum helps **accelerate learning** by considering previous updates:  \n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + \\eta \\frac{dL}{dw}\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - v_t\n",
    "$$\n",
    "\n",
    "### **üî¢ Manual Calculation**\n",
    "Let‚Äôs initialize $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "v_1 = (0.9 \\times 0) + (0.1 \\times 8) = 0.8\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - 0.8 = 3.2\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "v_2 = (0.9 \\times 0.8) + (0.1 \\times 6.4) = 0.72 + 0.64 = 1.36\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.2 - 1.36 = 1.84\n",
    "$$\n",
    "\n",
    "Momentum **smoothens updates** and avoids oscillations! üö≤\n",
    "\n",
    "\n",
    "\n",
    "## **4Ô∏è‚É£ RMSprop ‚Äì Adjusts Learning Rate Dynamically**\n",
    "RMSprop scales the learning rate based on the squared gradient:  \n",
    "\n",
    "$$\n",
    "v_t = \\beta v_{t-1} + (1 - \\beta) (\\frac{dL}{dw})^2\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - \\frac{\\eta}{\\sqrt{v_t} + \\epsilon} \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "\n",
    "### **üî¢ Manual Calculation**\n",
    "Let‚Äôs initialize $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "v_1 = (0.9 \\times 0) + (0.1 \\times 8^2) = 6.4\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - \\frac{0.1}{\\sqrt{6.4} + 10^{-8}} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - \\frac{0.1}{2.53} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - 0.32 = 3.68\n",
    "$$\n",
    "\n",
    "2nd Iteration:  \n",
    "$$\n",
    "v_2 = (0.9 \\times 6.4) + (0.1 \\times 6.4^2) = 10.24\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 3.68 - \\frac{0.1}{\\sqrt{10.24} + 10^{-8}} \\times 6.4\n",
    "$$\n",
    "$$\n",
    "= 3.68 - \\frac{0.1}{3.2} \\times 6.4\n",
    "$$\n",
    "$$\n",
    "= 3.68 - 0.2 = 3.48\n",
    "$$\n",
    "\n",
    "RMSprop **adapts learning rates to each step** üìâ.\n",
    "\n",
    "\n",
    "\n",
    "## **5Ô∏è‚É£ Adam ‚Äì The Best of Momentum + RMSprop**\n",
    "Adam uses **two moving averages**:  \n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\cdot \\frac{dL}{dw}\n",
    "$$\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\cdot (\\frac{dL}{dw})^2\n",
    "$$\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = w - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t\n",
    "$$\n",
    "\n",
    "### **üî¢ Manual Calculation**\n",
    "Let‚Äôs initialize $ m_0 = 0 $, $ v_0 = 0 $:\n",
    "\n",
    "1st Iteration:  \n",
    "$$\n",
    "m_1 = (0.9 \\times 0) + (0.1 \\times 8) = 0.8\n",
    "$$\n",
    "$$\n",
    "v_1 = (0.999 \\times 0) + (0.001 \\times 8^2) = 0.064\n",
    "$$\n",
    "Bias correction:\n",
    "$$\n",
    "\\hat{m}_1 = \\frac{0.8}{1 - 0.9} = 8, \\quad \\hat{v}_1 = \\frac{0.064}{1 - 0.999} = 64\n",
    "$$\n",
    "$$\n",
    "w_{\\text{new}} = 4 - \\frac{0.1}{\\sqrt{64} + 10^{-8}} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - \\frac{0.1}{8} \\times 8\n",
    "$$\n",
    "$$\n",
    "= 4 - 0.1 = 3.9\n",
    "$$\n",
    "\n",
    "Adam **smooths learning and adapts step sizes** ü§ñ.\n",
    "\n",
    "## **Final Summary**\n",
    "| Optimizer  | Manual Calculation Process | Benefit |\n",
    "|------------|-----------------------------|----------|\n",
    "| **GD** | $ w = w - \\eta \\cdot \\text{grad} $ | Simple but slow |\n",
    "| **SGD** | Same as GD but on **random** data | Faster but noisy |\n",
    "| **Momentum** | Uses velocity to **accelerate** learning | Smooth updates |\n",
    "| **RMSprop** | Uses squared gradients for adaptive learning | Avoids overshooting |\n",
    "| **Adam** | Combines Momentum + RMSprop | Best for most cases |\n",
    "\n",
    "\n",
    "\n",
    "### **üéØ Conclusion**\n",
    "- You **can** manually calculate how each optimizer updates weights!  \n",
    "- **Gradient Descent** is simple but slow.  \n",
    "- **Momentum** speeds things up.  \n",
    "- **RMSprop & Adam** adapt learning rates and work better in complex cases.  \n",
    "- **Adam is usually the best default choice!**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
