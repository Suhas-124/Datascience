{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🧠 Perceptron: The Foundation of Neural Networks | Full Explanation**  \n",
    "\n",
    "The **Perceptron** is the simplest type of **artificial neural network**, acting as a **building block for deep learning models**. It was introduced by **Frank Rosenblatt in 1958** and is the basis for more complex neural networks like **Multi-Layer Perceptrons (MLPs)** and **Deep Learning models**.  \n",
    "\n",
    "Let's dive deep into how perceptrons work, their structure, limitations, and applications! 🚀🔥  \n",
    "\n",
    "\n",
    "\n",
    "## **🌍 What is a Perceptron?**  \n",
    "A **Perceptron** is a type of **binary classifier** that **learns** to make decisions by processing inputs through **weights and an activation function**. It is a fundamental concept in **supervised learning** used for **classification tasks**.  \n",
    "\n",
    "🔹 **Example:**  \n",
    "A perceptron can classify whether an email is **spam or not spam** 📧, based on input features like **keywords, sender, and frequency of words**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Structure of a Perceptron**  \n",
    "\n",
    "A perceptron consists of:  \n",
    "✅ **Inputs (Features)** – Represent data points (e.g., image pixels, text words).  \n",
    "✅ **Weights (𝑤)** – Adjust the importance of each input.  \n",
    "✅ **Bias (b)** – Helps shift the activation function for better learning.  \n",
    "✅ **Summation Function (Σ)** – Computes the weighted sum of inputs.  \n",
    "✅ **Activation Function** – Determines the final decision (e.g., Step Function).  \n",
    "✅ **Output (Prediction)** – The final classification (e.g., 0 or 1).  \n",
    "\n",
    "📌 **Mathematical Formula:**  \n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$\n",
    "where:  \n",
    "🔹 **x₁, x₂, ..., xₙ** = Inputs  \n",
    "🔹 **w₁, w₂, ..., wₙ** = Weights  \n",
    "🔹 **b** = Bias  \n",
    "🔹 **f()** = Activation Function  \n",
    "\n",
    "\n",
    "\n",
    "## **⚡ How Perceptron Works? (Step-by-Step)**\n",
    "1️⃣ **Initialize Weights & Bias** – Set initial values (random or zeros).  \n",
    "2️⃣ **Compute Weighted Sum** – Multiply inputs with weights and add bias.  \n",
    "3️⃣ **Apply Activation Function** – Decide the output based on a threshold.  \n",
    "4️⃣ **Update Weights (Learning Process)** – Adjust weights using errors.  \n",
    "5️⃣ **Repeat for All Training Data** – Learn patterns and improve accuracy.  \n",
    "\n",
    "📌 **Example Calculation:**  \n",
    "Assume we have:  \n",
    "🔹 **Inputs:** x₁ = 1, x₂ = -1  \n",
    "🔹 **Weights:** w₁ = 0.5, w₂ = 0.3  \n",
    "🔹 **Bias:** b = -0.1  \n",
    "\n",
    "$$\n",
    "\\text{Weighted Sum} = (1 \\times 0.5) + (-1 \\times 0.3) + (-0.1) = 0.1\n",
    "$$  \n",
    "\n",
    "If we use a **step activation function**:  \n",
    "🔹 **If Sum ≥ 0 → Output = 1**  \n",
    "🔹 **If Sum < 0 → Output = 0**  \n",
    "\n",
    "Since **0.1 ≥ 0**, the output is **1 (Positive Class)**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🧪 Types of Perceptrons**\n",
    "1️⃣ **Single Layer Perceptron**  \n",
    "✅ Has **one layer** (only input and output).  \n",
    "✅ Can solve **linear problems** (e.g., AND, OR logic gates).  \n",
    "✅ Cannot solve **non-linear problems** (e.g., XOR gate).  \n",
    "\n",
    "2️⃣ **Multi-Layer Perceptron (MLP)**  \n",
    "✅ Has **multiple layers** (input, hidden, output).  \n",
    "✅ Can learn **complex patterns** (used in deep learning).  \n",
    "✅ Uses **backpropagation** for learning.  \n",
    "\n",
    "\n",
    "\n",
    "## **🚧 Limitations of Perceptron**\n",
    "🔴 **Cannot Solve Non-Linear Problems** – Example: **XOR function** cannot be solved using a single-layer perceptron.  \n",
    "🔴 **Limited Learning Ability** – Works only for **linearly separable data**.  \n",
    "🔴 **Step Activation Function is Too Simple** – More advanced activations (ReLU, Sigmoid) are needed for complex tasks.  \n",
    "\n",
    "🔹 **Solution?** Use **Multi-Layer Perceptrons (MLPs) with activation functions** like **ReLU, Sigmoid, and Tanh**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Advantages of Perceptron**\n",
    "✅ **Fast & Efficient** – Simple computation, making it fast for binary classification.  \n",
    "✅ **Foundation for Neural Networks** – Forms the basis of **MLPs and deep learning**.  \n",
    "✅ **Works Well for Linearly Separable Data** – Can classify simple patterns accurately.  \n",
    "\n",
    "\n",
    "## **📌 Perceptron vs. Modern Neural Networks**\n",
    "| Feature | Perceptron | Modern Neural Networks |\n",
    "|---------|------------|------------------------|\n",
    "| **Layers** | Single Layer | Multiple Layers |\n",
    "| **Learning Algorithm** | Simple Weight Update | Backpropagation |\n",
    "| **Activation Function** | Step Function | ReLU, Sigmoid, Softmax |\n",
    "| **Handles Complex Data?** | No | Yes |\n",
    "| **Can Solve XOR?** | ❌ No | ✅ Yes |\n",
    "\n",
    "\n",
    "\n",
    "## **📊 Real-World Applications**\n",
    "🔹 **Spam Detection** – Classify emails as spam or not.  \n",
    "🔹 **Medical Diagnosis** – Identify whether a patient has a disease.  \n",
    "🔹 **Fraud Detection** – Detect fraudulent transactions.  \n",
    "🔹 **Face Recognition** – Simple classification of facial features.  \n",
    "\n",
    "\n",
    "\n",
    "# **🎯 Summary**\n",
    "🔹 **The Perceptron is the simplest neural network model** and works best for **linear classification tasks**.  \n",
    "🔹 It **learns** by updating weights using a simple rule.  \n",
    "🔹 **Single-layer perceptrons cannot solve non-linear problems** like XOR.  \n",
    "🔹 **Multi-layer perceptrons (MLPs) solve complex problems** using multiple layers and activation functions.  \n",
    "\n",
    "\n",
    "![](images/perceptron.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🧠 Neuron vs. Perceptron | Key Differences & Explanation**  \n",
    "\n",
    "Both **Neurons** and **Perceptrons** are fundamental concepts in artificial neural networks (ANNs), but they are **not the same**. Let’s break them down in a clear, structured way! 🚀🔥  \n",
    "\n",
    "\n",
    "\n",
    "## **🌍 What is a Neuron?**  \n",
    "\n",
    "A **Neuron** is the basic computational unit of a **biological brain** 🧠 and **artificial neural networks (ANNs)**.  \n",
    "\n",
    "📌 **Key Features:**  \n",
    "✅ Takes multiple **inputs** (features).  \n",
    "✅ Computes a **weighted sum** of inputs.  \n",
    "✅ Passes the result through an **activation function**.  \n",
    "✅ Produces an **output** (decision).  \n",
    "\n",
    "### **🛠 Structure of an Artificial Neuron**  \n",
    "A neuron in an artificial neural network (ANN) consists of:  \n",
    "1️⃣ **Inputs (x₁, x₂, …, xₙ)** – Represent data (e.g., image pixels, words in text).  \n",
    "2️⃣ **Weights (w₁, w₂, …, wₙ)** – Adjust the importance of each input.  \n",
    "3️⃣ **Bias (b)** – Helps shift the activation function.  \n",
    "4️⃣ **Summation Function (Σ)** – Computes weighted sum:  \n",
    "   $$\n",
    "   z = (w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "   $$\n",
    "5️⃣ **Activation Function (f(z))** – Applies a transformation (e.g., ReLU, Sigmoid, Tanh).  \n",
    "6️⃣ **Output (y)** – The final decision (e.g., classification result).  \n",
    "\n",
    "🔍 **Example:** A neuron in an image recognition model **detects edges** in a photo 📸.  \n",
    "\n",
    "\n",
    "\n",
    "## **⚡ What is a Perceptron?**  \n",
    "\n",
    "A **Perceptron** is a **type of artificial neuron** and was the **first** computational model used in neural networks. It follows a **step-by-step** approach to make decisions.  \n",
    "\n",
    "📌 **Key Features:**  \n",
    "✅ **Simplest form of a neural network** (single-layer).  \n",
    "✅ Uses a **Step Function** (or threshold activation).  \n",
    "✅ Works only for **binary classification** (e.g., Spam or Not Spam 📧).  \n",
    "✅ Can **only solve linearly separable problems**.  \n",
    "\n",
    "### **🛠 Structure of a Perceptron**  \n",
    "A perceptron follows the same structure as a neuron but:  \n",
    "- Uses a **Step Activation Function**:  \n",
    "  $$\n",
    "  f(z) =\n",
    "  \\begin{cases} \n",
    "  1, & \\text{if } z \\geq 0 \\\\\n",
    "  0, & \\text{if } z < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- Can **only classify data that is linearly separable** (e.g., AND, OR logic gates but NOT XOR).  \n",
    "- Uses a **simple learning rule** (weight updates using errors).  \n",
    "\n",
    "🔍 **Example:** A perceptron can classify whether a review is **positive or negative** based on words used in a sentence.  \n",
    "\n",
    "## **🔍 Neuron vs. Perceptron | Key Differences**\n",
    "| Feature | Neuron | Perceptron |\n",
    "|---------|--------|------------|\n",
    "| **Definition** | A single unit in an artificial neural network | A type of artificial neuron used for classification |\n",
    "| **Complexity** | More advanced, used in deep learning | Simpler, used in early neural networks |\n",
    "| **Activation Function** | Sigmoid, ReLU, Tanh, Softmax | Step Function |\n",
    "| **Output** | Can be **continuous** or **discrete** | Always **binary** (0 or 1) |\n",
    "| **Problem Solving** | Can solve **linear & non-linear** problems | Can only solve **linearly separable** problems |\n",
    "| **Use Case** | Used in **deep learning** (MLPs, CNNs, RNNs) | Used for **simple classification** tasks |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **🧪 Example for Better Understanding**  \n",
    "\n",
    "**🟢 Neuron (Modern ANN) Example:**  \n",
    "💡 Imagine a **face recognition system** detecting **multiple features** (eyes, nose, mouth). Each neuron **learns different aspects** of the face.  \n",
    "\n",
    "**🔴 Perceptron Example:**  \n",
    "💡 A simple perceptron can classify an **email as spam or not spam** based on **word frequency**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Summary**\n",
    "🔹 **A perceptron is just a simple type of neuron** used in early neural networks.  \n",
    "🔹 **Neurons in modern neural networks** are more advanced and use complex **activation functions**.  \n",
    "🔹 **Perceptrons are limited** to solving only **linear problems**, while neurons in deep learning **handle complex tasks** like image recognition and NLP.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🚀 Problems with the Perceptron Trick**\n",
    "The **Perceptron Trick** is an update rule used in the **Perceptron Algorithm** to adjust weights whenever a misclassification occurs. However, it has **limitations** that affect its practical use in modern machine learning.\n",
    "\n",
    "\n",
    "\n",
    "## **📌 Key Problems with the Perceptron Trick**\n",
    "### **1️⃣ Works Only for Linearly Separable Data**\n",
    "💡 **Issue:**  \n",
    "- The perceptron can only solve problems where data points **can be separated by a straight line** (or hyperplane in higher dimensions).  \n",
    "- If the data is **not linearly separable**, the perceptron **never converges** and keeps updating weights forever.\n",
    "\n",
    "🔍 **Example:**  \n",
    "- **AND, OR gates** ✅ → **Linearly separable** → Perceptron works!  \n",
    "- **XOR gate** ❌ → **Not linearly separable** → Perceptron fails!  \n",
    "\n",
    "🎯 **Solution?**  \n",
    "Use **multi-layer perceptrons (MLPs)** with activation functions like ReLU or Sigmoid.\n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ No Probability or Confidence in Predictions**\n",
    "💡 **Issue:**  \n",
    "- The perceptron only gives a **binary output** (0 or 1) without a probability score.\n",
    "- It **does not indicate how confident** the prediction is.\n",
    "\n",
    "🔍 **Example:**  \n",
    "- If we use **Logistic Regression** instead, it gives a probability like **70% chance of Class 1**.\n",
    "\n",
    "🎯 **Solution?**  \n",
    "Use **logistic regression** or **softmax activation** in modern neural networks.\n",
    "\n",
    "\n",
    "\n",
    "### **3️⃣ Cannot Handle Overlapping Classes (No Margin)**\n",
    "💡 **Issue:**  \n",
    "- The perceptron does **not maximize the margin** (distance between classes).\n",
    "- It finds **just one possible boundary**, but not necessarily the **best** one.\n",
    "\n",
    "🔍 **Example:**  \n",
    "- **Support Vector Machines (SVMs)** aim to **maximize the margin** for better generalization.\n",
    "\n",
    "🎯 **Solution?**  \n",
    "Use **SVMs** or **deep learning models** for better separation.\n",
    "\n",
    "\n",
    "\n",
    "### **4️⃣ Can Get Stuck in Infinite Loops (If Data is Not Separable)**\n",
    "💡 **Issue:**  \n",
    "- If the dataset is **not linearly separable**, the perceptron **keeps updating weights forever** and **never stops**.\n",
    "- The learning rule does not have a mechanism to **detect non-separability**.\n",
    "\n",
    "🔍 **Example:**  \n",
    "- If we train a perceptron on **XOR data**, it will **keep adjusting weights endlessly**.\n",
    "\n",
    "🎯 **Solution?**  \n",
    "- Use **stochastic gradient descent (SGD)** to minimize an error function.\n",
    "- Use **multi-layer perceptrons (MLPs) with hidden layers**.\n",
    "\n",
    "\n",
    "\n",
    "### **5️⃣ No Learning Beyond Simple Patterns**\n",
    "💡 **Issue:**  \n",
    "- The perceptron only learns **simple, straight-line relationships**.\n",
    "- It cannot learn **complex, non-linear patterns**.\n",
    "\n",
    "🔍 **Example:**  \n",
    "- **Recognizing handwritten digits (MNIST dataset)** → The perceptron **fails completely** because digits are complex.\n",
    "\n",
    "🎯 **Solution?**  \n",
    "- Use **Deep Neural Networks (DNNs)** with **hidden layers**.\n",
    "\n",
    "### **🚀 Summary Table**\n",
    "| Problem | Why It Fails | Solution |\n",
    "|---------|------------|----------|\n",
    "| Only works for linearly separable data | Cannot solve XOR-like problems | Use MLPs with activation functions |\n",
    "| No probability confidence | Only gives 0/1 outputs | Use logistic regression or softmax |\n",
    "| No margin maximization | Can misclassify overlapping data | Use SVMs or deep networks |\n",
    "| Stuck in infinite loops | Cannot detect non-separability | Use SGD or MLPs |\n",
    "| Cannot learn complex patterns | Only finds straight-line solutions | Use deep learning models |\n",
    "\n",
    "\n",
    "## **🔥 Final Thoughts**\n",
    "While the **Perceptron Trick** was an important early idea, **modern deep learning** has moved beyond it.  \n",
    "Today, we use:\n",
    "✅ **Activation functions (ReLU, Sigmoid, Softmax)**  \n",
    "✅ **Gradient-based optimization (SGD, Adam, etc.)**  \n",
    "✅ **Deep Neural Networks (DNNs) with multiple layers**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🚀 Loss Function in Machine Learning & Deep Learning (Full Explanation)**  \n",
    "\n",
    "## **🔹 What is a Loss Function?**  \n",
    "A **Loss Function** is a mathematical function that measures **how far off** a model's predictions are from the actual values (ground truth). It tells us **how \"bad\" the model is performing** by calculating the error.  \n",
    "\n",
    "### **🛠️ How It Works?**  \n",
    "1️⃣ The model makes a **prediction** (ŷ).  \n",
    "2️⃣ The loss function compares the **prediction (ŷ) with the actual value (y)**.  \n",
    "3️⃣ It calculates an **error score** (loss).  \n",
    "4️⃣ The optimizer **adjusts weights** to minimize this loss.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Why is a Loss Function Important?**  \n",
    "✅ Helps **train models** by showing errors.  \n",
    "✅ Guides **gradient descent** in updating weights.  \n",
    "✅ Prevents **overfitting or underfitting** by selecting appropriate loss functions.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 Types of Loss Functions**\n",
    "Loss functions vary based on the type of machine learning problem:  \n",
    "\n",
    "### **1️⃣ Regression Loss Functions (For Continuous Outputs)**\n",
    "Used when predicting real-valued numbers, e.g., house prices, stock prices.  \n",
    "\n",
    "| **Loss Function**  | **Formula** | **Use Case** |\n",
    "|--------------------|------------|-------------|\n",
    "| **Mean Squared Error (MSE)**  | $ \\frac{1}{N} \\sum (y - \\hat{y})^2 $  | General regression tasks |\n",
    "| **Mean Absolute Error (MAE)**  | $ \\frac{1}{N} \\sum |y - \\hat{y}| $  | When outliers are present |\n",
    "| **Huber Loss**  | Hybrid of MSE & MAE | When outliers exist but need smooth training |\n",
    "\n",
    "🔍 **MSE vs. MAE**  \n",
    "- **MSE** penalizes large errors **more** than MAE (due to squaring).  \n",
    "- **MAE** is **more robust** to outliers than MSE.  \n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ Classification Loss Functions (For Categorical Outputs)**\n",
    "Used for predicting discrete classes, e.g., **spam vs. not spam**, **cat vs. dog**.\n",
    "\n",
    "| **Loss Function**  | **Formula** | **Use Case** |\n",
    "|--------------------|------------|-------------|\n",
    "| **Binary Cross-Entropy**  | $ -\\frac{1}{N} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})] $  | Binary classification (Yes/No) |\n",
    "| **Categorical Cross-Entropy**  | $ -\\sum y_i \\log(\\hat{y}_i) $  | Multi-class classification |\n",
    "| **Sparse Categorical Cross-Entropy**  | Similar to Categorical CE but works with integer labels | Multi-class classification (Efficient for large classes) |\n",
    "\n",
    "🔍 **Cross-Entropy Explanation**  \n",
    "- Measures the difference between **true labels** and **predicted probabilities**.  \n",
    "- **Higher loss** → Wrong prediction.  \n",
    "- **Lower loss** → Correct prediction.  \n",
    "\n",
    "\n",
    "\n",
    "## **🧠 How is Loss Function Used in Training?**\n",
    "1️⃣ Model makes a **prediction (ŷ)**.  \n",
    "2️⃣ Compute **loss** using a loss function.  \n",
    "3️⃣ Use **Backpropagation + Gradient Descent** to update model weights.  \n",
    "4️⃣ Repeat until loss is minimized.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔬 Python Code Example: Loss Function in Action**\n",
    "### **Example 1: Mean Squared Error (MSE) for Regression**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Actual values (y) and predicted values (ŷ)\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# Compute MSE\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "📝 **Output:**  \n",
    "```\n",
    "Mean Squared Error: 0.375\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Example 2: Cross-Entropy Loss for Classification**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Actual label (one-hot encoded for 3 classes)\n",
    "y_true = np.array([0, 1, 0])  \n",
    "\n",
    "# Predicted probabilities for each class\n",
    "y_pred = np.array([0.2, 0.7, 0.1])\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = -np.sum(y_true * np.log(y_pred))\n",
    "print(\"Cross-Entropy Loss:\", loss)\n",
    "```\n",
    "📝 **Output:**  \n",
    "```\n",
    "Cross-Entropy Loss: 0.3567\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Summary Table: Choosing the Right Loss Function**\n",
    "| **Problem Type**  | **Loss Function**  | **Best For** |\n",
    "|------------------|------------------|-------------|\n",
    "| Regression  | **MSE (Mean Squared Error)**  | General regression problems |\n",
    "| Regression  | **MAE (Mean Absolute Error)**  | When dealing with outliers |\n",
    "| Classification  | **Binary Cross-Entropy**  | Binary classification (e.g., spam detection) |\n",
    "| Classification  | **Categorical Cross-Entropy**  | Multi-class problems (e.g., digit recognition) |\n",
    "| Classification  | **Sparse Categorical Cross-Entropy**  | Multi-class problems with large labels |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Final Thoughts**\n",
    "- **Loss functions** guide models in learning the correct patterns.  \n",
    "- **Choosing the right loss function** is critical for getting **good model performance**.  \n",
    "- In **deep learning**, loss functions are often paired with **optimizers** like **SGD, Adam, RMSprop**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🎯 Perceptron Loss Function (Full Explanation)**  \n",
    "\n",
    "The **Perceptron Loss Function** plays a vital role in **training the perceptron model** by guiding how the model should update its weights to improve predictions. Unlike other machine learning models, the perceptron uses a **simple yet effective loss function** that focuses on misclassifications.\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 What is the Perceptron Loss Function?**\n",
    "\n",
    "The **Perceptron Loss Function** (also called **Perceptron Criterion**) is used to penalize the model whenever it misclassifies a data point. It calculates **whether a sample is misclassified** or not and **updates the weights accordingly**.\n",
    "\n",
    "### **Key Idea:**  \n",
    "- If the model classifies a point correctly, no weight update happens.  \n",
    "- If the model classifies a point **incorrectly**, the weights are adjusted in the **direction that corrects** the mistake.  \n",
    "- The **loss function** only cares about the **misclassifications**, making it **simple** but effective for certain types of problems.\n",
    "\n",
    "\n",
    "\n",
    "## **🧠 Formula of Perceptron Loss**\n",
    "\n",
    "For a single training example, the **Perceptron Loss Function** can be defined as:\n",
    "\n",
    "$$\n",
    "L = \\begin{cases} \n",
    "  0 & \\text{if } y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) > 0 \\\\\n",
    "  -y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) & \\text{if } y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ = True label of the sample (either 1 or -1)\n",
    "- $ \\mathbf{x}_i $ = Input feature vector of the sample\n",
    "- $ \\mathbf{w} $ = Weight vector\n",
    "- $ b $ = Bias term\n",
    "- $ \\mathbf{w} \\cdot \\mathbf{x}_i + b $ = **Linear function** (dot product of weights and features, plus bias)\n",
    "- The loss is **zero** when the model predicts correctly (i.e., $ y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) > 0 $).\n",
    "- The loss is **negative** when the model predicts incorrectly and the weights are adjusted to minimize this negative value.\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 How Does the Perceptron Loss Function Work?**\n",
    "\n",
    "### **1️⃣ Correct Classification**\n",
    "When a point is correctly classified (i.e., the model's output is in the correct direction), there is **no loss**, and **no update** is made to the weights.\n",
    "\n",
    "- **Example:**  \n",
    "  For a point $ (x_1, x_2) $ with the true label $ y = 1 $, if the perceptron’s prediction is also **1**, the loss will be zero.\n",
    "  \n",
    "### **2️⃣ Incorrect Classification**\n",
    "When the model makes an **incorrect prediction**, the **loss is non-zero**, and the weights are updated. The loss increases as the model's output moves farther from the true label.\n",
    "\n",
    "- **Example:**  \n",
    "  If the true label is $ y = 1 $ but the perceptron predicts $ -1 $, the loss will be negative, and we will update the weights.\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Perceptron Loss in Action with Weight Update**\n",
    "\n",
    "### **How Weight Update Works:**\n",
    "When the model misclassifies a point, it adjusts its weights to move the decision boundary towards the misclassified point. The **weight update rule** is:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot y_i \\cdot \\mathbf{x}_i\n",
    "$$\n",
    "Where:\n",
    "- $ \\eta $ = Learning rate (controls how big the weight update is)\n",
    "- $ y_i $ = True label of the sample\n",
    "- $ \\mathbf{x}_i $ = Feature vector of the sample\n",
    "\n",
    "\n",
    "\n",
    "## **🎨 Visualizing Perceptron Loss Function**\n",
    "Here’s how the perceptron works step-by-step:  \n",
    "\n",
    "1. **Training Data**: We start with training data that is either linearly separable or not.  \n",
    "2. **Initial Weights**: The perceptron begins with random weights.\n",
    "3. **Prediction**: It calculates the prediction by applying the weights and activation function.  \n",
    "4. **Loss Calculation**: If the point is misclassified, it calculates the **loss**.\n",
    "5. **Weight Update**: The weights are adjusted in such a way that the point is correctly classified in future iterations.\n",
    "\n",
    "\n",
    "\n",
    "## **📊 Example of Perceptron Loss Function:**\n",
    "\n",
    "Consider a simple 2D dataset with two classes:\n",
    "- Class 1 ($ y = 1 $): (1, 1), (2, 2)\n",
    "- Class -1 ($ y = -1 $): (-1, -1), (-2, -2)\n",
    "\n",
    "Now, let’s assume that the initial perceptron’s weight vector is $ \\mathbf{w} = [0.5, -0.5] $ and bias $ b = 0 $.\n",
    "\n",
    "### Step 1: **Initial Prediction for Point (1, 1) with $ y = 1 $**\n",
    "\n",
    "$$\n",
    "\\text{Linear function: } \\mathbf{w} \\cdot \\mathbf{x}_i + b = (0.5 \\times 1) + (-0.5 \\times 1) + 0 = 0\n",
    "$$\n",
    "\n",
    "- The **prediction is 0** (no activation), so the perceptron misclassifies it.\n",
    "- The **loss is non-zero**, and we update the weights.\n",
    "\n",
    "### Step 2: **Update Weights**\n",
    "The weights are updated as follows:\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} + \\eta \\cdot y_i \\cdot \\mathbf{x}_i\n",
    "$$\n",
    "If $ \\eta = 1 $, the new weights become:\n",
    "$$\n",
    "\\mathbf{w} = [0.5, -0.5] + 1 \\cdot 1 \\cdot [1, 1] = [1.5, 0.5]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🎉 Summary of Perceptron Loss Function**\n",
    "\n",
    "- **Simple and Effective:** Only penalizes misclassified points.\n",
    "- **Zero Loss for Correct Predictions:** The perceptron only updates when it makes mistakes.\n",
    "- **Weight Update:** When the model misclassifies a point, it adjusts the weights in the direction that **corrects the error**.\n",
    "- **Doesn't Handle Complex Relationships:** The perceptron loss works well for linearly separable data, but struggles with non-linear patterns.\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Final Thought:**\n",
    "The **Perceptron Loss Function** is **straightforward** but has **limitations** when dealing with complex patterns. It laid the foundation for **more advanced loss functions** in **deep learning** (like cross-entropy) and helps understand the core ideas behind **classification tasks**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
