{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸ§  Perceptron: The Foundation of Neural Networks | Full Explanation**  \n",
    "\n",
    "The **Perceptron** is the simplest type of **artificial neural network**, acting as a **building block for deep learning models**. It was introduced by **Frank Rosenblatt in 1958** and is the basis for more complex neural networks like **Multi-Layer Perceptrons (MLPs)** and **Deep Learning models**.  \n",
    "\n",
    "Let's dive deep into how perceptrons work, their structure, limitations, and applications! ğŸš€ğŸ”¥  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒ What is a Perceptron?**  \n",
    "A **Perceptron** is a type of **binary classifier** that **learns** to make decisions by processing inputs through **weights and an activation function**. It is a fundamental concept in **supervised learning** used for **classification tasks**.  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "A perceptron can classify whether an email is **spam or not spam** ğŸ“§, based on input features like **keywords, sender, and frequency of words**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ› ï¸ Structure of a Perceptron**  \n",
    "\n",
    "A perceptron consists of:  \n",
    "âœ… **Inputs (Features)** â€“ Represent data points (e.g., image pixels, text words).  \n",
    "âœ… **Weights (ğ‘¤)** â€“ Adjust the importance of each input.  \n",
    "âœ… **Bias (b)** â€“ Helps shift the activation function for better learning.  \n",
    "âœ… **Summation Function (Î£)** â€“ Computes the weighted sum of inputs.  \n",
    "âœ… **Activation Function** â€“ Determines the final decision (e.g., Step Function).  \n",
    "âœ… **Output (Prediction)** â€“ The final classification (e.g., 0 or 1).  \n",
    "\n",
    "ğŸ“Œ **Mathematical Formula:**  \n",
    "$$\n",
    "y = f(w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "$$\n",
    "where:  \n",
    "ğŸ”¹ **xâ‚, xâ‚‚, ..., xâ‚™** = Inputs  \n",
    "ğŸ”¹ **wâ‚, wâ‚‚, ..., wâ‚™** = Weights  \n",
    "ğŸ”¹ **b** = Bias  \n",
    "ğŸ”¹ **f()** = Activation Function  \n",
    "\n",
    "\n",
    "\n",
    "## **âš¡ How Perceptron Works? (Step-by-Step)**\n",
    "1ï¸âƒ£ **Initialize Weights & Bias** â€“ Set initial values (random or zeros).  \n",
    "2ï¸âƒ£ **Compute Weighted Sum** â€“ Multiply inputs with weights and add bias.  \n",
    "3ï¸âƒ£ **Apply Activation Function** â€“ Decide the output based on a threshold.  \n",
    "4ï¸âƒ£ **Update Weights (Learning Process)** â€“ Adjust weights using errors.  \n",
    "5ï¸âƒ£ **Repeat for All Training Data** â€“ Learn patterns and improve accuracy.  \n",
    "\n",
    "ğŸ“Œ **Example Calculation:**  \n",
    "Assume we have:  \n",
    "ğŸ”¹ **Inputs:** xâ‚ = 1, xâ‚‚ = -1  \n",
    "ğŸ”¹ **Weights:** wâ‚ = 0.5, wâ‚‚ = 0.3  \n",
    "ğŸ”¹ **Bias:** b = -0.1  \n",
    "\n",
    "$$\n",
    "\\text{Weighted Sum} = (1 \\times 0.5) + (-1 \\times 0.3) + (-0.1) = 0.1\n",
    "$$  \n",
    "\n",
    "If we use a **step activation function**:  \n",
    "ğŸ”¹ **If Sum â‰¥ 0 â†’ Output = 1**  \n",
    "ğŸ”¹ **If Sum < 0 â†’ Output = 0**  \n",
    "\n",
    "Since **0.1 â‰¥ 0**, the output is **1 (Positive Class)**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§ª Types of Perceptrons**\n",
    "1ï¸âƒ£ **Single Layer Perceptron**  \n",
    "âœ… Has **one layer** (only input and output).  \n",
    "âœ… Can solve **linear problems** (e.g., AND, OR logic gates).  \n",
    "âœ… Cannot solve **non-linear problems** (e.g., XOR gate).  \n",
    "\n",
    "2ï¸âƒ£ **Multi-Layer Perceptron (MLP)**  \n",
    "âœ… Has **multiple layers** (input, hidden, output).  \n",
    "âœ… Can learn **complex patterns** (used in deep learning).  \n",
    "âœ… Uses **backpropagation** for learning.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš§ Limitations of Perceptron**\n",
    "ğŸ”´ **Cannot Solve Non-Linear Problems** â€“ Example: **XOR function** cannot be solved using a single-layer perceptron.  \n",
    "ğŸ”´ **Limited Learning Ability** â€“ Works only for **linearly separable data**.  \n",
    "ğŸ”´ **Step Activation Function is Too Simple** â€“ More advanced activations (ReLU, Sigmoid) are needed for complex tasks.  \n",
    "\n",
    "ğŸ”¹ **Solution?** Use **Multi-Layer Perceptrons (MLPs) with activation functions** like **ReLU, Sigmoid, and Tanh**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Advantages of Perceptron**\n",
    "âœ… **Fast & Efficient** â€“ Simple computation, making it fast for binary classification.  \n",
    "âœ… **Foundation for Neural Networks** â€“ Forms the basis of **MLPs and deep learning**.  \n",
    "âœ… **Works Well for Linearly Separable Data** â€“ Can classify simple patterns accurately.  \n",
    "\n",
    "\n",
    "## **ğŸ“Œ Perceptron vs. Modern Neural Networks**\n",
    "| Feature | Perceptron | Modern Neural Networks |\n",
    "|---------|------------|------------------------|\n",
    "| **Layers** | Single Layer | Multiple Layers |\n",
    "| **Learning Algorithm** | Simple Weight Update | Backpropagation |\n",
    "| **Activation Function** | Step Function | ReLU, Sigmoid, Softmax |\n",
    "| **Handles Complex Data?** | No | Yes |\n",
    "| **Can Solve XOR?** | âŒ No | âœ… Yes |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š Real-World Applications**\n",
    "ğŸ”¹ **Spam Detection** â€“ Classify emails as spam or not.  \n",
    "ğŸ”¹ **Medical Diagnosis** â€“ Identify whether a patient has a disease.  \n",
    "ğŸ”¹ **Fraud Detection** â€“ Detect fraudulent transactions.  \n",
    "ğŸ”¹ **Face Recognition** â€“ Simple classification of facial features.  \n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ¯ Summary**\n",
    "ğŸ”¹ **The Perceptron is the simplest neural network model** and works best for **linear classification tasks**.  \n",
    "ğŸ”¹ It **learns** by updating weights using a simple rule.  \n",
    "ğŸ”¹ **Single-layer perceptrons cannot solve non-linear problems** like XOR.  \n",
    "ğŸ”¹ **Multi-layer perceptrons (MLPs) solve complex problems** using multiple layers and activation functions.  \n",
    "\n",
    "\n",
    "![](images/perceptron.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸ§  Neuron vs. Perceptron | Key Differences & Explanation**  \n",
    "\n",
    "Both **Neurons** and **Perceptrons** are fundamental concepts in artificial neural networks (ANNs), but they are **not the same**. Letâ€™s break them down in a clear, structured way! ğŸš€ğŸ”¥  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒ What is a Neuron?**  \n",
    "\n",
    "A **Neuron** is the basic computational unit of a **biological brain** ğŸ§  and **artificial neural networks (ANNs)**.  \n",
    "\n",
    "ğŸ“Œ **Key Features:**  \n",
    "âœ… Takes multiple **inputs** (features).  \n",
    "âœ… Computes a **weighted sum** of inputs.  \n",
    "âœ… Passes the result through an **activation function**.  \n",
    "âœ… Produces an **output** (decision).  \n",
    "\n",
    "### **ğŸ›  Structure of an Artificial Neuron**  \n",
    "A neuron in an artificial neural network (ANN) consists of:  \n",
    "1ï¸âƒ£ **Inputs (xâ‚, xâ‚‚, â€¦, xâ‚™)** â€“ Represent data (e.g., image pixels, words in text).  \n",
    "2ï¸âƒ£ **Weights (wâ‚, wâ‚‚, â€¦, wâ‚™)** â€“ Adjust the importance of each input.  \n",
    "3ï¸âƒ£ **Bias (b)** â€“ Helps shift the activation function.  \n",
    "4ï¸âƒ£ **Summation Function (Î£)** â€“ Computes weighted sum:  \n",
    "   $$\n",
    "   z = (w_1x_1 + w_2x_2 + ... + w_nx_n + b)\n",
    "   $$\n",
    "5ï¸âƒ£ **Activation Function (f(z))** â€“ Applies a transformation (e.g., ReLU, Sigmoid, Tanh).  \n",
    "6ï¸âƒ£ **Output (y)** â€“ The final decision (e.g., classification result).  \n",
    "\n",
    "ğŸ” **Example:** A neuron in an image recognition model **detects edges** in a photo ğŸ“¸.  \n",
    "\n",
    "\n",
    "\n",
    "## **âš¡ What is a Perceptron?**  \n",
    "\n",
    "A **Perceptron** is a **type of artificial neuron** and was the **first** computational model used in neural networks. It follows a **step-by-step** approach to make decisions.  \n",
    "\n",
    "ğŸ“Œ **Key Features:**  \n",
    "âœ… **Simplest form of a neural network** (single-layer).  \n",
    "âœ… Uses a **Step Function** (or threshold activation).  \n",
    "âœ… Works only for **binary classification** (e.g., Spam or Not Spam ğŸ“§).  \n",
    "âœ… Can **only solve linearly separable problems**.  \n",
    "\n",
    "### **ğŸ›  Structure of a Perceptron**  \n",
    "A perceptron follows the same structure as a neuron but:  \n",
    "- Uses a **Step Activation Function**:  \n",
    "  $$\n",
    "  f(z) =\n",
    "  \\begin{cases} \n",
    "  1, & \\text{if } z \\geq 0 \\\\\n",
    "  0, & \\text{if } z < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "- Can **only classify data that is linearly separable** (e.g., AND, OR logic gates but NOT XOR).  \n",
    "- Uses a **simple learning rule** (weight updates using errors).  \n",
    "\n",
    "ğŸ” **Example:** A perceptron can classify whether a review is **positive or negative** based on words used in a sentence.  \n",
    "\n",
    "## **ğŸ” Neuron vs. Perceptron | Key Differences**\n",
    "| Feature | Neuron | Perceptron |\n",
    "|---------|--------|------------|\n",
    "| **Definition** | A single unit in an artificial neural network | A type of artificial neuron used for classification |\n",
    "| **Complexity** | More advanced, used in deep learning | Simpler, used in early neural networks |\n",
    "| **Activation Function** | Sigmoid, ReLU, Tanh, Softmax | Step Function |\n",
    "| **Output** | Can be **continuous** or **discrete** | Always **binary** (0 or 1) |\n",
    "| **Problem Solving** | Can solve **linear & non-linear** problems | Can only solve **linearly separable** problems |\n",
    "| **Use Case** | Used in **deep learning** (MLPs, CNNs, RNNs) | Used for **simple classification** tasks |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§ª Example for Better Understanding**  \n",
    "\n",
    "**ğŸŸ¢ Neuron (Modern ANN) Example:**  \n",
    "ğŸ’¡ Imagine a **face recognition system** detecting **multiple features** (eyes, nose, mouth). Each neuron **learns different aspects** of the face.  \n",
    "\n",
    "**ğŸ”´ Perceptron Example:**  \n",
    "ğŸ’¡ A simple perceptron can classify an **email as spam or not spam** based on **word frequency**.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Summary**\n",
    "ğŸ”¹ **A perceptron is just a simple type of neuron** used in early neural networks.  \n",
    "ğŸ”¹ **Neurons in modern neural networks** are more advanced and use complex **activation functions**.  \n",
    "ğŸ”¹ **Perceptrons are limited** to solving only **linear problems**, while neurons in deep learning **handle complex tasks** like image recognition and NLP.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸš€ Problems with the Perceptron Trick**\n",
    "The **Perceptron Trick** is an update rule used in the **Perceptron Algorithm** to adjust weights whenever a misclassification occurs. However, it has **limitations** that affect its practical use in modern machine learning.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Key Problems with the Perceptron Trick**\n",
    "### **1ï¸âƒ£ Works Only for Linearly Separable Data**\n",
    "ğŸ’¡ **Issue:**  \n",
    "- The perceptron can only solve problems where data points **can be separated by a straight line** (or hyperplane in higher dimensions).  \n",
    "- If the data is **not linearly separable**, the perceptron **never converges** and keeps updating weights forever.\n",
    "\n",
    "ğŸ” **Example:**  \n",
    "- **AND, OR gates** âœ… â†’ **Linearly separable** â†’ Perceptron works!  \n",
    "- **XOR gate** âŒ â†’ **Not linearly separable** â†’ Perceptron fails!  \n",
    "\n",
    "ğŸ¯ **Solution?**  \n",
    "Use **multi-layer perceptrons (MLPs)** with activation functions like ReLU or Sigmoid.\n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ No Probability or Confidence in Predictions**\n",
    "ğŸ’¡ **Issue:**  \n",
    "- The perceptron only gives a **binary output** (0 or 1) without a probability score.\n",
    "- It **does not indicate how confident** the prediction is.\n",
    "\n",
    "ğŸ” **Example:**  \n",
    "- If we use **Logistic Regression** instead, it gives a probability like **70% chance of Class 1**.\n",
    "\n",
    "ğŸ¯ **Solution?**  \n",
    "Use **logistic regression** or **softmax activation** in modern neural networks.\n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Cannot Handle Overlapping Classes (No Margin)**\n",
    "ğŸ’¡ **Issue:**  \n",
    "- The perceptron does **not maximize the margin** (distance between classes).\n",
    "- It finds **just one possible boundary**, but not necessarily the **best** one.\n",
    "\n",
    "ğŸ” **Example:**  \n",
    "- **Support Vector Machines (SVMs)** aim to **maximize the margin** for better generalization.\n",
    "\n",
    "ğŸ¯ **Solution?**  \n",
    "Use **SVMs** or **deep learning models** for better separation.\n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Can Get Stuck in Infinite Loops (If Data is Not Separable)**\n",
    "ğŸ’¡ **Issue:**  \n",
    "- If the dataset is **not linearly separable**, the perceptron **keeps updating weights forever** and **never stops**.\n",
    "- The learning rule does not have a mechanism to **detect non-separability**.\n",
    "\n",
    "ğŸ” **Example:**  \n",
    "- If we train a perceptron on **XOR data**, it will **keep adjusting weights endlessly**.\n",
    "\n",
    "ğŸ¯ **Solution?**  \n",
    "- Use **stochastic gradient descent (SGD)** to minimize an error function.\n",
    "- Use **multi-layer perceptrons (MLPs) with hidden layers**.\n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ No Learning Beyond Simple Patterns**\n",
    "ğŸ’¡ **Issue:**  \n",
    "- The perceptron only learns **simple, straight-line relationships**.\n",
    "- It cannot learn **complex, non-linear patterns**.\n",
    "\n",
    "ğŸ” **Example:**  \n",
    "- **Recognizing handwritten digits (MNIST dataset)** â†’ The perceptron **fails completely** because digits are complex.\n",
    "\n",
    "ğŸ¯ **Solution?**  \n",
    "- Use **Deep Neural Networks (DNNs)** with **hidden layers**.\n",
    "\n",
    "### **ğŸš€ Summary Table**\n",
    "| Problem | Why It Fails | Solution |\n",
    "|---------|------------|----------|\n",
    "| Only works for linearly separable data | Cannot solve XOR-like problems | Use MLPs with activation functions |\n",
    "| No probability confidence | Only gives 0/1 outputs | Use logistic regression or softmax |\n",
    "| No margin maximization | Can misclassify overlapping data | Use SVMs or deep networks |\n",
    "| Stuck in infinite loops | Cannot detect non-separability | Use SGD or MLPs |\n",
    "| Cannot learn complex patterns | Only finds straight-line solutions | Use deep learning models |\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Final Thoughts**\n",
    "While the **Perceptron Trick** was an important early idea, **modern deep learning** has moved beyond it.  \n",
    "Today, we use:\n",
    "âœ… **Activation functions (ReLU, Sigmoid, Softmax)**  \n",
    "âœ… **Gradient-based optimization (SGD, Adam, etc.)**  \n",
    "âœ… **Deep Neural Networks (DNNs) with multiple layers**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸš€ Loss Function in Machine Learning & Deep Learning (Full Explanation)**  \n",
    "\n",
    "## **ğŸ”¹ What is a Loss Function?**  \n",
    "A **Loss Function** is a mathematical function that measures **how far off** a model's predictions are from the actual values (ground truth). It tells us **how \"bad\" the model is performing** by calculating the error.  \n",
    "\n",
    "### **ğŸ› ï¸ How It Works?**  \n",
    "1ï¸âƒ£ The model makes a **prediction** (Å·).  \n",
    "2ï¸âƒ£ The loss function compares the **prediction (Å·) with the actual value (y)**.  \n",
    "3ï¸âƒ£ It calculates an **error score** (loss).  \n",
    "4ï¸âƒ£ The optimizer **adjusts weights** to minimize this loss.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Why is a Loss Function Important?**  \n",
    "âœ… Helps **train models** by showing errors.  \n",
    "âœ… Guides **gradient descent** in updating weights.  \n",
    "âœ… Prevents **overfitting or underfitting** by selecting appropriate loss functions.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Types of Loss Functions**\n",
    "Loss functions vary based on the type of machine learning problem:  \n",
    "\n",
    "### **1ï¸âƒ£ Regression Loss Functions (For Continuous Outputs)**\n",
    "Used when predicting real-valued numbers, e.g., house prices, stock prices.  \n",
    "\n",
    "| **Loss Function**  | **Formula** | **Use Case** |\n",
    "|--------------------|------------|-------------|\n",
    "| **Mean Squared Error (MSE)**  | $ \\frac{1}{N} \\sum (y - \\hat{y})^2 $  | General regression tasks |\n",
    "| **Mean Absolute Error (MAE)**  | $ \\frac{1}{N} \\sum |y - \\hat{y}| $  | When outliers are present |\n",
    "| **Huber Loss**  | Hybrid of MSE & MAE | When outliers exist but need smooth training |\n",
    "\n",
    "ğŸ” **MSE vs. MAE**  \n",
    "- **MSE** penalizes large errors **more** than MAE (due to squaring).  \n",
    "- **MAE** is **more robust** to outliers than MSE.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Classification Loss Functions (For Categorical Outputs)**\n",
    "Used for predicting discrete classes, e.g., **spam vs. not spam**, **cat vs. dog**.\n",
    "\n",
    "| **Loss Function**  | **Formula** | **Use Case** |\n",
    "|--------------------|------------|-------------|\n",
    "| **Binary Cross-Entropy**  | $ -\\frac{1}{N} \\sum [y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})] $  | Binary classification (Yes/No) |\n",
    "| **Categorical Cross-Entropy**  | $ -\\sum y_i \\log(\\hat{y}_i) $  | Multi-class classification |\n",
    "| **Sparse Categorical Cross-Entropy**  | Similar to Categorical CE but works with integer labels | Multi-class classification (Efficient for large classes) |\n",
    "\n",
    "ğŸ” **Cross-Entropy Explanation**  \n",
    "- Measures the difference between **true labels** and **predicted probabilities**.  \n",
    "- **Higher loss** â†’ Wrong prediction.  \n",
    "- **Lower loss** â†’ Correct prediction.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  How is Loss Function Used in Training?**\n",
    "1ï¸âƒ£ Model makes a **prediction (Å·)**.  \n",
    "2ï¸âƒ£ Compute **loss** using a loss function.  \n",
    "3ï¸âƒ£ Use **Backpropagation + Gradient Descent** to update model weights.  \n",
    "4ï¸âƒ£ Repeat until loss is minimized.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¬ Python Code Example: Loss Function in Action**\n",
    "### **Example 1: Mean Squared Error (MSE) for Regression**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Actual values (y) and predicted values (Å·)\n",
    "y_true = np.array([3, -0.5, 2, 7])\n",
    "y_pred = np.array([2.5, 0.0, 2, 8])\n",
    "\n",
    "# Compute MSE\n",
    "mse = np.mean((y_true - y_pred) ** 2)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "```\n",
    "ğŸ“ **Output:**  \n",
    "```\n",
    "Mean Squared Error: 0.375\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Example 2: Cross-Entropy Loss for Classification**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Actual label (one-hot encoded for 3 classes)\n",
    "y_true = np.array([0, 1, 0])  \n",
    "\n",
    "# Predicted probabilities for each class\n",
    "y_pred = np.array([0.2, 0.7, 0.1])\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "loss = -np.sum(y_true * np.log(y_pred))\n",
    "print(\"Cross-Entropy Loss:\", loss)\n",
    "```\n",
    "ğŸ“ **Output:**  \n",
    "```\n",
    "Cross-Entropy Loss: 0.3567\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Summary Table: Choosing the Right Loss Function**\n",
    "| **Problem Type**  | **Loss Function**  | **Best For** |\n",
    "|------------------|------------------|-------------|\n",
    "| Regression  | **MSE (Mean Squared Error)**  | General regression problems |\n",
    "| Regression  | **MAE (Mean Absolute Error)**  | When dealing with outliers |\n",
    "| Classification  | **Binary Cross-Entropy**  | Binary classification (e.g., spam detection) |\n",
    "| Classification  | **Categorical Cross-Entropy**  | Multi-class problems (e.g., digit recognition) |\n",
    "| Classification  | **Sparse Categorical Cross-Entropy**  | Multi-class problems with large labels |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Final Thoughts**\n",
    "- **Loss functions** guide models in learning the correct patterns.  \n",
    "- **Choosing the right loss function** is critical for getting **good model performance**.  \n",
    "- In **deep learning**, loss functions are often paired with **optimizers** like **SGD, Adam, RMSprop**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ğŸ¯ Perceptron Loss Function (Full Explanation)**  \n",
    "\n",
    "The **Perceptron Loss Function** plays a vital role in **training the perceptron model** by guiding how the model should update its weights to improve predictions. Unlike other machine learning models, the perceptron uses a **simple yet effective loss function** that focuses on misclassifications.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ What is the Perceptron Loss Function?**\n",
    "\n",
    "The **Perceptron Loss Function** (also called **Perceptron Criterion**) is used to penalize the model whenever it misclassifies a data point. It calculates **whether a sample is misclassified** or not and **updates the weights accordingly**.\n",
    "\n",
    "### **Key Idea:**  \n",
    "- If the model classifies a point correctly, no weight update happens.  \n",
    "- If the model classifies a point **incorrectly**, the weights are adjusted in the **direction that corrects** the mistake.  \n",
    "- The **loss function** only cares about the **misclassifications**, making it **simple** but effective for certain types of problems.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§  Formula of Perceptron Loss**\n",
    "\n",
    "For a single training example, the **Perceptron Loss Function** can be defined as:\n",
    "\n",
    "$$\n",
    "L = \\begin{cases} \n",
    "  0 & \\text{if } y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) > 0 \\\\\n",
    "  -y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) & \\text{if } y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ = True label of the sample (either 1 or -1)\n",
    "- $ \\mathbf{x}_i $ = Input feature vector of the sample\n",
    "- $ \\mathbf{w} $ = Weight vector\n",
    "- $ b $ = Bias term\n",
    "- $ \\mathbf{w} \\cdot \\mathbf{x}_i + b $ = **Linear function** (dot product of weights and features, plus bias)\n",
    "- The loss is **zero** when the model predicts correctly (i.e., $ y_i \\cdot (\\mathbf{w} \\cdot \\mathbf{x}_i + b) > 0 $).\n",
    "- The loss is **negative** when the model predicts incorrectly and the weights are adjusted to minimize this negative value.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ How Does the Perceptron Loss Function Work?**\n",
    "\n",
    "### **1ï¸âƒ£ Correct Classification**\n",
    "When a point is correctly classified (i.e., the model's output is in the correct direction), there is **no loss**, and **no update** is made to the weights.\n",
    "\n",
    "- **Example:**  \n",
    "  For a point $ (x_1, x_2) $ with the true label $ y = 1 $, if the perceptronâ€™s prediction is also **1**, the loss will be zero.\n",
    "  \n",
    "### **2ï¸âƒ£ Incorrect Classification**\n",
    "When the model makes an **incorrect prediction**, the **loss is non-zero**, and the weights are updated. The loss increases as the model's output moves farther from the true label.\n",
    "\n",
    "- **Example:**  \n",
    "  If the true label is $ y = 1 $ but the perceptron predicts $ -1 $, the loss will be negative, and we will update the weights.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Perceptron Loss in Action with Weight Update**\n",
    "\n",
    "### **How Weight Update Works:**\n",
    "When the model misclassifies a point, it adjusts its weights to move the decision boundary towards the misclassified point. The **weight update rule** is:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot y_i \\cdot \\mathbf{x}_i\n",
    "$$\n",
    "Where:\n",
    "- $ \\eta $ = Learning rate (controls how big the weight update is)\n",
    "- $ y_i $ = True label of the sample\n",
    "- $ \\mathbf{x}_i $ = Feature vector of the sample\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¨ Visualizing Perceptron Loss Function**\n",
    "Hereâ€™s how the perceptron works step-by-step:  \n",
    "\n",
    "1. **Training Data**: We start with training data that is either linearly separable or not.  \n",
    "2. **Initial Weights**: The perceptron begins with random weights.\n",
    "3. **Prediction**: It calculates the prediction by applying the weights and activation function.  \n",
    "4. **Loss Calculation**: If the point is misclassified, it calculates the **loss**.\n",
    "5. **Weight Update**: The weights are adjusted in such a way that the point is correctly classified in future iterations.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Š Example of Perceptron Loss Function:**\n",
    "\n",
    "Consider a simple 2D dataset with two classes:\n",
    "- Class 1 ($ y = 1 $): (1, 1), (2, 2)\n",
    "- Class -1 ($ y = -1 $): (-1, -1), (-2, -2)\n",
    "\n",
    "Now, letâ€™s assume that the initial perceptronâ€™s weight vector is $ \\mathbf{w} = [0.5, -0.5] $ and bias $ b = 0 $.\n",
    "\n",
    "### Step 1: **Initial Prediction for Point (1, 1) with $ y = 1 $**\n",
    "\n",
    "$$\n",
    "\\text{Linear function: } \\mathbf{w} \\cdot \\mathbf{x}_i + b = (0.5 \\times 1) + (-0.5 \\times 1) + 0 = 0\n",
    "$$\n",
    "\n",
    "- The **prediction is 0** (no activation), so the perceptron misclassifies it.\n",
    "- The **loss is non-zero**, and we update the weights.\n",
    "\n",
    "### Step 2: **Update Weights**\n",
    "The weights are updated as follows:\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} + \\eta \\cdot y_i \\cdot \\mathbf{x}_i\n",
    "$$\n",
    "If $ \\eta = 1 $, the new weights become:\n",
    "$$\n",
    "\\mathbf{w} = [0.5, -0.5] + 1 \\cdot 1 \\cdot [1, 1] = [1.5, 0.5]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ‰ Summary of Perceptron Loss Function**\n",
    "\n",
    "- **Simple and Effective:** Only penalizes misclassified points.\n",
    "- **Zero Loss for Correct Predictions:** The perceptron only updates when it makes mistakes.\n",
    "- **Weight Update:** When the model misclassifies a point, it adjusts the weights in the direction that **corrects the error**.\n",
    "- **Doesn't Handle Complex Relationships:** The perceptron loss works well for linearly separable data, but struggles with non-linear patterns.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Final Thought:**\n",
    "The **Perceptron Loss Function** is **straightforward** but has **limitations** when dealing with complex patterns. It laid the foundation for **more advanced loss functions** in **deep learning** (like cross-entropy) and helps understand the core ideas behind **classification tasks**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
