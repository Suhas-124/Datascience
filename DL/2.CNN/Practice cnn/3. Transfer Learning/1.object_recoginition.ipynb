{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 2. Define a preprocessing function for tf.data.Dataset\n",
    "def preprocess_image_label(image, label):\n",
    "    image = tf.image.resize(image, (224, 224))  # Resize to 224x224 for VGG16\n",
    "    image = preprocess_input(image)              # Apply VGG16-specific preprocessing\n",
    "    return image, label\n",
    "\n",
    "# 3. Create tf.data.Dataset pipelines\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_image_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").shuffle(buffer_size=10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_image_label, num_parallel_calls=tf.data.AUTOTUNE\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 4. Load the pre-trained VGG16 model (without the top classification layer)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# 5. Build the custom model on top of VGG16\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 6. Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# 7. Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 8. Train the model (no need for steps_per_epoch or validation_steps)\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=val_dataset\n",
    ")\n",
    "\n",
    "# 9. Plot training and validation accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain the custom preprocessing function you provided in a clear and simple way.\n",
    "\n",
    "The function you're looking at is:\n",
    "\n",
    "```python\n",
    "def custom_preprocess(image):\n",
    "    image = tf.image.resize(image, (224, 224))  # Resize to 224x224 for VGG16\n",
    "    image = preprocess_input(image)  # Apply VGG16-specific preprocessing\n",
    "    return image\n",
    "```\n",
    "\n",
    "This function is designed to prepare an image so that it can be used with the VGG16 model, which is a pre-trained deep learning model commonly used for tasks like image classification. Let’s break down what it does step by step:\n",
    "\n",
    "### 1. **Resizing the Image**\n",
    "- **What happens**: The line `tf.image.resize(image, (224, 224))` takes the input image and resizes it to a size of 224x224 pixels.\n",
    "- **Why it’s needed**: VGG16 is built to work with images that are exactly 224x224 pixels. If you give it an image of a different size, it won’t know how to process it properly, and you’ll either get an error or incorrect results. Think of it like fitting a puzzle piece into the right slot—VGG16 expects a specific \"shape\" of image.\n",
    "\n",
    "### 2. **Applying VGG16-Specific Preprocessing**\n",
    "- **What happens**: The line `preprocess_input(image)` adjusts the image’s colors and values in a way that matches how VGG16 was originally trained.\n",
    "- **Details of this step**:\n",
    "  - **Color conversion**: It changes the image from RGB (red, green, blue) to BGR (blue, green, red) because that’s the format VGG16 was trained on.\n",
    "  - **Normalization**: It subtracts specific numbers (mean values) from each color channel (blue, green, red). These numbers come from the ImageNet dataset, which was used to train VGG16. This process \"centers\" the data around zero, making it easier for the model to analyze.\n",
    "- **Why it’s needed**: When VGG16 was trained, all the images it learned from were processed in this exact way. If you don’t preprocess your image the same way, it’s like speaking a different language to the model—it won’t understand the input properly, and its predictions will be off.\n",
    "\n",
    "### 3. **What the Function Returns**\n",
    "- After resizing and preprocessing, the function returns the modified image. This image is now ready to be fed into the VGG16 model for tasks like training or making predictions.\n",
    "\n",
    "### Why This Matters\n",
    "- VGG16 is a pre-trained model, meaning it’s already learned a lot from a huge dataset (ImageNet). To take advantage of what it’s learned, your images need to look like the ones it was trained on. This function ensures that:\n",
    "  - The size matches (224x224).\n",
    "  - The colors and values are adjusted to fit VGG16’s expectations.\n",
    "\n",
    "### Where It’s Used\n",
    "- This function is typically used with something like `ImageDataGenerator`, a tool in TensorFlow/Keras that feeds images to the model during training or prediction. It makes sure every image—whether from your training data or a new one you want to test—is processed consistently.\n",
    "\n",
    "### In Simple Terms\n",
    "Imagine you’re sending a letter to someone who only reads a specific font size and language. You’d need to adjust your letter (resize it and translate it) so they can understand it. That’s what this function does for images going into VGG16—it gets them into the right \"format\" so the model can \"read\" them correctly.\n",
    "\n",
    "Does that clear things up? Let me know if you’d like more details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain this code step by step so you can understand what’s happening with `train_datagen` and `val_datagen` in your query:\n",
    "\n",
    "```python\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=custom_preprocess\n",
    ")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=custom_preprocess\n",
    ")\n",
    "```\n",
    "\n",
    "This code is setting up two **data generators** using the `ImageDataGenerator` class from TensorFlow/Keras. These generators are tools that help prepare and feed images to a deep learning model efficiently. Here’s a breakdown of what’s going on:\n",
    "\n",
    "\n",
    "\n",
    "### What is `ImageDataGenerator`?\n",
    "`ImageDataGenerator` is a utility in Keras that:\n",
    "- **Loads images**: It can pull images from a directory or an array.\n",
    "- **Preprocesses images**: It applies transformations or adjustments to the images (like resizing or normalization).\n",
    "- **Delivers images in batches**: Instead of loading all images into memory at once (which could overwhelm your computer), it processes and sends small groups (batches) of images to the model as needed.\n",
    "\n",
    "This is super helpful when you’re working with large datasets, like in deep learning for tasks such as image classification.\n",
    "\n",
    "\n",
    "\n",
    "### What Does This Code Do?\n",
    "In your code:\n",
    "- **`train_datagen`**: This is a generator for the **training data** (the images used to teach the model).\n",
    "- **`val_datagen`**: This is a generator for the **validation data** (the images used to test how well the model is learning).\n",
    "\n",
    "Both generators are created with a parameter called `preprocessing_function`, which is set to `custom_preprocess`. Let’s dive into what that means.\n",
    "\n",
    "\n",
    "\n",
    "### The `preprocessing_function` Parameter\n",
    "- **What it does**: The `preprocessing_function=custom_preprocess` tells `ImageDataGenerator` to run every image through a function called `custom_preprocess` before sending it to the model.\n",
    "- **Why it’s there**: The `custom_preprocess` function (which isn’t shown here but is defined elsewhere in your code) likely does specific preprocessing steps. For example, if you’re using a pre-trained model like VGG16, it might:\n",
    "  - Resize images to 224x224 pixels (the size VGG16 expects).\n",
    "  - Adjust the colors or normalize the pixel values to match what VGG16 was trained on.\n",
    "- **When it happens**: This preprocessing is applied automatically to every image as it’s loaded, whether for training or validation.\n",
    "\n",
    "In short, this ensures all images are in the right format for your model to use them effectively.\n",
    "\n",
    "\n",
    "\n",
    "### Why Two Generators: `train_datagen` vs. `val_datagen`?\n",
    "- **`train_datagen`** (for training data):\n",
    "  - Prepares the images you use to train your model.\n",
    "  - In some cases, you might add extra transformations (like flipping or rotating images) to make the model better at generalizing, but here, it only uses `custom_preprocess`.\n",
    "- **`val_datagen`** (for validation data):\n",
    "  - Prepares the images you use to check your model’s performance during training.\n",
    "  - Typically, you don’t add random transformations to validation data because you want to see how the model does on clean, consistent images. Here, it also just uses `custom_preprocess`.\n",
    "\n",
    "Even though the code looks identical for both, they’re separate objects that will later be paired with different datasets (training vs. validation).\n",
    "\n",
    "\n",
    "\n",
    "### How Are These Generators Used?\n",
    "After setting up `train_datagen` and `val_datagen`, you’d typically use them with methods like `.flow()` or `.flow_from_directory()` to create batches of preprocessed images. For example:\n",
    "\n",
    "```python\n",
    "train_generator = train_datagen.flow(x_train, y_train, batch_size=32)\n",
    "val_generator = val_datagen.flow(x_val, y_val, batch_size=32)\n",
    "```\n",
    "\n",
    "- **`x_train` and `y_train`**: Your training images and their labels.\n",
    "- **`x_val` and `y_val`**: Your validation images and their labels.\n",
    "- **`batch_size=32`**: The generator processes and delivers 32 images at a time.\n",
    "\n",
    "Each image loaded by these generators will automatically go through `custom_preprocess` before being sent to the model.\n",
    "\n",
    "\n",
    "\n",
    "### Why Not Preprocess Images Manually?\n",
    "You might wonder, \"Why not just preprocess all the images myself and then train the model?\" Here’s why `ImageDataGenerator` is better:\n",
    "- **Memory efficiency**: Loading and preprocessing thousands of images at once could crash your system. The generator does it in small batches instead.\n",
    "- **Convenience**: It automates the process, so you don’t have to write extra code to handle resizing or normalization.\n",
    "- **Flexibility**: You can tweak preprocessing or add transformations later by just adjusting the generator.\n",
    "\n",
    "\n",
    "\n",
    "### Simple Analogy\n",
    "Think of `ImageDataGenerator` as a kitchen assistant:\n",
    "- It grabs raw ingredients (your images).\n",
    "- Runs them through a preparation step (the `custom_preprocess` function) to make them ready (e.g., chopping them to the right size and seasoning them).\n",
    "- Hands them to the chef (your model) in small servings (batches) instead of dumping everything at once.\n",
    "\n",
    "In this case, both `train_datagen` and `val_datagen` are assistants using the same recipe (`custom_preprocess`) to prepare food for different meals (training and validation).\n",
    "\n",
    "\n",
    "\n",
    "### Key Points to Understand\n",
    "1. **`train_datagen` and `val_datagen`** are tools to prepare images for training and validation.\n",
    "2. They use `custom_preprocess` to automatically adjust every image (e.g., resizing to 224x224 and normalizing for VGG16).\n",
    "3. This setup saves memory and makes your workflow smoother by preprocessing images on the fly.\n",
    "\n",
    "Does this clear things up? If you’re still confused about any part—like what `custom_preprocess` might do or how the generators are used later—feel free to ask!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
