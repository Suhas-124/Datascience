{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🧩 Encoder-Decoder Architecture: A Complete Breakdown 🔥**  \n",
    "\n",
    "The **Encoder-Decoder architecture** is one of the **most powerful deep learning models**, primarily used in **sequence-to-sequence (Seq2Seq) tasks** like **machine translation, text summarization, and speech recognition**. 🚀  \n",
    "\n",
    "Let’s **break it down** step by step, covering **each component in depth** with **illustrations, formulas, and intuitive explanations**! 🎯  \n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ What is an Encoder-Decoder Model? 🤔**  \n",
    "An **Encoder-Decoder model** processes an **input sequence** and generates an **output sequence**. It consists of:  \n",
    "\n",
    "### ✅ **Encoder**: Reads and compresses the input into a **fixed-size context vector** (representation).  \n",
    "### ✅ **Decoder**: Uses this context to generate the output **step by step**.  \n",
    "\n",
    "💡 **Example:**  \n",
    "> **English Sentence:** `\"I love AI\"` → **Model** → **French Translation:** `\"J'aime l'IA\"`\n",
    "\n",
    "🛠 **Applications of Encoder-Decoder Models:**  \n",
    "✔️ **Machine Translation** (Google Translate)  \n",
    "✔️ **Text Summarization**  \n",
    "✔️ **Speech-to-Text**  \n",
    "✔️ **Chatbots & Conversational AI**  \n",
    "\n",
    "\n",
    "\n",
    "## **2️⃣ High-Level Flow of an Encoder-Decoder Model**  \n",
    "```\n",
    "Input Sequence → [Encoder] → [Context Vector] → [Decoder] → Output Sequence\n",
    "```\n",
    "\n",
    "🔹 Example: Translating `\"Hello world\"` into French  \n",
    "```\n",
    "Input:  [\"Hello\", \"world\"] \n",
    "Encoder: 🔄 Converts to vector representation\n",
    "Context:  📦 Stores compressed information\n",
    "Decoder:  🔄 Converts back to output sequence\n",
    "Output:  [\"Bonjour\", \"monde\"]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **3️⃣ Encoder: Understanding the Input 🔄**  \n",
    "\n",
    "The **Encoder** takes an input sequence and transforms it into a **fixed-length representation**.  \n",
    "\n",
    "### **🔹 Components of the Encoder**\n",
    "1️⃣ **Word Embeddings** – Convert words into numerical vectors.  \n",
    "2️⃣ **Recurrent Layers (LSTM, GRU, Transformer Encoder)** – Process sequences.  \n",
    "3️⃣ **Final Hidden State (Context Vector)** – Encodes sentence meaning.  \n",
    "\n",
    "💡 **Example:**  \n",
    "For the sentence `\"I love AI\"`, each word is converted into an embedding:  \n",
    "```\n",
    "\"I\"   → [0.1, 0.2, 0.3, ...]\n",
    "\"love\" → [0.5, 0.6, 0.1, ...]\n",
    "\"AI\"   → [0.7, 0.8, 0.2, ...]\n",
    "```\n",
    "These embeddings are passed through **LSTM/GRU layers**, and the final **hidden state** is extracted as the **context vector**.\n",
    "\n",
    "**Mathematically**, in an RNN-based encoder:\n",
    "$$\n",
    "h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$$\n",
    "Where:  \n",
    "- $ x_t $ = Word embedding of the $ t $th word  \n",
    "- $ h_t $ = Hidden state at time step $ t $  \n",
    "- $ W, U, b $ = Learnable parameters  \n",
    "\n",
    "🔹 **Final Output of the Encoder**: The last hidden state acts as the **context vector**.  \n",
    "\n",
    "\n",
    "\n",
    "## **4️⃣ Context Vector: The Heart of the Model ❤️**  \n",
    "\n",
    "The **context vector** is the **final hidden state** of the encoder that captures the meaning of the input sequence.  \n",
    "\n",
    "🔹 **Problem in Basic Encoder-Decoder:**  \n",
    "- A single **fixed-size context vector** struggles with **long sentences** (information loss).  \n",
    "- **Solution:** Attention Mechanism (explained later 🚀).  \n",
    "\n",
    "\n",
    "\n",
    "## **5️⃣ Decoder: Generating the Output 🔄**  \n",
    "\n",
    "The **Decoder** takes the **context vector** from the encoder and generates the **output sequence** step by step.  \n",
    "\n",
    "### **🔹 Components of the Decoder**\n",
    "1️⃣ **Initial State:** Uses the **context vector** as the first hidden state.  \n",
    "2️⃣ **Recurrent Layers (LSTM, GRU, Transformer Decoder)** – Generate tokens sequentially.  \n",
    "3️⃣ **Softmax Layer** – Converts hidden states into word probabilities.  \n",
    "\n",
    "💡 **Example:**  \n",
    "```\n",
    "Step 1: Context → \"J'\"\n",
    "Step 2: \"J'\" → \"aime\"\n",
    "Step 3: \"aime\" → \"l'IA\"\n",
    "```\n",
    "\n",
    "### **Mathematically, the decoder works as follows:**\n",
    "$$\n",
    "s_t = f(W \\cdot y_{t-1} + U \\cdot s_{t-1} + V \\cdot c + b)\n",
    "$$\n",
    "Where:  \n",
    "- $ s_t $ = Hidden state at step $ t $  \n",
    "- $ y_{t-1} $ = Previous word generated  \n",
    "- $ c $ = Context vector from encoder  \n",
    "\n",
    "\n",
    "\n",
    "## **6️⃣ Attention Mechanism: Fixing the Context Vector Problem ⚡**  \n",
    "\n",
    "🔹 **Why Do We Need Attention?**  \n",
    "- Instead of using a **single fixed-size context vector**, **attention** allows the decoder to focus on **different parts of the input** at each step.  \n",
    "- This improves performance for **long sentences**!  \n",
    "\n",
    "### **🔹 How Attention Works**\n",
    "1️⃣ The decoder assigns a **weight** to each encoder hidden state.  \n",
    "2️⃣ The weights determine how much focus the decoder should give to each input word.  \n",
    "3️⃣ The final context vector is computed as a **weighted sum** of encoder hidden states.  \n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "$$\n",
    "$$\n",
    "c_t = \\sum_{i} \\alpha_{t,i} h_i\n",
    "$$\n",
    "Where:  \n",
    "- $ e_{t,i} $ = Score function (how important word $ i $ is for output step $ t $)  \n",
    "- $ \\alpha_{t,i} $ = Attention weight  \n",
    "- $ h_i $ = Encoder hidden state  \n",
    "\n",
    "### **💡 Example (Translating \"I love AI\" → \"J'aime l'IA\")**\n",
    "```\n",
    "Step 1: Focus on \"I\" → Generate \"J'\"\n",
    "Step 2: Focus on \"love\" → Generate \"aime\"\n",
    "Step 3: Focus on \"AI\" → Generate \"l'IA\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7️⃣ Encoder-Decoder Variants 🚀**\n",
    "There are multiple variations of Encoder-Decoder models:\n",
    "\n",
    "### **1️⃣ RNN-Based Encoder-Decoder**\n",
    "✔️ Uses LSTMs/GRUs for both encoder & decoder.  \n",
    "✔️ Simple but struggles with **long sequences**.  \n",
    "\n",
    "### **2️⃣ Attention-Based Encoder-Decoder**\n",
    "✔️ Introduces **attention** to improve long-sequence learning.  \n",
    "✔️ Used in **Neural Machine Translation (NMT)**.\n",
    "\n",
    "### **3️⃣ Transformer (Self-Attention)**\n",
    "✔️ **Removes recurrence** and uses **Self-Attention** for parallelization.  \n",
    "✔️ **State-of-the-art** for NLP tasks (**BERT, GPT, T5, etc.**).  \n",
    "\n",
    "## **8️⃣ Summary Table 📜**\n",
    "| Model Type | Key Feature | Strengths | Weaknesses |\n",
    "|------------|------------|-----------|------------|\n",
    "| **Basic RNN** | Context Vector | Simple | Poor for long sentences |\n",
    "| **LSTM/GRU** | Better Memory | Handles longer sequences | Still slow |\n",
    "| **Attention** | Weighted focus on words | Captures long-range dependencies | More computations |\n",
    "| **Transformer** | Self-Attention | Parallelized, Faster | High Memory Usage |\n",
    "\n",
    "\n",
    "# **🚀 Final Thoughts**\n",
    "✅ **Encoder-Decoder is the backbone of many AI applications**!  \n",
    "✅ **Attention has revolutionized sequence processing.**  \n",
    "✅ **Transformers (like GPT, BERT) are the next step forward.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can manually go through the steps of an **encoder-decoder** model with a simple sentence, but doing the full manual calculation for an entire **real-world model** would be extremely lengthy. Instead, let's break it down step by step for a **very simple model** using a toy example with **small vectors and basic mathematical operations**.\n",
    "\n",
    "\n",
    "### **Example Sentence:**\n",
    "👉 **\"Hi.\"**  \n",
    "\n",
    "Let's assume we are using a **basic Seq2Seq model with RNNs (LSTMs)**. We'll manually go through the process.\n",
    "\n",
    "## **Step 1: Word to Vector (Tokenization & Embedding)**\n",
    "Each word needs to be converted into a numerical representation. Let's assume we have the following **word embeddings**:\n",
    "\n",
    "| Word | One-Hot Encoding | Embedding Vector (2D for simplicity) |\n",
    "|------|-----------------|-------------------|\n",
    "| Hi   | [1, 0]         | [0.5, 0.8]        |\n",
    "\n",
    "We assume a **2D embedding vector** `[0.5, 0.8]` for the word \"Hi.\"\n",
    "\n",
    "\n",
    "## **Step 2: Encoding (LSTM Forward Pass)**\n",
    "Now, let's pass this embedding `[0.5, 0.8]` through an **LSTM encoder** step by step.\n",
    "\n",
    "### **LSTM Equation:**\n",
    "An LSTM consists of multiple **gates**:  \n",
    "\n",
    "1. **Forget Gate:**  \n",
    "   $$\n",
    "   f_t = \\sigma(W_f \\cdot h_{t-1} + U_f \\cdot x_t + b_f)\n",
    "   $$\n",
    "2. **Input Gate:**  \n",
    "   $$\n",
    "   i_t = \\sigma(W_i \\cdot h_{t-1} + U_i \\cdot x_t + b_i)\n",
    "   $$\n",
    "3. **Candidate Cell State:**  \n",
    "   $$\n",
    "   \\tilde{C_t} = \\tanh(W_c \\cdot h_{t-1} + U_c \\cdot x_t + b_c)\n",
    "   $$\n",
    "4. **Final Cell State:**  \n",
    "   $$\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "   $$\n",
    "5. **Output Gate:**  \n",
    "   $$\n",
    "   o_t = \\sigma(W_o \\cdot h_{t-1} + U_o \\cdot x_t + b_o)\n",
    "   $$\n",
    "6. **Hidden State:**  \n",
    "   $$\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   $$\n",
    "\n",
    "For simplicity, let's assume our **initial hidden state** and **cell state** are both **zero vectors**:  \n",
    "\n",
    "$$\n",
    "h_0 = [0, 0], \\quad C_0 = [0, 0]\n",
    "$$\n",
    "\n",
    "We now calculate the values using randomly chosen **LSTM weight matrices**.\n",
    "\n",
    "\n",
    "\n",
    "### **Manual LSTM Calculation**\n",
    "Let's assume the following **random weights** for a 2D LSTM:\n",
    "\n",
    "$$\n",
    "W_f = \\begin{bmatrix} 0.3 & 0.7 \\\\ 0.5 & 0.2 \\end{bmatrix}, \\quad\n",
    "U_f = \\begin{bmatrix} 0.6 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix}, \\quad\n",
    "b_f = \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We compute the **forget gate**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f h_0 + U_f x_t + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} 0.3 & 0.7 \\\\ 0.5 & 0.2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.6 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} (0.6 \\cdot 0.5 + 0.1 \\cdot 0.8) + 0.2 \\\\ (0.4 \\cdot 0.5 + 0.3 \\cdot 0.8) + 0.1 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} (0.3 + 0.08) + 0.2 \\\\ (0.2 + 0.24) + 0.1 \\end{bmatrix} \\right)\n",
    "= \\sigma\\left(\\begin{bmatrix} 0.58 \\\\ 0.54 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Using the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "f_t = \\begin{bmatrix} 0.64 \\\\ 0.63 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, we compute **input gate**, **cell state update**, and **output gate**, leading to:\n",
    "\n",
    "$$\n",
    "h_t = \\begin{bmatrix} 0.72 \\\\ 0.68 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This hidden state **encodes** the word \"Hi.\"\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Decoding (Generating Output)**\n",
    "We now pass this **encoded hidden state** to the **decoder**, which generates the output sequence.\n",
    "\n",
    "Let's assume the target output is **\"Hola.\"** (Spanish translation of \"Hi.\")\n",
    "\n",
    "The decoder, another **LSTM**, takes the encoded vector and generates words step by step.\n",
    "\n",
    "Using the **decoder LSTM** with random weights, we perform similar **LSTM calculations** and obtain a final output vector:\n",
    "\n",
    "$$\n",
    "y_t = \\begin{bmatrix} 0.9 \\\\ 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which corresponds to the word \"Hola\" in our vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Output: \"Hola.\"** 🎉\n",
    "The decoder produces `\"Hola.\"` as the **translated sequence** from `\"Hi.\"` using the learned Seq2Seq model!\n",
    "\n",
    "\n",
    "\n",
    "### **Summary of Steps:**\n",
    "1. Convert `\"Hi.\"` into a **word embedding**.\n",
    "2. Pass it through the **LSTM Encoder**, computing **gates and cell states**.\n",
    "3. The final **hidden state** represents the entire input sequence.\n",
    "4. Pass this **encoded representation** to the **LSTM Decoder**.\n",
    "5. The decoder generates words **one at a time** until reaching the **end of the sequence**.\n",
    "6. The final output is **\"Hola.\"**\n",
    "\n",
    "\n",
    "\n",
    "💡 **Note:**  \n",
    "- In real-world models, the computations involve **higher dimensions** (e.g., 512 or 1024).\n",
    "- Transformers use **self-attention** instead of **RNNs**, making them **parallelizable** and **efficient**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 **The Attention Mechanism: A Game Changer in Sequence-to-Sequence Models** 🎯  \n",
    "\n",
    "Imagine you’re translating a long sentence from **English** to **French**. A traditional **Encoder-Decoder (Seq2Seq) model** reads the entire English sentence, compresses it into a **single fixed-length vector** (the context vector), and then tries to generate the French translation word by word.  \n",
    "\n",
    "⚠️ **But there’s a problem!**  \n",
    "When the sentence is long, the fixed-size context vector **struggles** to retain all relevant information, leading to **poor translations** and loss of context.  \n",
    "\n",
    "👉 **Enter the Attention Mechanism!** 🚀  \n",
    "The **Attention Mechanism** solves this by allowing the decoder to focus on **different parts of the input sequence at each decoding step**, rather than relying on a single compressed vector.  \n",
    "\n",
    "\n",
    "\n",
    "## **💡 How Does Attention Work? (Step-by-Step Guide)**\n",
    "Let’s break it down in a **simple and intuitive** way.  \n",
    "\n",
    "### **1️⃣ Encoder Stage: Read and Store Information**\n",
    "The encoder processes the **input sequence** word by word and generates a **hidden state** at each step.\n",
    "\n",
    "Example: Translating **\"I love machine learning\"** to French **\"J'adore l'apprentissage automatique\"**  \n",
    "\n",
    "🔹 The encoder takes each word and **outputs a hidden state**:  \n",
    "- **h₁** for \"I\"  \n",
    "- **h₂** for \"love\"  \n",
    "- **h₃** for \"machine\"  \n",
    "- **h₄** for \"learning\"  \n",
    "\n",
    "📌 Instead of storing only the **final hidden state**, attention keeps track of **ALL hidden states**:  \n",
    "💾 **Memory** = {h₁, h₂, h₃, h₄}  \n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ Decoder Stage: Generate Output Word by Word**\n",
    "The decoder **doesn’t just rely on a single fixed vector**. Instead, for each word it generates, it selectively attends to **relevant parts** of the input sequence.\n",
    "\n",
    "Let’s say we want to generate the first French word: **\"J'adore\"**  \n",
    "\n",
    "🚀 **Instead of using just one vector, the decoder dynamically \"looks\" at different words in the input!**  \n",
    "\n",
    "### **3️⃣ Compute Attention Scores**\n",
    "For each decoder step, we compute **attention scores** that determine how much focus the decoder should give to each input word.  \n",
    "\n",
    "💡 How? We compare the decoder’s **current state** with each encoder hidden state to generate a **score** using:  \n",
    "- **Dot product**  \n",
    "- **Additive attention (Bahdanau, 2014)**  \n",
    "- **Multiplicative attention (Luong, 2015)**  \n",
    "\n",
    "💡 Example:  \n",
    "- The first output word **\"J'adore\"** mainly depends on **\"I love\"** → Higher weight for **h₁, h₂**  \n",
    "- The second word **\"l'apprentissage\"** depends on **\"machine learning\"** → Higher weight for **h₃, h₄**  \n",
    "\n",
    "### **4️⃣ Compute Attention Weights**\n",
    "📌 Normalize the scores using **softmax** to get a probability distribution.  \n",
    "Example (hypothetical weights for \"J'adore\"):  \n",
    "\n",
    "| Input Word | Raw Score | Softmax Weight (α) |\n",
    "|------------|-----------|--------------------|\n",
    "| \"I\" (h₁) | 2.3 | 0.30 |\n",
    "| \"love\" (h₂) | 2.5 | 0.35 |\n",
    "| \"machine\" (h₃) | 1.2 | 0.20 |\n",
    "| \"learning\" (h₄) | 0.8 | 0.15 |\n",
    "\n",
    "💡 **Higher weight = More focus!**  \n",
    "- Here, **h₁ and h₂ (I love)** get the most attention for **\"J'adore\"**.  \n",
    "\n",
    "\n",
    "### **5️⃣ Compute Context Vector**\n",
    "Multiply each **hidden state** by its attention weight and sum them:  \n",
    "\n",
    "$$\n",
    "\\text{Context Vector} = \\sum_{i=1}^{n} \\alpha_i \\cdot h_i\n",
    "$$\n",
    "\n",
    "🔹 This gives the decoder a **weighted sum of encoder hidden states** → A **dynamic, context-aware vector**!  \n",
    "\n",
    "\n",
    "\n",
    "### **6️⃣ Generate the Next Word**\n",
    "- The decoder **uses the context vector** + **previous output** to generate the next word.  \n",
    "- This repeats until the entire output sequence is generated.  \n",
    "\n",
    "🔥 **End Result? A much better, context-aware translation!** 🔥  \n",
    "\n",
    "\n",
    "\n",
    "## **💡 Why Use Attention Instead of Basic Seq2Seq?**\n",
    "✅ **Handles Long Sentences**: No more fixed-size bottleneck! Attention dynamically selects the most relevant information.  \n",
    "✅ **Improves Context Understanding**: Words are attended to **based on meaning**, preventing information loss.  \n",
    "✅ **Parallelization (in Transformers)**: Unlike RNNs, attention can be computed in parallel, making it much faster.  \n",
    "✅ **More Human-Like**: It mimics **how humans read**—we focus on **important words**, not the entire sentence at once.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔷 Where is Attention Used?**\n",
    "📌 **Machine Translation (Google Translate)**  \n",
    "📌 **Speech Recognition (DeepSpeech, Whisper)**  \n",
    "📌 **Text Summarization (BART, Pegasus)**  \n",
    "📌 **Image Captioning (Show, Attend, and Tell)**  \n",
    "\n",
    "\n",
    "\n",
    "### **🔮 Final Thoughts**\n",
    "The **Attention Mechanism** revolutionized deep learning by allowing models to selectively focus on important parts of the input, leading to **better performance, efficiency, and accuracy**. It **paved the way for Transformers**, which now dominate NLP tasks like ChatGPT, BERT, and GPT models!  \n",
    "\n",
    "🚀 **So next time you ask ChatGPT a question, remember—it’s powered by ATTENTION!** 🚀  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **🧠 Attention Mechanism in Simple Layman Terms**  \n",
    "\n",
    "Imagine you are reading a long book 📖 and later, someone asks you a question about a specific part of the story.  \n",
    "\n",
    "- If you had to **memorize the entire book** before answering, you'd likely forget details.  \n",
    "- But if you could **look back at the book** whenever needed, you’d give a much better answer!  \n",
    "\n",
    "💡 **That’s exactly what the Attention Mechanism does!**  \n",
    "\n",
    "\n",
    "\n",
    "## **💡 The Problem with Basic Encoder-Decoder (Seq2Seq)**\n",
    "A traditional **encoder-decoder** model is like trying to read an entire book **once** and then retelling it from memory.  \n",
    "\n",
    "📌 **Example**:  \n",
    "You hear the sentence:  \n",
    "👉 **\"The cat sat on the mat because it was tired.\"**  \n",
    "Now, you must **remember** everything before you start translating it to another language.  \n",
    "\n",
    "😨 **The problem?**  \n",
    "- If the sentence is too long, the decoder forgets important details.  \n",
    "- The model has to **squeeze** all information into a single memory unit (context vector).  \n",
    "\n",
    "🚀 **Solution? Let’s use Attention!**  \n",
    "\n",
    "\n",
    "\n",
    "## **🧐 What Does Attention Do?**\n",
    "Instead of remembering **everything at once**, Attention lets the model **focus on relevant words** at each step.  \n",
    "\n",
    "💡 Think of it like **a highlighter in a book**—you don’t remember the whole book, just the key parts when needed.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 How Does Attention Work?**\n",
    "Let’s say we are translating:  \n",
    "👉 **\"The cat sat on the mat because it was tired.\"**  \n",
    "into French:  \n",
    "👉 **\"Le chat s'est assis sur le tapis parce qu'il était fatigué.\"**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1️⃣ - Read the Words (Encoder)**\n",
    "The model reads the English sentence **one word at a time** and stores small memory chunks (**hidden states**) for each word.  \n",
    "\n",
    "| Word | Hidden Memory |\n",
    "|------|--------------|\n",
    "| The | h₁ |\n",
    "| cat | h₂ |\n",
    "| sat | h₃ |\n",
    "| on | h₄ |\n",
    "| the | h₅ |\n",
    "| mat | h₆ |\n",
    "| because | h₇ |\n",
    "| it | h₈ |\n",
    "| was | h₉ |\n",
    "| tired | h₁₀ |\n",
    "\n",
    "📌 **Each word has its own hidden state (like taking notes while reading).**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2️⃣ - Start Translating (Decoder)**\n",
    "Now, we start generating the translation **one word at a time**.  \n",
    "\n",
    "🔹 To generate the first French word (**\"Le\"**), instead of looking at the **whole English sentence**, the decoder **focuses more on** \"The cat\".  \n",
    "\n",
    "✅ **Attention Mechanism assigns different importance (weights) to each word!**  \n",
    "\n",
    "For **\"Le\"**, it focuses mostly on **\"The\"**  \n",
    "For **\"chat\"**, it focuses on **\"cat\"**  \n",
    "For **\"assis\"**, it focuses on **\"sat\"**, and so on...  \n",
    "\n",
    "📌 Instead of remembering everything, the model **dynamically looks at different words** when translating each word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3️⃣ - Assign Attention Weights**\n",
    "The model calculates **how important** each word is for the current translation step.  \n",
    "\n",
    "Example (when generating \"chat\"):  \n",
    "\n",
    "| English Word | Attention Weight (%) |\n",
    "|-------------|----------------------|\n",
    "| The | 10% |\n",
    "| cat | 70% ✅ |\n",
    "| sat | 15% |\n",
    "| on | 5% |\n",
    "\n",
    "💡 The model pays **most attention** to **\"cat\"** when generating **\"chat\"**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 4️⃣ - Continue Translating**\n",
    "For the next word (**\"s'est assis\"**), attention shifts focus to **\"sat\"** instead of \"cat\".  \n",
    "\n",
    "✅ This continues until the full translation is complete!  \n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Why is Attention Better than Traditional Encoder-Decoder?**\n",
    "✅ **No more memory bottlenecks** → It doesn’t try to fit the whole sentence into one vector.  \n",
    "✅ **Better translations** → The model focuses on **relevant** words at each step.  \n",
    "✅ **Handles long sentences well** → No more forgetting important details!  \n",
    "✅ **Works in real-world NLP tasks** → Used in Google Translate, ChatGPT, and more!  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Attention is Everywhere!**\n",
    "Attention is so powerful that it led to **Transformers**, which power modern AI models like:  \n",
    "💡 **BERT, GPT, Whisper, ChatGPT, and Google Translate!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's manually go through **how the attention mechanism works** using a small example.  \n",
    "\n",
    "We'll break it down step by step **with actual numbers**. Get ready for some math! 🧮✨  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 Sentence Example**\n",
    "Let's take a short English sentence:  \n",
    "👉 **\"She loves cats\"**  \n",
    "\n",
    "And let's assume we want to **translate it** into another language, like French.  \n",
    "\n",
    "💡 Our goal:  \n",
    "1. **Manually calculate attention scores**  \n",
    "2. **Show how attention selects words for the decoder**  \n",
    "\n",
    "## **⚙️ Step 1: Word Embeddings & Hidden States**\n",
    "Each word in the input sequence is **converted into a vector** (word embeddings).  \n",
    "For simplicity, we'll assume we have **predefined 3D embeddings** for each word:\n",
    "\n",
    "| Word  | Embedding (3D) |\n",
    "|-------|--------------|\n",
    "| She   | (0.1, 0.2, 0.3) |\n",
    "| Loves | (0.5, 0.4, 0.1) |\n",
    "| Cats  | (0.2, 0.7, 0.6) |\n",
    "\n",
    "These embeddings are processed by the **encoder**, which outputs **hidden states** (h₁, h₂, h₃):  \n",
    "\n",
    "| Word  | Hidden State (3D) |\n",
    "|-------|----------------|\n",
    "| She   | (0.2, 0.1, 0.5) |\n",
    "| Loves | (0.6, 0.3, 0.2) |\n",
    "| Cats  | (0.4, 0.8, 0.3) |\n",
    "\n",
    "The **decoder** will use these hidden states to generate the translation.\n",
    "\n",
    "\n",
    "## **⚙️ Step 2: Compute Attention Scores**  \n",
    "💡 **Goal:** Determine **which input words are important** when generating a translated word.\n",
    "\n",
    "📌 **Formula for attention score (before softmax):**  \n",
    "$$\n",
    "e_{ij} = q_j \\cdot h_i\n",
    "$$  \n",
    "where:  \n",
    "- $ q_j $ = decoder's current hidden state (query)  \n",
    "- $ h_i $ = encoder's hidden states (keys)  \n",
    "- $ e_{ij} $ = raw attention score (dot product of query and keys)\n",
    "\n",
    "Let's assume the decoder's hidden state **(query vector for first translated word)** is:  \n",
    "👉 $ q = (0.3, 0.5, 0.2) $\n",
    "\n",
    "Now, let's compute attention scores:\n",
    "\n",
    "$$\n",
    "e_1 = q \\cdot h_1 = (0.3, 0.5, 0.2) \\cdot (0.2, 0.1, 0.5)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.2) + (0.5 \\times 0.1) + (0.2 \\times 0.5) = 0.06 + 0.05 + 0.1 = 0.21\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_2 = q \\cdot h_2 = (0.3, 0.5, 0.2) \\cdot (0.6, 0.3, 0.2)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.6) + (0.5 \\times 0.3) + (0.2 \\times 0.2) = 0.18 + 0.15 + 0.04 = 0.37\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_3 = q \\cdot h_3 = (0.3, 0.5, 0.2) \\cdot (0.4, 0.8, 0.3)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.4) + (0.5 \\times 0.8) + (0.2 \\times 0.3) = 0.12 + 0.4 + 0.06 = 0.58\n",
    "$$\n",
    "\n",
    "📌 **Raw attention scores:**  \n",
    "$$\n",
    "e_1 = 0.21, \\quad e_2 = 0.37, \\quad e_3 = 0.58\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **⚙️ Step 3: Apply Softmax to Get Attention Weights**\n",
    "Now, we convert these raw scores into probabilities using the **Softmax function**:\n",
    "\n",
    "$$\n",
    "a_i = \\frac{e^e_i}{\\sum e^e_j}\n",
    "$$\n",
    "\n",
    "First, compute exponentials:\n",
    "\n",
    "$$\n",
    "e^{0.21} \\approx 1.234, \\quad e^{0.37} \\approx 1.447, \\quad e^{0.58} \\approx 1.786\n",
    "$$\n",
    "\n",
    "Sum of exponentials:\n",
    "\n",
    "$$\n",
    "1.234 + 1.447 + 1.786 = 4.467\n",
    "$$\n",
    "\n",
    "Now, compute softmax values:\n",
    "\n",
    "$$\n",
    "a_1 = \\frac{1.234}{4.467} \\approx 0.276\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_2 = \\frac{1.447}{4.467} \\approx 0.324\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_3 = \\frac{1.786}{4.467} \\approx 0.400\n",
    "$$\n",
    "\n",
    "📌 **Final attention weights:**  \n",
    "$$\n",
    "a_1 = 0.276, \\quad a_2 = 0.324, \\quad a_3 = 0.400\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **⚙️ Step 4: Compute Context Vector**\n",
    "The **context vector** is a weighted sum of the encoder's hidden states:\n",
    "\n",
    "$$\n",
    "C = a_1 h_1 + a_2 h_2 + a_3 h_3\n",
    "$$\n",
    "\n",
    "Each term:\n",
    "\n",
    "$$\n",
    "(0.276 \\times (0.2, 0.1, 0.5)) = (0.0552, 0.0276, 0.138)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.324 \\times (0.6, 0.3, 0.2)) = (0.1944, 0.0972, 0.0648)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.400 \\times (0.4, 0.8, 0.3)) = (0.16, 0.32, 0.12)\n",
    "$$\n",
    "\n",
    "Summing up:\n",
    "\n",
    "$$\n",
    "C = (0.0552 + 0.1944 + 0.16, \\quad 0.0276 + 0.0972 + 0.32, \\quad 0.138 + 0.0648 + 0.12)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C = (0.41, 0.445, 0.3228)\n",
    "$$\n",
    "\n",
    "📌 **Final Context Vector:**  \n",
    "👉 $ C = (0.41, 0.445, 0.3228) $  \n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 5: Use Context Vector to Generate the Next Word**\n",
    "The **context vector** $ C $ is now used as input for the decoder to generate the **first translated word** in French.\n",
    "\n",
    "This process repeats for every next word in the translated sentence!\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Summary of Manual Calculation**\n",
    "1. **Compute attention scores** by dot product of decoder hidden state and encoder hidden states.  \n",
    "2. **Apply softmax** to normalize attention scores into probabilities.  \n",
    "3. **Weight encoder hidden states** using attention values to get a **context vector**.  \n",
    "4. **Feed context vector to decoder** to generate the next word.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why Attention is Powerful?**\n",
    "✅ **Focuses on relevant words at each step** 🏹  \n",
    "✅ **Handles long sentences better** 📜  \n",
    "✅ **Improves translation & NLP tasks** 🚀  \n",
    "✅ **Used in modern AI like Transformers (GPT, BERT, etc.)** 🤖  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **📌 Bahdanau Attention (Additive Attention) - Full Explanation**  \n",
    "\n",
    "Bahdanau attention (a.k.a. **Additive Attention**) was introduced by **Dzmitry Bahdanau et al. (2015)** to improve the traditional **encoder-decoder model** in sequence-to-sequence tasks like **machine translation**.  \n",
    "\n",
    "### **🚀 Why do we need Bahdanau Attention?**  \n",
    "In the traditional encoder-decoder architecture:  \n",
    "✅ The **encoder compresses the entire input sentence** into a **single fixed-length context vector**.  \n",
    "✅ The **decoder generates words** based only on that **one vector**.  \n",
    "🚨 **Problem:** If the input sentence is long, a single context vector **loses information**! 😵  \n",
    "\n",
    "🎯 **Solution:** Bahdanau Attention dynamically assigns different attention weights to each word **at each decoding step**!  \n",
    "\n",
    "## **📌 Steps in Bahdanau Attention**\n",
    "Let's break it down step by step **with formulas and an example**!  \n",
    "\n",
    "### **🛠️ Step 1: Encoder Processes Input Sentence**\n",
    "We have an input sentence:  \n",
    "👉 **\"She loves cats\"**  \n",
    "\n",
    "Each word is converted into a **hidden state** using a Bi-directional LSTM/GRU encoder.  \n",
    "\n",
    "| Word  | Hidden State ($ h_i $) |\n",
    "|-------|----------------|\n",
    "| She   | $ h_1 = (0.2, 0.1, 0.5) $ |\n",
    "| Loves | $ h_2 = (0.6, 0.3, 0.2) $ |\n",
    "| Cats  | $ h_3 = (0.4, 0.8, 0.3) $ |\n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 2: Compute Alignment Scores**\n",
    "📌 **Instead of a simple dot product like in traditional attention, Bahdanau uses a feedforward neural network to compute attention scores.**  \n",
    "\n",
    "🔹 We calculate an **alignment score** $ e_i $ for each hidden state:  \n",
    "$$\n",
    "e_i = v_a^T \\tanh(W_a [h_i; s_{t-1}])\n",
    "$$\n",
    "where:  \n",
    "- $ W_a $ and $ v_a $ are **learnable weight matrices**.  \n",
    "- $ h_i $ is the encoder's hidden state for word $ i $.  \n",
    "- $ s_{t-1} $ is the **previous decoder hidden state** (query).  \n",
    "- $ [h_i; s_{t-1}] $ means concatenation.  \n",
    "- $ e_i $ is a scalar **score** that tells us **how important** word $ i $ is at timestep $ t $.  \n",
    "\n",
    "💡 This score is computed **for every input word** at every decoding step.\n",
    "\n",
    "#### **Example Calculation**  \n",
    "Assume we have:  \n",
    "- $ W_a = \\begin{bmatrix} 0.1 & 0.3 & 0.2 \\\\ 0.4 & 0.1 & 0.5 \\end{bmatrix} $  \n",
    "- $ v_a^T = (0.2, 0.6) $  \n",
    "- $ s_{t-1} = (0.3, 0.5, 0.2) $  \n",
    "\n",
    "Let's compute **$ e_1 $ for \"She\"**:  \n",
    "\n",
    "1️⃣ **Concatenate $ h_1 $ and $ s_{t-1} $:**  \n",
    "$$\n",
    "[h_1; s_{t-1}] = (0.2, 0.1, 0.5, 0.3, 0.5, 0.2)\n",
    "$$\n",
    "2️⃣ **Multiply with $ W_a $ and apply $ \\tanh $:**  \n",
    "$$\n",
    "W_a \\times [h_1; s_{t-1}] = \\tanh( [0.31, 0.43] )\n",
    "$$\n",
    "$$\n",
    "= (0.3, 0.4)  \\quad \\text{(applying tanh)}\n",
    "$$\n",
    "3️⃣ **Compute $ e_1 $:**  \n",
    "$$\n",
    "e_1 = (0.2, 0.6) \\cdot (0.3, 0.4) = 0.2(0.3) + 0.6(0.4) = 0.03 + 0.24 = 0.27\n",
    "$$\n",
    "\n",
    "Similarly, compute $ e_2 $ and $ e_3 $.  \n",
    "Let's assume:\n",
    "$$\n",
    "e_1 = 0.27, \\quad e_2 = 0.35, \\quad e_3 = 0.42\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 3: Compute Attention Weights**\n",
    "Now, we apply **Softmax** to convert these scores into probabilities:  \n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{e_i}}{\\sum e^{e_j}}\n",
    "$$\n",
    "\n",
    "Computing exponentials:  \n",
    "$$\n",
    "e^{0.27} \\approx 1.31, \\quad e^{0.35} \\approx 1.42, \\quad e^{0.42} \\approx 1.52\n",
    "$$\n",
    "\n",
    "Sum:\n",
    "$$\n",
    "1.31 + 1.42 + 1.52 = 4.25\n",
    "$$\n",
    "\n",
    "Final attention weights:\n",
    "$$\n",
    "\\alpha_1 = \\frac{1.31}{4.25} \\approx 0.308, \\quad \\alpha_2 = \\frac{1.42}{4.25} \\approx 0.334, \\quad \\alpha_3 = \\frac{1.52}{4.25} \\approx 0.358\n",
    "$$\n",
    "\n",
    "📌 **These values tell us how much attention to pay to each word!**  \n",
    "- **\"She\"**: 30.8%  \n",
    "- **\"Loves\"**: 33.4%  \n",
    "- **\"Cats\"**: 35.8%  \n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 4: Compute Context Vector**\n",
    "We compute a **weighted sum of the encoder hidden states**:  \n",
    "$$\n",
    "C_t = \\sum_{i} \\alpha_i h_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = (0.308 \\times h_1) + (0.334 \\times h_2) + (0.358 \\times h_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.308 \\times (0.2, 0.1, 0.5)) + (0.334 \\times (0.6, 0.3, 0.2)) + (0.358 \\times (0.4, 0.8, 0.3))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.0616, 0.0308, 0.154) + (0.2004, 0.1002, 0.0668) + (0.1432, 0.2864, 0.1074)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.4052, 0.4174, 0.3282)\n",
    "$$\n",
    "\n",
    "📌 **Final Context Vector** $ C_t = (0.4052, 0.4174, 0.3282) $  \n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 5: Generate Next Word in the Translation**\n",
    "This **context vector $ C_t $** is combined with the decoder's hidden state and passed through a softmax layer to predict the **next translated word**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why Bahdanau Attention is Better?**\n",
    "✅ **Removes fixed-length bottleneck** (no need for a single context vector).  \n",
    "✅ **Focuses on relevant words dynamically** at each step.  \n",
    "✅ **Works better for long sequences.**  \n",
    "✅ **Used in modern AI models like Transformers.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Summary of Bahdanau Attention**\n",
    "1️⃣ **Compute attention scores** using a neural network.  \n",
    "2️⃣ **Apply softmax** to get attention weights.  \n",
    "3️⃣ **Compute weighted sum** to get a context vector.  \n",
    "4️⃣ **Feed context vector to decoder** to generate output.  \n",
    "\n",
    "\n",
    "## **🌟 Conclusion**\n",
    "Bahdanau Attention is like a **smart spotlight** that helps the decoder **focus** on different words **at each step**, making translations much better! 🌟🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🚀 Luong Attention Mechanism (Multiplicative Attention) - Full Explanation**  \n",
    "\n",
    "The **Luong Attention Mechanism** was introduced by **Minh-Thang Luong et al. (2015)** to improve Bahdanau Attention. Unlike Bahdanau’s method, which uses an additional neural network to compute attention scores, **Luong Attention directly computes attention scores using dot products**, making it computationally efficient.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 Why Do We Need Luong Attention?**\n",
    "💡 **Problems with Bahdanau Attention:**  \n",
    "1. **Computational Overhead** 🖥️: Uses an additional neural network to compute attention scores.  \n",
    "2. **More Parameters to Train** 🎛️: Due to extra weight matrices.  \n",
    "\n",
    "💡 **Luong’s Solution:**  \n",
    "✅ Uses **simpler and faster dot-product operations** to calculate attention.  \n",
    "✅ Works better when input and output sequences have a **similar structure** (e.g., English-to-French translation).  \n",
    "\n",
    "## **🛠️ How Luong Attention Works?**\n",
    "Let's go step by step with **a sentence example and manual calculations**!  \n",
    "\n",
    "### **🔹 Given Input Sentence:**  \n",
    "👉 **\"She loves cats\"**  \n",
    "\n",
    "Each word is processed by an **encoder (LSTM/GRU)** to generate **hidden states**:\n",
    "\n",
    "| Word  | Hidden State ($ h_i $) |\n",
    "|-------|----------------|\n",
    "| She   | $ h_1 = (0.2, 0.1, 0.5) $ |\n",
    "| Loves | $ h_2 = (0.6, 0.3, 0.2) $ |\n",
    "| Cats  | $ h_3 = (0.4, 0.8, 0.3) $ |\n",
    "\n",
    "Let’s assume the decoder has already generated some output words and is now predicting the next word. The decoder has a hidden state:  \n",
    "$$\n",
    "s_t = (0.3, 0.5, 0.2)\n",
    "$$\n",
    "\n",
    "\n",
    "## **📌 Luong Attention Has Two Types**\n",
    "1️⃣ **Global Attention**: Attends to all encoder hidden states.  \n",
    "2️⃣ **Local Attention**: Attends only to a subset of encoder hidden states (less common).  \n",
    "\n",
    "We’ll explain **Global Attention**, as it’s the most widely used.\n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 1: Compute Alignment Scores**\n",
    "Luong proposes **three** ways to compute scores:  \n",
    "\n",
    "1️⃣ **Dot Product**:  \n",
    "$$\n",
    "e_i = h_i^T s_t\n",
    "$$\n",
    "\n",
    "2️⃣ **General (with learnable weights $ W_a $)**:  \n",
    "$$\n",
    "e_i = s_t^T W_a h_i\n",
    "$$\n",
    "\n",
    "3️⃣ **Concatenation (Bahdanau-style but simplified)**:  \n",
    "$$\n",
    "e_i = v_a^T \\tanh(W_a [h_i; s_t])\n",
    "$$\n",
    "\n",
    "💡 **Most common method?** **Dot Product**, since it’s fast and works well.\n",
    "\n",
    "### **Example Calculation (Dot Product)**\n",
    "For each encoder hidden state, compute the dot product with the decoder hidden state $ s_t $:\n",
    "\n",
    "$$\n",
    "e_1 = h_1^T s_t = (0.2, 0.1, 0.5) \\cdot (0.3, 0.5, 0.2)\n",
    "$$\n",
    "$$\n",
    "= (0.2 \\times 0.3) + (0.1 \\times 0.5) + (0.5 \\times 0.2)\n",
    "$$\n",
    "$$\n",
    "= 0.06 + 0.05 + 0.10 = 0.21\n",
    "$$\n",
    "\n",
    "Similarly,  \n",
    "$$\n",
    "e_2 = (0.6, 0.3, 0.2) \\cdot (0.3, 0.5, 0.2) = 0.18 + 0.15 + 0.04 = 0.37\n",
    "$$\n",
    "$$\n",
    "e_3 = (0.4, 0.8, 0.3) \\cdot (0.3, 0.5, 0.2) = 0.12 + 0.40 + 0.06 = 0.58\n",
    "$$\n",
    "\n",
    "Now we have:\n",
    "$$\n",
    "e_1 = 0.21, \\quad e_2 = 0.37, \\quad e_3 = 0.58\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 2: Compute Attention Weights**\n",
    "To convert these scores into probabilities, apply **softmax**:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{e_i}}{\\sum e^{e_j}}\n",
    "$$\n",
    "\n",
    "Computing exponentials:\n",
    "$$\n",
    "e^{0.21} \\approx 1.23, \\quad e^{0.37} \\approx 1.45, \\quad e^{0.58} \\approx 1.79\n",
    "$$\n",
    "\n",
    "Sum:\n",
    "$$\n",
    "1.23 + 1.45 + 1.79 = 4.47\n",
    "$$\n",
    "\n",
    "Final attention weights:\n",
    "$$\n",
    "\\alpha_1 = \\frac{1.23}{4.47} \\approx 0.275, \\quad \\alpha_2 = \\frac{1.45}{4.47} \\approx 0.324, \\quad \\alpha_3 = \\frac{1.79}{4.47} \\approx 0.401\n",
    "$$\n",
    "\n",
    "📌 **These values tell us how much attention to pay to each word!**  \n",
    "- **\"She\"**: 27.5%  \n",
    "- **\"Loves\"**: 32.4%  \n",
    "- **\"Cats\"**: 40.1%  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 3: Compute Context Vector**\n",
    "The **context vector** $ C_t $ is computed as a **weighted sum** of the encoder hidden states:\n",
    "\n",
    "$$\n",
    "C_t = \\sum_{i} \\alpha_i h_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = (0.275 \\times h_1) + (0.324 \\times h_2) + (0.401 \\times h_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.275 \\times (0.2, 0.1, 0.5)) + (0.324 \\times (0.6, 0.3, 0.2)) + (0.401 \\times (0.4, 0.8, 0.3))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.055, 0.0275, 0.1375) + (0.1944, 0.0972, 0.0648) + (0.1604, 0.3208, 0.1203)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.4098, 0.4455, 0.3226)\n",
    "$$\n",
    "\n",
    "📌 **Final Context Vector** $ C_t = (0.4098, 0.4455, 0.3226) $  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 4: Compute Final Decoder Hidden State**\n",
    "Luong suggests **two ways** to use the context vector $ C_t $:\n",
    "\n",
    "1️⃣ **Concatenation Method (Most Common)**\n",
    "$$\n",
    "\\tilde{s}_t = \\tanh(W_c [C_t; s_t])\n",
    "$$\n",
    "This means we **concatenate** $ C_t $ and $ s_t $, then pass it through a neural network.\n",
    "\n",
    "2️⃣ **Multiplication Method**\n",
    "$$\n",
    "\\tilde{s}_t = C_t + s_t\n",
    "$$\n",
    "Just a direct sum (less commonly used).\n",
    "\n",
    "The final hidden state is used to **generate the next word**.\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why is Luong Attention Better?**\n",
    "✅ **More Efficient** 🚀: Uses simple **dot products** instead of extra neural networks.  \n",
    "✅ **More Flexible** 🎛️: Works well with different encoder-decoder structures.  \n",
    "✅ **Better for Structured Data** 📊: If input-output sequences have a similar structure, it outperforms Bahdanau Attention.  \n",
    "\n",
    "\n",
    "\n",
    "## **🌟 Summary of Luong Attention**\n",
    "1️⃣ **Compute attention scores** using a dot product.  \n",
    "2️⃣ **Apply softmax** to get attention weights.  \n",
    "3️⃣ **Compute weighted sum** to get a context vector.  \n",
    "4️⃣ **Combine context with decoder state** to predict the next word.  \n",
    "\n",
    "🚀 **Luong Attention is used in many NLP models like OpenNMT and early versions of Transformers!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔥 **Luong vs. Bahdanau Attention - The Key Differences** 🔥  \n",
    "\n",
    "| Feature 🔍 | **Bahdanau Attention (Additive)** | **Luong Attention (Multiplicative)** |\n",
    "|-----------|---------------------------------|--------------------------------|\n",
    "| **Inventor** 👨‍🔬 | Dzmitry Bahdanau (2014) | Minh-Thang Luong (2015) |\n",
    "| **Computation of Scores** 🧮 | Uses a small feedforward neural network to compute alignment scores (**additive attention**) | Uses dot product or a weight matrix to compute scores (**multiplicative attention**) |\n",
    "| **Formula for Score Calculation** 📏 | \\[ e_i = v_a^T \\tanh(W_a [h_i; s_t]) \\] | **Dot**: \\( e_i = h_i^T s_t \\)  **General**: \\( e_i = s_t^T W_a h_i \\)  **Concatenation**: \\( e_i = v_a^T \\tanh(W_a [h_i; s_t]) \\) |\n",
    "| **Computational Efficiency** ⚡ | **Slower** (uses extra parameters for feedforward NN) | **Faster** (uses simple dot product) |\n",
    "| **When to Use?** 🎯 | ✅ **Good for variable-length sequences** ✅ Works well for **long sentences** (better handling of alignment) | ✅ **Better for structured sequences** ✅ Works well when **input and output have similar structures** |\n",
    "| **Complexity** 📊 | More complex (extra weights and non-linearity) | Simpler (only matrix multiplications) |\n",
    "| **Common Applications** 🤖 | Used in **early Neural Machine Translation (NMT)** models (e.g., **Seq2Seq** for long text) | Used in **modern machine translation** (e.g., **OpenNMT, Google’s NMT system**) |\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Which One is Better?**\n",
    "- **Use Bahdanau (Additive) Attention** if you have long or **variable-length** sentences and need more flexibility in learning alignments.  \n",
    "- **Use Luong (Multiplicative) Attention** if **speed and efficiency** are important and your input/output structures are similar.\n",
    "\n",
    "🚀 **Luong Attention is often preferred in practice due to its efficiency!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bahdanau vs. Luong Attention – Explained in Simple Terms** 🎯  \n",
    "\n",
    "Imagine you are a teacher helping students (decoder) answer questions based on a textbook (encoder). The teacher **does not** just memorize the entire textbook (like the original encoder-decoder model). Instead, they **focus on important sections** while answering. This focus is **attention**!  \n",
    "\n",
    "Now, let’s compare **Bahdanau** and **Luong** attention with real-life examples.  \n",
    "\n",
    "\n",
    "### **📘 Bahdanau Attention (Additive Attention) – \"Thoughtful Teacher\"**  \n",
    "A **thoughtful teacher** reads every part of the book **carefully** before answering. The teacher:  \n",
    "✅ **Thinks deeply** about which sections are important  \n",
    "✅ **Mixes different ideas together** before giving an answer  \n",
    "✅ Uses **more effort and extra steps** to decide what’s important  \n",
    "\n",
    "**Example:**  \n",
    "- A student asks, “What is gravity?”  \n",
    "- The teacher looks at different **paragraphs** in a physics book, **compares them carefully**, and then **blends** the ideas to give the answer.  \n",
    "\n",
    "👨‍🏫 **Bahdanau is good when answers need deep reasoning and multiple references** but **is a bit slow** because of extra thinking.  \n",
    "\n",
    "\n",
    "\n",
    "### **📗 Luong Attention (Multiplicative Attention) – \"Fast Teacher\"**  \n",
    "A **fast teacher** quickly checks **only the most relevant** section of the book and gives an answer. The teacher:  \n",
    "✅ Looks at the book but **does not overthink**  \n",
    "✅ **Matches the question directly** to relevant sections  \n",
    "✅ **Uses quick calculations** (multiplication) instead of blending ideas  \n",
    "\n",
    "**Example:**  \n",
    "- A student asks, “What is Newton’s First Law?”  \n",
    "- The teacher quickly **scans the index**, finds the right section, and reads it **without too much extra processing**.  \n",
    "\n",
    "👨‍🏫 **Luong is good when answers can be found quickly** in **directly matching sections**, making it **faster** but sometimes less flexible.  \n",
    "\n",
    "### **⏳ Key Difference in Simple Words**  \n",
    "| 🧐 **Aspect** | 🤔 **Bahdanau (Additive)** | ⚡ **Luong (Multiplicative)** |\n",
    "|-------------|--------------------------|--------------------------|\n",
    "| **How it works?** | **Thinks deeply** before choosing focus | **Quickly picks the most relevant** section |\n",
    "| **Computation?** | **Extra steps (slower)**, carefully blends ideas | **Simple math (faster)**, direct comparison |\n",
    "| **Example** | **A teacher checking multiple pages carefully before answering** | **A teacher quickly finding the right page and reading from it** |\n",
    "| **Best for?** | **Long and complex answers** | **Quick and straightforward answers** |\n",
    "\n",
    "\n",
    "### **🔥 Which One to Use?**\n",
    "- If **your task is complex and requires looking at different parts of input carefully** → **Use Bahdanau**  \n",
    "- If **your task is structured and the answer is directly linked to one part of the input** → **Use Luong**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 **Transformers in Deep Learning: A Complete Guide**  \n",
    "\n",
    "Transformers are a game-changing deep learning architecture that has revolutionized **Natural Language Processing (NLP)** and beyond. First introduced in the paper **\"Attention Is All You Need\"** by Vaswani et al. (2017), transformers have since powered state-of-the-art AI models like **BERT, GPT, T5, and Vision Transformers (ViTs).**  \n",
    "\n",
    "\n",
    "\n",
    "# 🔥 **What Are Transformers?**  \n",
    "\n",
    "A **Transformer** is a neural network model that relies on a mechanism called **self-attention** to process input data **in parallel**, making it highly efficient and powerful. Unlike earlier models such as **RNNs (Recurrent Neural Networks) and LSTMs**, which process data sequentially, transformers can analyze **entire input sequences at once**, drastically improving speed and accuracy.\n",
    "\n",
    "> 🌟 **Key Idea**: Instead of processing words one by one like RNNs, transformers look at the entire sentence at once and determine the importance of each word to others using **attention mechanisms.**\n",
    "\n",
    "\n",
    "\n",
    "# 🧠 **How Transformers Work? (Simplified)**\n",
    "Transformers consist of an **encoder-decoder structure**, each with **multi-head self-attention and feed-forward layers**.\n",
    "\n",
    "### 🔹 **Encoder (Understanding Input)**\n",
    "- Takes input (e.g., a sentence) and processes it using self-attention.\n",
    "- Captures relationships between words, even if they are far apart.\n",
    "\n",
    "### 🔹 **Self-Attention Mechanism**\n",
    "- **Example**: In the sentence *\"The cat sat on the mat.\"*, the model understands that *\"cat\"* and *\"sat\"* are more related than *\"cat\"* and *\"mat\"*.\n",
    "- Assigns **attention scores** to words based on their importance.\n",
    "\n",
    "### 🔹 **Decoder (Generating Output)**\n",
    "- Generates predictions **word-by-word** while looking at the encoder’s output.\n",
    "- Used in **translation tasks (English → French), text generation (GPT models), etc.**.\n",
    "\n",
    "### 🔹 **Positional Encoding**\n",
    "- Since transformers process all words at once, they need a way to track word order.\n",
    "- They add **positional embeddings** to retain sequential information.\n",
    "\n",
    "\n",
    "\n",
    "# 💡 **Why Are Transformers Used? (Advantages)**  \n",
    "✅ **Parallel Processing** – Unlike RNNs, transformers process entire input sequences at once, making training **faster** and more efficient.  \n",
    "\n",
    "✅ **Long-Range Dependencies** – They capture relationships between words across **long texts**, solving RNNs' **vanishing gradient problem**.  \n",
    "\n",
    "✅ **State-of-the-Art Performance** – Models like **BERT, GPT-4, and T5** achieve **human-like performance** in NLP tasks.  \n",
    "\n",
    "✅ **Versatility** – Used for **text, images, speech, and even protein structure prediction (AlphaFold)**.  \n",
    "\n",
    "✅ **Scalability** – Transformers are the backbone of **large AI models**, scaling up with billions of parameters (e.g., GPT-4 has 1.76 trillion parameters!).  \n",
    "\n",
    "✅ **No Sequential Bottleneck** – Unlike RNNs, transformers **do not require sequential computation**, making them highly efficient for training on **GPUs and TPUs**.\n",
    "\n",
    "\n",
    "\n",
    "# ⚠️ **Challenges of Transformers (Disadvantages)**  \n",
    "❌ **High Computational Cost** – Training large models like **GPT-4 or BERT** requires **massive GPUs and TPUs**.  \n",
    "\n",
    "❌ **Huge Memory Requirements** – Self-attention requires **quadratic** memory growth with input size, making long-text processing expensive.  \n",
    "\n",
    "❌ **Data-Hungry** – Transformers need **huge datasets** to generalize well, unlike traditional models.  \n",
    "\n",
    "❌ **Lack of Interpretability** – Unlike simpler models like decision trees, transformers act as **black boxes**, making it hard to understand why they make certain decisions.  \n",
    "\n",
    "❌ **Ethical Concerns** – Large-scale models can **amplify biases** present in training data and **generate misinformation**.\n",
    "\n",
    "\n",
    "\n",
    "# 🌍 **Real-World Applications of Transformers**  \n",
    "\n",
    "### 💬 **1. Natural Language Processing (NLP)**\n",
    "- **Machine Translation** (Google Translate using Transformer models)\n",
    "- **Chatbots & Virtual Assistants** (ChatGPT, Bard, Alexa)\n",
    "- **Text Summarization** (Abstractive & Extractive summarization)\n",
    "- **Speech Recognition** (ASR models like Whisper, Kaldi)\n",
    "\n",
    "### 🤖 **2. AI-Generated Content**\n",
    "- **Text Generation** (GPT-4 for AI writing, chatbots, story generation)\n",
    "- **Code Completion** (GitHub Copilot, OpenAI Codex)\n",
    "\n",
    "### 🎥 **3. Computer Vision**\n",
    "- **Image Recognition** (Vision Transformers (ViT), DINO)\n",
    "- **Video Processing** (Detecting objects & scenes in videos)\n",
    "\n",
    "### 🔊 **4. Speech & Audio Processing**\n",
    "- **Speech-to-Text** (ASR models like Whisper, DeepSpeech)\n",
    "- **Text-to-Speech (TTS)** (Google WaveNet, VALL-E)\n",
    "\n",
    "### 🧬 **5. Biology & Healthcare**\n",
    "- **Drug Discovery** (AI-driven drug design)\n",
    "- **Protein Folding** (AlphaFold 2 revolutionizing bioinformatics)\n",
    "\n",
    "### 📈 **6. Finance & Stock Market**\n",
    "- **Algorithmic Trading** (Predicting stock trends using NLP-based news analysis)\n",
    "- **Fraud Detection** (Analyzing financial transactions)\n",
    "\n",
    "\n",
    "\n",
    "# 🔮 **The Future of Transformers**\n",
    "Transformers are shaping the future of **AI and deep learning**. With innovations like **efficient attention mechanisms (e.g., Linformer, BigBird), sparse transformers, and multimodal models**, we can expect **smarter AI that understands text, images, and speech better than ever.**\n",
    "\n",
    "🚀 **The possibilities are endless!** From **AI tutors** to **autonomous robots**, transformers will continue to redefine how we interact with technology.\n",
    "\n",
    "\n",
    "\n",
    "# 🎯 **Final Thoughts**\n",
    "Transformers are a **revolutionary architecture** that outperforms traditional models in **speed, accuracy, and versatility**. Despite challenges like **high computational costs**, they are **pushing the boundaries** of AI applications across **NLP, vision, speech, and even science!**\n",
    "\n",
    "![](images/transformers.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔥 **Self-Attention in Transformers: A Deep Dive**  \n",
    "\n",
    "Self-attention is the **core mechanism** behind transformers, allowing them to **weigh the importance of different words** in a sentence while processing text. It enables models to **capture long-range dependencies**, unlike RNNs and LSTMs, which struggle with distant word relationships.  \n",
    "\n",
    "\n",
    "\n",
    "# 🤔 **What is Self-Attention?**  \n",
    "Self-attention allows each word in a sentence to focus on **other relevant words** to understand the context better. It helps a transformer model determine **which words matter the most** when making predictions.  \n",
    "\n",
    "### **Example: Translating a Sentence**  \n",
    "Let’s take a sentence:  \n",
    "\n",
    "💬 **\"The cat sat on the mat.\"**  \n",
    "\n",
    "A traditional model might process this word by word, but **self-attention** ensures that **\"sat\"** is more connected to **\"cat\"** than to **\"mat\"**, making the model **more context-aware**.  \n",
    "\n",
    "\n",
    "\n",
    "# 🚀 **How Does Self-Attention Work?**  \n",
    "The self-attention mechanism follows a step-by-step process:  \n",
    "\n",
    "### **1️⃣ Convert Words into Vectors (Embeddings)**\n",
    "- Words are converted into **word embeddings** (vectors) using techniques like **Word2Vec, FastText, or BERT embeddings**.\n",
    "- These embeddings capture **semantic meaning**.\n",
    "\n",
    "### **2️⃣ Create Query, Key, and Value (Q, K, V) Matrices**\n",
    "Each word embedding is transformed into **three vectors**:  \n",
    "- **Query (Q):** What this word is searching for  \n",
    "- **Key (K):** What this word has to offer  \n",
    "- **Value (V):** The actual word representation  \n",
    "\n",
    "Each of these is learned using **weight matrices**, which the transformer **learns** during training.  \n",
    "\n",
    "> 🎯 **Example:**  \n",
    "> - \"The\" → Q1, K1, V1  \n",
    "> - \"cat\" → Q2, K2, V2  \n",
    "> - \"sat\" → Q3, K3, V3  \n",
    "\n",
    "### **3️⃣ Compute Attention Scores**\n",
    "Now, we **compare the Query of one word with the Key of every other word** to determine **how much attention one word should give to another**.  \n",
    "- This is done using the **dot product** between Query and Key:  \n",
    "\n",
    "$$\n",
    "\\text{Attention Score} = Q_i \\cdot K_j\n",
    "$$\n",
    "\n",
    "Each word's Query is compared with all other words' Keys, forming an **Attention Score Matrix**.\n",
    "\n",
    "\n",
    "\n",
    "### **4️⃣ Apply Softmax to Normalize Scores**\n",
    "To make sure the attention scores add up to 1, we apply a **Softmax function**, turning raw scores into **probabilities**.  \n",
    "\n",
    "$$\n",
    "\\text{Softmax}(QK^T) = \\frac{e^{score}}{\\sum e^{score}}\n",
    "$$\n",
    "\n",
    "Words with higher probabilities receive **more attention**!  \n",
    "\n",
    "\n",
    "\n",
    "### **5️⃣ Multiply Attention Scores with Value (V)**\n",
    "Each word’s attention scores are multiplied with the **Value (V) vectors** to compute the final representation of the word.  \n",
    "\n",
    "> 🔍 **Why use Value (V)?**  \n",
    "> - Q and K **decide attention**, but **V contains the actual meaning of the word**!  \n",
    "\n",
    "\n",
    "\n",
    "### **6️⃣ Combine All Weighted Values to Get Output**\n",
    "Once each word is represented with its attended information, we sum them up and get the final **attention-weighted representation** of each word.  \n",
    "\n",
    "This allows words like **\"cat\"** and **\"sat\"** to be closely related, while **\"on\"** and **\"mat\"** get lower attention.\n",
    "\n",
    "\n",
    "\n",
    "# 🔥 **Multi-Head Self-Attention: The Next Level!**  \n",
    "Instead of doing self-attention once, **multi-head attention** applies self-attention **multiple times in parallel**, capturing **different aspects of relationships** between words.\n",
    "\n",
    "- Some heads may focus on **syntax** (e.g., subject-verb agreement).  \n",
    "- Others may focus on **meaning** (e.g., relationships between entities).  \n",
    "\n",
    "After processing, all these heads are **concatenated** and passed through a **feed-forward layer**.\n",
    "\n",
    "\n",
    "\n",
    "# ⚡ **Why is Self-Attention Powerful?**  \n",
    "\n",
    "✅ **Captures Long-Range Dependencies** – Unlike RNNs, transformers can learn relationships between words **far apart** in a sentence.  \n",
    "\n",
    "✅ **Parallel Computation** – Unlike sequential RNNs, self-attention processes the whole sequence **at once**, making it **faster**.  \n",
    "\n",
    "✅ **Context-Aware Representations** – It dynamically **adjusts** based on surrounding words, unlike static word embeddings.  \n",
    "\n",
    "✅ **Handles Ambiguity** – Words like *\"bank\"* (river vs. finance) can be understood **based on context**.  \n",
    "\n",
    "\n",
    "\n",
    "# 🔥 **Self-Attention in Action: A Simple Example**  \n",
    "\n",
    "Imagine processing:  \n",
    "💬 **\"The animal didn't cross the street because it was too tired.\"**  \n",
    "\n",
    "What does **\"it\"** refer to? 🧐  \n",
    "\n",
    "- Traditional models might struggle.  \n",
    "- With **self-attention**, \"it\" assigns higher attention to **\"animal\"**, helping the model **understand context better**.\n",
    "\n",
    "\n",
    "\n",
    "# 🔮 **Final Thoughts**  \n",
    "Self-attention is the **backbone** of transformers, enabling them to process text efficiently and with **context-awareness**. It powers **state-of-the-art AI models** like **BERT, GPT, T5, and Vision Transformers (ViTs)**, making them the **dominant architecture in AI today**. 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let’s break down **self-attention** in the simplest way possible! 😊  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Imagine You’re in a Classroom!**\n",
    "You are in a classroom, and the teacher asks a question:  \n",
    "\n",
    "**\"Who won the World Cup in 2011?\"**  \n",
    "\n",
    "Now, everyone in the class starts thinking 🤔. Some students might **remember the answer quickly**, while others may need a **hint**.  \n",
    "\n",
    "This is exactly what self-attention does! **Each word in a sentence “looks at” the other words** to understand which ones are important.  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 How Does It Work? (Super Simple)**\n",
    "Let’s take an example sentence:  \n",
    "\n",
    "💬 **\"The cat sat on the mat.\"**  \n",
    "\n",
    "Each word in this sentence tries to **figure out which other words are important** for understanding its meaning.  \n",
    "\n",
    "🔹 When **\"cat\"** is looking around, it realizes that **\"sat\"** is more important than **\"mat\"**, because \"sat\" tells us what the cat is doing.  \n",
    "\n",
    "🔹 When **\"on\"** looks around, it sees **\"mat\"** is more important because it tells us **where** the cat sat.  \n",
    "\n",
    "\n",
    "\n",
    "## **💡 The Key Idea: Words Pay Attention to Each Other!**\n",
    "Instead of treating every word equally, **self-attention helps words focus on the most relevant words** to understand the sentence better.  \n",
    "\n",
    "Think of it like a **group discussion**:  \n",
    "- Each student (word) listens to what others are saying.  \n",
    "- Some voices are more important, so they listen **more closely** to them.  \n",
    "- This helps everyone understand the topic **better and faster**!  \n",
    "\n",
    "\n",
    "\n",
    "## **🔄 Self-Attention in Action**\n",
    "1️⃣ Each word in a sentence **asks**: *\"Which words are important to me?\"*  \n",
    "2️⃣ It **checks** all other words and **gives them scores** (higher scores = more important).  \n",
    "3️⃣ It **focuses more** on high-scored words while forming the final sentence understanding.  \n",
    "\n",
    "\n",
    "\n",
    "## **👀 Real-Life Example: How We Use Self-Attention**\n",
    "Let’s say your friend texts you:  \n",
    "\n",
    "💬 **\"I went to a party last night. It was amazing!\"**  \n",
    "\n",
    "🔹 **\"It\"** → What does \"it\" refer to? 🤔  \n",
    "- Your brain **does self-attention** and realizes **\"it\" refers to \"party\"**, not \"night\" or \"went\".  \n",
    "\n",
    "That’s exactly how self-attention helps AI models understand text! 🤖  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Why is Self-Attention So Powerful?**\n",
    "✅ **Understands Context** – Words like \"bank\" (river or money?) are understood **based on nearby words**.  \n",
    "✅ **Handles Long Sentences** – Unlike older models (RNNs), it doesn’t forget earlier words.  \n",
    "✅ **Super Fast** – Looks at **all words at once** instead of one by one.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔮 Final Thought**\n",
    "Think of self-attention like **highlighting important words** while reading a book. It helps transformers **focus on what truly matters** instead of treating every word the same.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's break down **Query (Q), Key (K), and Value (V)** in Transformers **step by step** in a **simple and intuitive way**.  \n",
    "\n",
    "\n",
    "\n",
    "### **🧠 Why Do We Need Q, K, V?**  \n",
    "Imagine you're in a **library** 📚, and you're **looking for a book** about \"Deep Learning\".  \n",
    "\n",
    "1️⃣ **Query (Q)** → What you are searching for → **(\"Deep Learning\")**  \n",
    "2️⃣ **Key (K)** → The labels on books in the library  \n",
    "3️⃣ **Value (V)** → The actual book content  \n",
    "\n",
    "👉 **The idea**: You **compare** your Query (Q) with the Keys (K) on the bookshelves. The books **most relevant** to your query get the **highest score**, and you read their content (V) with more attention.  \n",
    "\n",
    "This is exactly how **self-attention in Transformers** works! 🚀  \n",
    "\n",
    "## **💡 How Q, K, V Work in Transformers**\n",
    "Each word in a sentence is **transformed into three vectors**:  \n",
    "- **Query (Q)** – What this word is searching for in other words.  \n",
    "- **Key (K)** – How relevant this word is to other words.  \n",
    "- **Value (V)** – The actual information of this word.  \n",
    "\n",
    "💬 **Example Sentence:**  \n",
    "👉 \"The cat sat on the mat.\"  \n",
    "\n",
    "Now, let's focus on the word **\"cat\"** 🐱:  \n",
    "\n",
    "| Word  | Query (Q) | Key (K) | Value (V) |\n",
    "|--------|----------|----------|----------|\n",
    "| The   | Looks for relevant words | Matches with \"The\" | \"The\" itself |\n",
    "| **Cat** 🐱 | Looks for context | Matches \"sat\" | \"Cat\" itself |\n",
    "| Sat   | Looks for subject | Matches \"cat\" | \"Sat\" itself |\n",
    "| On    | Looks for location | Matches \"mat\" | \"On\" itself |\n",
    "| Mat   | Looks for subject | Matches \"on\" | \"Mat\" itself |\n",
    "\n",
    "\n",
    "\n",
    "## **🔢 How Does Attention Work? (Step-by-Step)**\n",
    "💡 **Step 1: Calculate Attention Scores**  \n",
    "Each word's **Query (Q)** is compared with every other word's **Key (K)** to get a similarity score. The more similar they are, the more attention the word pays to it.  \n",
    "\n",
    "💡 **Step 2: Apply Softmax to Get Attention Weights**  \n",
    "The scores are converted into a probability distribution (softmax) so that the focus is distributed properly.  \n",
    "\n",
    "💡 **Step 3: Multiply by Values (V)**  \n",
    "Each word's **Value (V)** is weighted based on attention scores. Words that get higher attention contribute more to the final output.  \n",
    "\n",
    "💡 **Step 4: Update the Word Representation**  \n",
    "The final representation of each word is updated based on its weighted combination of all words in the sentence.  \n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Why Is This Powerful?**\n",
    "✅ **Captures Context** – Words can dynamically change their meaning based on surrounding words.  \n",
    "✅ **Handles Long Sentences** – Unlike RNNs, Transformers can understand **distant relationships** between words.  \n",
    "✅ **Improves NLP Tasks** – Used in **translation, chatbots, text summarization, etc.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Final Takeaway**\n",
    "Think of **Q, K, V** as how we **search for, match, and retrieve information** in daily life. **Self-attention in Transformers** follows the same logic to understand text **contextually and efficiently**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let’s break down **Scaled Dot-Product Attention** in Transformers **step by step** in the simplest way possible! 😊  \n",
    "\n",
    "\n",
    "\n",
    "### **🔍 Why Do We Need Scaled Dot-Product Attention?**  \n",
    "Before jumping into the formula, let's first understand **why** we need **Scaled Dot-Product Attention**.  \n",
    "\n",
    "Imagine you are in a classroom, and the teacher asks a question:  \n",
    "👉 **\"Who discovered gravity?\"**  \n",
    "\n",
    "Your brain **immediately connects** this to \"Isaac Newton\" 🍏.  \n",
    "\n",
    "✅ You ignore unnecessary words.  \n",
    "✅ You focus only on the **important words** in the sentence.  \n",
    "\n",
    "This is exactly what **Scaled Dot-Product Attention** does! It helps the Transformer **focus on the right words efficiently**. 🚀  \n",
    "\n",
    "\n",
    "\n",
    "### **🔢 Step-by-Step: Scaled Dot-Product Attention**\n",
    "The attention mechanism takes three inputs:  \n",
    "- **Query (Q)** → What each word is looking for.  \n",
    "- **Key (K)** → What information each word has.  \n",
    "- **Value (V)** → The actual meaning of each word.  \n",
    "\n",
    "👉 **Attention(Q, K, V) = Softmax( (Q × Kᵀ) / √d ) × V**  \n",
    "\n",
    "Let’s break this formula down step by step.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1️⃣: Compute Q × Kᵀ (Dot Product of Queries and Keys)**  \n",
    "Each word **compares itself** with all other words to see **which words are important**.  \n",
    "\n",
    "💬 **Example Sentence:**  \n",
    "👉 \"The cat sat on the mat.\"  \n",
    "\n",
    "If **Q (cat)** interacts with **K (sat, mat, etc.)**, we get similarity scores:  \n",
    "\n",
    "| Words Compared | Dot Product Score |\n",
    "|---------------|------------------|\n",
    "| Cat & The   | 0.2  |\n",
    "| Cat & Cat   | 1.0  |\n",
    "| Cat & Sat   | 0.8  |\n",
    "| Cat & On    | 0.1  |\n",
    "| Cat & Mat   | 0.5  |\n",
    "\n",
    "💡 **Higher scores = more important words!**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2️⃣: Scale by √d (Why Do We Scale?)**  \n",
    "👉 If the dot product values are **too large**, softmax will give **extremely high weights** to some words and ignore others.  \n",
    "👉 To prevent this, we **divide by √d**, where **d is the embedding size**.  \n",
    "\n",
    "This **balances** the attention distribution, so we don’t focus too much on just one word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3️⃣: Apply Softmax (Convert Scores to Probabilities)**  \n",
    "Softmax makes sure that all attention scores **add up to 1** (like probabilities).  \n",
    "\n",
    "🔹 High values become **closer to 1** (high attention).  \n",
    "🔹 Low values become **closer to 0** (low attention).  \n",
    "\n",
    "| Word Pair | Scaled Score | Softmax Output (Attention Weight) |\n",
    "|-----------|-------------|--------------------------------|\n",
    "| Cat & The | 0.2 → 0.05 | 0.10 |\n",
    "| Cat & Cat | 1.0 → 0.25 | 0.40 |\n",
    "| Cat & Sat | 0.8 → 0.20 | 0.30 |\n",
    "| Cat & On  | 0.1 → 0.02 | 0.05 |\n",
    "| Cat & Mat | 0.5 → 0.12 | 0.15 |\n",
    "\n",
    "💡 **Now, the Transformer knows how much focus to give to each word!**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 4️⃣: Multiply by V (Weighted Sum of Values)**  \n",
    "Finally, we **multiply** these attention scores with **V (Values)** to get the final representation of the word.  \n",
    "\n",
    "🔹 Words that got **higher attention weights** contribute **more** to the final meaning.  \n",
    "\n",
    "**Final Output:**\n",
    "- **Cat’s updated representation** now **incorporates** information from **Sat, Mat**, and other relevant words.  \n",
    "\n",
    "\n",
    "\n",
    "### **🚀 Why is Scaled Dot-Product Attention So Powerful?**\n",
    "✅ **Captures Important Relationships** → Finds meaningful word connections.  \n",
    "✅ **Balances Attention Distribution** → Prevents one word from dominating.  \n",
    "✅ **Computationally Efficient** → Works in parallel, unlike older models (RNNs).  \n",
    "\n",
    "\n",
    "\n",
    "### **🔥 Final Takeaway**\n",
    "Think of **Scaled Dot-Product Attention** as a **smart highlighter** 🖍️ that helps the Transformer **focus on the most important words** in a sentence, making the model **understand language better**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate the **geometric intuition of self-attention** using a **simple sentence**. I'll keep it **easy and visual** so that you get a clear **intuition** of how self-attention works in **vector space**. 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## **🔍 Problem Setup:**\n",
    "We take a simple sentence:  \n",
    "\n",
    "👉 **\"I love NLP\"**  \n",
    "\n",
    "💡 **Goal:** Compute self-attention **manually** using vectors, dot product, and softmax!  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1️⃣: Convert Words into Vector Representations**\n",
    "Each word is transformed into a **vector** (we assume these are pre-trained embeddings).  \n",
    "\n",
    "Let's assign some **simple 2D vectors** for each word:  \n",
    "\n",
    "| Word  | Vector Representation (Embeddings) |\n",
    "|-------|------------------------------------|\n",
    "| **I**    | [1, 2]  |\n",
    "| **Love** ❤️ | [2, 3]  |\n",
    "| **NLP** 🤖 | [3, 1]  |\n",
    "\n",
    "These vectors **live in a 2D space**, and we will perform self-attention using **dot product, softmax, and weighted sum**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2️⃣: Compute Queries (Q), Keys (K), and Values (V)**  \n",
    "Each word has:  \n",
    "- **Query (Q)** → What this word is searching for  \n",
    "- **Key (K)** → How relevant this word is  \n",
    "- **Value (V)** → The actual content of the word  \n",
    "\n",
    "For simplicity, let's **assume Q = K = V**, so we take the same word vectors as Q, K, and V.\n",
    "\n",
    "| Word  | Query (Q)  | Key (K)  | Value (V)  |\n",
    "|-------|-----------|-----------|-----------|\n",
    "| **I**    | [1, 2]  | [1, 2]  | [1, 2]  |\n",
    "| **Love** ❤️ | [2, 3]  | [2, 3]  | [2, 3]  |\n",
    "| **NLP** 🤖 | [3, 1]  | [3, 1]  | [3, 1]  |\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3️⃣: Compute Attention Scores using Dot Product (Q × Kᵀ)**  \n",
    "Each word's **Query (Q)** is compared with every other word’s **Key (K)** using the **dot product**.  \n",
    "\n",
    "#### **Dot Product Formula:**  \n",
    "$$\n",
    "\\text{Score} = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "Let’s compute the dot product for all words:\n",
    "\n",
    "#### **Dot product for \"I\" with all words (Q = [1,2])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **I & I** | (1×1) + (2×2) = 1 + 4  | **5** |\n",
    "| **I & Love** | (1×2) + (2×3) = 2 + 6  | **8** |\n",
    "| **I & NLP** | (1×3) + (2×1) = 3 + 2  | **5** |\n",
    "\n",
    "#### **Dot product for \"Love\" with all words (Q = [2,3])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **Love & I** | (2×1) + (3×2) = 2 + 6  | **8** |\n",
    "| **Love & Love** | (2×2) + (3×3) = 4 + 9  | **13** |\n",
    "| **Love & NLP** | (2×3) + (3×1) = 6 + 3  | **9** |\n",
    "\n",
    "#### **Dot product for \"NLP\" with all words (Q = [3,1])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **NLP & I** | (3×1) + (1×2) = 3 + 2  | **5** |\n",
    "| **NLP & Love** | (3×2) + (1×3) = 6 + 3  | **9** |\n",
    "| **NLP & NLP** | (3×3) + (1×1) = 9 + 1  | **10** |\n",
    "\n",
    "So, we get the **attention score matrix**:\n",
    "\n",
    "$$\n",
    "S =\n",
    "\\begin{bmatrix}\n",
    "5 & 8 & 5 \\\\\n",
    "8 & 13 & 9 \\\\\n",
    "5 & 9 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4️⃣: Apply Scaling (Divide by √d)**\n",
    "The embedding dimension (**d**) here is **2** (since our vectors are 2D).  \n",
    "\n",
    "$$\n",
    "\\text{Scale Factor} = \\sqrt{2} = 1.41\n",
    "$$\n",
    "\n",
    "We **divide each score** by 1.41 to balance the attention distribution:\n",
    "\n",
    "| Scaled Score Matrix |\n",
    "|---------------------|\n",
    "| **5 / 1.41 = 3.54**   **8 / 1.41 = 5.67**  **5 / 1.41 = 3.54**  |\n",
    "| **8 / 1.41 = 5.67**   **13 / 1.41 = 9.22**  **9 / 1.41 = 6.38**  |\n",
    "| **5 / 1.41 = 3.54**   **9 / 1.41 = 6.38**  **10 / 1.41 = 7.09**  |\n",
    "\n",
    "\n",
    "\n",
    "### **Step 5️⃣: Apply Softmax to Get Attention Weights**\n",
    "Now, we apply **softmax** to normalize the scores into probabilities.  \n",
    "\n",
    "Softmax formula:  \n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum e^{x_i}}\n",
    "$$\n",
    "\n",
    "For example, applying softmax to the first row:\n",
    "$$\n",
    "e^{3.54} = 34.5, \\quad e^{5.67} = 289.6, \\quad e^{3.54} = 34.5\n",
    "$$\n",
    "Sum = **34.5 + 289.6 + 34.5 = 358.6**  \n",
    "\n",
    "Now, compute **softmax values**:\n",
    "- **I → I:** **34.5 / 358.6 = 0.096**  \n",
    "- **I → Love:** **289.6 / 358.6 = 0.81**  \n",
    "- **I → NLP:** **34.5 / 358.6 = 0.096**  \n",
    "\n",
    "Similarly, we compute for all words to get the **final attention matrix**:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0.096 & 0.81 & 0.096 \\\\\n",
    "0.19 & 0.64 & 0.17 \\\\\n",
    "0.10 & 0.45 & 0.45\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 6️⃣: Compute Final Output by Multiplying with Values (V)**\n",
    "Final representation for **\"I\"** is:\n",
    "\n",
    "$$\n",
    "\\text{I} = (0.096 \\times [1,2]) + (0.81 \\times [2,3]) + (0.096 \\times [3,1])\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.096, 0.192] + [1.62, 2.43] + [0.288, 0.096]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [2.00, 2.71]\n",
    "$$\n",
    "\n",
    "Similarly, compute for **Love** and **NLP** to get updated embeddings.\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Final Takeaway (Geometric View)**\n",
    "1️⃣ Each word **compares itself** with all others using **dot product**.  \n",
    "2️⃣ The **softmax** turns these into attention weights (how much attention to pay).  \n",
    "3️⃣ The final word representation is a **weighted sum** of other words based on attention scores.  \n",
    "\n",
    "💡 **Self-attention gives words new, context-rich embeddings!** 🚀  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🌟 Why is **\"Self-Attention\"** Called \"Self\"?  \n",
    "\n",
    "\"Self-attention\" is called **\"self\"** because, unlike traditional attention mechanisms that focus on different parts of an input sequence **relative to another sequence** (e.g., encoder-decoder attention), self-attention operates **within** the same sequence.  \n",
    "\n",
    "Each token (word or feature) in the sequence attends to **all other tokens, including itself** to compute its new representation. This allows the model to capture **global dependencies**, regardless of their position in the sequence.  \n",
    "\n",
    "🔹 **Example Sentence:**  \n",
    "*\"The cat sat on the mat.\"*  \n",
    "\n",
    "✅ The word **\"cat\"** can pay attention to **\"sat\"** to understand the action.  \n",
    "✅ The word **\"mat\"** can attend to **\"on\"** for spatial context.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Self-Attention vs. Luong Attention**  \n",
    "\n",
    "### ✨ **1. Self-Attention (Transformer Attention)**\n",
    "🛠 **Used in:** Transformers (e.g., **BERT, GPT**).  \n",
    "🌎 **Key Idea:** Every token **attends to all other tokens** in the input sequence.  \n",
    "🔗 **Best for:** Capturing **long-range dependencies**.  \n",
    "⚡ **Fully Parallelizable** – No sequential dependencies!  \n",
    "\n",
    "#### 🔍 **How It Works?**\n",
    "1️⃣ Compute **Query (Q), Key (K), and Value (V)** matrices from the input.  \n",
    "2️⃣ Compute **attention scores** using:  \n",
    "   $$\n",
    "   \\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "   $$  \n",
    "3️⃣ Multiply scores with **Value (V)** matrix to get the new representation.  \n",
    "\n",
    "\n",
    "\n",
    "### 🎯 **2. Luong Attention (Traditional Attention)**\n",
    "🛠 **Used in:** **Seq2Seq (LSTM, GRU)** with attention.  \n",
    "🎯 **Key Idea:** Focuses on aligning **encoder outputs** with the **decoder state**.  \n",
    "📉 **Step-wise Calculation** – Not fully parallelizable like self-attention.  \n",
    "📍 **Best for:** Capturing dependencies **between encoder & decoder**.  \n",
    "\n",
    "#### 🔍 **How It Works?**\n",
    "1️⃣ At each decoder time step, compare the **decoder hidden state** with all **encoder outputs** to get attention scores.  \n",
    "2️⃣ Compute **context vector** as a weighted sum of encoder outputs.  \n",
    "3️⃣ Combine **context vector** with the **decoder hidden state** to predict the next token.  \n",
    "\n",
    "## 🔥 **Key Differences: Self-Attention vs. Luong Attention**\n",
    "| Feature 🏆        | Self-Attention (Transformer) ⚡ | Luong Attention (Seq2Seq) 🔄 |\n",
    "|-----------------|----------------------------|---------------------------|\n",
    "| **Works within** | Same sequence (e.g., input sentence) | Encoder-Decoder interaction |\n",
    "| **Computes Attention** | All tokens attend to all tokens | Decoder attends to encoder outputs |\n",
    "| **Parallelization** | ✅ Fully parallelizable | ❌ Step-wise (not parallelizable) |\n",
    "| **Dependency Range** | 🌍 Long-range dependencies | 🔎 Limited dependency range |\n",
    "| **Use Case** | 🤖 Transformers (BERT, GPT) | 📜 Seq2Seq (LSTMs, GRUs) |\n",
    "\n",
    "\n",
    "## 🧐 **When Should You Use Which?**  \n",
    "✅ **Use Self-Attention** when handling **long-range dependencies** (e.g., **machine translation, text generation, speech recognition**).  \n",
    "✅ **Use Luong Attention** in **RNN-based Seq2Seq models**, where tight **encoder-decoder alignment** is necessary.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Multi-Head Attention in Transformers – **Explained Visually & Clearly** 🎨🚀  \n",
    "\n",
    "Multi-Head Attention is a **superpower** 🦸‍♂️ of Transformers! It allows the model to focus on **different parts of the input simultaneously**, capturing multiple perspectives of the data. Let’s break it down!  \n",
    "\n",
    "\n",
    "## 🌟 **What is Multi-Head Attention?**  \n",
    "🔹 Imagine reading a complex book 📖. Instead of focusing on one word at a time, your brain can analyze **multiple aspects** of the text:  \n",
    "- The **main theme** 🧐  \n",
    "- The **characters' emotions** 😊😡  \n",
    "- The **story’s timeline** ⏳  \n",
    "\n",
    "Multi-Head Attention does the same! Instead of computing a **single** attention score, it learns **multiple attention patterns in parallel** to understand different relationships in the data.  \n",
    "\n",
    "🔍 **Key Idea**:  \n",
    "👉 Instead of applying **one** self-attention mechanism, we apply **multiple** attention mechanisms (heads) **in parallel** and combine their outputs.  \n",
    "\n",
    "\n",
    "\n",
    "## 🏗️ **How Does Multi-Head Attention Work?**  \n",
    "\n",
    "### 🔹 **Step 1: Compute Query, Key, and Value (Q, K, V) Matrices**  \n",
    "Each input token (word/feature) is transformed into **three** vectors:  \n",
    "- **Query (Q)** → \"What am I looking for?\"  \n",
    "- **Key (K)** → \"What do I have?\"  \n",
    "- **Value (V)** → \"What information do I carry?\"  \n",
    "\n",
    "💡 **These matrices are obtained by multiplying the input embeddings with learned weight matrices**:  \n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "Where:  \n",
    "- **X** = input embeddings  \n",
    "- **W_Q, W_K, W_V** = weight matrices for Query, Key, and Value  \n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Step 2: Compute Scaled Dot-Product Attention**  \n",
    "To determine **how much each word should pay attention to others**, we compute attention scores using the **dot-product** of Query and Key:  \n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$  \n",
    "\n",
    "👉 The **Softmax function** converts these scores into probabilities, determining **which tokens should be attended to more**.  \n",
    "\n",
    "💡 **Why divide by** $ \\sqrt{d_k} $ **?**  \n",
    "- It prevents large values in the dot-product from causing extremely sharp Softmax distributions.  \n",
    "\n",
    "### 🔹 **Step 3: Split into Multiple Attention Heads**  \n",
    "Instead of using **one** set of $ Q, K, V $, we **split** them into multiple \"heads\" 🧠 that process different parts of the input independently.  \n",
    "\n",
    "Example with 3 heads:  \n",
    "| Head 🧠 | Focus 🎯 |  \n",
    "|--------|---------|  \n",
    "| **Head 1** | Word order & position 📍 |  \n",
    "| **Head 2** | Meaning & synonyms 📝 |  \n",
    "| **Head 3** | Context & dependencies 🔄 |  \n",
    "\n",
    "Each head runs **its own attention mechanism**, capturing different types of relationships!  \n",
    "\n",
    "\n",
    "\n",
    "### 🔹 **Step 4: Concatenate & Project the Heads**  \n",
    "After computing attention in **each head**, we **concatenate** them together and pass them through a final weight matrix $ W_O $ to merge the information.  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Head}_1, \\text{Head}_2, ..., \\text{Head}_h) W_O\n",
    "$$\n",
    "\n",
    "Now, we have **a richer, more detailed representation of our input**! 🎯  \n",
    "\n",
    "\n",
    "\n",
    "## 🏆 **Why Use Multi-Head Attention?**  \n",
    "✅ **Improves learning capacity** – Each head captures different aspects of the sequence.  \n",
    "✅ **Enhances representation power** – More perspectives = **better understanding**.  \n",
    "✅ **Enables parallel processing** – Multiple heads work **simultaneously**, making training efficient!  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Multi-Head Attention vs. Single-Head Attention**\n",
    "| Feature 🏆 | **Multi-Head Attention** 🎯 | **Single-Head Attention** 🔄 |  \n",
    "|------------|----------------------------|---------------------------|  \n",
    "| **Focus** | Multiple attention perspectives 🧠 | Only one focus 🔍 |  \n",
    "| **Captures** | Complex dependencies 🔄 | Limited relationships 📏 |  \n",
    "| **Performance** | More expressive 💡 | Less effective 😕 |  \n",
    "| **Used In** | Transformers (BERT, GPT) 🤖 | Simpler RNN models 📜 |  \n",
    "\n",
    "\n",
    "## 🚀 **Where is Multi-Head Attention Used?**\n",
    "🔥 **Transformers** (BERT, GPT, T5)  \n",
    "🎙️ **Speech Recognition** (ASR models)  \n",
    "📜 **Machine Translation** (Google Translate)  \n",
    "📊 **Time-Series Forecasting**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, it’s possible! Let’s take a simple sentence and manually calculate how **Multi-Head Attention** works step by step. I'll keep the numbers simple for easier understanding.\n",
    "\n",
    "\n",
    "\n",
    "### **Sentence:**\n",
    "👉 **\"The cat sat.\"** (3 words)\n",
    "\n",
    "For simplicity, assume:\n",
    "- Each word is represented as a **3-dimensional vector**.\n",
    "- We use **2 attention heads**.\n",
    "- The dimension of each head’s query/key/value is **2** (after projection).\n",
    "\n",
    "## **Step 1: Word Embeddings**\n",
    "Each word is converted into an embedding vector (simplified numbers):\n",
    "\n",
    "| Word   | Embedding (3D) |\n",
    "|--------|--------------|\n",
    "| **The** | [1, 0, 1]  |\n",
    "| **Cat** | [0, 1, 0]  |\n",
    "| **Sat** | [1, 1, 0]  |\n",
    "\n",
    "**Matrix form (X):**  \n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 2: Compute Query, Key, and Value Matrices**\n",
    "Each input is projected into **Q, K, V** matrices using weight matrices.\n",
    "\n",
    "For **Head 1**, let’s assume:\n",
    "\n",
    "$$\n",
    "W_Q^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}, \\quad\n",
    "W_K^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix}, \\quad\n",
    "W_V^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, calculate **Q, K, V**:\n",
    "\n",
    "$$\n",
    "Q^{(1)} = X W_Q^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K^{(1)} = X W_K^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "2 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^{(1)} = X W_V^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Compute Attention Scores**\n",
    "We use the **Scaled Dot-Product Attention Formula**:\n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "1. Compute **QK^T**:\n",
    "\n",
    "$$\n",
    "QK^T =\n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 1 & 2 \\\\ \n",
    "2 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "4 & 2 & 5 \\\\ \n",
    "2 & 0 & 1 \\\\ \n",
    "3 & 1 & 3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Scale by \\( \\sqrt{d_k} = \\sqrt{2} \\approx 1.41 \\):\n",
    "\n",
    "$$\n",
    "\\frac{QK^T}{1.41} =\n",
    "\\begin{bmatrix} \n",
    "2.83 & 1.41 & 3.54 \\\\ \n",
    "1.41 & 0 & 0.71 \\\\ \n",
    "2.12 & 0.71 & 2.12 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Apply **Softmax** row-wise:\n",
    "\n",
    "Softmax normalizes each row into probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Softmax} \\left( \n",
    "\\begin{bmatrix} \n",
    "2.83 & 1.41 & 3.54 \\\\ \n",
    "1.41 & 0 & 0.71 \\\\ \n",
    "2.12 & 0.71 & 2.12 \n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "0.3 & 0.1 & 0.6 \\\\ \n",
    "0.4 & 0.2 & 0.4 \\\\ \n",
    "0.4 & 0.2 & 0.4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Compute Weighted Sum with V**\n",
    "Now, multiply **softmax scores** with **V**:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \n",
    "\\begin{bmatrix} \n",
    "0.3 & 0.1 & 0.6 \\\\ \n",
    "0.4 & 0.2 & 0.4 \\\\ \n",
    "0.4 & 0.2 & 0.4 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 1.7 \\\\ \n",
    "1 & 1.2 \\\\ \n",
    "1 & 1.2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Repeat for Other Heads & Merge**\n",
    "Each head produces a different attention output. If we had **another head**, we’d repeat steps **with different W_Q, W_K, W_V**.  \n",
    "\n",
    "Finally, we **concatenate** outputs from all heads and project them using a weight matrix \\( W_O \\).\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Final Takeaways**\n",
    "✅ **Multi-Head Attention** allows different attention heads to focus on **different aspects** of the input.  \n",
    "✅ Instead of **one** attention mechanism, we compute **multiple heads in parallel** and combine them.  \n",
    "✅ It helps the model learn **long-range dependencies efficiently**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌟 **Positional Encoding in Transformers: Full Explanation** 🌟  \n",
    "\n",
    "## 🔹 **Why Do We Need Positional Encoding?**  \n",
    "\n",
    "Unlike **RNNs (LSTMs, GRUs)**, Transformers **do not** process words in a sequential order. Instead, they process the **entire input at once** using **self-attention**.  \n",
    "\n",
    "👉 This creates a problem:  \n",
    "- **Self-attention is permutation-invariant** 🌀 → It **doesn’t know the word order**!  \n",
    "- **Example Issue:**  \n",
    "  - `\"The cat sat.\"` and `\"Sat cat the.\"` would **look the same** to the model! 😱  \n",
    "\n",
    "### 🚀 **Solution: Positional Encoding!**  \n",
    "Positional Encoding **adds information about word order** by injecting **unique position values** into each word embedding. This allows Transformers to **differentiate between word positions** while keeping full parallelization.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **How Does Positional Encoding Work?**  \n",
    "\n",
    "Each input word **embedding** is a vector (e.g., 512 dimensions in GPT, BERT).  \n",
    "👉 **Positional Encoding is another vector** (same size) added to it.  \n",
    "\n",
    "Instead of learning these values like normal weights, **Transformers use a fixed formula** based on **sine & cosine functions** to encode word positions.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **Mathematical Formula of Positional Encoding**  \n",
    "\n",
    "For a given position **$ pos $** (word index) and dimension **$ i $** (feature index), the **positional encoding** is:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ pos $ = position of the word in the sentence (e.g., **0 for first word, 1 for second**).\n",
    "- $ i $ = dimension index (even or odd).\n",
    "- $ d $ = total embedding size (e.g., **512** in GPT).\n",
    "- **Sin for even indices, Cosine for odd indices**.\n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **Why Use Sine & Cosine?**  \n",
    "\n",
    "1️⃣ **Captures Relative Positions:**  \n",
    "   - The difference between positions remains **consistent**, which helps the model learn relationships between words.  \n",
    "\n",
    "2️⃣ **Handles Long Sentences:**  \n",
    "   - The formula ensures unique encodings for **long sequences**, unlike simple index numbers.  \n",
    "\n",
    "3️⃣ **Smooth Variations:**  \n",
    "   - Since sine and cosine oscillate smoothly, small position shifts cause **small changes** in embeddings → Makes the model more robust!\n",
    "\n",
    "## 🔹 **Example: Calculating Positional Encoding**  \n",
    "\n",
    "Let’s assume **3 words**:  \n",
    "👉 `\"The\" (pos = 0)`, `\"Cat\" (pos = 1)`, `\"Sat\" (pos = 2)`  \n",
    "\n",
    "And embedding size **d = 4** (keeping it small for simplicity).\n",
    "\n",
    "#### **Step 1: Compute Positional Encoding**\n",
    "Using the formula, let’s compute:\n",
    "\n",
    "| Position | PE(0) (sin) | PE(1) (cos) | PE(2) (sin) | PE(3) (cos) |\n",
    "|----------|------------|------------|------------|------------|\n",
    "| 0 (The)  | sin(0) = 0 | cos(0) = 1 | sin(0) = 0 | cos(0) = 1 |\n",
    "| 1 (Cat)  | sin(1/10000⁰) ≈ 1 | cos(1/10000⁰) ≈ 1 | sin(1/10000¹) ≈ 0.0001 | cos(1/10000¹) ≈ 1 |\n",
    "| 2 (Sat)  | sin(2/10000⁰) ≈ 2 | cos(2/10000⁰) ≈ 1 | sin(2/10000¹) ≈ 0.0002 | cos(2/10000¹) ≈ 1 |\n",
    "\n",
    "#### **Step 2: Add Positional Encoding to Word Embeddings**\n",
    "Now, we add these **positional encodings** to the word **embeddings**.\n",
    "\n",
    "| Word  | Embedding (e.g., [1.2, 0.8, 2.5, 1.5]) | + Positional Encoding | = Final Input to Transformer |\n",
    "|-------|--------------------------------|-----------------|------------------|\n",
    "| The   | [1.2, 0.8, 2.5, 1.5] | [0, 1, 0, 1] | [1.2, 1.8, 2.5, 2.5] |\n",
    "| Cat   | [0.5, 1.1, 2.0, 1.3] | [1, 1, 0.0001, 1] | [1.5, 2.1, 2.0001, 2.3] |\n",
    "| Sat   | [1.0, 0.9, 2.3, 1.7] | [2, 1, 0.0002, 1] | [3.0, 1.9, 2.3002, 2.7] |\n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **Visualization of Positional Encoding**\n",
    "🎨 Here’s a heatmap of **Positional Encoding** over **50 positions** with **512 dimensions**:  \n",
    "\n",
    "![Positional Encoding Heatmap](images/pe.png)  \n",
    "\n",
    "- **X-axis** = position (word index).  \n",
    "- **Y-axis** = embedding dimensions.  \n",
    "- **Patterns of waves** represent the **sine & cosine variations** across positions.  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **Key Takeaways**\n",
    "✅ **Positional Encoding solves the word order problem** in Transformers.  \n",
    "✅ **Uses sine & cosine functions** to create unique position vectors.  \n",
    "✅ **Enables long-range dependencies** and smooth transitions.  \n",
    "✅ **Added to word embeddings** before self-attention.  \n",
    "\n",
    "\n",
    "### 🏆 **Final Thought: Why Not Learn Positional Encoding?**\n",
    "- **Fixed Positional Encoding** (like sine/cosine) works well for **long texts** and avoids extra training parameters.  \n",
    "- Some models (like **ALBERT, T5**) use **learnable positional embeddings**, but **vanilla Transformers** use this sine/cosine approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why Do We Use Layer Normalization Instead of Batch Normalization in Transformers?**  \n",
    "\n",
    "In deep learning, **normalization** helps stabilize training by ensuring that activations are well-scaled and centered. While **Batch Normalization (BN)** works well for CNNs and RNNs, **Layer Normalization (LN)** is preferred for Transformers. But why? 🤔  \n",
    "\n",
    "Let’s break it down! 🚀  \n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Key Reasons Why Transformers Use Layer Normalization Instead of Batch Normalization**  \n",
    "\n",
    "### 1️⃣ **BN Depends on Mini-Batch Statistics, LN Does Not!**  \n",
    "- **Batch Normalization** normalizes inputs across the **batch dimension**, meaning it relies on the statistics (mean & variance) of a batch of examples.  \n",
    "- **Layer Normalization** normalizes across the **features of a single input (token)**, making it **independent of batch size**.  \n",
    "\n",
    "💡 **Why is this important?**  \n",
    "- **In Transformers, we process a single input at inference time (e.g., one sentence at a time).** If we used Batch Norm, statistics from a single sample wouldn’t be stable, leading to inconsistent results.  \n",
    "- **Layer Norm works even when batch size = 1**, making it ideal for NLP tasks where input sizes vary.  \n",
    "\n",
    "\n",
    "\n",
    "### 2️⃣ **Batch Norm Doesn’t Work Well with Variable Sequence Lengths**  \n",
    "- **BN computes mean & variance per batch**, but **in NLP, sentence lengths vary** (e.g., \"Hello world\" vs. \"This is a long sentence\").  \n",
    "- Padding sequences in BN can distort batch statistics, making it harder to learn meaningful representations.  \n",
    "- **LN normalizes each sequence independently**, so it avoids these issues.  \n",
    "\n",
    "💡 **Why is this important?**  \n",
    "In NLP, inputs are variable-length sequences, and **BN struggles with this**. LN, however, handles it smoothly!  \n",
    "\n",
    "\n",
    "\n",
    "### 3️⃣ **BN Breaks in Autoregressive Models Like GPT**  \n",
    "- In models like **GPT (causal Transformer)**, we generate tokens **one by one** during inference.  \n",
    "- **Batch Norm requires full batches to compute statistics, but in autoregressive models, we generate one token at a time!**  \n",
    "- **Layer Norm does not depend on batches, so it works perfectly in autoregressive tasks.**  \n",
    "\n",
    "💡 **Why is this important?**  \n",
    "BN would fail when generating text token-by-token, but LN does not!  \n",
    "\n",
    "\n",
    "\n",
    "### 4️⃣ **LN Works Better for Attention Mechanisms**  \n",
    "- Transformers **use self-attention**, where each token interacts with all others in the sequence.  \n",
    "- **Batch Norm computes batch-level statistics, which can introduce unwanted interactions** between different sentences in a batch.  \n",
    "- **Layer Norm operates at the token level**, preserving the meaning of self-attention outputs.  \n",
    "\n",
    "💡 **Why is this important?**  \n",
    "Since **each token should focus on relevant words**, normalizing within the token (LN) is better than normalizing across the batch (BN).  \n",
    "\n",
    "\n",
    "\n",
    "## 🔬 **How Does Layer Normalization Work?**  \n",
    "\n",
    "Layer Normalization normalizes **each input token’s features** across all dimensions (instead of across the batch).  \n",
    "\n",
    "For an input vector **x** with **d** features:\n",
    "\n",
    "1️⃣ **Compute the mean** of the features:  \n",
    "   $$\n",
    "   \\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i\n",
    "   $$\n",
    "   \n",
    "2️⃣ **Compute the variance** of the features:  \n",
    "   $$\n",
    "   \\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "   $$\n",
    "\n",
    "3️⃣ **Normalize** each feature:  \n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "   (Where **ε** is a small value to avoid division by zero.)\n",
    "\n",
    "4️⃣ **Apply learnable parameters** (scale & shift):  \n",
    "   $$\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "   $$\n",
    "   - **γ (gamma):** Scaling factor (learned parameter).  \n",
    "   - **β (beta):** Bias/shift (learned parameter).  \n",
    "\n",
    "🔹 **This ensures that each token is normalized based on its own features, independent of other samples!**  \n",
    "\n",
    "\n",
    "\n",
    "## 🛠 **Example: Manual Calculation of Layer Norm**  \n",
    "Let’s say we have a token embedding vector:  \n",
    "\n",
    "$$\n",
    "x = [3, 5, 7, 9]\n",
    "$$\n",
    "(4 feature dimensions per token)  \n",
    "\n",
    "🔹 **Step 1: Compute mean**  \n",
    "$$\n",
    "\\mu = \\frac{3 + 5 + 7 + 9}{4} = \\frac{24}{4} = 6\n",
    "$$\n",
    "\n",
    "🔹 **Step 2: Compute variance**  \n",
    "$$\n",
    "\\sigma^2 = \\frac{(3-6)^2 + (5-6)^2 + (7-6)^2 + (9-6)^2}{4}\n",
    "$$\n",
    "$$\n",
    "= \\frac{9 + 1 + 1 + 9}{4} = \\frac{20}{4} = 5\n",
    "$$\n",
    "\n",
    "🔹 **Step 3: Normalize each feature**  \n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - 6}{\\sqrt{5}}\n",
    "$$\n",
    "$$\n",
    "\\hat{x} = \\left[ \\frac{3-6}{\\sqrt{5}}, \\frac{5-6}{\\sqrt{5}}, \\frac{7-6}{\\sqrt{5}}, \\frac{9-6}{\\sqrt{5}} \\right]\n",
    "$$\n",
    "$$\n",
    "\\hat{x} = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "🔹 **Step 4: Apply learned parameters (γ & β)**  \n",
    "If **γ = [1, 1, 1, 1]** and **β = [0, 0, 0, 0]**, then:  \n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "✨ **Final normalized vector:**  \n",
    "$$\n",
    "y = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "🚀 **Now this vector is normalized and ready for the next layer in the Transformer!**  \n",
    "\n",
    "## 🎯 **Key Differences: Layer Norm vs. Batch Norm**\n",
    "| Feature              | Layer Normalization (LN) | Batch Normalization (BN) |\n",
    "|----------------------|------------------------|------------------------|\n",
    "| **Normalization Across** | Features (per token)  | Batch (all samples) |\n",
    "| **Works with Batch Size = 1?** | ✅ Yes  | ❌ No |\n",
    "| **Handles Variable Lengths?** | ✅ Yes  | ❌ No |\n",
    "| **Autoregressive Models (e.g., GPT)?** | ✅ Yes | ❌ No |\n",
    "| **Computes Mean & Variance** | Across features (per token) | Across batch (all samples) |\n",
    "| **Best For** | Transformers, NLP | CNNs, Computer Vision |\n",
    "\n",
    "\n",
    "\n",
    "## 🏆 **Final Takeaways**\n",
    "🔹 **Batch Norm works well in CNNs but fails in NLP due to varying sequence lengths & autoregressive decoding.**  \n",
    "🔹 **Layer Norm normalizes each token’s features, making it batch-independent and perfect for Transformers.**  \n",
    "🔹 **This allows Transformers like BERT & GPT to work efficiently across different tasks without relying on batch statistics.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 **The Encoder Part of a Transformer – Deep Dive!** 🚀  \n",
    "\n",
    "Transformers revolutionized deep learning, especially in NLP, by using self-attention to process entire sequences **in parallel** instead of sequentially like RNNs. The **encoder** is a key component of this architecture, responsible for **understanding** input text and converting it into meaningful representations.  \n",
    "\n",
    "Let’s break down the encoder’s architecture in **depth** and understand **each step with a manual example**! 😃  \n",
    "\n",
    "\n",
    "\n",
    "## 🔹 **Overall Structure of the Encoder**\n",
    "A Transformer encoder consists of **multiple identical layers** (e.g., 6 in BERT-base, 12 in BERT-large). Each layer has:  \n",
    "1. **Input Embedding + Positional Encoding**  \n",
    "2. **Multi-Head Self-Attention**  \n",
    "3. **Add & Norm (Layer Normalization + Residual Connection)**  \n",
    "4. **Feed-Forward Neural Network (FFN)**  \n",
    "5. **Add & Norm Again (Layer Normalization + Residual Connection)**  \n",
    "\n",
    "Each encoder layer **refines** the representation, making it more powerful for downstream tasks.  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 1: Input Processing**\n",
    "### **🔹 Tokenization & Embedding**\n",
    "Let’s say our input sentence is:  \n",
    "👉 **\"The cat sat on the mat\"**  \n",
    "\n",
    "1️⃣ First, it is tokenized into subwords (e.g., using WordPiece in BERT):  \n",
    "   $$\n",
    "   [\\text{\"The\"}, \\text{\"cat\"}, \\text{\"sat\"}, \\text{\"on\"}, \\text{\"the\"}, \\text{\"mat\"}]\n",
    "   $$\n",
    "\n",
    "2️⃣ Each token is then converted into an **embedding vector** (e.g., size 512 in BERT).  \n",
    "   - If our embedding matrix has **d = 512**, then:  \n",
    "     $$\n",
    "     X \\in \\mathbb{R}^{6 \\times 512}\n",
    "     $$\n",
    "     This means each of the 6 tokens is now a 512-dimensional vector.\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 2: Positional Encoding**  \n",
    "Since transformers **do not have recurrence**, we add **positional encoding** to preserve word order.  \n",
    "\n",
    "- Positional encoding uses **sine and cosine functions** to generate unique position values for each word.  \n",
    "- This is **added** to the word embeddings, so the final input to the encoder is:  \n",
    "  $$\n",
    "  X' = X + PE\n",
    "  $$\n",
    "\n",
    "🚀 Now, the words are both **meaningful (word embeddings)** and **aware of their positions (positional encoding)**.\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 3: Multi-Head Self-Attention (The Heart of the Encoder!)**  \n",
    "The key idea: **Each word attends to all other words in the sentence** to understand their relationships.  \n",
    "\n",
    "### **🔹 Step 3.1: Compute Queries, Keys, and Values**  \n",
    "Each input word **X'** (a vector of size 512) is transformed into three matrices:  \n",
    "- **Query (Q)**\n",
    "- **Key (K)**\n",
    "- **Value (V)**\n",
    "\n",
    "Using **learnable weight matrices**:\n",
    "$$\n",
    "Q = X' W_Q, \\quad K = X' W_K, \\quad V = X' W_V\n",
    "$$\n",
    "(Each weight matrix is of size **512 × 64** for 8 attention heads.)\n",
    "\n",
    "### **🔹 Step 3.2: Compute Attention Scores**  \n",
    "We compute **scaled dot-product attention** using:  \n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "🔹 **Breaking it down manually**  \n",
    "Let’s assume:  \n",
    "- Token \"cat\" has a query vector **Q_cat = [2, 3]**  \n",
    "- \"sat\" has a key vector **K_sat = [1, 1]**  \n",
    "- The dot-product is:  \n",
    "  $$\n",
    "  Q_{\\text{cat}} \\cdot K_{\\text{sat}} = (2 \\times 1) + (3 \\times 1) = 5\n",
    "  $$\n",
    "- We scale it by **sqrt(d_k) = sqrt(64) = 8**:  \n",
    "  $$\n",
    "  \\frac{5}{8} = 0.625\n",
    "  $$\n",
    "- Apply **softmax**:  \n",
    "  $$\n",
    "  \\text{softmax}(0.625) = 0.65\n",
    "  $$\n",
    "  This means \"cat\" attends to \"sat\" **with 65% importance**! 🎯  \n",
    "\n",
    "This is done for **all words attending to all others**, producing an **attention matrix**.\n",
    "\n",
    "### **🔹 Step 3.3: Compute the Weighted Sum of Values**  \n",
    "Each word's new representation is computed as:  \n",
    "$$\n",
    "\\sum \\text{(attention score)} \\times \\text{Value vector}\n",
    "$$\n",
    "\n",
    "For multi-head attention, this is done **8 times in parallel**, capturing different relationships in different subspaces! 🚀\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 4: Add & Norm (Residual Connection + Layer Norm)**  \n",
    "The **output of self-attention is added back to the input (residual connection)**:  \n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(X' + \\text{Self-Attention Output})\n",
    "$$\n",
    "\n",
    "This ensures smooth gradient flow and prevents vanishing gradients! ✅\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 5: Feed-Forward Network (FFN)**  \n",
    "Each word's representation **passes through a simple MLP**:  \n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "Where:  \n",
    "- **W1, W2** are learned weight matrices  \n",
    "- **ReLU** adds non-linearity  \n",
    "\n",
    "This allows each token to **refine its representation independently**!\n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Step 6: Add & Norm (Again!)**  \n",
    "Just like before, we apply **residual connection** and **layer normalization**:  \n",
    "$$\n",
    "\\text{Final Output} = \\text{LayerNorm}(\\text{FFN Output} + \\text{Input to FFN})\n",
    "$$\n",
    "\n",
    "🚀 Now, the **encoder has finished processing the input!** This output is passed to the **next encoder layer (if any)** or to the **decoder (in sequence-to-sequence models).**  \n",
    "\n",
    "## **🔍 Summary of the Encoder Pipeline**\n",
    "| Step | Operation | Purpose |\n",
    "|------|-----------|---------|\n",
    "| **1** | Tokenization & Embedding | Convert words to vectors |\n",
    "| **2** | Positional Encoding | Add word position information |\n",
    "| **3** | Multi-Head Self-Attention | Let each word attend to all others |\n",
    "| **4** | Add & Norm | Stabilize training |\n",
    "| **5** | Feed-Forward Network | Transform representations |\n",
    "| **6** | Add & Norm | Further stabilization |\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why Is the Encoder So Powerful?**\n",
    "✔ **Captures Long-Range Dependencies:** Unlike RNNs, which struggle with long sequences, self-attention **connects all words instantly**.  \n",
    "✔ **Handles Parallel Processing:** Unlike sequential models, Transformers **process all tokens at once**, making them much faster!  \n",
    "✔ **Works for Any Input Length:** Because of positional encoding, Transformers don’t need fixed-length inputs.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually calculating how a Transformer encoder processes a sentence is a big task, but let’s do it step by step for a **single-layer encoder** with **one attention head** for simplicity.  \n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 1: Sentence and Embedding**\n",
    "Let’s take a simple sentence:  \n",
    "👉 **\"The cat sat\"** (3 words)\n",
    "\n",
    "Each word gets an embedding. Suppose we use a **4-dimensional embedding** for simplicity:\n",
    "\n",
    "| Word | Embedding (d=4) |\n",
    "|------|----------------|\n",
    "| The  | [0.2, 0.4, 0.8, 0.6] |\n",
    "| Cat  | [0.5, 0.1, 0.9, 0.7] |\n",
    "| Sat  | [0.3, 0.8, 0.2, 0.4] |\n",
    "\n",
    "So, the input matrix **X** is:\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.4 & 0.8 & 0.6 \\\\\n",
    "0.5 & 0.1 & 0.9 & 0.7 \\\\\n",
    "0.3 & 0.8 & 0.2 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 2: Positional Encoding**\n",
    "Since Transformers don’t have recurrence, they use **positional encoding** to capture the order of words.  \n",
    "\n",
    "Using the formula:  \n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos / 10000^{2i/d})\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos(pos / 10000^{2i/d})\n",
    "$$\n",
    "where:\n",
    "- **pos** = word position (0, 1, 2)\n",
    "- **d** = 4 (embedding size)\n",
    "\n",
    "For simplicity, let's assume the **precomputed positional encoding**:\n",
    "\n",
    "| Position | PE (d=4) |\n",
    "|----------|---------|\n",
    "| 0 (The)  | [0.0, 1.0, 0.0, 1.0] |\n",
    "| 1 (Cat)  | [0.84, 0.54, 0.08, 0.99] |\n",
    "| 2 (Sat)  | [0.90, 0.43, 0.16, 0.99] |\n",
    "\n",
    "Now, **add PE to embeddings**:\n",
    "$$\n",
    "X' = X + PE\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =\n",
    "\\begin{bmatrix}\n",
    "0.2 + 0.0 & 0.4 + 1.0 & 0.8 + 0.0 & 0.6 + 1.0 \\\\\n",
    "0.5 + 0.84 & 0.1 + 0.54 & 0.9 + 0.08 & 0.7 + 0.99 \\\\\n",
    "0.3 + 0.90 & 0.8 + 0.43 & 0.2 + 0.16 & 0.4 + 0.99\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 1.4 & 0.8 & 1.6 \\\\\n",
    "1.34 & 0.64 & 0.98 & 1.69 \\\\\n",
    "1.2 & 1.23 & 0.36 & 1.39\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is now **passed to the self-attention mechanism**.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 3: Compute Queries, Keys, and Values**\n",
    "We compute Queries (Q), Keys (K), and Values (V) using weight matrices.  \n",
    "Let’s assume the **weight matrices** are:\n",
    "\n",
    "$$\n",
    "W_Q =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.5 & 0.7 \\\\\n",
    "0.2 & 0.4 & 0.6 & 0.8 \\\\\n",
    "0.9 & 0.7 & 0.5 & 0.3 \\\\\n",
    "0.8 & 0.6 & 0.4 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similar matrices exist for **W_K** and **W_V**.\n",
    "\n",
    "Compute queries:  \n",
    "$$\n",
    "Q = X' W_Q\n",
    "$$\n",
    "\n",
    "Multiply:\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\times 0.1) + (1.4 \\times 0.2) + (0.8 \\times 0.9) + (1.6 \\times 0.8) & \\dots \\\\\n",
    "(1.34 \\times 0.1) + (0.64 \\times 0.2) + (0.98 \\times 0.9) + (1.69 \\times 0.8) & \\dots \\\\\n",
    "(1.2 \\times 0.1) + (1.23 \\times 0.2) + (0.36 \\times 0.9) + (1.39 \\times 0.8) & \\dots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Repeating for K and V.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 4: Compute Attention Scores**\n",
    "Now we compute the **attention scores** using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Let’s assume:\n",
    "- Query for \"cat\" is **Q_cat = [1.2, 0.5]**\n",
    "- Key for \"sat\" is **K_sat = [0.9, 1.1]**\n",
    "- Dot product:\n",
    "  $$\n",
    "  (1.2 \\times 0.9) + (0.5 \\times 1.1) = 1.08 + 0.55 = 1.63\n",
    "  $$\n",
    "- Scale by \\(\\sqrt{4} = 2\\)\n",
    "  $$\n",
    "  \\frac{1.63}{2} = 0.815\n",
    "  $$\n",
    "- Apply softmax:\n",
    "  $$\n",
    "  \\frac{e^{0.815}}{e^{0.815} + e^{0.7} + e^{0.5}} = 0.42\n",
    "  $$\n",
    "  So, \"cat\" attends to \"sat\" **with 42% weight**.\n",
    "\n",
    "Repeat for all pairs and compute **weighted sum** with values **V**.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 5: Add & Normalize**\n",
    "$$\n",
    "X'' = \\text{LayerNorm}(X' + \\text{Self-Attention Output})\n",
    "$$\n",
    "\n",
    "Normalize across each feature.\n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Step 6: Feed-Forward Network**\n",
    "Each word **passes through an MLP**:\n",
    "\n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "Apply residual connection and **LayerNorm again**.\n",
    "\n",
    "\n",
    "## **🚀 Final Output**\n",
    "Now, we have transformed input embeddings **into contextual representations**!  \n",
    "\n",
    "Each word now **understands its relationship** with all others!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Masked Multi-Head Attention in Transformers – Full Explanation 🚀**\n",
    "\n",
    "### **What is Masked Multi-Head Attention?**\n",
    "Masked Multi-Head Attention is a special variant of **Multi-Head Self-Attention (MHSA)** used **only in the decoder** of a Transformer. The key difference is that it **prevents \"cheating\"** by ensuring that at each decoding step, a token **cannot attend to future tokens**.  \n",
    "\n",
    "### **Why Do We Need It?**\n",
    "In the Transformer **decoder**, we generate output tokens **one by one** (auto-regressive generation).  \n",
    "- Example: If we translate **\"I love coding\"** to French, we should predict **\"J'aime\"** before seeing **\"coder\"**.\n",
    "- Without masking, the model could peek at future words, making training unrealistic.\n",
    "\n",
    "💡 **Masked attention ensures the model only learns from past words**, just like how humans speak!\n",
    "\n",
    "\n",
    "\n",
    "# **🌟 Step-by-Step Breakdown of Masked Multi-Head Attention**\n",
    "Now, let's dive into **how it works** mathematically and intuitively!\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 1: Input Embeddings and Positional Encoding**\n",
    "The input sentence (in target language) is converted into **word embeddings** and **positional encoding** is added.\n",
    "\n",
    "Example sentence (English → French Translation):  \n",
    "**\"I love coding\"** → **\"J'aime coder\"**\n",
    "\n",
    "| Word  | Embedding (d=4) |\n",
    "|--------|---------------|\n",
    "| J'aime | [0.5, 0.1, 0.8, 0.6] |\n",
    "| coder | [0.7, 0.2, 0.4, 0.9] |\n",
    "\n",
    "Positional encoding is added:  \n",
    "$$\n",
    "X' = X + PE\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 2: Compute Queries, Keys, and Values**\n",
    "We compute the **queries (Q), keys (K), and values (V)** using learnable weight matrices.\n",
    "\n",
    "$$\n",
    "Q = X' W_Q, \\quad K = X' W_K, \\quad V = X' W_V\n",
    "$$\n",
    "\n",
    "Example matrices:\n",
    "\n",
    "$$\n",
    "W_Q = \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix}\n",
    "\\quad\n",
    "W_K = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying embeddings by **W_Q, W_K, W_V**, we get:\n",
    "\n",
    "| Word  | Q   | K   | V   |\n",
    "|--------|-----|-----|-----|\n",
    "| J'aime | [1.2, 0.8] | [1.4, 0.9] | [0.9, 1.1] |\n",
    "| coder  | [1.5, 1.0] | [1.7, 1.2] | [1.2, 1.4] |\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 3: Compute Attention Scores**\n",
    "Attention scores are computed using:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K) = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\text{Score}(J'aime, coder) = \\frac{(1.2 \\times 1.7) + (0.8 \\times 1.2)}{\\sqrt{2}} = \\frac{2.04 + 0.96}{1.41} = 2.13\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 4: Apply the Mask!**\n",
    "💡 **Here’s where masking comes in!**  \n",
    "\n",
    "We apply a **mask matrix** to ensure each token can only attend to itself and previous tokens.\n",
    "\n",
    "For **two words**, the mask matrix looks like:\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "0 & -\\infty \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The **-∞** prevents the word **\"J'aime\"** from looking at **\"coder\"**.\n",
    "\n",
    "**Modified scores after masking**:\n",
    "\n",
    "$$\n",
    "S' =\n",
    "\\begin{bmatrix}\n",
    "\\text{Score}(J'aime, J'aime) & -\\infty \\\\\n",
    "\\text{Score}(coder, J'aime) & \\text{Score}(coder, coder)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying **softmax**, the masked token gets probability **0**.\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 5: Compute Final Attention Output**\n",
    "We multiply the **attention scores** by **V** to get final attention output.\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Softmax}(S') V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 6: Multi-Head Attention**\n",
    "Instead of using **one** attention head, **multiple heads** process the input in parallel, capturing different aspects of meaning.\n",
    "\n",
    "Example:\n",
    "- **Head 1** focuses on **word order**.\n",
    "- **Head 2** focuses on **semantic similarity**.\n",
    "\n",
    "**Final output is a concatenation** of all attention heads.\n",
    "\n",
    "\n",
    "\n",
    "## **🔹 Step 7: Add & Normalize**\n",
    "$$\n",
    "X'' = \\text{LayerNorm}(X' + \\text{Masked Multi-Head Attention Output})\n",
    "$$\n",
    "\n",
    "\n",
    "# **🔥 Summary**\n",
    "✅ **Prevents future tokens from being seen**  \n",
    "✅ **Allows auto-regressive generation**  \n",
    "✅ **Multiple heads capture rich context**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a full **manual calculation** of **multi-head attention** on a real sentence is **possible** but requires many steps, involving matrix multiplications, softmax, and weighted sums. I'll **simplify** it while keeping all essential calculations.\n",
    "\n",
    "\n",
    "\n",
    "# **Manual Multi-Head Attention Calculation on a Sentence**\n",
    "Let's take a **simple sentence**:  \n",
    "\n",
    "**\"I love AI\"**  \n",
    "\n",
    "We will calculate **multi-head self-attention** step-by-step with two heads.\n",
    "\n",
    "## **Step 1: Convert Words to Embeddings**\n",
    "Each word is represented as a vector (randomly chosen for simplicity).\n",
    "\n",
    "| Word   | Embedding (d=4) |\n",
    "|--------|----------------|\n",
    "| I      | [0.2, 0.3, 0.4, 0.5] |\n",
    "| love   | [0.7, 0.1, 0.8, 0.6] |\n",
    "| AI     | [0.5, 0.9, 0.3, 0.7] |\n",
    "\n",
    "**We use d_model = 4 (dimension of embeddings) and two attention heads.**\n",
    "\n",
    "\n",
    "## **Step 2: Compute Queries, Keys, and Values**\n",
    "Each head has different **weight matrices** for Query (Q), Key (K), and Value (V).  \n",
    "Let’s define two sets of weight matrices for **Head 1** and **Head 2**.\n",
    "\n",
    "### **Head 1:**\n",
    "$$\n",
    "W_Q^1 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\\\ 0.9 & 0.1 & 0.2 & 0.3 \\\\ 0.4 & 0.5 & 0.6 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_K^1 = \\begin{bmatrix} 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 \\\\ 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_V^1 = \\begin{bmatrix} 0.3 & 0.4 & 0.5 & 0.6 \\\\ 0.7 & 0.8 & 0.9 & 0.1 \\\\ 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Compute Queries, Keys, and Values for Head 1**\n",
    "Using:\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "For word **\"I\"**:\n",
    "\n",
    "$$\n",
    "Q_{I} = [0.2, 0.3, 0.4, 0.5] \\times W_Q^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q_{I} = [ (0.2×0.1 + 0.3×0.5 + 0.4×0.9 + 0.5×0.4), (0.2×0.2 + 0.3×0.6 + 0.4×0.1 + 0.5×0.5), ...]\n",
    "$$\n",
    "\n",
    "Similarly, compute for **K and V**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Compute Attention Scores**\n",
    "Using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "(Here, \\( d_k = 2 \\) because we split embeddings for two heads)\n",
    "\n",
    "1. Compute **QK^T** (dot product of Queries and Keys).\n",
    "2. Apply **scaling** (\\( \\sqrt{d_k} \\)).\n",
    "3. Apply **softmax**.\n",
    "4. Multiply by **V**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Compute for Head 2**\n",
    "Repeat Steps 2 and 3 using different **W_Q^2, W_K^2, W_V^2**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Concatenate and Apply Final Linear Transformation**\n",
    "Concatenate the two heads’ outputs and apply a final transformation.\n",
    "\n",
    "$$\n",
    "\\text{Output} = [\\text{Head}_1, \\text{Head}_2] W_O\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "# **Final Thoughts**\n",
    "✅ We performed step-by-step calculations for **multi-head self-attention**.  \n",
    "✅ This shows how Transformers learn **context** across multiple perspectives! 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross-Attention in Transformers – Full Explanation** 🎯  \n",
    "\n",
    "Cross-attention is a crucial mechanism in **transformers**, especially in models like **encoder-decoder architectures (e.g., T5, BART, and Transformer-based Machine Translation)**. It enables the **decoder to focus on relevant encoder outputs** while generating each token of the output.\n",
    "\n",
    "\n",
    "\n",
    "# **📌 Why Do We Need Cross-Attention?**\n",
    "1. **Bridging Encoder and Decoder** 🔗  \n",
    "   - The encoder processes the **input sequence** and generates **contextual representations**.\n",
    "   - The decoder **does not directly access the input** but must **attend** to the encoder's output to generate relevant output tokens.\n",
    "\n",
    "2. **Handling Contextual Dependencies** 🧠  \n",
    "   - Some output tokens depend on long-distance dependencies from the input.  \n",
    "   - Cross-attention ensures that the decoder has **direct access** to all encoder outputs.\n",
    "\n",
    "3. **Improving Translation & Summarization** 📝  \n",
    "   - In **machine translation**, the decoder must generate words in the target language while referring to the encoder outputs.  \n",
    "   - In **text summarization**, the decoder selects important parts of the input text.\n",
    "\n",
    "\n",
    "\n",
    "# **⚙️ How Does Cross-Attention Work?**\n",
    "Cross-attention follows the same **scaled dot-product attention** mechanism as self-attention but with a key difference:\n",
    "\n",
    "- **In self-attention**, the queries (Q), keys (K), and values (V) come from the same input sequence.\n",
    "- **In cross-attention**, the queries (Q) come from the **decoder**, while the keys (K) and values (V) come from the **encoder outputs**.\n",
    "\n",
    "### **Formula for Attention Scores**\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- **Q (Query)** comes from the decoder's previous hidden state.  \n",
    "- **K (Key) and V (Value)** come from the encoder's final hidden states.  \n",
    "- **$ d_k $** is the key dimension, used for scaling.\n",
    "\n",
    "\n",
    "\n",
    "# **🔬 Step-by-Step Process of Cross-Attention**\n",
    "Let’s break it down:\n",
    "\n",
    "### **1️⃣ Encoder Produces Contextual Representations**\n",
    "- The encoder processes the input sequence and produces a set of output embeddings.\n",
    "- Example:  \n",
    "  Suppose we have the input:  \n",
    "  **\"The cat sat on the mat.\"**  \n",
    "  The encoder generates **hidden states** for each word.\n",
    "\n",
    "  ```\n",
    "  Encoder Outputs:\n",
    "  [E1, E2, E3, E4, E5, E6]\n",
    "  ```\n",
    "\n",
    "### **2️⃣ Decoder Generates Queries**\n",
    "- The decoder is generating output words **one at a time**.\n",
    "- At each step, it takes the previously generated words and computes a **query (Q)**.\n",
    "\n",
    "  ```\n",
    "  Query (Q) from decoder hidden state:\n",
    "  Q = Decoder_hidden_state_t\n",
    "  ```\n",
    "\n",
    "### **3️⃣ Compute Attention Scores**\n",
    "- Compute **dot product** between Query (Q) and all encoder Key (K) vectors.\n",
    "- Apply **softmax** to get attention scores.\n",
    "\n",
    "### **4️⃣ Weighted Sum of Encoder Outputs**\n",
    "- Multiply attention scores with encoder **Value (V)** vectors.\n",
    "- This forms the **context vector**, which contains the most relevant information for generating the next token.\n",
    "\n",
    "### **5️⃣ Decoder Uses Context Vector to Generate Next Token**\n",
    "- The decoder uses this weighted context vector to decide the next word in the output sequence.\n",
    "\n",
    "\n",
    "\n",
    "# **🤖 Example: Machine Translation Using Cross-Attention**\n",
    "Imagine we are translating:  \n",
    "📝 **Input (English):** \"I love AI\"  \n",
    "🌍 **Output (French):** \"J'aime l'IA\"  \n",
    "\n",
    "### **Encoder Process** (Self-Attention on Input)  \n",
    "```\n",
    "Input:  [\"I\", \"love\", \"AI\"]\n",
    "Embeddings → Self-Attention → Encoder Hidden States\n",
    "```\n",
    "\n",
    "The encoder outputs:  \n",
    "```\n",
    "[E1, E2, E3] (hidden representations for \"I\", \"love\", \"AI\")\n",
    "```\n",
    "\n",
    "### **Decoder Process (with Cross-Attention)**\n",
    "- **Step 1**: Decoder generates **Q (query) for \"J'\"**  \n",
    "  ```\n",
    "  Q1 = Decoder_hidden_state_1\n",
    "  ```\n",
    "  - Compute attention scores with encoder outputs `[E1, E2, E3]`.\n",
    "  - Get **context vector** and generate \"J'\".\n",
    "\n",
    "- **Step 2**: Decoder generates **Q (query) for \"aime\"**  \n",
    "  ```\n",
    "  Q2 = Decoder_hidden_state_2\n",
    "  ```\n",
    "  - Compute new attention scores with encoder outputs `[E1, E2, E3]`.\n",
    "  - Get **context vector** and generate \"aime\".\n",
    "\n",
    "- **Step 3**: Decoder generates **Q (query) for \"l'IA\"**  \n",
    "  ```\n",
    "  Q3 = Decoder_hidden_state_3\n",
    "  ```\n",
    "  - Compute attention scores again.\n",
    "  - Get **context vector** and generate \"l'IA\".\n",
    "\n",
    "Final Output:  \n",
    "✅ **\"J'aime l'IA\"** 🎉\n",
    "\n",
    "# **🆚 Self-Attention vs. Cross-Attention**\n",
    "| Feature        | Self-Attention | Cross-Attention |\n",
    "|---------------|---------------|----------------|\n",
    "| **Where?**    | Encoder & Decoder | Decoder only |\n",
    "| **Query (Q)?** | From same sequence | From decoder hidden states |\n",
    "| **Key (K), Value (V)?** | From same sequence | From encoder outputs |\n",
    "| **Purpose?**  | Relate words within same sequence | Connect encoder & decoder |\n",
    "\n",
    "\n",
    "# **🚀 Key Takeaways**\n",
    "✔ **Cross-attention is essential** for sequence-to-sequence tasks like machine translation.  \n",
    "✔ The **decoder uses cross-attention** to focus on relevant parts of the encoder's output.  \n",
    "✔ It enables **better alignment** between input and output sequences.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-Attention in Simple Layman Terms**  \n",
    "\n",
    "Think of **cross-attention** like a **translator** who listens to one language (input) and speaks in another (output).  \n",
    "\n",
    "Let’s say you have an **English teacher** and a **French student**:  \n",
    "- The **teacher (encoder)** speaks in **English**.  \n",
    "- The **student (decoder)** listens and translates into **French**.  \n",
    "- The student must **pay attention** to the right words from the teacher **before speaking**.  \n",
    "\n",
    "💡 **Cross-attention is how the student listens to the teacher!**  \n",
    "\n",
    "\n",
    "\n",
    "### **How It Works in Transformers**\n",
    "A Transformer has **two main parts**:  \n",
    "1. **Encoder** → Reads and understands the input sentence.  \n",
    "2. **Decoder** → Generates the output sentence, **paying attention to the encoder’s words** using **cross-attention**.  \n",
    "\n",
    "🔹 In **self-attention**, the decoder looks at **its own words**.  \n",
    "🔹 In **cross-attention**, the decoder looks at **the encoder’s words** before deciding what to say next.  \n",
    "\n",
    "\n",
    "\n",
    "### **Example: English to French Translation**\n",
    "Imagine the Transformer translating:  \n",
    "**\"I love apples\"** → **\"J'aime les pommes\"**  \n",
    "\n",
    "🔹 The **encoder** processes **\"I love apples\"** and stores its meaning.  \n",
    "🔹 The **decoder** starts generating French words, but before picking the next word, it **looks at the most relevant parts of the English sentence**.  \n",
    "\n",
    "#### **Step-by-Step Process:**\n",
    "1️⃣ The decoder starts with **\"J'\"**.  \n",
    "2️⃣ It **attends to** (\"I love apples\") and decides the next word **\"aime\"**.  \n",
    "3️⃣ It again checks (\"I love apples\") and picks **\"les\"**.  \n",
    "4️⃣ Finally, it attends again and picks **\"pommes\"**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Analogy: Ordering Food at a Restaurant 🍔**  \n",
    "Imagine you're at a restaurant and **don’t know what to order**.  \n",
    "- You look at the **menu (encoder)**, which has all options.  \n",
    "- You **cross-check** it with what you want.  \n",
    "- You then tell the waiter your choice (decoder).  \n",
    "\n",
    "The **menu = encoder**, and **your choice depends on looking at the menu first = cross-attention**!  \n",
    "\n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "✅ **Self-attention** = Looking at your own notes to write a story.  \n",
    "✅ **Cross-attention** = Looking at a book (encoder) to answer questions.  \n",
    "✅ **Used in decoders** (like language translation & AI chatbots).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 **Understanding Transformer Decoder Architecture in Depth**  \n",
    "\n",
    "The **decoder** in a Transformer is responsible for **generating text step by step**, using the encoded input information. It is widely used in **machine translation, text generation, and other NLP tasks**.\n",
    "\n",
    "Let’s break it down step by step and understand **how it works** in detail.  \n",
    "\n",
    "\n",
    "\n",
    "## 🏗 **Transformer Decoder Architecture Overview**  \n",
    "\n",
    "A **Transformer decoder** consists of multiple **decoder layers** (e.g., 6 in the original paper). Each layer has three main sub-components:  \n",
    "\n",
    "### 🔹 **1. Masked Multi-Head Self-Attention**  \n",
    "➡ The decoder **attends to itself**, looking at previously generated tokens while ensuring it **doesn’t peek ahead** (future tokens are masked).  \n",
    "\n",
    "### 🔹 **2. Cross-Attention (Encoder-Decoder Attention)**  \n",
    "➡ The decoder **attends to the encoder’s output**, focusing on the most relevant parts of the input sentence.  \n",
    "\n",
    "### 🔹 **3. Feed-Forward Network (FFN)**  \n",
    "➡ A fully connected layer applied independently to each position to transform features.  \n",
    "\n",
    "\n",
    "\n",
    "### 📌 **Detailed Step-by-Step Flow**  \n",
    "\n",
    "Imagine we are **translating an English sentence to French**:\n",
    "\n",
    "💬 **Input (English):** `\"The cat sat on the mat.\"`  \n",
    "📝 **Output (French, step by step):** `\"Le chat est assis sur le tapis.\"`\n",
    "\n",
    "At each step, the decoder generates one word at a time while looking at the encoder's output.\n",
    "\n",
    "### 🔥 **Step 1: Token Embeddings & Positional Encoding**\n",
    "- The decoder **starts with an empty sequence**.\n",
    "- Each generated word (token) is converted into a vector using an **embedding layer**.\n",
    "- **Positional encoding** is added to retain **word order** information.\n",
    "\n",
    "👉 Example:  \n",
    "```\n",
    "Step 1: [\"Le\"]\n",
    "Step 2: [\"Le\", \"chat\"]\n",
    "Step 3: [\"Le\", \"chat\", \"est\"]\n",
    "...\n",
    "```\n",
    "Each token is processed **one at a time**.\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Step 2: Masked Multi-Head Self-Attention 🛑**  \n",
    "The decoder applies **self-attention**, but it must ensure **no future words are visible** (to prevent cheating!).  \n",
    "\n",
    "✅ **Why is it masked?**  \n",
    "- If we are at **Step 2** generating `\"chat\"`, we should **not see** `\"est\", \"assis\", \"sur\", \"le tapis\"`.  \n",
    "- This prevents the model from accessing future tokens, ensuring **auto-regressive decoding**.  \n",
    "\n",
    "🚀 **Self-Attention Formula:**  \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask} \\right) V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Step 3: Cross-Attention (Encoder-Decoder Attention)**  \n",
    "Now, the decoder needs to understand the **input sentence** to generate the correct translation.  \n",
    "\n",
    "✅ **How does it work?**  \n",
    "- The decoder **attends to the encoder outputs**.\n",
    "- Each decoder token decides **which input words are most relevant**.  \n",
    "- This ensures **the correct meaning is captured**.\n",
    "\n",
    "🔹 **Example:**  \n",
    "For **\"chat\"**, the model attends strongly to **\"cat\"** in the encoder’s output.  \n",
    "\n",
    "🚀 **Cross-Attention Formula:**  \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "where:  \n",
    "- **Query (Q)** comes from the decoder.  \n",
    "- **Key (K) and Value (V)** come from the encoder.\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Step 4: Feed-Forward Network (FFN)**\n",
    "Each position is passed through a **fully connected network** to further process the information.  \n",
    "\n",
    "FFN is applied **independently to each position**:\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "🚀 **Why is this needed?**  \n",
    "- Adds **non-linearity**, helping the model capture complex patterns.\n",
    "- Allows transformation of feature space for **better predictions**.\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Step 5: Layer Normalization & Residual Connections**\n",
    "To **stabilize training**, we add:  \n",
    "✅ **Residual connections** (skip connections) to allow information flow.  \n",
    "✅ **Layer normalization** to normalize activations for faster convergence.\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Step 6: Softmax & Word Prediction**\n",
    "After passing through **multiple decoder layers**, the final output is a probability distribution over the vocabulary.\n",
    "\n",
    "$$\n",
    "\\text{P(word)} = \\text{softmax}(W_{\\text{out}} h_{\\text{final}})\n",
    "$$\n",
    "\n",
    "👉 The highest probability word is chosen as the next word in the sequence.\n",
    "\n",
    "\n",
    "\n",
    "## 🔥 **Putting It All Together**\n",
    "At each decoding step:\n",
    "1️⃣ **Masked Self-Attention** → The decoder attends to past words only.  \n",
    "2️⃣ **Cross-Attention** → The decoder attends to the encoder’s input.  \n",
    "3️⃣ **FFN & Layer Norm** → Helps learn patterns.  \n",
    "4️⃣ **Softmax & Word Selection** → Predicts the next word.  \n",
    "5️⃣ **Repeat until END token is generated.**  \n",
    "\n",
    "\n",
    "\n",
    "## 🎯 **Key Takeaways**\n",
    "✅ The **decoder generates words step by step**, ensuring proper sentence structure.  \n",
    "✅ **Masked self-attention prevents cheating** by hiding future words.  \n",
    "✅ **Cross-attention helps align input and output sentences**.  \n",
    "✅ **Layer normalization + residual connections stabilize training**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually calculating how the **Transformer Decoder** processes a sentence is quite detailed, but I’ll break it down step by step with full calculations.  \n",
    "\n",
    "We’ll take a simple sentence:  \n",
    "\n",
    "**Sentence:** `\"I love AI\"`  \n",
    "\n",
    "### **Transformer Decoder Architecture Overview**  \n",
    "The Transformer Decoder consists of the following main components:  \n",
    "1. **Tokenization & Embedding** – Convert words into numerical representations.  \n",
    "2. **Positional Encoding** – Encode word positions into vectors.  \n",
    "3. **Masked Multi-Head Self-Attention** – Prevent the decoder from seeing future words.  \n",
    "4. **Cross-Attention (Encoder-Decoder Attention)** – Focus on relevant encoder outputs.  \n",
    "5. **Feedforward Neural Network** – Enhance feature representations.  \n",
    "6. **Layer Normalization & Residual Connections** – Stabilize and optimize learning.  \n",
    "7. **Final Softmax Layer** – Generate probabilities for the next token.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 1: Tokenization & Embedding**  \n",
    "Each word is first converted into a token using a vocabulary mapping. Let's assume:  \n",
    "\n",
    "| Word  | Token ID |\n",
    "|--------|----------|\n",
    "| I      | 1        |\n",
    "| love   | 2        |\n",
    "| AI     | 3        |\n",
    "\n",
    "Using an embedding matrix (random values for illustration), let’s assume a **3D embedding (d_model = 3) for simplicity**:  \n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}  \n",
    "0.1 & 0.2 & 0.3 \\\\  \n",
    "0.4 & 0.5 & 0.6 \\\\  \n",
    "0.7 & 0.8 & 0.9  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each token maps to an embedding row:  \n",
    "- `\"I\"` → [0.1, 0.2, 0.3]  \n",
    "- `\"love\"` → [0.4, 0.5, 0.6]  \n",
    "- `\"AI\"` → [0.7, 0.8, 0.9]  \n",
    "\n",
    "### **Step 2: Positional Encoding**  \n",
    "Since Transformers don’t have recurrence, we need to add position information using:  \n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "For simplicity, let’s assume **d_model = 3** and compute for each position manually:  \n",
    "\n",
    "#### **Position 0 (\"I\")**  \n",
    "$$\n",
    "PE_0 = [\\sin(0), \\cos(0), \\sin(0)] = [0, 1, 0]\n",
    "$$\n",
    "\n",
    "#### **Position 1 (\"love\")**  \n",
    "$$\n",
    "PE_1 = [\\sin(1/10000^{0}), \\cos(1/10000^{0}), \\sin(1/10000^{1/3})] \n",
    "$$\n",
    "$$\n",
    "PE_1 ≈ [0.0001, 0.9999, 0.001]\n",
    "$$\n",
    "\n",
    "#### **Position 2 (\"AI\")**  \n",
    "$$\n",
    "PE_2 = [\\sin(2/10000^{0}), \\cos(2/10000^{0}), \\sin(2/10000^{1/3})]  \n",
    "$$\n",
    "$$\n",
    "PE_2 ≈ [0.0002, 0.9998, 0.002]\n",
    "$$\n",
    "\n",
    "### **Step 3: Add Positional Encoding**  \n",
    "Now, we add PE to embeddings:  \n",
    "\n",
    "| Word  | Embedding | Positional Encoding | Sum |\n",
    "|--------|-----------|----------------------|-----|\n",
    "| `\"I\"` | [0.1, 0.2, 0.3] | [0, 1, 0] | [0.1, 1.2, 0.3] |\n",
    "| `\"love\"` | [0.4, 0.5, 0.6] | [0.0001, 0.9999, 0.001] | [0.4001, 1.4999, 0.601] |\n",
    "| `\"AI\"` | [0.7, 0.8, 0.9] | [0.0002, 0.9998, 0.002] | [0.7002, 1.7998, 0.902] |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Masked Multi-Head Self-Attention**  \n",
    "### **4.1 Compute Query (Q), Key (K), and Value (V) Matrices**  \n",
    "Assume weight matrices for Q, K, V:  \n",
    "\n",
    "$$\n",
    "W_Q = \\begin{bmatrix} 0.2 & 0.3 & 0.5 \\\\ 0.1 & 0.6 & 0.8 \\\\ 0.7 & 0.2 & 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_K = \\begin{bmatrix} 0.3 & 0.5 & 0.2 \\\\ 0.6 & 0.1 & 0.4 \\\\ 0.8 & 0.3 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_V = \\begin{bmatrix} 0.5 & 0.2 & 0.6 \\\\ 0.3 & 0.8 & 0.1 \\\\ 0.7 & 0.4 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute Q, K, V for **\"I\"** (first token):  \n",
    "\n",
    "$$\n",
    "Q = X W_Q = \\begin{bmatrix} 0.1 & 1.2 & 0.3 \\end{bmatrix} \\times W_Q\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [ (0.1*0.2 + 1.2*0.1 + 0.3*0.7), (0.1*0.3 + 1.2*0.6 + 0.3*0.2), (0.1*0.5 + 1.2*0.8 + 0.3*0.4)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.02 + 0.12 + 0.21, 0.03 + 0.72 + 0.06, 0.05 + 0.96 + 0.12]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.35, 0.81, 1.13]\n",
    "$$\n",
    "\n",
    "Similarly, compute K and V.  \n",
    "\n",
    "### **4.2 Compute Attention Scores**  \n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "Then apply **masking** (to prevent seeing future words) and **softmax** to get attention scores.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Cross-Attention (Encoder-Decoder Attention)**  \n",
    "- The decoder uses the **encoder’s outputs** as Key (K) and Value (V).  \n",
    "- The decoder’s own Query (Q) attends to the encoder’s outputs.  \n",
    "- Follow the same attention formula.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 6: Feedforward Network (FFN)**  \n",
    "Each token’s output is passed through:  \n",
    "\n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "Assume:  \n",
    "\n",
    "$$\n",
    "W_1 = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.6 & 0.8 \\\\ 0.3 & 0.9 \\end{bmatrix}, \\quad b_1 = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2 = \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.2 & 0.6 \\end{bmatrix}, \\quad b_2 = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 7: Final Softmax Layer**  \n",
    "Finally, the output is passed through **softmax** to predict the next word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Final Summary**  \n",
    "1. **Tokenization & Embedding**  \n",
    "2. **Positional Encoding**  \n",
    "3. **Masked Self-Attention**  \n",
    "4. **Cross-Attention (Encoder-Decoder Attention)**  \n",
    "5. **Feedforward Network**  \n",
    "6. **Final Softmax**  \n",
    "\n",
    "This gives probabilities for the next token prediction! 🎯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Inference: How It Works in Detail**\n",
    "Transformer inference is the process of using a trained transformer model (such as GPT, BERT, or T5) to generate predictions, complete sentences, or classify text. The inference phase is different from training because it focuses only on **forward propagation**, without backpropagation or weight updates.\n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Steps in Transformer Inference**\n",
    "When a transformer performs inference, it goes through several key steps:\n",
    "\n",
    "1. **Input Tokenization & Encoding**\n",
    "2. **Positional Encoding**\n",
    "3. **Passing Through the Encoder (for Encoder-Decoder models)**\n",
    "4. **Decoding Step-by-Step (Auto-Regressive Nature)**\n",
    "5. **Generating the Next Token Using Softmax**\n",
    "6. **Iterating Until the End of Sentence Token (`<EOS>`)**\n",
    "7. **Final Output Processing**\n",
    "\n",
    "We’ll go through each step with **detailed calculations**. 🚀\n",
    "\n",
    "\n",
    "\n",
    "### **1️⃣ Input Tokenization & Encoding**\n",
    "Before passing data into a transformer model, the input text is **tokenized** into subwords or word pieces. \n",
    "\n",
    "Example Sentence:  \n",
    "📌 `\"I love AI\"`  \n",
    "\n",
    "Assume our vocabulary has the following **token IDs**:  \n",
    "| Word  | Token ID |\n",
    "|--------|----------|\n",
    "| I      | 1        |\n",
    "| love   | 2        |\n",
    "| AI     | 3        |\n",
    "\n",
    "So, the input is represented as:\n",
    "```plaintext\n",
    "[1, 2, 3]\n",
    "```\n",
    "\n",
    "Now, each token ID is mapped to a **word embedding** vector from an embedding matrix \\(E\\).\n",
    "\n",
    "Example embedding matrix (d_model = 3 for simplicity):\n",
    "$$\n",
    "E = \\begin{bmatrix}  \n",
    "0.1 & 0.2 & 0.3 \\\\  \n",
    "0.4 & 0.5 & 0.6 \\\\  \n",
    "0.7 & 0.8 & 0.9  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "So the embeddings are:\n",
    "- `\"I\"` → **[0.1, 0.2, 0.3]**\n",
    "- `\"love\"` → **[0.4, 0.5, 0.6]**\n",
    "- `\"AI\"` → **[0.7, 0.8, 0.9]**\n",
    "\n",
    "\n",
    "\n",
    "### **2️⃣ Positional Encoding**\n",
    "Transformers **do not** have recurrence like RNNs, so we need **positional encoding** to encode word order.\n",
    "\n",
    "The formula for **Positional Encoding (PE)** is:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "For **3-dimensional** embeddings, the positional encodings are computed as:\n",
    "\n",
    "#### **Position 0 (\"I\")**  \n",
    "$$\n",
    "PE_0 = [\\sin(0), \\cos(0), \\sin(0)] = [0, 1, 0]\n",
    "$$\n",
    "\n",
    "#### **Position 1 (\"love\")**  \n",
    "$$\n",
    "PE_1 = [\\sin(1/10000^0), \\cos(1/10000^0), \\sin(1/10000^{1/3})] \n",
    "$$\n",
    "$$\n",
    "PE_1 ≈ [0.0001, 0.9999, 0.001]\n",
    "$$\n",
    "\n",
    "#### **Position 2 (\"AI\")**  \n",
    "$$\n",
    "PE_2 = [\\sin(2/10000^0), \\cos(2/10000^0), \\sin(2/10000^{1/3})]  \n",
    "$$\n",
    "$$\n",
    "PE_2 ≈ [0.0002, 0.9998, 0.002]\n",
    "$$\n",
    "\n",
    "Now we **add** these positional encodings to the embeddings:\n",
    "\n",
    "| Word  | Embedding | Positional Encoding | Sum |\n",
    "|--------|-----------|----------------------|-----|\n",
    "| `\"I\"` | [0.1, 0.2, 0.3] | [0, 1, 0] | **[0.1, 1.2, 0.3]** |\n",
    "| `\"love\"` | [0.4, 0.5, 0.6] | [0.0001, 0.9999, 0.001] | **[0.4001, 1.4999, 0.601]** |\n",
    "| `\"AI\"` | [0.7, 0.8, 0.9] | [0.0002, 0.9998, 0.002] | **[0.7002, 1.7998, 0.902]** |\n",
    "\n",
    "\n",
    "\n",
    "### **3️⃣ Passing Through the Encoder**\n",
    "The encoder processes the input using **multi-head self-attention** and a **feedforward network**.\n",
    "\n",
    "#### **Multi-Head Self-Attention**\n",
    "Each input token gets transformed using Query (Q), Key (K), and Value (V) matrices.\n",
    "\n",
    "Assume the matrices:\n",
    "\n",
    "$$\n",
    "W_Q = \\begin{bmatrix} 0.2 & 0.3 & 0.5 \\\\ 0.1 & 0.6 & 0.8 \\\\ 0.7 & 0.2 & 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_K = \\begin{bmatrix} 0.3 & 0.5 & 0.2 \\\\ 0.6 & 0.1 & 0.4 \\\\ 0.8 & 0.3 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_V = \\begin{bmatrix} 0.5 & 0.2 & 0.6 \\\\ 0.3 & 0.8 & 0.1 \\\\ 0.7 & 0.4 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For each word, we compute **Q = XW_Q**, **K = XW_K**, **V = XW_V**, then compute **attention scores**:\n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "After self-attention, the output is passed through a **feedforward network (FFN)**.\n",
    "\n",
    "\n",
    "\n",
    "### **4️⃣ Decoding Step-by-Step (Auto-Regressive Nature)**\n",
    "The decoder **predicts one word at a time**. It uses:\n",
    "- **Masked Multi-Head Self-Attention**\n",
    "- **Cross-Attention with Encoder Outputs**\n",
    "- **Feedforward Layer**\n",
    "\n",
    "The decoder starts with:\n",
    "```plaintext\n",
    "[\"<START>\"]\n",
    "```\n",
    "And generates words **one by one**, masking future words.\n",
    "\n",
    "Each output is fed back into the decoder until it reaches `<EOS>`.\n",
    "\n",
    "\n",
    "\n",
    "### **5️⃣ Generating the Next Token Using Softmax**\n",
    "The final decoder output is transformed into **logits** (raw scores for each word in the vocabulary). \n",
    "\n",
    "Softmax converts logits into probabilities:\n",
    "$$\n",
    "P_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$$\n",
    "\n",
    "The word with the **highest probability** is selected as the next token.\n",
    "\n",
    "Example:\n",
    "| Token  | Logit | Softmax Probability |\n",
    "|--------|--------|------------------|\n",
    "| \"AI\"   | 6.1    | 0.70 |\n",
    "| \"Robot\"| 4.2    | 0.20 |\n",
    "| \"Human\"| 3.5    | 0.10 |\n",
    "\n",
    "So, `\"AI\"` is the next word.\n",
    "\n",
    "\n",
    "\n",
    "### **6️⃣ Iterating Until `<EOS>`**\n",
    "The process repeats until the decoder generates an **end-of-sequence (`<EOS>`)** token.\n",
    "\n",
    "Example output:\n",
    "```plaintext\n",
    "[\"I\", \"love\", \"AI\", \"<EOS>\"]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **🎯 Summary of Transformer Inference**\n",
    "✅ **Tokenization & Encoding** – Convert input text into embeddings  \n",
    "✅ **Positional Encoding** – Add position info to embeddings  \n",
    "✅ **Encoder Processes Input** – Uses self-attention & feedforward layers  \n",
    "✅ **Decoder Generates Tokens** – Uses masked attention & cross-attention  \n",
    "✅ **Softmax Determines Next Word**  \n",
    "✅ **Repeat Until `<EOS>`**  \n",
    "\n",
    "This is how transformers **generate predictions** in NLP tasks like text generation, translation, and chatbots! 🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Inference in Simple Layman Terms**  \n",
    "\n",
    "Think of a **transformer model** like a **smart storyteller** 🤖📖. It has already learned a **huge book of patterns** during training, and now, during **inference**, it simply **predicts the next word** based on what you’ve given it.  \n",
    "\n",
    "Let’s break it down step by step using an analogy!  \n",
    "\n",
    "\n",
    "\n",
    "### **🎭 Step 1: You Give an Input (Like Asking a Friend a Question)**\n",
    "Imagine you have a friend who is really good at guessing what comes next in a conversation. You say:  \n",
    "\n",
    "> **\"I love\"**  \n",
    "\n",
    "Now, your friend **thinks carefully** about what word might come next.  \n",
    "\n",
    "\n",
    "\n",
    "### **📖 Step 2: Tokenization – Breaking Words into Small Pieces**\n",
    "Before our transformer can understand the text, it **breaks it down into numbers** (because computers love numbers, not words!).  \n",
    "\n",
    "For example:  \n",
    "- **\"I\" → Token 1**  \n",
    "- **\"love\" → Token 2**  \n",
    "- **\"AI\" → Token 3**  \n",
    "\n",
    "So, **\"I love AI\"** becomes **[1, 2, 3]** in a format the transformer understands.  \n",
    "\n",
    "\n",
    "\n",
    "### **📌 Step 3: Positional Encoding – Remembering Word Order**\n",
    "Unlike humans, computers don’t naturally **remember order** (they see words as a bag of numbers). So, we add **positional encoding** to **tell the transformer where each word is in the sentence**.  \n",
    "\n",
    "Think of it like numbering words in a notebook:  \n",
    "- **\"I\" (1st word) → Position 1**  \n",
    "- **\"love\" (2nd word) → Position 2**  \n",
    "- **\"AI\" (3rd word) → Position 3**  \n",
    "\n",
    "Now, the transformer knows both the **meaning of words** and **where they are** in the sentence!  \n",
    "\n",
    "\n",
    "\n",
    "### **🤔 Step 4: Understanding the Input (Encoder)**\n",
    "The **encoder** takes the input words and **figures out their relationships**. It does this using **self-attention**, which means:  \n",
    "\n",
    "💡 **Each word \"looks at\" every other word** in the sentence and decides which ones are important.  \n",
    "\n",
    "For example, in **\"I love AI\"**, the transformer might realize:  \n",
    "- \"I\" is not very important.  \n",
    "- \"love\" is strongly connected to \"AI\".  \n",
    "\n",
    "It creates a **mathematical score** for each word’s importance and stores this information.  \n",
    "\n",
    "\n",
    "\n",
    "### **📝 Step 5: Decoding – Predicting the Next Word**\n",
    "Now, let’s say we want the transformer to complete the sentence **\"I love\" → ???**.  \n",
    "\n",
    "💡 **The decoder now guesses the next word** using the information from the encoder.  \n",
    "\n",
    "🚀 It starts with:  \n",
    "- **\"I love\"** → **Looks at all the words it knows.**  \n",
    "- Checks past patterns it has learned.  \n",
    "- It predicts: **\"AI\"** (or another relevant word like \"coding\" or \"music\").  \n",
    "\n",
    "### **🎯 Step 6: Softmax – Picking the Best Word**\n",
    "The decoder doesn’t pick the next word randomly. Instead, it assigns a **probability score** to each possible word:  \n",
    "\n",
    "| Possible Next Word | Score (%) |\n",
    "|-------------------|----------|\n",
    "| AI               | 80%      |\n",
    "| coding           | 15%      |\n",
    "| music           | 5%       |\n",
    "\n",
    "Since **\"AI\" has the highest score (80%)**, the model selects it. 🎉  \n",
    "\n",
    "\n",
    "### **🔁 Step 7: Repeating Until the Sentence is Complete**\n",
    "The decoder keeps generating one word at a time until it sees an **end-of-sentence token (`<EOS>`)**.  \n",
    "\n",
    "For example:  \n",
    "- \"I love\" → **AI** (from decoder)  \n",
    "- \"I love AI\" → **<EOS>** (End of sentence)  \n",
    "\n",
    "Final Output:  \n",
    "> **\"I love AI\"** ✅  \n",
    "\n",
    "\n",
    "\n",
    "### **🤖 Summary (Think of Transformer as a Smart Storyteller)**\n",
    "1. **You give it words** → \"I love\"  \n",
    "2. **It breaks them into numbers** → [1, 2]  \n",
    "3. **It remembers word order** → [1st, 2nd word]  \n",
    "4. **It understands the meaning** → \"Love is related to AI\"  \n",
    "5. **It predicts the next word** → \"AI\"  \n",
    "6. **It picks the best word based on probability**  \n",
    "7. **It stops when the sentence is complete**  \n",
    "\n",
    "That’s how **transformer inference works!** 🎉🚀  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
