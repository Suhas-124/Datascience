{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ **The Attention Mechanism: A Game Changer in Sequence-to-Sequence Models** ğŸ¯  \n",
    "\n",
    "Imagine youâ€™re translating a long sentence from **English** to **French**. A traditional **Encoder-Decoder (Seq2Seq) model** reads the entire English sentence, compresses it into a **single fixed-length vector** (the context vector), and then tries to generate the French translation word by word.  \n",
    "\n",
    "âš ï¸ **But thereâ€™s a problem!**  \n",
    "When the sentence is long, the fixed-size context vector **struggles** to retain all relevant information, leading to **poor translations** and loss of context.  \n",
    "\n",
    "ğŸ‘‰ **Enter the Attention Mechanism!** ğŸš€  \n",
    "The **Attention Mechanism** solves this by allowing the decoder to focus on **different parts of the input sequence at each decoding step**, rather than relying on a single compressed vector.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ’¡ How Does Attention Work? (Step-by-Step Guide)**\n",
    "Letâ€™s break it down in a **simple and intuitive** way.  \n",
    "\n",
    "### **1ï¸âƒ£ Encoder Stage: Read and Store Information**\n",
    "The encoder processes the **input sequence** word by word and generates a **hidden state** at each step.\n",
    "\n",
    "Example: Translating **\"I love machine learning\"** to French **\"J'adore l'apprentissage automatique\"**  \n",
    "\n",
    "ğŸ”¹ The encoder takes each word and **outputs a hidden state**:  \n",
    "- **hâ‚** for \"I\"  \n",
    "- **hâ‚‚** for \"love\"  \n",
    "- **hâ‚ƒ** for \"machine\"  \n",
    "- **hâ‚„** for \"learning\"  \n",
    "\n",
    "ğŸ“Œ Instead of storing only the **final hidden state**, attention keeps track of **ALL hidden states**:  \n",
    "ğŸ’¾ **Memory** = {hâ‚, hâ‚‚, hâ‚ƒ, hâ‚„}  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Decoder Stage: Generate Output Word by Word**\n",
    "The decoder **doesnâ€™t just rely on a single fixed vector**. Instead, for each word it generates, it selectively attends to **relevant parts** of the input sequence.\n",
    "\n",
    "Letâ€™s say we want to generate the first French word: **\"J'adore\"**  \n",
    "\n",
    "ğŸš€ **Instead of using just one vector, the decoder dynamically \"looks\" at different words in the input!**  \n",
    "\n",
    "### **3ï¸âƒ£ Compute Attention Scores**\n",
    "For each decoder step, we compute **attention scores** that determine how much focus the decoder should give to each input word.  \n",
    "\n",
    "ğŸ’¡ How? We compare the decoderâ€™s **current state** with each encoder hidden state to generate a **score** using:  \n",
    "- **Dot product**  \n",
    "- **Additive attention (Bahdanau, 2014)**  \n",
    "- **Multiplicative attention (Luong, 2015)**  \n",
    "\n",
    "ğŸ’¡ Example:  \n",
    "- The first output word **\"J'adore\"** mainly depends on **\"I love\"** â†’ Higher weight for **hâ‚, hâ‚‚**  \n",
    "- The second word **\"l'apprentissage\"** depends on **\"machine learning\"** â†’ Higher weight for **hâ‚ƒ, hâ‚„**  \n",
    "\n",
    "### **4ï¸âƒ£ Compute Attention Weights**\n",
    "ğŸ“Œ Normalize the scores using **softmax** to get a probability distribution.  \n",
    "Example (hypothetical weights for \"J'adore\"):  \n",
    "\n",
    "| Input Word | Raw Score | Softmax Weight (Î±) |\n",
    "|------------|-----------|--------------------|\n",
    "| \"I\" (hâ‚) | 2.3 | 0.30 |\n",
    "| \"love\" (hâ‚‚) | 2.5 | 0.35 |\n",
    "| \"machine\" (hâ‚ƒ) | 1.2 | 0.20 |\n",
    "| \"learning\" (hâ‚„) | 0.8 | 0.15 |\n",
    "\n",
    "ğŸ’¡ **Higher weight = More focus!**  \n",
    "- Here, **hâ‚ and hâ‚‚ (I love)** get the most attention for **\"J'adore\"**.  \n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ Compute Context Vector**\n",
    "Multiply each **hidden state** by its attention weight and sum them:  \n",
    "\n",
    "$$\n",
    "\\text{Context Vector} = \\sum_{i=1}^{n} \\alpha_i \\cdot h_i\n",
    "$$\n",
    "\n",
    "ğŸ”¹ This gives the decoder a **weighted sum of encoder hidden states** â†’ A **dynamic, context-aware vector**!  \n",
    "\n",
    "\n",
    "\n",
    "### **6ï¸âƒ£ Generate the Next Word**\n",
    "- The decoder **uses the context vector** + **previous output** to generate the next word.  \n",
    "- This repeats until the entire output sequence is generated.  \n",
    "\n",
    "ğŸ”¥ **End Result? A much better, context-aware translation!** ğŸ”¥  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ’¡ Why Use Attention Instead of Basic Seq2Seq?**\n",
    "âœ… **Handles Long Sentences**: No more fixed-size bottleneck! Attention dynamically selects the most relevant information.  \n",
    "âœ… **Improves Context Understanding**: Words are attended to **based on meaning**, preventing information loss.  \n",
    "âœ… **Parallelization (in Transformers)**: Unlike RNNs, attention can be computed in parallel, making it much faster.  \n",
    "âœ… **More Human-Like**: It mimics **how humans read**â€”we focus on **important words**, not the entire sentence at once.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”· Where is Attention Used?**\n",
    "ğŸ“Œ **Machine Translation (Google Translate)**  \n",
    "ğŸ“Œ **Speech Recognition (DeepSpeech, Whisper)**  \n",
    "ğŸ“Œ **Text Summarization (BART, Pegasus)**  \n",
    "ğŸ“Œ **Image Captioning (Show, Attend, and Tell)**  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”® Final Thoughts**\n",
    "The **Attention Mechanism** revolutionized deep learning by allowing models to selectively focus on important parts of the input, leading to **better performance, efficiency, and accuracy**. It **paved the way for Transformers**, which now dominate NLP tasks like ChatGPT, BERT, and GPT models!  \n",
    "\n",
    "ğŸš€ **So next time you ask ChatGPT a question, rememberâ€”itâ€™s powered by ATTENTION!** ğŸš€  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's manually go through **how the attention mechanism works** using a small example.  \n",
    "\n",
    "We'll break it down step by step **with actual numbers**. Get ready for some math! ğŸ§®âœ¨  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ Sentence Example**\n",
    "Let's take a short English sentence:  \n",
    "ğŸ‘‰ **\"She loves cats\"**  \n",
    "\n",
    "And let's assume we want to **translate it** into another language, like French.  \n",
    "\n",
    "ğŸ’¡ Our goal:  \n",
    "1. **Manually calculate attention scores**  \n",
    "2. **Show how attention selects words for the decoder**  \n",
    "\n",
    "## **âš™ï¸ Step 1: Word Embeddings & Hidden States**\n",
    "Each word in the input sequence is **converted into a vector** (word embeddings).  \n",
    "For simplicity, we'll assume we have **predefined 3D embeddings** for each word:\n",
    "\n",
    "| Word  | Embedding (3D) |\n",
    "|-------|--------------|\n",
    "| She   | (0.1, 0.2, 0.3) |\n",
    "| Loves | (0.5, 0.4, 0.1) |\n",
    "| Cats  | (0.2, 0.7, 0.6) |\n",
    "\n",
    "These embeddings are processed by the **encoder**, which outputs **hidden states** (hâ‚, hâ‚‚, hâ‚ƒ):  \n",
    "\n",
    "| Word  | Hidden State (3D) |\n",
    "|-------|----------------|\n",
    "| She   | (0.2, 0.1, 0.5) |\n",
    "| Loves | (0.6, 0.3, 0.2) |\n",
    "| Cats  | (0.4, 0.8, 0.3) |\n",
    "\n",
    "The **decoder** will use these hidden states to generate the translation.\n",
    "\n",
    "\n",
    "## **âš™ï¸ Step 2: Compute Attention Scores**  \n",
    "ğŸ’¡ **Goal:** Determine **which input words are important** when generating a translated word.\n",
    "\n",
    "ğŸ“Œ **Formula for attention score (before softmax):**  \n",
    "$$\n",
    "e_{ij} = q_j \\cdot h_i\n",
    "$$  \n",
    "where:  \n",
    "- $ q_j $ = decoder's current hidden state (query)  \n",
    "- $ h_i $ = encoder's hidden states (keys)  \n",
    "- $ e_{ij} $ = raw attention score (dot product of query and keys)\n",
    "\n",
    "Let's assume the decoder's hidden state **(query vector for first translated word)** is:  \n",
    "ğŸ‘‰ $ q = (0.3, 0.5, 0.2) $\n",
    "\n",
    "Now, let's compute attention scores:\n",
    "\n",
    "$$\n",
    "e_1 = q \\cdot h_1 = (0.3, 0.5, 0.2) \\cdot (0.2, 0.1, 0.5)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.2) + (0.5 \\times 0.1) + (0.2 \\times 0.5) = 0.06 + 0.05 + 0.1 = 0.21\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_2 = q \\cdot h_2 = (0.3, 0.5, 0.2) \\cdot (0.6, 0.3, 0.2)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.6) + (0.5 \\times 0.3) + (0.2 \\times 0.2) = 0.18 + 0.15 + 0.04 = 0.37\n",
    "$$\n",
    "\n",
    "$$\n",
    "e_3 = q \\cdot h_3 = (0.3, 0.5, 0.2) \\cdot (0.4, 0.8, 0.3)\n",
    "$$  \n",
    "$$\n",
    "= (0.3 \\times 0.4) + (0.5 \\times 0.8) + (0.2 \\times 0.3) = 0.12 + 0.4 + 0.06 = 0.58\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Raw attention scores:**  \n",
    "$$\n",
    "e_1 = 0.21, \\quad e_2 = 0.37, \\quad e_3 = 0.58\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **âš™ï¸ Step 3: Apply Softmax to Get Attention Weights**\n",
    "Now, we convert these raw scores into probabilities using the **Softmax function**:\n",
    "\n",
    "$$\n",
    "a_i = \\frac{e^e_i}{\\sum e^e_j}\n",
    "$$\n",
    "\n",
    "First, compute exponentials:\n",
    "\n",
    "$$\n",
    "e^{0.21} \\approx 1.234, \\quad e^{0.37} \\approx 1.447, \\quad e^{0.58} \\approx 1.786\n",
    "$$\n",
    "\n",
    "Sum of exponentials:\n",
    "\n",
    "$$\n",
    "1.234 + 1.447 + 1.786 = 4.467\n",
    "$$\n",
    "\n",
    "Now, compute softmax values:\n",
    "\n",
    "$$\n",
    "a_1 = \\frac{1.234}{4.467} \\approx 0.276\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_2 = \\frac{1.447}{4.467} \\approx 0.324\n",
    "$$\n",
    "\n",
    "$$\n",
    "a_3 = \\frac{1.786}{4.467} \\approx 0.400\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Final attention weights:**  \n",
    "$$\n",
    "a_1 = 0.276, \\quad a_2 = 0.324, \\quad a_3 = 0.400\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **âš™ï¸ Step 4: Compute Context Vector**\n",
    "The **context vector** is a weighted sum of the encoder's hidden states:\n",
    "\n",
    "$$\n",
    "C = a_1 h_1 + a_2 h_2 + a_3 h_3\n",
    "$$\n",
    "\n",
    "Each term:\n",
    "\n",
    "$$\n",
    "(0.276 \\times (0.2, 0.1, 0.5)) = (0.0552, 0.0276, 0.138)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.324 \\times (0.6, 0.3, 0.2)) = (0.1944, 0.0972, 0.0648)\n",
    "$$\n",
    "\n",
    "$$\n",
    "(0.400 \\times (0.4, 0.8, 0.3)) = (0.16, 0.32, 0.12)\n",
    "$$\n",
    "\n",
    "Summing up:\n",
    "\n",
    "$$\n",
    "C = (0.0552 + 0.1944 + 0.16, \\quad 0.0276 + 0.0972 + 0.32, \\quad 0.138 + 0.0648 + 0.12)\n",
    "$$\n",
    "\n",
    "$$\n",
    "C = (0.41, 0.445, 0.3228)\n",
    "$$\n",
    "\n",
    "ğŸ“Œ **Final Context Vector:**  \n",
    "ğŸ‘‰ $ C = (0.41, 0.445, 0.3228) $  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Step 5: Use Context Vector to Generate the Next Word**\n",
    "The **context vector** $ C $ is now used as input for the decoder to generate the **first translated word** in French.\n",
    "\n",
    "This process repeats for every next word in the translated sentence!\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Summary of Manual Calculation**\n",
    "1. **Compute attention scores** by dot product of decoder hidden state and encoder hidden states.  \n",
    "2. **Apply softmax** to normalize attention scores into probabilities.  \n",
    "3. **Weight encoder hidden states** using attention values to get a **context vector**.  \n",
    "4. **Feed context vector to decoder** to generate the next word.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Why Attention is Powerful?**\n",
    "âœ… **Focuses on relevant words at each step** ğŸ¹  \n",
    "âœ… **Handles long sentences better** ğŸ“œ  \n",
    "âœ… **Improves translation & NLP tasks** ğŸš€  \n",
    "âœ… **Used in modern AI like Transformers (GPT, BERT, etc.)** ğŸ¤–  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ğŸ§  Attention Mechanism in Simple Layman Terms**  \n",
    "\n",
    "Imagine you are reading a long book ğŸ“– and later, someone asks you a question about a specific part of the story.  \n",
    "\n",
    "- If you had to **memorize the entire book** before answering, you'd likely forget details.  \n",
    "- But if you could **look back at the book** whenever needed, youâ€™d give a much better answer!  \n",
    "\n",
    "ğŸ’¡ **Thatâ€™s exactly what the Attention Mechanism does!**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ’¡ The Problem with Basic Encoder-Decoder (Seq2Seq)**\n",
    "A traditional **encoder-decoder** model is like trying to read an entire book **once** and then retelling it from memory.  \n",
    "\n",
    "ğŸ“Œ **Example**:  \n",
    "You hear the sentence:  \n",
    "ğŸ‘‰ **\"The cat sat on the mat because it was tired.\"**  \n",
    "Now, you must **remember** everything before you start translating it to another language.  \n",
    "\n",
    "ğŸ˜¨ **The problem?**  \n",
    "- If the sentence is too long, the decoder forgets important details.  \n",
    "- The model has to **squeeze** all information into a single memory unit (context vector).  \n",
    "\n",
    "ğŸš€ **Solution? Letâ€™s use Attention!**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ§ What Does Attention Do?**\n",
    "Instead of remembering **everything at once**, Attention lets the model **focus on relevant words** at each step.  \n",
    "\n",
    "ğŸ’¡ Think of it like **a highlighter in a book**â€”you donâ€™t remember the whole book, just the key parts when needed.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ“Œ How Does Attention Work?**\n",
    "Letâ€™s say we are translating:  \n",
    "ğŸ‘‰ **\"The cat sat on the mat because it was tired.\"**  \n",
    "into French:  \n",
    "ğŸ‘‰ **\"Le chat s'est assis sur le tapis parce qu'il Ã©tait fatiguÃ©.\"**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1ï¸âƒ£ - Read the Words (Encoder)**\n",
    "The model reads the English sentence **one word at a time** and stores small memory chunks (**hidden states**) for each word.  \n",
    "\n",
    "| Word | Hidden Memory |\n",
    "|------|--------------|\n",
    "| The | hâ‚ |\n",
    "| cat | hâ‚‚ |\n",
    "| sat | hâ‚ƒ |\n",
    "| on | hâ‚„ |\n",
    "| the | hâ‚… |\n",
    "| mat | hâ‚† |\n",
    "| because | hâ‚‡ |\n",
    "| it | hâ‚ˆ |\n",
    "| was | hâ‚‰ |\n",
    "| tired | hâ‚â‚€ |\n",
    "\n",
    "ğŸ“Œ **Each word has its own hidden state (like taking notes while reading).**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2ï¸âƒ£ - Start Translating (Decoder)**\n",
    "Now, we start generating the translation **one word at a time**.  \n",
    "\n",
    "ğŸ”¹ To generate the first French word (**\"Le\"**), instead of looking at the **whole English sentence**, the decoder **focuses more on** \"The cat\".  \n",
    "\n",
    "âœ… **Attention Mechanism assigns different importance (weights) to each word!**  \n",
    "\n",
    "For **\"Le\"**, it focuses mostly on **\"The\"**  \n",
    "For **\"chat\"**, it focuses on **\"cat\"**  \n",
    "For **\"assis\"**, it focuses on **\"sat\"**, and so on...  \n",
    "\n",
    "ğŸ“Œ Instead of remembering everything, the model **dynamically looks at different words** when translating each word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3ï¸âƒ£ - Assign Attention Weights**\n",
    "The model calculates **how important** each word is for the current translation step.  \n",
    "\n",
    "Example (when generating \"chat\"):  \n",
    "\n",
    "| English Word | Attention Weight (%) |\n",
    "|-------------|----------------------|\n",
    "| The | 10% |\n",
    "| cat | 70% âœ… |\n",
    "| sat | 15% |\n",
    "| on | 5% |\n",
    "\n",
    "ğŸ’¡ The model pays **most attention** to **\"cat\"** when generating **\"chat\"**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 4ï¸âƒ£ - Continue Translating**\n",
    "For the next word (**\"s'est assis\"**), attention shifts focus to **\"sat\"** instead of \"cat\".  \n",
    "\n",
    "âœ… This continues until the full translation is complete!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸš€ Why is Attention Better than Traditional Encoder-Decoder?**\n",
    "âœ… **No more memory bottlenecks** â†’ It doesnâ€™t try to fit the whole sentence into one vector.  \n",
    "âœ… **Better translations** â†’ The model focuses on **relevant** words at each step.  \n",
    "âœ… **Handles long sentences well** â†’ No more forgetting important details!  \n",
    "âœ… **Works in real-world NLP tasks** â†’ Used in Google Translate, ChatGPT, and more!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Attention is Everywhere!**\n",
    "Attention is so powerful that it led to **Transformers**, which power modern AI models like:  \n",
    "ğŸ’¡ **BERT, GPT, Whisper, ChatGPT, and Google Translate!**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
