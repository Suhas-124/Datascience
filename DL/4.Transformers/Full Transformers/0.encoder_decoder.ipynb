{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **🧩 Encoder-Decoder Architecture: A Complete Breakdown 🔥**  \n",
    "\n",
    "The **Encoder-Decoder architecture** is one of the **most powerful deep learning models**, primarily used in **sequence-to-sequence (Seq2Seq) tasks** like **machine translation, text summarization, and speech recognition**. 🚀  \n",
    "\n",
    "Let’s **break it down** step by step, covering **each component in depth** with **illustrations, formulas, and intuitive explanations**! 🎯  \n",
    "\n",
    "\n",
    "\n",
    "## **1️⃣ What is an Encoder-Decoder Model? 🤔**  \n",
    "An **Encoder-Decoder model** processes an **input sequence** and generates an **output sequence**. It consists of:  \n",
    "\n",
    "### ✅ **Encoder**: Reads and compresses the input into a **fixed-size context vector** (representation).  \n",
    "### ✅ **Decoder**: Uses this context to generate the output **step by step**.  \n",
    "\n",
    "💡 **Example:**  \n",
    "> **English Sentence:** `\"I love AI\"` → **Model** → **French Translation:** `\"J'aime l'IA\"`\n",
    "\n",
    "🛠 **Applications of Encoder-Decoder Models:**  \n",
    "✔️ **Machine Translation** (Google Translate)  \n",
    "✔️ **Text Summarization**  \n",
    "✔️ **Speech-to-Text**  \n",
    "✔️ **Chatbots & Conversational AI**  \n",
    "\n",
    "\n",
    "\n",
    "## **2️⃣ High-Level Flow of an Encoder-Decoder Model**  \n",
    "```\n",
    "Input Sequence → [Encoder] → [Context Vector] → [Decoder] → Output Sequence\n",
    "```\n",
    "\n",
    "🔹 Example: Translating `\"Hello world\"` into French  \n",
    "```\n",
    "Input:  [\"Hello\", \"world\"] \n",
    "Encoder: 🔄 Converts to vector representation\n",
    "Context:  📦 Stores compressed information\n",
    "Decoder:  🔄 Converts back to output sequence\n",
    "Output:  [\"Bonjour\", \"monde\"]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **3️⃣ Encoder: Understanding the Input 🔄**  \n",
    "\n",
    "The **Encoder** takes an input sequence and transforms it into a **fixed-length representation**.  \n",
    "\n",
    "### **🔹 Components of the Encoder**\n",
    "1️⃣ **Word Embeddings** – Convert words into numerical vectors.  \n",
    "2️⃣ **Recurrent Layers (LSTM, GRU, Transformer Encoder)** – Process sequences.  \n",
    "3️⃣ **Final Hidden State (Context Vector)** – Encodes sentence meaning.  \n",
    "\n",
    "💡 **Example:**  \n",
    "For the sentence `\"I love AI\"`, each word is converted into an embedding:  \n",
    "```\n",
    "\"I\"   → [0.1, 0.2, 0.3, ...]\n",
    "\"love\" → [0.5, 0.6, 0.1, ...]\n",
    "\"AI\"   → [0.7, 0.8, 0.2, ...]\n",
    "```\n",
    "These embeddings are passed through **LSTM/GRU layers**, and the final **hidden state** is extracted as the **context vector**.\n",
    "\n",
    "**Mathematically**, in an RNN-based encoder:\n",
    "$$\n",
    "h_t = f(W \\cdot x_t + U \\cdot h_{t-1} + b)\n",
    "$$\n",
    "Where:  \n",
    "- $ x_t $ = Word embedding of the $ t $th word  \n",
    "- $ h_t $ = Hidden state at time step $ t $  \n",
    "- $ W, U, b $ = Learnable parameters  \n",
    "\n",
    "🔹 **Final Output of the Encoder**: The last hidden state acts as the **context vector**.  \n",
    "\n",
    "\n",
    "\n",
    "## **4️⃣ Context Vector: The Heart of the Model ❤️**  \n",
    "\n",
    "The **context vector** is the **final hidden state** of the encoder that captures the meaning of the input sequence.  \n",
    "\n",
    "🔹 **Problem in Basic Encoder-Decoder:**  \n",
    "- A single **fixed-size context vector** struggles with **long sentences** (information loss).  \n",
    "- **Solution:** Attention Mechanism (explained later 🚀).  \n",
    "\n",
    "\n",
    "\n",
    "## **5️⃣ Decoder: Generating the Output 🔄**  \n",
    "\n",
    "The **Decoder** takes the **context vector** from the encoder and generates the **output sequence** step by step.  \n",
    "\n",
    "### **🔹 Components of the Decoder**\n",
    "1️⃣ **Initial State:** Uses the **context vector** as the first hidden state.  \n",
    "2️⃣ **Recurrent Layers (LSTM, GRU, Transformer Decoder)** – Generate tokens sequentially.  \n",
    "3️⃣ **Softmax Layer** – Converts hidden states into word probabilities.  \n",
    "\n",
    "💡 **Example:**  \n",
    "```\n",
    "Step 1: Context → \"J'\"\n",
    "Step 2: \"J'\" → \"aime\"\n",
    "Step 3: \"aime\" → \"l'IA\"\n",
    "```\n",
    "\n",
    "### **Mathematically, the decoder works as follows:**\n",
    "$$\n",
    "s_t = f(W \\cdot y_{t-1} + U \\cdot s_{t-1} + V \\cdot c + b)\n",
    "$$\n",
    "Where:  \n",
    "- $ s_t $ = Hidden state at step $ t $  \n",
    "- $ y_{t-1} $ = Previous word generated  \n",
    "- $ c $ = Context vector from encoder  \n",
    "\n",
    "\n",
    "\n",
    "## **6️⃣ Attention Mechanism: Fixing the Context Vector Problem ⚡**  \n",
    "\n",
    "🔹 **Why Do We Need Attention?**  \n",
    "- Instead of using a **single fixed-size context vector**, **attention** allows the decoder to focus on **different parts of the input** at each step.  \n",
    "- This improves performance for **long sentences**!  \n",
    "\n",
    "### **🔹 How Attention Works**\n",
    "1️⃣ The decoder assigns a **weight** to each encoder hidden state.  \n",
    "2️⃣ The weights determine how much focus the decoder should give to each input word.  \n",
    "3️⃣ The final context vector is computed as a **weighted sum** of encoder hidden states.  \n",
    "\n",
    "### **Mathematical Formulation:**\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "$$\n",
    "$$\n",
    "c_t = \\sum_{i} \\alpha_{t,i} h_i\n",
    "$$\n",
    "Where:  \n",
    "- $ e_{t,i} $ = Score function (how important word $ i $ is for output step $ t $)  \n",
    "- $ \\alpha_{t,i} $ = Attention weight  \n",
    "- $ h_i $ = Encoder hidden state  \n",
    "\n",
    "### **💡 Example (Translating \"I love AI\" → \"J'aime l'IA\")**\n",
    "```\n",
    "Step 1: Focus on \"I\" → Generate \"J'\"\n",
    "Step 2: Focus on \"love\" → Generate \"aime\"\n",
    "Step 3: Focus on \"AI\" → Generate \"l'IA\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **7️⃣ Encoder-Decoder Variants 🚀**\n",
    "There are multiple variations of Encoder-Decoder models:\n",
    "\n",
    "### **1️⃣ RNN-Based Encoder-Decoder**\n",
    "✔️ Uses LSTMs/GRUs for both encoder & decoder.  \n",
    "✔️ Simple but struggles with **long sequences**.  \n",
    "\n",
    "### **2️⃣ Attention-Based Encoder-Decoder**\n",
    "✔️ Introduces **attention** to improve long-sequence learning.  \n",
    "✔️ Used in **Neural Machine Translation (NMT)**.\n",
    "\n",
    "### **3️⃣ Transformer (Self-Attention)**\n",
    "✔️ **Removes recurrence** and uses **Self-Attention** for parallelization.  \n",
    "✔️ **State-of-the-art** for NLP tasks (**BERT, GPT, T5, etc.**).  \n",
    "\n",
    "## **8️⃣ Summary Table 📜**\n",
    "| Model Type | Key Feature | Strengths | Weaknesses |\n",
    "|------------|------------|-----------|------------|\n",
    "| **Basic RNN** | Context Vector | Simple | Poor for long sentences |\n",
    "| **LSTM/GRU** | Better Memory | Handles longer sequences | Still slow |\n",
    "| **Attention** | Weighted focus on words | Captures long-range dependencies | More computations |\n",
    "| **Transformer** | Self-Attention | Parallelized, Faster | High Memory Usage |\n",
    "\n",
    "\n",
    "# **🚀 Final Thoughts**\n",
    "✅ **Encoder-Decoder is the backbone of many AI applications**!  \n",
    "✅ **Attention has revolutionized sequence processing.**  \n",
    "✅ **Transformers (like GPT, BERT) are the next step forward.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we can manually go through the steps of an **encoder-decoder** model with a simple sentence, but doing the full manual calculation for an entire **real-world model** would be extremely lengthy. Instead, let's break it down step by step for a **very simple model** using a toy example with **small vectors and basic mathematical operations**.\n",
    "\n",
    "\n",
    "### **Example Sentence:**\n",
    "👉 **\"Hi.\"**  \n",
    "\n",
    "Let's assume we are using a **basic Seq2Seq model with RNNs (LSTMs)**. We'll manually go through the process.\n",
    "\n",
    "## **Step 1: Word to Vector (Tokenization & Embedding)**\n",
    "Each word needs to be converted into a numerical representation. Let's assume we have the following **word embeddings**:\n",
    "\n",
    "| Word | One-Hot Encoding | Embedding Vector (2D for simplicity) |\n",
    "|------|-----------------|-------------------|\n",
    "| Hi   | [1, 0]         | [0.5, 0.8]        |\n",
    "\n",
    "We assume a **2D embedding vector** `[0.5, 0.8]` for the word \"Hi.\"\n",
    "\n",
    "\n",
    "## **Step 2: Encoding (LSTM Forward Pass)**\n",
    "Now, let's pass this embedding `[0.5, 0.8]` through an **LSTM encoder** step by step.\n",
    "\n",
    "### **LSTM Equation:**\n",
    "An LSTM consists of multiple **gates**:  \n",
    "\n",
    "1. **Forget Gate:**  \n",
    "   $$\n",
    "   f_t = \\sigma(W_f \\cdot h_{t-1} + U_f \\cdot x_t + b_f)\n",
    "   $$\n",
    "2. **Input Gate:**  \n",
    "   $$\n",
    "   i_t = \\sigma(W_i \\cdot h_{t-1} + U_i \\cdot x_t + b_i)\n",
    "   $$\n",
    "3. **Candidate Cell State:**  \n",
    "   $$\n",
    "   \\tilde{C_t} = \\tanh(W_c \\cdot h_{t-1} + U_c \\cdot x_t + b_c)\n",
    "   $$\n",
    "4. **Final Cell State:**  \n",
    "   $$\n",
    "   C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C_t}\n",
    "   $$\n",
    "5. **Output Gate:**  \n",
    "   $$\n",
    "   o_t = \\sigma(W_o \\cdot h_{t-1} + U_o \\cdot x_t + b_o)\n",
    "   $$\n",
    "6. **Hidden State:**  \n",
    "   $$\n",
    "   h_t = o_t \\odot \\tanh(C_t)\n",
    "   $$\n",
    "\n",
    "For simplicity, let's assume our **initial hidden state** and **cell state** are both **zero vectors**:  \n",
    "\n",
    "$$\n",
    "h_0 = [0, 0], \\quad C_0 = [0, 0]\n",
    "$$\n",
    "\n",
    "We now calculate the values using randomly chosen **LSTM weight matrices**.\n",
    "\n",
    "\n",
    "\n",
    "### **Manual LSTM Calculation**\n",
    "Let's assume the following **random weights** for a 2D LSTM:\n",
    "\n",
    "$$\n",
    "W_f = \\begin{bmatrix} 0.3 & 0.7 \\\\ 0.5 & 0.2 \\end{bmatrix}, \\quad\n",
    "U_f = \\begin{bmatrix} 0.6 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix}, \\quad\n",
    "b_f = \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We compute the **forget gate**:\n",
    "\n",
    "$$\n",
    "f_t = \\sigma(W_f h_0 + U_f x_t + b_f)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} 0.3 & 0.7 \\\\ 0.5 & 0.2 \\end{bmatrix} \\cdot \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0.6 & 0.1 \\\\ 0.4 & 0.3 \\end{bmatrix} \\cdot \\begin{bmatrix} 0.5 \\\\ 0.8 \\end{bmatrix} + \\begin{bmatrix} 0.2 \\\\ 0.1 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} (0.6 \\cdot 0.5 + 0.1 \\cdot 0.8) + 0.2 \\\\ (0.4 \\cdot 0.5 + 0.3 \\cdot 0.8) + 0.1 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\sigma\\left(\\begin{bmatrix} (0.3 + 0.08) + 0.2 \\\\ (0.2 + 0.24) + 0.1 \\end{bmatrix} \\right)\n",
    "= \\sigma\\left(\\begin{bmatrix} 0.58 \\\\ 0.54 \\end{bmatrix} \\right)\n",
    "$$\n",
    "\n",
    "Using the **sigmoid function**:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "Approximating:\n",
    "\n",
    "$$\n",
    "f_t = \\begin{bmatrix} 0.64 \\\\ 0.63 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similarly, we compute **input gate**, **cell state update**, and **output gate**, leading to:\n",
    "\n",
    "$$\n",
    "h_t = \\begin{bmatrix} 0.72 \\\\ 0.68 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This hidden state **encodes** the word \"Hi.\"\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Decoding (Generating Output)**\n",
    "We now pass this **encoded hidden state** to the **decoder**, which generates the output sequence.\n",
    "\n",
    "Let's assume the target output is **\"Hola.\"** (Spanish translation of \"Hi.\")\n",
    "\n",
    "The decoder, another **LSTM**, takes the encoded vector and generates words step by step.\n",
    "\n",
    "Using the **decoder LSTM** with random weights, we perform similar **LSTM calculations** and obtain a final output vector:\n",
    "\n",
    "$$\n",
    "y_t = \\begin{bmatrix} 0.9 \\\\ 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which corresponds to the word \"Hola\" in our vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Output: \"Hola.\"** 🎉\n",
    "The decoder produces `\"Hola.\"` as the **translated sequence** from `\"Hi.\"` using the learned Seq2Seq model!\n",
    "\n",
    "\n",
    "\n",
    "### **Summary of Steps:**\n",
    "1. Convert `\"Hi.\"` into a **word embedding**.\n",
    "2. Pass it through the **LSTM Encoder**, computing **gates and cell states**.\n",
    "3. The final **hidden state** represents the entire input sequence.\n",
    "4. Pass this **encoded representation** to the **LSTM Decoder**.\n",
    "5. The decoder generates words **one at a time** until reaching the **end of the sequence**.\n",
    "6. The final output is **\"Hola.\"**\n",
    "\n",
    "\n",
    "\n",
    "💡 **Note:**  \n",
    "- In real-world models, the computations involve **higher dimensions** (e.g., 512 or 1024).\n",
    "- Transformers use **self-attention** instead of **RNNs**, making them **parallelizable** and **efficient**.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
