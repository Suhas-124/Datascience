{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• **The Encoder Part of a Transformer ‚Äì Deep Dive!** üöÄ  \n",
    "\n",
    "Transformers revolutionized deep learning, especially in NLP, by using self-attention to process entire sequences **in parallel** instead of sequentially like RNNs. The **encoder** is a key component of this architecture, responsible for **understanding** input text and converting it into meaningful representations.  \n",
    "\n",
    "Let‚Äôs break down the encoder‚Äôs architecture in **depth** and understand **each step with a manual example**! üòÉ  \n",
    "\n",
    "\n",
    "\n",
    "## üîπ **Overall Structure of the Encoder**\n",
    "A Transformer encoder consists of **multiple identical layers** (e.g., 6 in BERT-base, 12 in BERT-large). Each layer has:  \n",
    "1. **Input Embedding + Positional Encoding**  \n",
    "2. **Multi-Head Self-Attention**  \n",
    "3. **Add & Norm (Layer Normalization + Residual Connection)**  \n",
    "4. **Feed-Forward Neural Network (FFN)**  \n",
    "5. **Add & Norm Again (Layer Normalization + Residual Connection)**  \n",
    "\n",
    "Each encoder layer **refines** the representation, making it more powerful for downstream tasks.  \n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 1: Input Processing**\n",
    "### **üîπ Tokenization & Embedding**\n",
    "Let‚Äôs say our input sentence is:  \n",
    "üëâ **\"The cat sat on the mat\"**  \n",
    "\n",
    "1Ô∏è‚É£ First, it is tokenized into subwords (e.g., using WordPiece in BERT):  \n",
    "   $$\n",
    "   [\\text{\"The\"}, \\text{\"cat\"}, \\text{\"sat\"}, \\text{\"on\"}, \\text{\"the\"}, \\text{\"mat\"}]\n",
    "   $$\n",
    "\n",
    "2Ô∏è‚É£ Each token is then converted into an **embedding vector** (e.g., size 512 in BERT).  \n",
    "   - If our embedding matrix has **d = 512**, then:  \n",
    "     $$\n",
    "     X \\in \\mathbb{R}^{6 \\times 512}\n",
    "     $$\n",
    "     This means each of the 6 tokens is now a 512-dimensional vector.\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 2: Positional Encoding**  \n",
    "Since transformers **do not have recurrence**, we add **positional encoding** to preserve word order.  \n",
    "\n",
    "- Positional encoding uses **sine and cosine functions** to generate unique position values for each word.  \n",
    "- This is **added** to the word embeddings, so the final input to the encoder is:  \n",
    "  $$\n",
    "  X' = X + PE\n",
    "  $$\n",
    "\n",
    "üöÄ Now, the words are both **meaningful (word embeddings)** and **aware of their positions (positional encoding)**.\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 3: Multi-Head Self-Attention (The Heart of the Encoder!)**  \n",
    "The key idea: **Each word attends to all other words in the sentence** to understand their relationships.  \n",
    "\n",
    "### **üîπ Step 3.1: Compute Queries, Keys, and Values**  \n",
    "Each input word **X'** (a vector of size 512) is transformed into three matrices:  \n",
    "- **Query (Q)**\n",
    "- **Key (K)**\n",
    "- **Value (V)**\n",
    "\n",
    "Using **learnable weight matrices**:\n",
    "$$\n",
    "Q = X' W_Q, \\quad K = X' W_K, \\quad V = X' W_V\n",
    "$$\n",
    "(Each weight matrix is of size **512 √ó 64** for 8 attention heads.)\n",
    "\n",
    "### **üîπ Step 3.2: Compute Attention Scores**  \n",
    "We compute **scaled dot-product attention** using:  \n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "üîπ **Breaking it down manually**  \n",
    "Let‚Äôs assume:  \n",
    "- Token \"cat\" has a query vector **Q_cat = [2, 3]**  \n",
    "- \"sat\" has a key vector **K_sat = [1, 1]**  \n",
    "- The dot-product is:  \n",
    "  $$\n",
    "  Q_{\\text{cat}} \\cdot K_{\\text{sat}} = (2 \\times 1) + (3 \\times 1) = 5\n",
    "  $$\n",
    "- We scale it by **sqrt(d_k) = sqrt(64) = 8**:  \n",
    "  $$\n",
    "  \\frac{5}{8} = 0.625\n",
    "  $$\n",
    "- Apply **softmax**:  \n",
    "  $$\n",
    "  \\text{softmax}(0.625) = 0.65\n",
    "  $$\n",
    "  This means \"cat\" attends to \"sat\" **with 65% importance**! üéØ  \n",
    "\n",
    "This is done for **all words attending to all others**, producing an **attention matrix**.\n",
    "\n",
    "### **üîπ Step 3.3: Compute the Weighted Sum of Values**  \n",
    "Each word's new representation is computed as:  \n",
    "$$\n",
    "\\sum \\text{(attention score)} \\times \\text{Value vector}\n",
    "$$\n",
    "\n",
    "For multi-head attention, this is done **8 times in parallel**, capturing different relationships in different subspaces! üöÄ\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 4: Add & Norm (Residual Connection + Layer Norm)**  \n",
    "The **output of self-attention is added back to the input (residual connection)**:  \n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(X' + \\text{Self-Attention Output})\n",
    "$$\n",
    "\n",
    "This ensures smooth gradient flow and prevents vanishing gradients! ‚úÖ\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 5: Feed-Forward Network (FFN)**  \n",
    "Each word's representation **passes through a simple MLP**:  \n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "Where:  \n",
    "- **W1, W2** are learned weight matrices  \n",
    "- **ReLU** adds non-linearity  \n",
    "\n",
    "This allows each token to **refine its representation independently**!\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Step 6: Add & Norm (Again!)**  \n",
    "Just like before, we apply **residual connection** and **layer normalization**:  \n",
    "$$\n",
    "\\text{Final Output} = \\text{LayerNorm}(\\text{FFN Output} + \\text{Input to FFN})\n",
    "$$\n",
    "\n",
    "üöÄ Now, the **encoder has finished processing the input!** This output is passed to the **next encoder layer (if any)** or to the **decoder (in sequence-to-sequence models).**  \n",
    "\n",
    "## **üîç Summary of the Encoder Pipeline**\n",
    "| Step | Operation | Purpose |\n",
    "|------|-----------|---------|\n",
    "| **1** | Tokenization & Embedding | Convert words to vectors |\n",
    "| **2** | Positional Encoding | Add word position information |\n",
    "| **3** | Multi-Head Self-Attention | Let each word attend to all others |\n",
    "| **4** | Add & Norm | Stabilize training |\n",
    "| **5** | Feed-Forward Network | Transform representations |\n",
    "| **6** | Add & Norm | Further stabilization |\n",
    "\n",
    "\n",
    "\n",
    "## **üî• Why Is the Encoder So Powerful?**\n",
    "‚úî **Captures Long-Range Dependencies:** Unlike RNNs, which struggle with long sequences, self-attention **connects all words instantly**.  \n",
    "‚úî **Handles Parallel Processing:** Unlike sequential models, Transformers **process all tokens at once**, making them much faster!  \n",
    "‚úî **Works for Any Input Length:** Because of positional encoding, Transformers don‚Äôt need fixed-length inputs.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually calculating how a Transformer encoder processes a sentence is a big task, but let‚Äôs do it step by step for a **single-layer encoder** with **one attention head** for simplicity.  \n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 1: Sentence and Embedding**\n",
    "Let‚Äôs take a simple sentence:  \n",
    "üëâ **\"The cat sat\"** (3 words)\n",
    "\n",
    "Each word gets an embedding. Suppose we use a **4-dimensional embedding** for simplicity:\n",
    "\n",
    "| Word | Embedding (d=4) |\n",
    "|------|----------------|\n",
    "| The  | [0.2, 0.4, 0.8, 0.6] |\n",
    "| Cat  | [0.5, 0.1, 0.9, 0.7] |\n",
    "| Sat  | [0.3, 0.8, 0.2, 0.4] |\n",
    "\n",
    "So, the input matrix **X** is:\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.4 & 0.8 & 0.6 \\\\\n",
    "0.5 & 0.1 & 0.9 & 0.7 \\\\\n",
    "0.3 & 0.8 & 0.2 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 2: Positional Encoding**\n",
    "Since Transformers don‚Äôt have recurrence, they use **positional encoding** to capture the order of words.  \n",
    "\n",
    "Using the formula:  \n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos / 10000^{2i/d})\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos(pos / 10000^{2i/d})\n",
    "$$\n",
    "where:\n",
    "- **pos** = word position (0, 1, 2)\n",
    "- **d** = 4 (embedding size)\n",
    "\n",
    "For simplicity, let's assume the **precomputed positional encoding**:\n",
    "\n",
    "| Position | PE (d=4) |\n",
    "|----------|---------|\n",
    "| 0 (The)  | [0.0, 1.0, 0.0, 1.0] |\n",
    "| 1 (Cat)  | [0.84, 0.54, 0.08, 0.99] |\n",
    "| 2 (Sat)  | [0.90, 0.43, 0.16, 0.99] |\n",
    "\n",
    "Now, **add PE to embeddings**:\n",
    "$$\n",
    "X' = X + PE\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =\n",
    "\\begin{bmatrix}\n",
    "0.2 + 0.0 & 0.4 + 1.0 & 0.8 + 0.0 & 0.6 + 1.0 \\\\\n",
    "0.5 + 0.84 & 0.1 + 0.54 & 0.9 + 0.08 & 0.7 + 0.99 \\\\\n",
    "0.3 + 0.90 & 0.8 + 0.43 & 0.2 + 0.16 & 0.4 + 0.99\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X' =\n",
    "\\begin{bmatrix}\n",
    "0.2 & 1.4 & 0.8 & 1.6 \\\\\n",
    "1.34 & 0.64 & 0.98 & 1.69 \\\\\n",
    "1.2 & 1.23 & 0.36 & 1.39\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This is now **passed to the self-attention mechanism**.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 3: Compute Queries, Keys, and Values**\n",
    "We compute Queries (Q), Keys (K), and Values (V) using weight matrices.  \n",
    "Let‚Äôs assume the **weight matrices** are:\n",
    "\n",
    "$$\n",
    "W_Q =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.5 & 0.7 \\\\\n",
    "0.2 & 0.4 & 0.6 & 0.8 \\\\\n",
    "0.9 & 0.7 & 0.5 & 0.3 \\\\\n",
    "0.8 & 0.6 & 0.4 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Similar matrices exist for **W_K** and **W_V**.\n",
    "\n",
    "Compute queries:  \n",
    "$$\n",
    "Q = X' W_Q\n",
    "$$\n",
    "\n",
    "Multiply:\n",
    "$$\n",
    "Q =\n",
    "\\begin{bmatrix}\n",
    "(0.2 \\times 0.1) + (1.4 \\times 0.2) + (0.8 \\times 0.9) + (1.6 \\times 0.8) & \\dots \\\\\n",
    "(1.34 \\times 0.1) + (0.64 \\times 0.2) + (0.98 \\times 0.9) + (1.69 \\times 0.8) & \\dots \\\\\n",
    "(1.2 \\times 0.1) + (1.23 \\times 0.2) + (0.36 \\times 0.9) + (1.39 \\times 0.8) & \\dots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Repeating for K and V.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 4: Compute Attention Scores**\n",
    "Now we compute the **attention scores** using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Let‚Äôs assume:\n",
    "- Query for \"cat\" is **Q_cat = [1.2, 0.5]**\n",
    "- Key for \"sat\" is **K_sat = [0.9, 1.1]**\n",
    "- Dot product:\n",
    "  $$\n",
    "  (1.2 \\times 0.9) + (0.5 \\times 1.1) = 1.08 + 0.55 = 1.63\n",
    "  $$\n",
    "- Scale by \\(\\sqrt{4} = 2\\)\n",
    "  $$\n",
    "  \\frac{1.63}{2} = 0.815\n",
    "  $$\n",
    "- Apply softmax:\n",
    "  $$\n",
    "  \\frac{e^{0.815}}{e^{0.815} + e^{0.7} + e^{0.5}} = 0.42\n",
    "  $$\n",
    "  So, \"cat\" attends to \"sat\" **with 42% weight**.\n",
    "\n",
    "Repeat for all pairs and compute **weighted sum** with values **V**.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 5: Add & Normalize**\n",
    "$$\n",
    "X'' = \\text{LayerNorm}(X' + \\text{Self-Attention Output})\n",
    "$$\n",
    "\n",
    "Normalize across each feature.\n",
    "\n",
    "\n",
    "\n",
    "## **üöÄ Step 6: Feed-Forward Network**\n",
    "Each word **passes through an MLP**:\n",
    "\n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "Apply residual connection and **LayerNorm again**.\n",
    "\n",
    "\n",
    "## **üöÄ Final Output**\n",
    "Now, we have transformed input embeddings **into contextual representations**!  \n",
    "\n",
    "Each word now **understands its relationship** with all others!  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
