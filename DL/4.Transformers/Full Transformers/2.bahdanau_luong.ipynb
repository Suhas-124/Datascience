{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **📌 Bahdanau Attention (Additive Attention) - Full Explanation**  \n",
    "\n",
    "Bahdanau attention (a.k.a. **Additive Attention**) was introduced by **Dzmitry Bahdanau et al. (2015)** to improve the traditional **encoder-decoder model** in sequence-to-sequence tasks like **machine translation**.  \n",
    "\n",
    "### **🚀 Why do we need Bahdanau Attention?**  \n",
    "In the traditional encoder-decoder architecture:  \n",
    "✅ The **encoder compresses the entire input sentence** into a **single fixed-length context vector**.  \n",
    "✅ The **decoder generates words** based only on that **one vector**.  \n",
    "🚨 **Problem:** If the input sentence is long, a single context vector **loses information**! 😵  \n",
    "\n",
    "🎯 **Solution:** Bahdanau Attention dynamically assigns different attention weights to each word **at each decoding step**!  \n",
    "\n",
    "## **📌 Steps in Bahdanau Attention**\n",
    "Let's break it down step by step **with formulas and an example**!  \n",
    "\n",
    "### **🛠️ Step 1: Encoder Processes Input Sentence**\n",
    "We have an input sentence:  \n",
    "👉 **\"She loves cats\"**  \n",
    "\n",
    "Each word is converted into a **hidden state** using a Bi-directional LSTM/GRU encoder.  \n",
    "\n",
    "| Word  | Hidden State ($ h_i $) |\n",
    "|-------|----------------|\n",
    "| She   | $ h_1 = (0.2, 0.1, 0.5) $ |\n",
    "| Loves | $ h_2 = (0.6, 0.3, 0.2) $ |\n",
    "| Cats  | $ h_3 = (0.4, 0.8, 0.3) $ |\n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 2: Compute Alignment Scores**\n",
    "📌 **Instead of a simple dot product like in traditional attention, Bahdanau uses a feedforward neural network to compute attention scores.**  \n",
    "\n",
    "🔹 We calculate an **alignment score** $ e_i $ for each hidden state:  \n",
    "$$\n",
    "e_i = v_a^T \\tanh(W_a [h_i; s_{t-1}])\n",
    "$$\n",
    "where:  \n",
    "- $ W_a $ and $ v_a $ are **learnable weight matrices**.  \n",
    "- $ h_i $ is the encoder's hidden state for word $ i $.  \n",
    "- $ s_{t-1} $ is the **previous decoder hidden state** (query).  \n",
    "- $ [h_i; s_{t-1}] $ means concatenation.  \n",
    "- $ e_i $ is a scalar **score** that tells us **how important** word $ i $ is at timestep $ t $.  \n",
    "\n",
    "💡 This score is computed **for every input word** at every decoding step.\n",
    "\n",
    "#### **Example Calculation**  \n",
    "Assume we have:  \n",
    "- $ W_a = \\begin{bmatrix} 0.1 & 0.3 & 0.2 \\\\ 0.4 & 0.1 & 0.5 \\end{bmatrix} $  \n",
    "- $ v_a^T = (0.2, 0.6) $  \n",
    "- $ s_{t-1} = (0.3, 0.5, 0.2) $  \n",
    "\n",
    "Let's compute **$ e_1 $ for \"She\"**:  \n",
    "\n",
    "1️⃣ **Concatenate $ h_1 $ and $ s_{t-1} $:**  \n",
    "$$\n",
    "[h_1; s_{t-1}] = (0.2, 0.1, 0.5, 0.3, 0.5, 0.2)\n",
    "$$\n",
    "2️⃣ **Multiply with $ W_a $ and apply $ \\tanh $:**  \n",
    "$$\n",
    "W_a \\times [h_1; s_{t-1}] = \\tanh( [0.31, 0.43] )\n",
    "$$\n",
    "$$\n",
    "= (0.3, 0.4)  \\quad \\text{(applying tanh)}\n",
    "$$\n",
    "3️⃣ **Compute $ e_1 $:**  \n",
    "$$\n",
    "e_1 = (0.2, 0.6) \\cdot (0.3, 0.4) = 0.2(0.3) + 0.6(0.4) = 0.03 + 0.24 = 0.27\n",
    "$$\n",
    "\n",
    "Similarly, compute $ e_2 $ and $ e_3 $.  \n",
    "Let's assume:\n",
    "$$\n",
    "e_1 = 0.27, \\quad e_2 = 0.35, \\quad e_3 = 0.42\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 3: Compute Attention Weights**\n",
    "Now, we apply **Softmax** to convert these scores into probabilities:  \n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{e_i}}{\\sum e^{e_j}}\n",
    "$$\n",
    "\n",
    "Computing exponentials:  \n",
    "$$\n",
    "e^{0.27} \\approx 1.31, \\quad e^{0.35} \\approx 1.42, \\quad e^{0.42} \\approx 1.52\n",
    "$$\n",
    "\n",
    "Sum:\n",
    "$$\n",
    "1.31 + 1.42 + 1.52 = 4.25\n",
    "$$\n",
    "\n",
    "Final attention weights:\n",
    "$$\n",
    "\\alpha_1 = \\frac{1.31}{4.25} \\approx 0.308, \\quad \\alpha_2 = \\frac{1.42}{4.25} \\approx 0.334, \\quad \\alpha_3 = \\frac{1.52}{4.25} \\approx 0.358\n",
    "$$\n",
    "\n",
    "📌 **These values tell us how much attention to pay to each word!**  \n",
    "- **\"She\"**: 30.8%  \n",
    "- **\"Loves\"**: 33.4%  \n",
    "- **\"Cats\"**: 35.8%  \n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 4: Compute Context Vector**\n",
    "We compute a **weighted sum of the encoder hidden states**:  \n",
    "$$\n",
    "C_t = \\sum_{i} \\alpha_i h_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = (0.308 \\times h_1) + (0.334 \\times h_2) + (0.358 \\times h_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.308 \\times (0.2, 0.1, 0.5)) + (0.334 \\times (0.6, 0.3, 0.2)) + (0.358 \\times (0.4, 0.8, 0.3))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.0616, 0.0308, 0.154) + (0.2004, 0.1002, 0.0668) + (0.1432, 0.2864, 0.1074)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.4052, 0.4174, 0.3282)\n",
    "$$\n",
    "\n",
    "📌 **Final Context Vector** $ C_t = (0.4052, 0.4174, 0.3282) $  \n",
    "\n",
    "\n",
    "\n",
    "### **🛠️ Step 5: Generate Next Word in the Translation**\n",
    "This **context vector $ C_t $** is combined with the decoder's hidden state and passed through a softmax layer to predict the **next translated word**.  \n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why Bahdanau Attention is Better?**\n",
    "✅ **Removes fixed-length bottleneck** (no need for a single context vector).  \n",
    "✅ **Focuses on relevant words dynamically** at each step.  \n",
    "✅ **Works better for long sequences.**  \n",
    "✅ **Used in modern AI models like Transformers.**  \n",
    "\n",
    "\n",
    "\n",
    "## **🚀 Summary of Bahdanau Attention**\n",
    "1️⃣ **Compute attention scores** using a neural network.  \n",
    "2️⃣ **Apply softmax** to get attention weights.  \n",
    "3️⃣ **Compute weighted sum** to get a context vector.  \n",
    "4️⃣ **Feed context vector to decoder** to generate output.  \n",
    "\n",
    "\n",
    "## **🌟 Conclusion**\n",
    "Bahdanau Attention is like a **smart spotlight** that helps the decoder **focus** on different words **at each step**, making translations much better! 🌟🚀\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **🚀 Luong Attention Mechanism (Multiplicative Attention) - Full Explanation**  \n",
    "\n",
    "The **Luong Attention Mechanism** was introduced by **Minh-Thang Luong et al. (2015)** to improve Bahdanau Attention. Unlike Bahdanau’s method, which uses an additional neural network to compute attention scores, **Luong Attention directly computes attention scores using dot products**, making it computationally efficient.  \n",
    "\n",
    "\n",
    "\n",
    "## **📌 Why Do We Need Luong Attention?**\n",
    "💡 **Problems with Bahdanau Attention:**  \n",
    "1. **Computational Overhead** 🖥️: Uses an additional neural network to compute attention scores.  \n",
    "2. **More Parameters to Train** 🎛️: Due to extra weight matrices.  \n",
    "\n",
    "💡 **Luong’s Solution:**  \n",
    "✅ Uses **simpler and faster dot-product operations** to calculate attention.  \n",
    "✅ Works better when input and output sequences have a **similar structure** (e.g., English-to-French translation).  \n",
    "\n",
    "## **🛠️ How Luong Attention Works?**\n",
    "Let's go step by step with **a sentence example and manual calculations**!  \n",
    "\n",
    "### **🔹 Given Input Sentence:**  \n",
    "👉 **\"She loves cats\"**  \n",
    "\n",
    "Each word is processed by an **encoder (LSTM/GRU)** to generate **hidden states**:\n",
    "\n",
    "| Word  | Hidden State ($ h_i $) |\n",
    "|-------|----------------|\n",
    "| She   | $ h_1 = (0.2, 0.1, 0.5) $ |\n",
    "| Loves | $ h_2 = (0.6, 0.3, 0.2) $ |\n",
    "| Cats  | $ h_3 = (0.4, 0.8, 0.3) $ |\n",
    "\n",
    "Let’s assume the decoder has already generated some output words and is now predicting the next word. The decoder has a hidden state:  \n",
    "$$\n",
    "s_t = (0.3, 0.5, 0.2)\n",
    "$$\n",
    "\n",
    "\n",
    "## **📌 Luong Attention Has Two Types**\n",
    "1️⃣ **Global Attention**: Attends to all encoder hidden states.  \n",
    "2️⃣ **Local Attention**: Attends only to a subset of encoder hidden states (less common).  \n",
    "\n",
    "We’ll explain **Global Attention**, as it’s the most widely used.\n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 1: Compute Alignment Scores**\n",
    "Luong proposes **three** ways to compute scores:  \n",
    "\n",
    "1️⃣ **Dot Product**:  \n",
    "$$\n",
    "e_i = h_i^T s_t\n",
    "$$\n",
    "\n",
    "2️⃣ **General (with learnable weights $ W_a $)**:  \n",
    "$$\n",
    "e_i = s_t^T W_a h_i\n",
    "$$\n",
    "\n",
    "3️⃣ **Concatenation (Bahdanau-style but simplified)**:  \n",
    "$$\n",
    "e_i = v_a^T \\tanh(W_a [h_i; s_t])\n",
    "$$\n",
    "\n",
    "💡 **Most common method?** **Dot Product**, since it’s fast and works well.\n",
    "\n",
    "### **Example Calculation (Dot Product)**\n",
    "For each encoder hidden state, compute the dot product with the decoder hidden state $ s_t $:\n",
    "\n",
    "$$\n",
    "e_1 = h_1^T s_t = (0.2, 0.1, 0.5) \\cdot (0.3, 0.5, 0.2)\n",
    "$$\n",
    "$$\n",
    "= (0.2 \\times 0.3) + (0.1 \\times 0.5) + (0.5 \\times 0.2)\n",
    "$$\n",
    "$$\n",
    "= 0.06 + 0.05 + 0.10 = 0.21\n",
    "$$\n",
    "\n",
    "Similarly,  \n",
    "$$\n",
    "e_2 = (0.6, 0.3, 0.2) \\cdot (0.3, 0.5, 0.2) = 0.18 + 0.15 + 0.04 = 0.37\n",
    "$$\n",
    "$$\n",
    "e_3 = (0.4, 0.8, 0.3) \\cdot (0.3, 0.5, 0.2) = 0.12 + 0.40 + 0.06 = 0.58\n",
    "$$\n",
    "\n",
    "Now we have:\n",
    "$$\n",
    "e_1 = 0.21, \\quad e_2 = 0.37, \\quad e_3 = 0.58\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 2: Compute Attention Weights**\n",
    "To convert these scores into probabilities, apply **softmax**:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{e_i}}{\\sum e^{e_j}}\n",
    "$$\n",
    "\n",
    "Computing exponentials:\n",
    "$$\n",
    "e^{0.21} \\approx 1.23, \\quad e^{0.37} \\approx 1.45, \\quad e^{0.58} \\approx 1.79\n",
    "$$\n",
    "\n",
    "Sum:\n",
    "$$\n",
    "1.23 + 1.45 + 1.79 = 4.47\n",
    "$$\n",
    "\n",
    "Final attention weights:\n",
    "$$\n",
    "\\alpha_1 = \\frac{1.23}{4.47} \\approx 0.275, \\quad \\alpha_2 = \\frac{1.45}{4.47} \\approx 0.324, \\quad \\alpha_3 = \\frac{1.79}{4.47} \\approx 0.401\n",
    "$$\n",
    "\n",
    "📌 **These values tell us how much attention to pay to each word!**  \n",
    "- **\"She\"**: 27.5%  \n",
    "- **\"Loves\"**: 32.4%  \n",
    "- **\"Cats\"**: 40.1%  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 3: Compute Context Vector**\n",
    "The **context vector** $ C_t $ is computed as a **weighted sum** of the encoder hidden states:\n",
    "\n",
    "$$\n",
    "C_t = \\sum_{i} \\alpha_i h_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "C_t = (0.275 \\times h_1) + (0.324 \\times h_2) + (0.401 \\times h_3)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.275 \\times (0.2, 0.1, 0.5)) + (0.324 \\times (0.6, 0.3, 0.2)) + (0.401 \\times (0.4, 0.8, 0.3))\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.055, 0.0275, 0.1375) + (0.1944, 0.0972, 0.0648) + (0.1604, 0.3208, 0.1203)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (0.4098, 0.4455, 0.3226)\n",
    "$$\n",
    "\n",
    "📌 **Final Context Vector** $ C_t = (0.4098, 0.4455, 0.3226) $  \n",
    "\n",
    "\n",
    "\n",
    "## **🛠️ Step 4: Compute Final Decoder Hidden State**\n",
    "Luong suggests **two ways** to use the context vector $ C_t $:\n",
    "\n",
    "1️⃣ **Concatenation Method (Most Common)**\n",
    "$$\n",
    "\\tilde{s}_t = \\tanh(W_c [C_t; s_t])\n",
    "$$\n",
    "This means we **concatenate** $ C_t $ and $ s_t $, then pass it through a neural network.\n",
    "\n",
    "2️⃣ **Multiplication Method**\n",
    "$$\n",
    "\\tilde{s}_t = C_t + s_t\n",
    "$$\n",
    "Just a direct sum (less commonly used).\n",
    "\n",
    "The final hidden state is used to **generate the next word**.\n",
    "\n",
    "\n",
    "\n",
    "## **🔥 Why is Luong Attention Better?**\n",
    "✅ **More Efficient** 🚀: Uses simple **dot products** instead of extra neural networks.  \n",
    "✅ **More Flexible** 🎛️: Works well with different encoder-decoder structures.  \n",
    "✅ **Better for Structured Data** 📊: If input-output sequences have a similar structure, it outperforms Bahdanau Attention.  \n",
    "\n",
    "\n",
    "\n",
    "## **🌟 Summary of Luong Attention**\n",
    "1️⃣ **Compute attention scores** using a dot product.  \n",
    "2️⃣ **Apply softmax** to get attention weights.  \n",
    "3️⃣ **Compute weighted sum** to get a context vector.  \n",
    "4️⃣ **Combine context with decoder state** to predict the next word.  \n",
    "\n",
    "🚀 **Luong Attention is used in many NLP models like OpenNMT and early versions of Transformers!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔥 **Luong vs. Bahdanau Attention - The Key Differences** 🔥  \n",
    "\n",
    "| Feature 🔍 | **Bahdanau Attention (Additive)** | **Luong Attention (Multiplicative)** |\n",
    "|-----------|---------------------------------|--------------------------------|\n",
    "| **Inventor** 👨‍🔬 | Dzmitry Bahdanau (2014) | Minh-Thang Luong (2015) |\n",
    "| **Computation of Scores** 🧮 | Uses a small feedforward neural network to compute alignment scores (**additive attention**) | Uses dot product or a weight matrix to compute scores (**multiplicative attention**) |\n",
    "| **Formula for Score Calculation** 📏 | \\[ e_i = v_a^T \\tanh(W_a [h_i; s_t]) \\] | **Dot**: \\( e_i = h_i^T s_t \\)  **General**: \\( e_i = s_t^T W_a h_i \\)  **Concatenation**: \\( e_i = v_a^T \\tanh(W_a [h_i; s_t]) \\) |\n",
    "| **Computational Efficiency** ⚡ | **Slower** (uses extra parameters for feedforward NN) | **Faster** (uses simple dot product) |\n",
    "| **When to Use?** 🎯 | ✅ **Good for variable-length sequences** ✅ Works well for **long sentences** (better handling of alignment) | ✅ **Better for structured sequences** ✅ Works well when **input and output have similar structures** |\n",
    "| **Complexity** 📊 | More complex (extra weights and non-linearity) | Simpler (only matrix multiplications) |\n",
    "| **Common Applications** 🤖 | Used in **early Neural Machine Translation (NMT)** models (e.g., **Seq2Seq** for long text) | Used in **modern machine translation** (e.g., **OpenNMT, Google’s NMT system**) |\n",
    "\n",
    "\n",
    "\n",
    "### 🔥 **Which One is Better?**\n",
    "- **Use Bahdanau (Additive) Attention** if you have long or **variable-length** sentences and need more flexibility in learning alignments.  \n",
    "- **Use Luong (Multiplicative) Attention** if **speed and efficiency** are important and your input/output structures are similar.\n",
    "\n",
    "🚀 **Luong Attention is often preferred in practice due to its efficiency!**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bahdanau vs. Luong Attention – Explained in Simple Terms** 🎯  \n",
    "\n",
    "Imagine you are a teacher helping students (decoder) answer questions based on a textbook (encoder). The teacher **does not** just memorize the entire textbook (like the original encoder-decoder model). Instead, they **focus on important sections** while answering. This focus is **attention**!  \n",
    "\n",
    "Now, let’s compare **Bahdanau** and **Luong** attention with real-life examples.  \n",
    "\n",
    "\n",
    "### **📘 Bahdanau Attention (Additive Attention) – \"Thoughtful Teacher\"**  \n",
    "A **thoughtful teacher** reads every part of the book **carefully** before answering. The teacher:  \n",
    "✅ **Thinks deeply** about which sections are important  \n",
    "✅ **Mixes different ideas together** before giving an answer  \n",
    "✅ Uses **more effort and extra steps** to decide what’s important  \n",
    "\n",
    "**Example:**  \n",
    "- A student asks, “What is gravity?”  \n",
    "- The teacher looks at different **paragraphs** in a physics book, **compares them carefully**, and then **blends** the ideas to give the answer.  \n",
    "\n",
    "👨‍🏫 **Bahdanau is good when answers need deep reasoning and multiple references** but **is a bit slow** because of extra thinking.  \n",
    "\n",
    "\n",
    "\n",
    "### **📗 Luong Attention (Multiplicative Attention) – \"Fast Teacher\"**  \n",
    "A **fast teacher** quickly checks **only the most relevant** section of the book and gives an answer. The teacher:  \n",
    "✅ Looks at the book but **does not overthink**  \n",
    "✅ **Matches the question directly** to relevant sections  \n",
    "✅ **Uses quick calculations** (multiplication) instead of blending ideas  \n",
    "\n",
    "**Example:**  \n",
    "- A student asks, “What is Newton’s First Law?”  \n",
    "- The teacher quickly **scans the index**, finds the right section, and reads it **without too much extra processing**.  \n",
    "\n",
    "👨‍🏫 **Luong is good when answers can be found quickly** in **directly matching sections**, making it **faster** but sometimes less flexible.  \n",
    "\n",
    "### **⏳ Key Difference in Simple Words**  \n",
    "| 🧐 **Aspect** | 🤔 **Bahdanau (Additive)** | ⚡ **Luong (Multiplicative)** |\n",
    "|-------------|--------------------------|--------------------------|\n",
    "| **How it works?** | **Thinks deeply** before choosing focus | **Quickly picks the most relevant** section |\n",
    "| **Computation?** | **Extra steps (slower)**, carefully blends ideas | **Simple math (faster)**, direct comparison |\n",
    "| **Example** | **A teacher checking multiple pages carefully before answering** | **A teacher quickly finding the right page and reading from it** |\n",
    "| **Best for?** | **Long and complex answers** | **Quick and straightforward answers** |\n",
    "\n",
    "\n",
    "### **🔥 Which One to Use?**\n",
    "- If **your task is complex and requires looking at different parts of input carefully** → **Use Bahdanau**  \n",
    "- If **your task is structured and the answer is directly linked to one part of the input** → **Use Luong**  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
