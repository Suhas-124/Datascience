{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ **Transformers in Deep Learning: A Complete Guide**  \n",
    "\n",
    "Transformers are a game-changing deep learning architecture that has revolutionized **Natural Language Processing (NLP)** and beyond. First introduced in the paper **\"Attention Is All You Need\"** by Vaswani et al. (2017), transformers have since powered state-of-the-art AI models like **BERT, GPT, T5, and Vision Transformers (ViTs).**  \n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”¥ **What Are Transformers?**  \n",
    "\n",
    "A **Transformer** is a neural network model that relies on a mechanism called **self-attention** to process input data **in parallel**, making it highly efficient and powerful. Unlike earlier models such as **RNNs (Recurrent Neural Networks) and LSTMs**, which process data sequentially, transformers can analyze **entire input sequences at once**, drastically improving speed and accuracy.\n",
    "\n",
    "> ğŸŒŸ **Key Idea**: Instead of processing words one by one like RNNs, transformers look at the entire sentence at once and determine the importance of each word to others using **attention mechanisms.**\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ§  **How Transformers Work? (Simplified)**\n",
    "Transformers consist of an **encoder-decoder structure**, each with **multi-head self-attention and feed-forward layers**.\n",
    "\n",
    "### ğŸ”¹ **Encoder (Understanding Input)**\n",
    "- Takes input (e.g., a sentence) and processes it using self-attention.\n",
    "- Captures relationships between words, even if they are far apart.\n",
    "\n",
    "### ğŸ”¹ **Self-Attention Mechanism**\n",
    "- **Example**: In the sentence *\"The cat sat on the mat.\"*, the model understands that *\"cat\"* and *\"sat\"* are more related than *\"cat\"* and *\"mat\"*.\n",
    "- Assigns **attention scores** to words based on their importance.\n",
    "\n",
    "### ğŸ”¹ **Decoder (Generating Output)**\n",
    "- Generates predictions **word-by-word** while looking at the encoderâ€™s output.\n",
    "- Used in **translation tasks (English â†’ French), text generation (GPT models), etc.**.\n",
    "\n",
    "### ğŸ”¹ **Positional Encoding**\n",
    "- Since transformers process all words at once, they need a way to track word order.\n",
    "- They add **positional embeddings** to retain sequential information.\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ’¡ **Why Are Transformers Used? (Advantages)**  \n",
    "âœ… **Parallel Processing** â€“ Unlike RNNs, transformers process entire input sequences at once, making training **faster** and more efficient.  \n",
    "\n",
    "âœ… **Long-Range Dependencies** â€“ They capture relationships between words across **long texts**, solving RNNs' **vanishing gradient problem**.  \n",
    "\n",
    "âœ… **State-of-the-Art Performance** â€“ Models like **BERT, GPT-4, and T5** achieve **human-like performance** in NLP tasks.  \n",
    "\n",
    "âœ… **Versatility** â€“ Used for **text, images, speech, and even protein structure prediction (AlphaFold)**.  \n",
    "\n",
    "âœ… **Scalability** â€“ Transformers are the backbone of **large AI models**, scaling up with billions of parameters (e.g., GPT-4 has 1.76 trillion parameters!).  \n",
    "\n",
    "âœ… **No Sequential Bottleneck** â€“ Unlike RNNs, transformers **do not require sequential computation**, making them highly efficient for training on **GPUs and TPUs**.\n",
    "\n",
    "\n",
    "\n",
    "# âš ï¸ **Challenges of Transformers (Disadvantages)**  \n",
    "âŒ **High Computational Cost** â€“ Training large models like **GPT-4 or BERT** requires **massive GPUs and TPUs**.  \n",
    "\n",
    "âŒ **Huge Memory Requirements** â€“ Self-attention requires **quadratic** memory growth with input size, making long-text processing expensive.  \n",
    "\n",
    "âŒ **Data-Hungry** â€“ Transformers need **huge datasets** to generalize well, unlike traditional models.  \n",
    "\n",
    "âŒ **Lack of Interpretability** â€“ Unlike simpler models like decision trees, transformers act as **black boxes**, making it hard to understand why they make certain decisions.  \n",
    "\n",
    "âŒ **Ethical Concerns** â€“ Large-scale models can **amplify biases** present in training data and **generate misinformation**.\n",
    "\n",
    "\n",
    "\n",
    "# ğŸŒ **Real-World Applications of Transformers**  \n",
    "\n",
    "### ğŸ’¬ **1. Natural Language Processing (NLP)**\n",
    "- **Machine Translation** (Google Translate using Transformer models)\n",
    "- **Chatbots & Virtual Assistants** (ChatGPT, Bard, Alexa)\n",
    "- **Text Summarization** (Abstractive & Extractive summarization)\n",
    "- **Speech Recognition** (ASR models like Whisper, Kaldi)\n",
    "\n",
    "### ğŸ¤– **2. AI-Generated Content**\n",
    "- **Text Generation** (GPT-4 for AI writing, chatbots, story generation)\n",
    "- **Code Completion** (GitHub Copilot, OpenAI Codex)\n",
    "\n",
    "### ğŸ¥ **3. Computer Vision**\n",
    "- **Image Recognition** (Vision Transformers (ViT), DINO)\n",
    "- **Video Processing** (Detecting objects & scenes in videos)\n",
    "\n",
    "### ğŸ”Š **4. Speech & Audio Processing**\n",
    "- **Speech-to-Text** (ASR models like Whisper, DeepSpeech)\n",
    "- **Text-to-Speech (TTS)** (Google WaveNet, VALL-E)\n",
    "\n",
    "### ğŸ§¬ **5. Biology & Healthcare**\n",
    "- **Drug Discovery** (AI-driven drug design)\n",
    "- **Protein Folding** (AlphaFold 2 revolutionizing bioinformatics)\n",
    "\n",
    "### ğŸ“ˆ **6. Finance & Stock Market**\n",
    "- **Algorithmic Trading** (Predicting stock trends using NLP-based news analysis)\n",
    "- **Fraud Detection** (Analyzing financial transactions)\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”® **The Future of Transformers**\n",
    "Transformers are shaping the future of **AI and deep learning**. With innovations like **efficient attention mechanisms (e.g., Linformer, BigBird), sparse transformers, and multimodal models**, we can expect **smarter AI that understands text, images, and speech better than ever.**\n",
    "\n",
    "ğŸš€ **The possibilities are endless!** From **AI tutors** to **autonomous robots**, transformers will continue to redefine how we interact with technology.\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ¯ **Final Thoughts**\n",
    "Transformers are a **revolutionary architecture** that outperforms traditional models in **speed, accuracy, and versatility**. Despite challenges like **high computational costs**, they are **pushing the boundaries** of AI applications across **NLP, vision, speech, and even science!**\n",
    "\n",
    "![](transformers.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”¥ **Self-Attention in Transformers: A Deep Dive**  \n",
    "\n",
    "Self-attention is the **core mechanism** behind transformers, allowing them to **weigh the importance of different words** in a sentence while processing text. It enables models to **capture long-range dependencies**, unlike RNNs and LSTMs, which struggle with distant word relationships.  \n",
    "\n",
    "\n",
    "\n",
    "# ğŸ¤” **What is Self-Attention?**  \n",
    "Self-attention allows each word in a sentence to focus on **other relevant words** to understand the context better. It helps a transformer model determine **which words matter the most** when making predictions.  \n",
    "\n",
    "### **Example: Translating a Sentence**  \n",
    "Letâ€™s take a sentence:  \n",
    "\n",
    "ğŸ’¬ **\"The cat sat on the mat.\"**  \n",
    "\n",
    "A traditional model might process this word by word, but **self-attention** ensures that **\"sat\"** is more connected to **\"cat\"** than to **\"mat\"**, making the model **more context-aware**.  \n",
    "\n",
    "\n",
    "\n",
    "# ğŸš€ **How Does Self-Attention Work?**  \n",
    "The self-attention mechanism follows a step-by-step process:  \n",
    "\n",
    "### **1ï¸âƒ£ Convert Words into Vectors (Embeddings)**\n",
    "- Words are converted into **word embeddings** (vectors) using techniques like **Word2Vec, FastText, or BERT embeddings**.\n",
    "- These embeddings capture **semantic meaning**.\n",
    "\n",
    "### **2ï¸âƒ£ Create Query, Key, and Value (Q, K, V) Matrices**\n",
    "Each word embedding is transformed into **three vectors**:  \n",
    "- **Query (Q):** What this word is searching for  \n",
    "- **Key (K):** What this word has to offer  \n",
    "- **Value (V):** The actual word representation  \n",
    "\n",
    "Each of these is learned using **weight matrices**, which the transformer **learns** during training.  \n",
    "\n",
    "> ğŸ¯ **Example:**  \n",
    "> - \"The\" â†’ Q1, K1, V1  \n",
    "> - \"cat\" â†’ Q2, K2, V2  \n",
    "> - \"sat\" â†’ Q3, K3, V3  \n",
    "\n",
    "### **3ï¸âƒ£ Compute Attention Scores**\n",
    "Now, we **compare the Query of one word with the Key of every other word** to determine **how much attention one word should give to another**.  \n",
    "- This is done using the **dot product** between Query and Key:  \n",
    "\n",
    "$$\n",
    "\\text{Attention Score} = Q_i \\cdot K_j\n",
    "$$\n",
    "\n",
    "Each word's Query is compared with all other words' Keys, forming an **Attention Score Matrix**.\n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Apply Softmax to Normalize Scores**\n",
    "To make sure the attention scores add up to 1, we apply a **Softmax function**, turning raw scores into **probabilities**.  \n",
    "\n",
    "$$\n",
    "\\text{Softmax}(QK^T) = \\frac{e^{score}}{\\sum e^{score}}\n",
    "$$\n",
    "\n",
    "Words with higher probabilities receive **more attention**!  \n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ Multiply Attention Scores with Value (V)**\n",
    "Each wordâ€™s attention scores are multiplied with the **Value (V) vectors** to compute the final representation of the word.  \n",
    "\n",
    "> ğŸ” **Why use Value (V)?**  \n",
    "> - Q and K **decide attention**, but **V contains the actual meaning of the word**!  \n",
    "\n",
    "\n",
    "\n",
    "### **6ï¸âƒ£ Combine All Weighted Values to Get Output**\n",
    "Once each word is represented with its attended information, we sum them up and get the final **attention-weighted representation** of each word.  \n",
    "\n",
    "This allows words like **\"cat\"** and **\"sat\"** to be closely related, while **\"on\"** and **\"mat\"** get lower attention.\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”¥ **Multi-Head Self-Attention: The Next Level!**  \n",
    "Instead of doing self-attention once, **multi-head attention** applies self-attention **multiple times in parallel**, capturing **different aspects of relationships** between words.\n",
    "\n",
    "- Some heads may focus on **syntax** (e.g., subject-verb agreement).  \n",
    "- Others may focus on **meaning** (e.g., relationships between entities).  \n",
    "\n",
    "After processing, all these heads are **concatenated** and passed through a **feed-forward layer**.\n",
    "\n",
    "\n",
    "\n",
    "# âš¡ **Why is Self-Attention Powerful?**  \n",
    "\n",
    "âœ… **Captures Long-Range Dependencies** â€“ Unlike RNNs, transformers can learn relationships between words **far apart** in a sentence.  \n",
    "\n",
    "âœ… **Parallel Computation** â€“ Unlike sequential RNNs, self-attention processes the whole sequence **at once**, making it **faster**.  \n",
    "\n",
    "âœ… **Context-Aware Representations** â€“ It dynamically **adjusts** based on surrounding words, unlike static word embeddings.  \n",
    "\n",
    "âœ… **Handles Ambiguity** â€“ Words like *\"bank\"* (river vs. finance) can be understood **based on context**.  \n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”¥ **Self-Attention in Action: A Simple Example**  \n",
    "\n",
    "Imagine processing:  \n",
    "ğŸ’¬ **\"The animal didn't cross the street because it was too tired.\"**  \n",
    "\n",
    "What does **\"it\"** refer to? ğŸ§  \n",
    "\n",
    "- Traditional models might struggle.  \n",
    "- With **self-attention**, \"it\" assigns higher attention to **\"animal\"**, helping the model **understand context better**.\n",
    "\n",
    "\n",
    "\n",
    "# ğŸ”® **Final Thoughts**  \n",
    "Self-attention is the **backbone** of transformers, enabling them to process text efficiently and with **context-awareness**. It powers **state-of-the-art AI models** like **BERT, GPT, T5, and Vision Transformers (ViTs)**, making them the **dominant architecture in AI today**. ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Letâ€™s break down **self-attention** in the simplest way possible! ğŸ˜Š  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Imagine Youâ€™re in a Classroom!**\n",
    "You are in a classroom, and the teacher asks a question:  \n",
    "\n",
    "**\"Who won the World Cup in 2011?\"**  \n",
    "\n",
    "Now, everyone in the class starts thinking ğŸ¤”. Some students might **remember the answer quickly**, while others may need a **hint**.  \n",
    "\n",
    "This is exactly what self-attention does! **Each word in a sentence â€œlooks atâ€ the other words** to understand which ones are important.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ How Does It Work? (Super Simple)**\n",
    "Letâ€™s take an example sentence:  \n",
    "\n",
    "ğŸ’¬ **\"The cat sat on the mat.\"**  \n",
    "\n",
    "Each word in this sentence tries to **figure out which other words are important** for understanding its meaning.  \n",
    "\n",
    "ğŸ”¹ When **\"cat\"** is looking around, it realizes that **\"sat\"** is more important than **\"mat\"**, because \"sat\" tells us what the cat is doing.  \n",
    "\n",
    "ğŸ”¹ When **\"on\"** looks around, it sees **\"mat\"** is more important because it tells us **where** the cat sat.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ’¡ The Key Idea: Words Pay Attention to Each Other!**\n",
    "Instead of treating every word equally, **self-attention helps words focus on the most relevant words** to understand the sentence better.  \n",
    "\n",
    "Think of it like a **group discussion**:  \n",
    "- Each student (word) listens to what others are saying.  \n",
    "- Some voices are more important, so they listen **more closely** to them.  \n",
    "- This helps everyone understand the topic **better and faster**!  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”„ Self-Attention in Action**\n",
    "1ï¸âƒ£ Each word in a sentence **asks**: *\"Which words are important to me?\"*  \n",
    "2ï¸âƒ£ It **checks** all other words and **gives them scores** (higher scores = more important).  \n",
    "3ï¸âƒ£ It **focuses more** on high-scored words while forming the final sentence understanding.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ‘€ Real-Life Example: How We Use Self-Attention**\n",
    "Letâ€™s say your friend texts you:  \n",
    "\n",
    "ğŸ’¬ **\"I went to a party last night. It was amazing!\"**  \n",
    "\n",
    "ğŸ”¹ **\"It\"** â†’ What does \"it\" refer to? ğŸ¤”  \n",
    "- Your brain **does self-attention** and realizes **\"it\" refers to \"party\"**, not \"night\" or \"went\".  \n",
    "\n",
    "Thatâ€™s exactly how self-attention helps AI models understand text! ğŸ¤–  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Why is Self-Attention So Powerful?**\n",
    "âœ… **Understands Context** â€“ Words like \"bank\" (river or money?) are understood **based on nearby words**.  \n",
    "âœ… **Handles Long Sentences** â€“ Unlike older models (RNNs), it doesnâ€™t forget earlier words.  \n",
    "âœ… **Super Fast** â€“ Looks at **all words at once** instead of one by one.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”® Final Thought**\n",
    "Think of self-attention like **highlighting important words** while reading a book. It helps transformers **focus on what truly matters** instead of treating every word the same.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Let's break down **Query (Q), Key (K), and Value (V)** in Transformers **step by step** in a **simple and intuitive way**.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ§  Why Do We Need Q, K, V?**  \n",
    "Imagine you're in a **library** ğŸ“š, and you're **looking for a book** about \"Deep Learning\".  \n",
    "\n",
    "1ï¸âƒ£ **Query (Q)** â†’ What you are searching for â†’ **(\"Deep Learning\")**  \n",
    "2ï¸âƒ£ **Key (K)** â†’ The labels on books in the library  \n",
    "3ï¸âƒ£ **Value (V)** â†’ The actual book content  \n",
    "\n",
    "ğŸ‘‰ **The idea**: You **compare** your Query (Q) with the Keys (K) on the bookshelves. The books **most relevant** to your query get the **highest score**, and you read their content (V) with more attention.  \n",
    "\n",
    "This is exactly how **self-attention in Transformers** works! ğŸš€  \n",
    "\n",
    "## **ğŸ’¡ How Q, K, V Work in Transformers**\n",
    "Each word in a sentence is **transformed into three vectors**:  \n",
    "- **Query (Q)** â€“ What this word is searching for in other words.  \n",
    "- **Key (K)** â€“ How relevant this word is to other words.  \n",
    "- **Value (V)** â€“ The actual information of this word.  \n",
    "\n",
    "ğŸ’¬ **Example Sentence:**  \n",
    "ğŸ‘‰ \"The cat sat on the mat.\"  \n",
    "\n",
    "Now, let's focus on the word **\"cat\"** ğŸ±:  \n",
    "\n",
    "| Word  | Query (Q) | Key (K) | Value (V) |\n",
    "|--------|----------|----------|----------|\n",
    "| The   | Looks for relevant words | Matches with \"The\" | \"The\" itself |\n",
    "| **Cat** ğŸ± | Looks for context | Matches \"sat\" | \"Cat\" itself |\n",
    "| Sat   | Looks for subject | Matches \"cat\" | \"Sat\" itself |\n",
    "| On    | Looks for location | Matches \"mat\" | \"On\" itself |\n",
    "| Mat   | Looks for subject | Matches \"on\" | \"Mat\" itself |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¢ How Does Attention Work? (Step-by-Step)**\n",
    "ğŸ’¡ **Step 1: Calculate Attention Scores**  \n",
    "Each word's **Query (Q)** is compared with every other word's **Key (K)** to get a similarity score. The more similar they are, the more attention the word pays to it.  \n",
    "\n",
    "ğŸ’¡ **Step 2: Apply Softmax to Get Attention Weights**  \n",
    "The scores are converted into a probability distribution (softmax) so that the focus is distributed properly.  \n",
    "\n",
    "ğŸ’¡ **Step 3: Multiply by Values (V)**  \n",
    "Each word's **Value (V)** is weighted based on attention scores. Words that get higher attention contribute more to the final output.  \n",
    "\n",
    "ğŸ’¡ **Step 4: Update the Word Representation**  \n",
    "The final representation of each word is updated based on its weighted combination of all words in the sentence.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Why Is This Powerful?**\n",
    "âœ… **Captures Context** â€“ Words can dynamically change their meaning based on surrounding words.  \n",
    "âœ… **Handles Long Sentences** â€“ Unlike RNNs, Transformers can understand **distant relationships** between words.  \n",
    "âœ… **Improves NLP Tasks** â€“ Used in **translation, chatbots, text summarization, etc.**  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¥ Final Takeaway**\n",
    "Think of **Q, K, V** as how we **search for, match, and retrieve information** in daily life. **Self-attention in Transformers** follows the same logic to understand text **contextually and efficiently**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolutely! Letâ€™s break down **Scaled Dot-Product Attention** in Transformers **step by step** in the simplest way possible! ğŸ˜Š  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ” Why Do We Need Scaled Dot-Product Attention?**  \n",
    "Before jumping into the formula, let's first understand **why** we need **Scaled Dot-Product Attention**.  \n",
    "\n",
    "Imagine you are in a classroom, and the teacher asks a question:  \n",
    "ğŸ‘‰ **\"Who discovered gravity?\"**  \n",
    "\n",
    "Your brain **immediately connects** this to \"Isaac Newton\" ğŸ.  \n",
    "\n",
    "âœ… You ignore unnecessary words.  \n",
    "âœ… You focus only on the **important words** in the sentence.  \n",
    "\n",
    "This is exactly what **Scaled Dot-Product Attention** does! It helps the Transformer **focus on the right words efficiently**. ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¢ Step-by-Step: Scaled Dot-Product Attention**\n",
    "The attention mechanism takes three inputs:  \n",
    "- **Query (Q)** â†’ What each word is looking for.  \n",
    "- **Key (K)** â†’ What information each word has.  \n",
    "- **Value (V)** â†’ The actual meaning of each word.  \n",
    "\n",
    "ğŸ‘‰ **Attention(Q, K, V) = Softmax( (Q Ã— Káµ€) / âˆšd ) Ã— V**  \n",
    "\n",
    "Letâ€™s break this formula down step by step.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1ï¸âƒ£: Compute Q Ã— Káµ€ (Dot Product of Queries and Keys)**  \n",
    "Each word **compares itself** with all other words to see **which words are important**.  \n",
    "\n",
    "ğŸ’¬ **Example Sentence:**  \n",
    "ğŸ‘‰ \"The cat sat on the mat.\"  \n",
    "\n",
    "If **Q (cat)** interacts with **K (sat, mat, etc.)**, we get similarity scores:  \n",
    "\n",
    "| Words Compared | Dot Product Score |\n",
    "|---------------|------------------|\n",
    "| Cat & The   | 0.2  |\n",
    "| Cat & Cat   | 1.0  |\n",
    "| Cat & Sat   | 0.8  |\n",
    "| Cat & On    | 0.1  |\n",
    "| Cat & Mat   | 0.5  |\n",
    "\n",
    "ğŸ’¡ **Higher scores = more important words!**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2ï¸âƒ£: Scale by âˆšd (Why Do We Scale?)**  \n",
    "ğŸ‘‰ If the dot product values are **too large**, softmax will give **extremely high weights** to some words and ignore others.  \n",
    "ğŸ‘‰ To prevent this, we **divide by âˆšd**, where **d is the embedding size**.  \n",
    "\n",
    "This **balances** the attention distribution, so we donâ€™t focus too much on just one word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3ï¸âƒ£: Apply Softmax (Convert Scores to Probabilities)**  \n",
    "Softmax makes sure that all attention scores **add up to 1** (like probabilities).  \n",
    "\n",
    "ğŸ”¹ High values become **closer to 1** (high attention).  \n",
    "ğŸ”¹ Low values become **closer to 0** (low attention).  \n",
    "\n",
    "| Word Pair | Scaled Score | Softmax Output (Attention Weight) |\n",
    "|-----------|-------------|--------------------------------|\n",
    "| Cat & The | 0.2 â†’ 0.05 | 0.10 |\n",
    "| Cat & Cat | 1.0 â†’ 0.25 | 0.40 |\n",
    "| Cat & Sat | 0.8 â†’ 0.20 | 0.30 |\n",
    "| Cat & On  | 0.1 â†’ 0.02 | 0.05 |\n",
    "| Cat & Mat | 0.5 â†’ 0.12 | 0.15 |\n",
    "\n",
    "ğŸ’¡ **Now, the Transformer knows how much focus to give to each word!**  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 4ï¸âƒ£: Multiply by V (Weighted Sum of Values)**  \n",
    "Finally, we **multiply** these attention scores with **V (Values)** to get the final representation of the word.  \n",
    "\n",
    "ğŸ”¹ Words that got **higher attention weights** contribute **more** to the final meaning.  \n",
    "\n",
    "**Final Output:**\n",
    "- **Catâ€™s updated representation** now **incorporates** information from **Sat, Mat**, and other relevant words.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸš€ Why is Scaled Dot-Product Attention So Powerful?**\n",
    "âœ… **Captures Important Relationships** â†’ Finds meaningful word connections.  \n",
    "âœ… **Balances Attention Distribution** â†’ Prevents one word from dominating.  \n",
    "âœ… **Computationally Efficient** â†’ Works in parallel, unlike older models (RNNs).  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ”¥ Final Takeaway**\n",
    "Think of **Scaled Dot-Product Attention** as a **smart highlighter** ğŸ–ï¸ that helps the Transformer **focus on the most important words** in a sentence, making the model **understand language better**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes! Let's go step by step and manually calculate the **geometric intuition of self-attention** using a **simple sentence**. I'll keep it **easy and visual** so that you get a clear **intuition** of how self-attention works in **vector space**. ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ” Problem Setup:**\n",
    "We take a simple sentence:  \n",
    "\n",
    "ğŸ‘‰ **\"I love NLP\"**  \n",
    "\n",
    "ğŸ’¡ **Goal:** Compute self-attention **manually** using vectors, dot product, and softmax!  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1ï¸âƒ£: Convert Words into Vector Representations**\n",
    "Each word is transformed into a **vector** (we assume these are pre-trained embeddings).  \n",
    "\n",
    "Let's assign some **simple 2D vectors** for each word:  \n",
    "\n",
    "| Word  | Vector Representation (Embeddings) |\n",
    "|-------|------------------------------------|\n",
    "| **I**    | [1, 2]  |\n",
    "| **Love** â¤ï¸ | [2, 3]  |\n",
    "| **NLP** ğŸ¤– | [3, 1]  |\n",
    "\n",
    "These vectors **live in a 2D space**, and we will perform self-attention using **dot product, softmax, and weighted sum**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2ï¸âƒ£: Compute Queries (Q), Keys (K), and Values (V)**  \n",
    "Each word has:  \n",
    "- **Query (Q)** â†’ What this word is searching for  \n",
    "- **Key (K)** â†’ How relevant this word is  \n",
    "- **Value (V)** â†’ The actual content of the word  \n",
    "\n",
    "For simplicity, let's **assume Q = K = V**, so we take the same word vectors as Q, K, and V.\n",
    "\n",
    "| Word  | Query (Q)  | Key (K)  | Value (V)  |\n",
    "|-------|-----------|-----------|-----------|\n",
    "| **I**    | [1, 2]  | [1, 2]  | [1, 2]  |\n",
    "| **Love** â¤ï¸ | [2, 3]  | [2, 3]  | [2, 3]  |\n",
    "| **NLP** ğŸ¤– | [3, 1]  | [3, 1]  | [3, 1]  |\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3ï¸âƒ£: Compute Attention Scores using Dot Product (Q Ã— Káµ€)**  \n",
    "Each word's **Query (Q)** is compared with every other wordâ€™s **Key (K)** using the **dot product**.  \n",
    "\n",
    "#### **Dot Product Formula:**  \n",
    "$$\n",
    "\\text{Score} = Q \\cdot K^T\n",
    "$$\n",
    "\n",
    "Letâ€™s compute the dot product for all words:\n",
    "\n",
    "#### **Dot product for \"I\" with all words (Q = [1,2])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **I & I** | (1Ã—1) + (2Ã—2) = 1 + 4  | **5** |\n",
    "| **I & Love** | (1Ã—2) + (2Ã—3) = 2 + 6  | **8** |\n",
    "| **I & NLP** | (1Ã—3) + (2Ã—1) = 3 + 2  | **5** |\n",
    "\n",
    "#### **Dot product for \"Love\" with all words (Q = [2,3])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **Love & I** | (2Ã—1) + (3Ã—2) = 2 + 6  | **8** |\n",
    "| **Love & Love** | (2Ã—2) + (3Ã—3) = 4 + 9  | **13** |\n",
    "| **Love & NLP** | (2Ã—3) + (3Ã—1) = 6 + 3  | **9** |\n",
    "\n",
    "#### **Dot product for \"NLP\" with all words (Q = [3,1])**\n",
    "| Word Pair | Computation   | Score |\n",
    "|-----------|--------------|--------|\n",
    "| **NLP & I** | (3Ã—1) + (1Ã—2) = 3 + 2  | **5** |\n",
    "| **NLP & Love** | (3Ã—2) + (1Ã—3) = 6 + 3  | **9** |\n",
    "| **NLP & NLP** | (3Ã—3) + (1Ã—1) = 9 + 1  | **10** |\n",
    "\n",
    "So, we get the **attention score matrix**:\n",
    "\n",
    "$$\n",
    "S =\n",
    "\\begin{bmatrix}\n",
    "5 & 8 & 5 \\\\\n",
    "8 & 13 & 9 \\\\\n",
    "5 & 9 & 10\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4ï¸âƒ£: Apply Scaling (Divide by âˆšd)**\n",
    "The embedding dimension (**d**) here is **2** (since our vectors are 2D).  \n",
    "\n",
    "$$\n",
    "\\text{Scale Factor} = \\sqrt{2} = 1.41\n",
    "$$\n",
    "\n",
    "We **divide each score** by 1.41 to balance the attention distribution:\n",
    "\n",
    "| Scaled Score Matrix |\n",
    "|---------------------|\n",
    "| **5 / 1.41 = 3.54**   **8 / 1.41 = 5.67**  **5 / 1.41 = 3.54**  |\n",
    "| **8 / 1.41 = 5.67**   **13 / 1.41 = 9.22**  **9 / 1.41 = 6.38**  |\n",
    "| **5 / 1.41 = 3.54**   **9 / 1.41 = 6.38**  **10 / 1.41 = 7.09**  |\n",
    "\n",
    "\n",
    "\n",
    "### **Step 5ï¸âƒ£: Apply Softmax to Get Attention Weights**\n",
    "Now, we apply **softmax** to normalize the scores into probabilities.  \n",
    "\n",
    "Softmax formula:  \n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum e^{x_i}}\n",
    "$$\n",
    "\n",
    "For example, applying softmax to the first row:\n",
    "$$\n",
    "e^{3.54} = 34.5, \\quad e^{5.67} = 289.6, \\quad e^{3.54} = 34.5\n",
    "$$\n",
    "Sum = **34.5 + 289.6 + 34.5 = 358.6**  \n",
    "\n",
    "Now, compute **softmax values**:\n",
    "- **I â†’ I:** **34.5 / 358.6 = 0.096**  \n",
    "- **I â†’ Love:** **289.6 / 358.6 = 0.81**  \n",
    "- **I â†’ NLP:** **34.5 / 358.6 = 0.096**  \n",
    "\n",
    "Similarly, we compute for all words to get the **final attention matrix**:\n",
    "\n",
    "$$\n",
    "A =\n",
    "\\begin{bmatrix}\n",
    "0.096 & 0.81 & 0.096 \\\\\n",
    "0.19 & 0.64 & 0.17 \\\\\n",
    "0.10 & 0.45 & 0.45\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 6ï¸âƒ£: Compute Final Output by Multiplying with Values (V)**\n",
    "Final representation for **\"I\"** is:\n",
    "\n",
    "$$\n",
    "\\text{I} = (0.096 \\times [1,2]) + (0.81 \\times [2,3]) + (0.096 \\times [3,1])\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.096, 0.192] + [1.62, 2.43] + [0.288, 0.096]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [2.00, 2.71]\n",
    "$$\n",
    "\n",
    "Similarly, compute for **Love** and **NLP** to get updated embeddings.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ¯ Final Takeaway (Geometric View)**\n",
    "1ï¸âƒ£ Each word **compares itself** with all others using **dot product**.  \n",
    "2ï¸âƒ£ The **softmax** turns these into attention weights (how much attention to pay).  \n",
    "3ï¸âƒ£ The final word representation is a **weighted sum** of other words based on attention scores.  \n",
    "\n",
    "ğŸ’¡ **Self-attention gives words new, context-rich embeddings!** ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ Why is **\"Self-Attention\"** Called \"Self\"?  \n",
    "\n",
    "\"Self-attention\" is called **\"self\"** because, unlike traditional attention mechanisms that focus on different parts of an input sequence **relative to another sequence** (e.g., encoder-decoder attention), self-attention operates **within** the same sequence.  \n",
    "\n",
    "Each token (word or feature) in the sequence attends to **all other tokens, including itself** to compute its new representation. This allows the model to capture **global dependencies**, regardless of their position in the sequence.  \n",
    "\n",
    "ğŸ”¹ **Example Sentence:**  \n",
    "*\"The cat sat on the mat.\"*  \n",
    "\n",
    "âœ… The word **\"cat\"** can pay attention to **\"sat\"** to understand the action.  \n",
    "âœ… The word **\"mat\"** can attend to **\"on\"** for spatial context.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Self-Attention vs. Luong Attention**  \n",
    "\n",
    "### âœ¨ **1. Self-Attention (Transformer Attention)**\n",
    "ğŸ›  **Used in:** Transformers (e.g., **BERT, GPT**).  \n",
    "ğŸŒ **Key Idea:** Every token **attends to all other tokens** in the input sequence.  \n",
    "ğŸ”— **Best for:** Capturing **long-range dependencies**.  \n",
    "âš¡ **Fully Parallelizable** â€“ No sequential dependencies!  \n",
    "\n",
    "#### ğŸ” **How It Works?**\n",
    "1ï¸âƒ£ Compute **Query (Q), Key (K), and Value (V)** matrices from the input.  \n",
    "2ï¸âƒ£ Compute **attention scores** using:  \n",
    "   $$\n",
    "   \\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "   $$  \n",
    "3ï¸âƒ£ Multiply scores with **Value (V)** matrix to get the new representation.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ¯ **2. Luong Attention (Traditional Attention)**\n",
    "ğŸ›  **Used in:** **Seq2Seq (LSTM, GRU)** with attention.  \n",
    "ğŸ¯ **Key Idea:** Focuses on aligning **encoder outputs** with the **decoder state**.  \n",
    "ğŸ“‰ **Step-wise Calculation** â€“ Not fully parallelizable like self-attention.  \n",
    "ğŸ“ **Best for:** Capturing dependencies **between encoder & decoder**.  \n",
    "\n",
    "#### ğŸ” **How It Works?**\n",
    "1ï¸âƒ£ At each decoder time step, compare the **decoder hidden state** with all **encoder outputs** to get attention scores.  \n",
    "2ï¸âƒ£ Compute **context vector** as a weighted sum of encoder outputs.  \n",
    "3ï¸âƒ£ Combine **context vector** with the **decoder hidden state** to predict the next token.  \n",
    "\n",
    "## ğŸ”¥ **Key Differences: Self-Attention vs. Luong Attention**\n",
    "| Feature ğŸ†        | Self-Attention (Transformer) âš¡ | Luong Attention (Seq2Seq) ğŸ”„ |\n",
    "|-----------------|----------------------------|---------------------------|\n",
    "| **Works within** | Same sequence (e.g., input sentence) | Encoder-Decoder interaction |\n",
    "| **Computes Attention** | All tokens attend to all tokens | Decoder attends to encoder outputs |\n",
    "| **Parallelization** | âœ… Fully parallelizable | âŒ Step-wise (not parallelizable) |\n",
    "| **Dependency Range** | ğŸŒ Long-range dependencies | ğŸ” Limited dependency range |\n",
    "| **Use Case** | ğŸ¤– Transformers (BERT, GPT) | ğŸ“œ Seq2Seq (LSTMs, GRUs) |\n",
    "\n",
    "\n",
    "## ğŸ§ **When Should You Use Which?**  \n",
    "âœ… **Use Self-Attention** when handling **long-range dependencies** (e.g., **machine translation, text generation, speech recognition**).  \n",
    "âœ… **Use Luong Attention** in **RNN-based Seq2Seq models**, where tight **encoder-decoder alignment** is necessary.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ Multi-Head Attention in Transformers â€“ **Explained Visually & Clearly** ğŸ¨ğŸš€  \n",
    "\n",
    "Multi-Head Attention is a **superpower** ğŸ¦¸â€â™‚ï¸ of Transformers! It allows the model to focus on **different parts of the input simultaneously**, capturing multiple perspectives of the data. Letâ€™s break it down!  \n",
    "\n",
    "\n",
    "## ğŸŒŸ **What is Multi-Head Attention?**  \n",
    "ğŸ”¹ Imagine reading a complex book ğŸ“–. Instead of focusing on one word at a time, your brain can analyze **multiple aspects** of the text:  \n",
    "- The **main theme** ğŸ§  \n",
    "- The **characters' emotions** ğŸ˜ŠğŸ˜¡  \n",
    "- The **storyâ€™s timeline** â³  \n",
    "\n",
    "Multi-Head Attention does the same! Instead of computing a **single** attention score, it learns **multiple attention patterns in parallel** to understand different relationships in the data.  \n",
    "\n",
    "ğŸ” **Key Idea**:  \n",
    "ğŸ‘‰ Instead of applying **one** self-attention mechanism, we apply **multiple** attention mechanisms (heads) **in parallel** and combine their outputs.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ—ï¸ **How Does Multi-Head Attention Work?**  \n",
    "\n",
    "### ğŸ”¹ **Step 1: Compute Query, Key, and Value (Q, K, V) Matrices**  \n",
    "Each input token (word/feature) is transformed into **three** vectors:  \n",
    "- **Query (Q)** â†’ \"What am I looking for?\"  \n",
    "- **Key (K)** â†’ \"What do I have?\"  \n",
    "- **Value (V)** â†’ \"What information do I carry?\"  \n",
    "\n",
    "ğŸ’¡ **These matrices are obtained by multiplying the input embeddings with learned weight matrices**:  \n",
    "$$\n",
    "Q = XW_Q, \\quad K = XW_K, \\quad V = XW_V\n",
    "$$\n",
    "Where:  \n",
    "- **X** = input embeddings  \n",
    "- **W_Q, W_K, W_V** = weight matrices for Query, Key, and Value  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Step 2: Compute Scaled Dot-Product Attention**  \n",
    "To determine **how much each word should pay attention to others**, we compute attention scores using the **dot-product** of Query and Key:  \n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$  \n",
    "\n",
    "ğŸ‘‰ The **Softmax function** converts these scores into probabilities, determining **which tokens should be attended to more**.  \n",
    "\n",
    "ğŸ’¡ **Why divide by** $ \\sqrt{d_k} $ **?**  \n",
    "- It prevents large values in the dot-product from causing extremely sharp Softmax distributions.  \n",
    "\n",
    "### ğŸ”¹ **Step 3: Split into Multiple Attention Heads**  \n",
    "Instead of using **one** set of $ Q, K, V $, we **split** them into multiple \"heads\" ğŸ§  that process different parts of the input independently.  \n",
    "\n",
    "Example with 3 heads:  \n",
    "| Head ğŸ§  | Focus ğŸ¯ |  \n",
    "|--------|---------|  \n",
    "| **Head 1** | Word order & position ğŸ“ |  \n",
    "| **Head 2** | Meaning & synonyms ğŸ“ |  \n",
    "| **Head 3** | Context & dependencies ğŸ”„ |  \n",
    "\n",
    "Each head runs **its own attention mechanism**, capturing different types of relationships!  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¹ **Step 4: Concatenate & Project the Heads**  \n",
    "After computing attention in **each head**, we **concatenate** them together and pass them through a final weight matrix $ W_O $ to merge the information.  \n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{Head}_1, \\text{Head}_2, ..., \\text{Head}_h) W_O\n",
    "$$\n",
    "\n",
    "Now, we have **a richer, more detailed representation of our input**! ğŸ¯  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Why Use Multi-Head Attention?**  \n",
    "âœ… **Improves learning capacity** â€“ Each head captures different aspects of the sequence.  \n",
    "âœ… **Enhances representation power** â€“ More perspectives = **better understanding**.  \n",
    "âœ… **Enables parallel processing** â€“ Multiple heads work **simultaneously**, making training efficient!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Multi-Head Attention vs. Single-Head Attention**\n",
    "| Feature ğŸ† | **Multi-Head Attention** ğŸ¯ | **Single-Head Attention** ğŸ”„ |  \n",
    "|------------|----------------------------|---------------------------|  \n",
    "| **Focus** | Multiple attention perspectives ğŸ§  | Only one focus ğŸ” |  \n",
    "| **Captures** | Complex dependencies ğŸ”„ | Limited relationships ğŸ“ |  \n",
    "| **Performance** | More expressive ğŸ’¡ | Less effective ğŸ˜• |  \n",
    "| **Used In** | Transformers (BERT, GPT) ğŸ¤– | Simpler RNN models ğŸ“œ |  \n",
    "\n",
    "\n",
    "## ğŸš€ **Where is Multi-Head Attention Used?**\n",
    "ğŸ”¥ **Transformers** (BERT, GPT, T5)  \n",
    "ğŸ™ï¸ **Speech Recognition** (ASR models)  \n",
    "ğŸ“œ **Machine Translation** (Google Translate)  \n",
    "ğŸ“Š **Time-Series Forecasting**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-Head Attention in Simple Layman Terms**  \n",
    "\n",
    "Imagine you are a **detective** investigating a case, and you have a **team of experts** to help you. Each expert specializes in a different **perspective** of the case.\n",
    "\n",
    "- One expert focuses on **what happened before**.  \n",
    "- Another expert looks at **what might happen next**.  \n",
    "- Another checks **who is involved**.  \n",
    "- Another looks at **the location**.  \n",
    "\n",
    "Each expert examines the **same evidence** (sentence) but from a **different angle**. After gathering their findings, they **combine their insights** to get the full picture.  \n",
    "\n",
    "This is exactly how **multi-head attention** works in Transformers!  \n",
    "\n",
    "\n",
    "\n",
    "### **Breaking It Down Step by Step**  \n",
    "\n",
    "ğŸ’¡ **Letâ€™s say we have a sentence:**  \n",
    "*\"The cat sat on the mat.\"*\n",
    "\n",
    "A normal attention mechanism (like a single expert) might focus only on the **most important** words related to \"sat,\" such as \"cat.\" But **multi-head attention** allows multiple \"experts\" to focus on different relationships **at the same time**:\n",
    "\n",
    "1. **Head 1:** Focuses on the subject (\"cat\").  \n",
    "2. **Head 2:** Focuses on location (\"on the mat\").  \n",
    "3. **Head 3:** Focuses on tense (past action: \"sat\").  \n",
    "4. **Head 4:** Focuses on article/determiner (\"The\").  \n",
    "\n",
    "Each attention \"head\" sees **different patterns in the sentence**, and then all of them **combine their insights** to form a richer understanding of the meaning.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Is Multi-Head Attention Useful?**\n",
    "âœ… **Better Understanding** â€“ Instead of one perspective, the model looks at multiple perspectives at once.  \n",
    "âœ… **Handles Long Sentences** â€“ Different heads focus on different words, making it easier to understand long sentences.  \n",
    "âœ… **Improves Translation** â€“ When translating languages, different heads focus on word order, grammar, and context.  \n",
    "\n",
    "\n",
    "\n",
    "### **Final Analogy: Reading a Book**\n",
    "Imagine you are reading a book:  \n",
    "ğŸ“– **Single-head attention** is like reading it with **one mindset** (e.g., just following the story).  \n",
    "ğŸ“š **Multi-head attention** is like reading it with **multiple perspectives** at the same time (e.g., plot, character development, foreshadowing).  \n",
    "\n",
    "That's why Transformers are so powerful! ğŸš€  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, itâ€™s possible! Letâ€™s take a simple sentence and manually calculate how **Multi-Head Attention** works step by step. I'll keep the numbers simple for easier understanding.\n",
    "\n",
    "\n",
    "\n",
    "### **Sentence:**\n",
    "ğŸ‘‰ **\"The cat sat.\"** (3 words)\n",
    "\n",
    "For simplicity, assume:\n",
    "- Each word is represented as a **3-dimensional vector**.\n",
    "- We use **2 attention heads**.\n",
    "- The dimension of each headâ€™s query/key/value is **2** (after projection).\n",
    "\n",
    "## **Step 1: Word Embeddings**\n",
    "Each word is converted into an embedding vector (simplified numbers):\n",
    "\n",
    "| Word   | Embedding (3D) |\n",
    "|--------|--------------|\n",
    "| **The** | [1, 0, 1]  |\n",
    "| **Cat** | [0, 1, 0]  |\n",
    "| **Sat** | [1, 1, 0]  |\n",
    "\n",
    "**Matrix form (X):**  \n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "## **Step 2: Compute Query, Key, and Value Matrices**\n",
    "Each input is projected into **Q, K, V** matrices using weight matrices.\n",
    "\n",
    "For **Head 1**, letâ€™s assume:\n",
    "\n",
    "$$\n",
    "W_Q^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}, \\quad\n",
    "W_K^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix}, \\quad\n",
    "W_V^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, calculate **Q, K, V**:\n",
    "\n",
    "$$\n",
    "Q^{(1)} = X W_Q^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K^{(1)} = X W_K^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "2 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "V^{(1)} = X W_V^{(1)} =\n",
    "\\begin{bmatrix} \n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "0 & 1 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Compute Attention Scores**\n",
    "We use the **Scaled Dot-Product Attention Formula**:\n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "1. Compute **QK^T**:\n",
    "\n",
    "$$\n",
    "QK^T =\n",
    "\\begin{bmatrix} \n",
    "2 & 1 \\\\ \n",
    "0 & 1 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 1 & 2 \\\\ \n",
    "2 & 0 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "4 & 2 & 5 \\\\ \n",
    "2 & 0 & 1 \\\\ \n",
    "3 & 1 & 3 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "2. Scale by \\( \\sqrt{d_k} = \\sqrt{2} \\approx 1.41 \\):\n",
    "\n",
    "$$\n",
    "\\frac{QK^T}{1.41} =\n",
    "\\begin{bmatrix} \n",
    "2.83 & 1.41 & 3.54 \\\\ \n",
    "1.41 & 0 & 0.71 \\\\ \n",
    "2.12 & 0.71 & 2.12 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "3. Apply **Softmax** row-wise:\n",
    "\n",
    "Softmax normalizes each row into probabilities:\n",
    "\n",
    "$$\n",
    "\\text{Softmax} \\left( \n",
    "\\begin{bmatrix} \n",
    "2.83 & 1.41 & 3.54 \\\\ \n",
    "1.41 & 0 & 0.71 \\\\ \n",
    "2.12 & 0.71 & 2.12 \n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "0.3 & 0.1 & 0.6 \\\\ \n",
    "0.4 & 0.2 & 0.4 \\\\ \n",
    "0.4 & 0.2 & 0.4 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Compute Weighted Sum with V**\n",
    "Now, multiply **softmax scores** with **V**:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \n",
    "\\begin{bmatrix} \n",
    "0.3 & 0.1 & 0.6 \\\\ \n",
    "0.4 & 0.2 & 0.4 \\\\ \n",
    "0.4 & 0.2 & 0.4 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "1 & 2 \\\\ \n",
    "1 & 0 \\\\ \n",
    "1 & 1 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "1 & 1.7 \\\\ \n",
    "1 & 1.2 \\\\ \n",
    "1 & 1.2 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Repeat for Other Heads & Merge**\n",
    "Each head produces a different attention output. If we had **another head**, weâ€™d repeat steps **with different W_Q, W_K, W_V**.  \n",
    "\n",
    "Finally, we **concatenate** outputs from all heads and project them using a weight matrix \\( W_O \\).\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Takeaways**\n",
    "âœ… **Multi-Head Attention** allows different attention heads to focus on **different aspects** of the input.  \n",
    "âœ… Instead of **one** attention mechanism, we compute **multiple heads in parallel** and combine them.  \n",
    "âœ… It helps the model learn **long-range dependencies efficiently**!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **Positional Encoding in Transformers: Full Explanation** ğŸŒŸ  \n",
    "\n",
    "## ğŸ”¹ **Why Do We Need Positional Encoding?**  \n",
    "\n",
    "Unlike **RNNs (LSTMs, GRUs)**, Transformers **do not** process words in a sequential order. Instead, they process the **entire input at once** using **self-attention**.  \n",
    "\n",
    "ğŸ‘‰ This creates a problem:  \n",
    "- **Self-attention is permutation-invariant** ğŸŒ€ â†’ It **doesnâ€™t know the word order**!  \n",
    "- **Example Issue:**  \n",
    "  - `\"The cat sat.\"` and `\"Sat cat the.\"` would **look the same** to the model! ğŸ˜±  \n",
    "\n",
    "### ğŸš€ **Solution: Positional Encoding!**  \n",
    "Positional Encoding **adds information about word order** by injecting **unique position values** into each word embedding. This allows Transformers to **differentiate between word positions** while keeping full parallelization.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ **How Does Positional Encoding Work?**  \n",
    "\n",
    "Each input word **embedding** is a vector (e.g., 512 dimensions in GPT, BERT).  \n",
    "ğŸ‘‰ **Positional Encoding is another vector** (same size) added to it.  \n",
    "\n",
    "Instead of learning these values like normal weights, **Transformers use a fixed formula** based on **sine & cosine functions** to encode word positions.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ **Mathematical Formula of Positional Encoding**  \n",
    "\n",
    "For a given position **$ pos $** (word index) and dimension **$ i $** (feature index), the **positional encoding** is:\n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos \\left( \\frac{pos}{10000^{\\frac{2i}{d}}} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ pos $ = position of the word in the sentence (e.g., **0 for first word, 1 for second**).\n",
    "- $ i $ = dimension index (even or odd).\n",
    "- $ d $ = total embedding size (e.g., **512** in GPT).\n",
    "- **Sin for even indices, Cosine for odd indices**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ **Why Use Sine & Cosine?**  \n",
    "\n",
    "1ï¸âƒ£ **Captures Relative Positions:**  \n",
    "   - The difference between positions remains **consistent**, which helps the model learn relationships between words.  \n",
    "\n",
    "2ï¸âƒ£ **Handles Long Sentences:**  \n",
    "   - The formula ensures unique encodings for **long sequences**, unlike simple index numbers.  \n",
    "\n",
    "3ï¸âƒ£ **Smooth Variations:**  \n",
    "   - Since sine and cosine oscillate smoothly, small position shifts cause **small changes** in embeddings â†’ Makes the model more robust!\n",
    "\n",
    "## ğŸ”¹ **Example: Calculating Positional Encoding**  \n",
    "\n",
    "Letâ€™s assume **3 words**:  \n",
    "ğŸ‘‰ `\"The\" (pos = 0)`, `\"Cat\" (pos = 1)`, `\"Sat\" (pos = 2)`  \n",
    "\n",
    "And embedding size **d = 4** (keeping it small for simplicity).\n",
    "\n",
    "#### **Step 1: Compute Positional Encoding**\n",
    "Using the formula, letâ€™s compute:\n",
    "\n",
    "| Position | PE(0) (sin) | PE(1) (cos) | PE(2) (sin) | PE(3) (cos) |\n",
    "|----------|------------|------------|------------|------------|\n",
    "| 0 (The)  | sin(0) = 0 | cos(0) = 1 | sin(0) = 0 | cos(0) = 1 |\n",
    "| 1 (Cat)  | sin(1/10000â°) â‰ˆ 1 | cos(1/10000â°) â‰ˆ 1 | sin(1/10000Â¹) â‰ˆ 0.0001 | cos(1/10000Â¹) â‰ˆ 1 |\n",
    "| 2 (Sat)  | sin(2/10000â°) â‰ˆ 2 | cos(2/10000â°) â‰ˆ 1 | sin(2/10000Â¹) â‰ˆ 0.0002 | cos(2/10000Â¹) â‰ˆ 1 |\n",
    "\n",
    "#### **Step 2: Add Positional Encoding to Word Embeddings**\n",
    "Now, we add these **positional encodings** to the word **embeddings**.\n",
    "\n",
    "| Word  | Embedding (e.g., [1.2, 0.8, 2.5, 1.5]) | + Positional Encoding | = Final Input to Transformer |\n",
    "|-------|--------------------------------|-----------------|------------------|\n",
    "| The   | [1.2, 0.8, 2.5, 1.5] | [0, 1, 0, 1] | [1.2, 1.8, 2.5, 2.5] |\n",
    "| Cat   | [0.5, 1.1, 2.0, 1.3] | [1, 1, 0.0001, 1] | [1.5, 2.1, 2.0001, 2.3] |\n",
    "| Sat   | [1.0, 0.9, 2.3, 1.7] | [2, 1, 0.0002, 1] | [3.0, 1.9, 2.3002, 2.7] |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ **Visualization of Positional Encoding**\n",
    "ğŸ¨ Hereâ€™s a heatmap of **Positional Encoding** over **50 positions** with **512 dimensions**:  \n",
    "\n",
    "![Positional Encoding Heatmap](pe.png)  \n",
    "\n",
    "- **X-axis** = position (word index).  \n",
    "- **Y-axis** = embedding dimensions.  \n",
    "- **Patterns of waves** represent the **sine & cosine variations** across positions.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¹ **Key Takeaways**\n",
    "âœ… **Positional Encoding solves the word order problem** in Transformers.  \n",
    "âœ… **Uses sine & cosine functions** to create unique position vectors.  \n",
    "âœ… **Enables long-range dependencies** and smooth transitions.  \n",
    "âœ… **Added to word embeddings** before self-attention.  \n",
    "\n",
    "\n",
    "### ğŸ† **Final Thought: Why Not Learn Positional Encoding?**\n",
    "- **Fixed Positional Encoding** (like sine/cosine) works well for **long texts** and avoids extra training parameters.  \n",
    "- Some models (like **ALBERT, T5**) use **learnable positional embeddings**, but **vanilla Transformers** use this sine/cosine approach.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Why Do We Use Layer Normalization Instead of Batch Normalization in Transformers?**  \n",
    "\n",
    "In deep learning, **normalization** helps stabilize training by ensuring that activations are well-scaled and centered. While **Batch Normalization (BN)** works well for CNNs and RNNs, **Layer Normalization (LN)** is preferred for Transformers. But why? ğŸ¤”  \n",
    "\n",
    "Letâ€™s break it down! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Reasons Why Transformers Use Layer Normalization Instead of Batch Normalization**  \n",
    "\n",
    "### 1ï¸âƒ£ **BN Depends on Mini-Batch Statistics, LN Does Not!**  \n",
    "- **Batch Normalization** normalizes inputs across the **batch dimension**, meaning it relies on the statistics (mean & variance) of a batch of examples.  \n",
    "- **Layer Normalization** normalizes across the **features of a single input (token)**, making it **independent of batch size**.  \n",
    "\n",
    "ğŸ’¡ **Why is this important?**  \n",
    "- **In Transformers, we process a single input at inference time (e.g., one sentence at a time).** If we used Batch Norm, statistics from a single sample wouldnâ€™t be stable, leading to inconsistent results.  \n",
    "- **Layer Norm works even when batch size = 1**, making it ideal for NLP tasks where input sizes vary.  \n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ **Batch Norm Doesnâ€™t Work Well with Variable Sequence Lengths**  \n",
    "- **BN computes mean & variance per batch**, but **in NLP, sentence lengths vary** (e.g., \"Hello world\" vs. \"This is a long sentence\").  \n",
    "- Padding sequences in BN can distort batch statistics, making it harder to learn meaningful representations.  \n",
    "- **LN normalizes each sequence independently**, so it avoids these issues.  \n",
    "\n",
    "ğŸ’¡ **Why is this important?**  \n",
    "In NLP, inputs are variable-length sequences, and **BN struggles with this**. LN, however, handles it smoothly!  \n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ **BN Breaks in Autoregressive Models Like GPT**  \n",
    "- In models like **GPT (causal Transformer)**, we generate tokens **one by one** during inference.  \n",
    "- **Batch Norm requires full batches to compute statistics, but in autoregressive models, we generate one token at a time!**  \n",
    "- **Layer Norm does not depend on batches, so it works perfectly in autoregressive tasks.**  \n",
    "\n",
    "ğŸ’¡ **Why is this important?**  \n",
    "BN would fail when generating text token-by-token, but LN does not!  \n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ **LN Works Better for Attention Mechanisms**  \n",
    "- Transformers **use self-attention**, where each token interacts with all others in the sequence.  \n",
    "- **Batch Norm computes batch-level statistics, which can introduce unwanted interactions** between different sentences in a batch.  \n",
    "- **Layer Norm operates at the token level**, preserving the meaning of self-attention outputs.  \n",
    "\n",
    "ğŸ’¡ **Why is this important?**  \n",
    "Since **each token should focus on relevant words**, normalizing within the token (LN) is better than normalizing across the batch (BN).  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¬ **How Does Layer Normalization Work?**  \n",
    "\n",
    "Layer Normalization normalizes **each input tokenâ€™s features** across all dimensions (instead of across the batch).  \n",
    "\n",
    "For an input vector **x** with **d** features:\n",
    "\n",
    "1ï¸âƒ£ **Compute the mean** of the features:  \n",
    "   $$\n",
    "   \\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i\n",
    "   $$\n",
    "   \n",
    "2ï¸âƒ£ **Compute the variance** of the features:  \n",
    "   $$\n",
    "   \\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2\n",
    "   $$\n",
    "\n",
    "3ï¸âƒ£ **Normalize** each feature:  \n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "   (Where **Îµ** is a small value to avoid division by zero.)\n",
    "\n",
    "4ï¸âƒ£ **Apply learnable parameters** (scale & shift):  \n",
    "   $$\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "   $$\n",
    "   - **Î³ (gamma):** Scaling factor (learned parameter).  \n",
    "   - **Î² (beta):** Bias/shift (learned parameter).  \n",
    "\n",
    "ğŸ”¹ **This ensures that each token is normalized based on its own features, independent of other samples!**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ›  **Example: Manual Calculation of Layer Norm**  \n",
    "Letâ€™s say we have a token embedding vector:  \n",
    "\n",
    "$$\n",
    "x = [3, 5, 7, 9]\n",
    "$$\n",
    "(4 feature dimensions per token)  \n",
    "\n",
    "ğŸ”¹ **Step 1: Compute mean**  \n",
    "$$\n",
    "\\mu = \\frac{3 + 5 + 7 + 9}{4} = \\frac{24}{4} = 6\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Step 2: Compute variance**  \n",
    "$$\n",
    "\\sigma^2 = \\frac{(3-6)^2 + (5-6)^2 + (7-6)^2 + (9-6)^2}{4}\n",
    "$$\n",
    "$$\n",
    "= \\frac{9 + 1 + 1 + 9}{4} = \\frac{20}{4} = 5\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Step 3: Normalize each feature**  \n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - 6}{\\sqrt{5}}\n",
    "$$\n",
    "$$\n",
    "\\hat{x} = \\left[ \\frac{3-6}{\\sqrt{5}}, \\frac{5-6}{\\sqrt{5}}, \\frac{7-6}{\\sqrt{5}}, \\frac{9-6}{\\sqrt{5}} \\right]\n",
    "$$\n",
    "$$\n",
    "\\hat{x} = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "ğŸ”¹ **Step 4: Apply learned parameters (Î³ & Î²)**  \n",
    "If **Î³ = [1, 1, 1, 1]** and **Î² = [0, 0, 0, 0]**, then:  \n",
    "$$\n",
    "y = \\gamma \\hat{x} + \\beta = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "âœ¨ **Final normalized vector:**  \n",
    "$$\n",
    "y = [-1.34, -0.45, 0.45, 1.34]\n",
    "$$\n",
    "\n",
    "ğŸš€ **Now this vector is normalized and ready for the next layer in the Transformer!**  \n",
    "\n",
    "## ğŸ¯ **Key Differences: Layer Norm vs. Batch Norm**\n",
    "| Feature              | Layer Normalization (LN) | Batch Normalization (BN) |\n",
    "|----------------------|------------------------|------------------------|\n",
    "| **Normalization Across** | Features (per token)  | Batch (all samples) |\n",
    "| **Works with Batch Size = 1?** | âœ… Yes  | âŒ No |\n",
    "| **Handles Variable Lengths?** | âœ… Yes  | âŒ No |\n",
    "| **Autoregressive Models (e.g., GPT)?** | âœ… Yes | âŒ No |\n",
    "| **Computes Mean & Variance** | Across features (per token) | Across batch (all samples) |\n",
    "| **Best For** | Transformers, NLP | CNNs, Computer Vision |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ† **Final Takeaways**\n",
    "ğŸ”¹ **Batch Norm works well in CNNs but fails in NLP due to varying sequence lengths & autoregressive decoding.**  \n",
    "ğŸ”¹ **Layer Norm normalizes each tokenâ€™s features, making it batch-independent and perfect for Transformers.**  \n",
    "ğŸ”¹ **This allows Transformers like BERT & GPT to work efficiently across different tasks without relying on batch statistics.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Think of a Classroom with Students and Their Report Cards**  \n",
    "\n",
    "#### **Batch Normalization (BN) â†’ Normalizing Across Students**  \n",
    "Imagine a teacher is grading a test for **a group of students** (a batch).  \n",
    "\n",
    "- The teacher looks at **all students' scores** in math and finds the average.  \n",
    "- If the scores are too high or too low, the teacher adjusts **all students' scores** so they are more balanced.  \n",
    "- So, each studentâ€™s final score depends on how well or badly others did in the batch.  \n",
    "\n",
    "ğŸš¨ **Why BN is not great for Transformers?**  \n",
    "- Transformers process each word/token **independently** (like grading each student separately).  \n",
    "- BN needs a whole batch of students (batch of words), which doesnâ€™t work well in this case.  \n",
    "\n",
    "#### **Layer Normalization (LN) â†’ Normalizing Within One Student**  \n",
    "Now, imagine instead of normalizing across **students**, we normalize within **one studentâ€™s report card**:  \n",
    "\n",
    "- A student has grades in **math, science, English, history** (features of a token).  \n",
    "- If one subject score is too high or low, we adjust **only that studentâ€™s scores** so all subjects are balanced.  \n",
    "- **Each student is normalized separately, without depending on other students.**  \n",
    "\n",
    "ğŸš¨ **Why LN is better for Transformers?**  \n",
    "- In Transformers, each word (token) is like a student.  \n",
    "- We normalize each wordâ€™s features **individually**, without needing a batch.  \n",
    "- Works well when processing one word/token at a time.  \n",
    "\n",
    "### **Super Simple Takeaway**  \n",
    "- **BatchNorm (BN):** Adjusts scores by looking at a whole group of students.  \n",
    "- **LayerNorm (LN):** Adjusts scores by looking at only one studentâ€™s report card.  \n",
    "- **Transformers prefer LayerNorm because each word (token) should be treated separately, not based on a batch.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Letâ€™s simplify **residual connections in Transformers** using an easy analogy.  \n",
    "\n",
    "\n",
    "\n",
    "### **Think of Sending a Message Through Friends**  \n",
    "Imagine you want to send a message to your friend, but you have to pass it through **multiple people** before it reaches them. Each person **adds some information** or **modifies** the message slightly.  \n",
    "\n",
    "But what if one person messes up? ğŸ˜¬  \n",
    "To avoid this, you also **send a copy of the original message along with the modified version** so your friend can refer to it if needed.  \n",
    "\n",
    "\n",
    "\n",
    "### **How This Relates to Transformers?**  \n",
    "- In Transformers, data (words/tokens) **pass through multiple layers** (like passing the message through multiple people).  \n",
    "- Each layer **modifies the information** using attention and transformations.  \n",
    "- But deep networks can sometimes **change the information too much** and make it harder for the model to learn.  \n",
    "\n",
    "ğŸš€ **Solution? Residual Connections!**  \n",
    "- At every step, we **add the original input back to the modified output** before passing it to the next layer.  \n",
    "- This helps keep some of the original information, preventing too much distortion.  \n",
    "\n",
    "\n",
    "\n",
    "### **Simple Formula (Donâ€™t Worry, Itâ€™s Easy!)**  \n",
    "Instead of just using:  \n",
    "$$\n",
    "\\text{output} = \\text{transformed data}\n",
    "$$  \n",
    "We use:  \n",
    "$$\n",
    "\\text{output} = \\text{original input} + \\text{transformed data}\n",
    "$$  \n",
    "Then, we normalize it (LayerNorm) before sending it to the next layer.  \n",
    "\n",
    "\n",
    "\n",
    "### **Why is Residual Connection Useful?**  \n",
    "âœ… Prevents loss of important information.  \n",
    "âœ… Helps train deep models by making sure information flows smoothly.  \n",
    "âœ… Avoids problems like vanishing gradients (where information gets lost in deep layers).  \n",
    "\n",
    "\n",
    "\n",
    "### **Super Simple Takeaway**  \n",
    "ğŸ’¡ **Residual Connection = \"Backup Copy of Message\"**  \n",
    "It ensures that even if layers modify the input, we still keep some of the original information.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Variables:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Masked Multi-Head Attention in Transformers â€“ Full Explanation ğŸš€**\n",
    "\n",
    "### **What is Masked Multi-Head Attention?**\n",
    "Masked Multi-Head Attention is a special variant of **Multi-Head Self-Attention (MHSA)** used **only in the decoder** of a Transformer. The key difference is that it **prevents \"cheating\"** by ensuring that at each decoding step, a token **cannot attend to future tokens**.  \n",
    "\n",
    "### **Why Do We Need It?**\n",
    "In the Transformer **decoder**, we generate output tokens **one by one** (auto-regressive generation).  \n",
    "- Example: If we translate **\"I love coding\"** to French, we should predict **\"J'aime\"** before seeing **\"coder\"**.\n",
    "- Without masking, the model could peek at future words, making training unrealistic.\n",
    "\n",
    "ğŸ’¡ **Masked attention ensures the model only learns from past words**, just like how humans speak!\n",
    "\n",
    "\n",
    "\n",
    "# **ğŸŒŸ Step-by-Step Breakdown of Masked Multi-Head Attention**\n",
    "Now, let's dive into **how it works** mathematically and intuitively!\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 1: Input Embeddings and Positional Encoding**\n",
    "The input sentence (in target language) is converted into **word embeddings** and **positional encoding** is added.\n",
    "\n",
    "Example sentence (English â†’ French Translation):  \n",
    "**\"I love coding\"** â†’ **\"J'aime coder\"**\n",
    "\n",
    "| Word  | Embedding (d=4) |\n",
    "|--------|---------------|\n",
    "| J'aime | [0.5, 0.1, 0.8, 0.6] |\n",
    "| coder | [0.7, 0.2, 0.4, 0.9] |\n",
    "\n",
    "Positional encoding is added:  \n",
    "$$\n",
    "X' = X + PE\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 2: Compute Queries, Keys, and Values**\n",
    "We compute the **queries (Q), keys (K), and values (V)** using learnable weight matrices.\n",
    "\n",
    "$$\n",
    "Q = X' W_Q, \\quad K = X' W_K, \\quad V = X' W_V\n",
    "$$\n",
    "\n",
    "Example matrices:\n",
    "\n",
    "$$\n",
    "W_Q = \\begin{bmatrix} 0.2 & 0.3 \\\\ 0.4 & 0.5 \\end{bmatrix}\n",
    "\\quad\n",
    "W_K = \\begin{bmatrix} 0.6 & 0.7 \\\\ 0.8 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Multiplying embeddings by **W_Q, W_K, W_V**, we get:\n",
    "\n",
    "| Word  | Q   | K   | V   |\n",
    "|--------|-----|-----|-----|\n",
    "| J'aime | [1.2, 0.8] | [1.4, 0.9] | [0.9, 1.1] |\n",
    "| coder  | [1.5, 1.0] | [1.7, 1.2] | [1.2, 1.4] |\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 3: Compute Attention Scores**\n",
    "Attention scores are computed using:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K) = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$\n",
    "\\text{Score}(J'aime, coder) = \\frac{(1.2 \\times 1.7) + (0.8 \\times 1.2)}{\\sqrt{2}} = \\frac{2.04 + 0.96}{1.41} = 2.13\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 4: Apply the Mask!**\n",
    "ğŸ’¡ **Hereâ€™s where masking comes in!**  \n",
    "\n",
    "We apply a **mask matrix** to ensure each token can only attend to itself and previous tokens.\n",
    "\n",
    "For **two words**, the mask matrix looks like:\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "0 & -\\infty \\\\\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- The **-âˆ** prevents the word **\"J'aime\"** from looking at **\"coder\"**.\n",
    "\n",
    "**Modified scores after masking**:\n",
    "\n",
    "$$\n",
    "S' =\n",
    "\\begin{bmatrix}\n",
    "\\text{Score}(J'aime, J'aime) & -\\infty \\\\\n",
    "\\text{Score}(coder, J'aime) & \\text{Score}(coder, coder)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Applying **softmax**, the masked token gets probability **0**.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 5: Compute Final Attention Output**\n",
    "We multiply the **attention scores** by **V** to get final attention output.\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{Softmax}(S') V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 6: Multi-Head Attention**\n",
    "Instead of using **one** attention head, **multiple heads** process the input in parallel, capturing different aspects of meaning.\n",
    "\n",
    "Example:\n",
    "- **Head 1** focuses on **word order**.\n",
    "- **Head 2** focuses on **semantic similarity**.\n",
    "\n",
    "**Final output is a concatenation** of all attention heads.\n",
    "\n",
    "\n",
    "\n",
    "## **ğŸ”¹ Step 7: Add & Normalize**\n",
    "$$\n",
    "X'' = \\text{LayerNorm}(X' + \\text{Masked Multi-Head Attention Output})\n",
    "$$\n",
    "\n",
    "\n",
    "# **ğŸ”¥ Summary**\n",
    "âœ… **Prevents future tokens from being seen**  \n",
    "âœ… **Allows auto-regressive generation**  \n",
    "âœ… **Multiple heads capture rich context**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Masked Multi-Head Attention in the Transformer Decoder (Layman Explanation)**\n",
    "  \n",
    "Think of **masked multi-head attention** as a **student taking an exam** but **only allowed to see previous questions**, not future ones.  \n",
    "\n",
    "This is essential in the **Transformer decoder** when generating text, ensuring that words are predicted **one by one in order** without looking ahead.\n",
    "\n",
    "\n",
    "\n",
    "## **1ï¸âƒ£ Why is Masking Needed?**\n",
    "In tasks like **text generation (e.g., machine translation)**, the decoder generates words **step by step**.  \n",
    "For example, if translating:\n",
    "  \n",
    "**English** â†’ *\"I love apples.\"*  \n",
    "**French** â†’ *\"J'aime les pommes.\"*  \n",
    "\n",
    "At each step, the model should **only use past words**, not future ones.  \n",
    "Without masking, the decoder might **cheat** by looking at words it hasnâ€™t generated yet.\n",
    "\n",
    "\n",
    "\n",
    "## **2ï¸âƒ£ How Does Masked Multi-Head Attention Work?**\n",
    "The Transformer decoder has **two types of attention**:  \n",
    "1. **Masked Multi-Head Self-Attention** (prevents cheating ğŸ”’).  \n",
    "2. **Multi-Head Encoder-Decoder Attention** (helps understand input context ğŸ“–).\n",
    "\n",
    "ğŸ”¹ **In Masked Multi-Head Attention:**  \n",
    "âœ… Itâ€™s the **same as multi-head attention**, but with **a mask** applied.  \n",
    "âœ… The mask **blocks future words** by setting their scores to **-âˆ (negative infinity)**.  \n",
    "âœ… This ensures each word **only attends to previous words**.\n",
    "\n",
    "\n",
    "\n",
    "## **3ï¸âƒ£ How Masking Works in Practice**\n",
    "Consider generating:  \n",
    "*\"The cat sat on the mat.\"*  \n",
    "\n",
    "At each step, the model should **only see past words**:\n",
    "```\n",
    "Step 1: \"The\"      â†’ Can see: [\"The\"]\n",
    "Step 2: \"The cat\"  â†’ Can see: [\"The\", \"cat\"]\n",
    "Step 3: \"The cat sat\" â†’ Can see: [\"The\", \"cat\", \"sat\"]\n",
    "```\n",
    "The **future words** (\"on the mat\") are **masked** so the model doesnâ€™t peek ahead.\n",
    "\n",
    "\n",
    "\n",
    "## **4ï¸âƒ£ Simple Analogy: Watching a TV Series ğŸ“º**\n",
    "Imagine you are watching a suspense TV show **episode by episode**.  \n",
    "- **Without masking:** You **accidentally read spoilers** for the next episode.  \n",
    "- **With masking:** You **only see the current and past episodes**, so you donâ€™t spoil the surprise.  \n",
    "\n",
    "Masked attention ensures the model **doesnâ€™t spoil its own prediction** when generating text.\n",
    "\n",
    "\n",
    "\n",
    "## **5ï¸âƒ£ How Itâ€™s Implemented in Transformers**\n",
    "### **Formula for Masked Attention**:\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask} \\right) V\n",
    "$$\n",
    "Where:\n",
    "- **$ QK^T $** finds word relationships.\n",
    "- **$ \\text{mask} $** sets future words to **-âˆ**, making them ignored.\n",
    "- **Softmax ensures the model only attends to allowed words.**\n",
    "\n",
    "\n",
    "\n",
    "## **6ï¸âƒ£ Summary**\n",
    "âœ… **Multi-Head Attention** = Looks at all words freely.  \n",
    "âœ… **Masked Multi-Head Attention** = Looks **only at past words** (prevents cheating).  \n",
    "âœ… Used in **Transformer decoders** (e.g., GPT models) for **auto-regressive text generation**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing a full **manual calculation** of **multi-head attention** on a real sentence is **possible** but requires many steps, involving matrix multiplications, softmax, and weighted sums. I'll **simplify** it while keeping all essential calculations.\n",
    "\n",
    "\n",
    "\n",
    "# **Manual Multi-Head Attention Calculation on a Sentence**\n",
    "Let's take a **simple sentence**:  \n",
    "\n",
    "**\"I love AI\"**  \n",
    "\n",
    "We will calculate **multi-head self-attention** step-by-step with two heads.\n",
    "\n",
    "## **Step 1: Convert Words to Embeddings**\n",
    "Each word is represented as a vector (randomly chosen for simplicity).\n",
    "\n",
    "| Word   | Embedding (d=4) |\n",
    "|--------|----------------|\n",
    "| I      | [0.2, 0.3, 0.4, 0.5] |\n",
    "| love   | [0.7, 0.1, 0.8, 0.6] |\n",
    "| AI     | [0.5, 0.9, 0.3, 0.7] |\n",
    "\n",
    "**We use d_model = 4 (dimension of embeddings) and two attention heads.**\n",
    "\n",
    "\n",
    "## **Step 2: Compute Queries, Keys, and Values**\n",
    "Each head has different **weight matrices** for Query (Q), Key (K), and Value (V).  \n",
    "Letâ€™s define two sets of weight matrices for **Head 1** and **Head 2**.\n",
    "\n",
    "### **Head 1:**\n",
    "$$\n",
    "W_Q^1 = \\begin{bmatrix} 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\\\ 0.9 & 0.1 & 0.2 & 0.3 \\\\ 0.4 & 0.5 & 0.6 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_K^1 = \\begin{bmatrix} 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 \\\\ 0.1 & 0.2 & 0.3 & 0.4 \\\\ 0.5 & 0.6 & 0.7 & 0.8 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "W_V^1 = \\begin{bmatrix} 0.3 & 0.4 & 0.5 & 0.6 \\\\ 0.7 & 0.8 & 0.9 & 0.1 \\\\ 0.2 & 0.3 & 0.4 & 0.5 \\\\ 0.6 & 0.7 & 0.8 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### **Compute Queries, Keys, and Values for Head 1**\n",
    "Using:\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "For word **\"I\"**:\n",
    "\n",
    "$$\n",
    "Q_{I} = [0.2, 0.3, 0.4, 0.5] \\times W_Q^1\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q_{I} = [ (0.2Ã—0.1 + 0.3Ã—0.5 + 0.4Ã—0.9 + 0.5Ã—0.4), (0.2Ã—0.2 + 0.3Ã—0.6 + 0.4Ã—0.1 + 0.5Ã—0.5), ...]\n",
    "$$\n",
    "\n",
    "Similarly, compute for **K and V**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Compute Attention Scores**\n",
    "Using the formula:\n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "(Here, \\( d_k = 2 \\) because we split embeddings for two heads)\n",
    "\n",
    "1. Compute **QK^T** (dot product of Queries and Keys).\n",
    "2. Apply **scaling** (\\( \\sqrt{d_k} \\)).\n",
    "3. Apply **softmax**.\n",
    "4. Multiply by **V**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Compute for Head 2**\n",
    "Repeat Steps 2 and 3 using different **W_Q^2, W_K^2, W_V^2**.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Concatenate and Apply Final Linear Transformation**\n",
    "Concatenate the two headsâ€™ outputs and apply a final transformation.\n",
    "\n",
    "$$\n",
    "\\text{Output} = [\\text{Head}_1, \\text{Head}_2] W_O\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "# **Final Thoughts**\n",
    "âœ… We performed step-by-step calculations for **multi-head self-attention**.  \n",
    "âœ… This shows how Transformers learn **context** across multiple perspectives! ğŸš€\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cross-Attention in Transformers â€“ Full Explanation** ğŸ¯  \n",
    "\n",
    "Cross-attention is a crucial mechanism in **transformers**, especially in models like **encoder-decoder architectures (e.g., T5, BART, and Transformer-based Machine Translation)**. It enables the **decoder to focus on relevant encoder outputs** while generating each token of the output.\n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ“Œ Why Do We Need Cross-Attention?**\n",
    "1. **Bridging Encoder and Decoder** ğŸ”—  \n",
    "   - The encoder processes the **input sequence** and generates **contextual representations**.\n",
    "   - The decoder **does not directly access the input** but must **attend** to the encoder's output to generate relevant output tokens.\n",
    "\n",
    "2. **Handling Contextual Dependencies** ğŸ§   \n",
    "   - Some output tokens depend on long-distance dependencies from the input.  \n",
    "   - Cross-attention ensures that the decoder has **direct access** to all encoder outputs.\n",
    "\n",
    "3. **Improving Translation & Summarization** ğŸ“  \n",
    "   - In **machine translation**, the decoder must generate words in the target language while referring to the encoder outputs.  \n",
    "   - In **text summarization**, the decoder selects important parts of the input text.\n",
    "\n",
    "\n",
    "\n",
    "# **âš™ï¸ How Does Cross-Attention Work?**\n",
    "Cross-attention follows the same **scaled dot-product attention** mechanism as self-attention but with a key difference:\n",
    "\n",
    "- **In self-attention**, the queries (Q), keys (K), and values (V) come from the same input sequence.\n",
    "- **In cross-attention**, the queries (Q) come from the **decoder**, while the keys (K) and values (V) come from the **encoder outputs**.\n",
    "\n",
    "### **Formula for Attention Scores**\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- **Q (Query)** comes from the decoder's previous hidden state.  \n",
    "- **K (Key) and V (Value)** come from the encoder's final hidden states.  \n",
    "- **$ d_k $** is the key dimension, used for scaling.\n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ”¬ Step-by-Step Process of Cross-Attention**\n",
    "Letâ€™s break it down:\n",
    "\n",
    "### **1ï¸âƒ£ Encoder Produces Contextual Representations**\n",
    "- The encoder processes the input sequence and produces a set of output embeddings.\n",
    "- Example:  \n",
    "  Suppose we have the input:  \n",
    "  **\"The cat sat on the mat.\"**  \n",
    "  The encoder generates **hidden states** for each word.\n",
    "\n",
    "  ```\n",
    "  Encoder Outputs:\n",
    "  [E1, E2, E3, E4, E5, E6]\n",
    "  ```\n",
    "\n",
    "### **2ï¸âƒ£ Decoder Generates Queries**\n",
    "- The decoder is generating output words **one at a time**.\n",
    "- At each step, it takes the previously generated words and computes a **query (Q)**.\n",
    "\n",
    "  ```\n",
    "  Query (Q) from decoder hidden state:\n",
    "  Q = Decoder_hidden_state_t\n",
    "  ```\n",
    "\n",
    "### **3ï¸âƒ£ Compute Attention Scores**\n",
    "- Compute **dot product** between Query (Q) and all encoder Key (K) vectors.\n",
    "- Apply **softmax** to get attention scores.\n",
    "\n",
    "### **4ï¸âƒ£ Weighted Sum of Encoder Outputs**\n",
    "- Multiply attention scores with encoder **Value (V)** vectors.\n",
    "- This forms the **context vector**, which contains the most relevant information for generating the next token.\n",
    "\n",
    "### **5ï¸âƒ£ Decoder Uses Context Vector to Generate Next Token**\n",
    "- The decoder uses this weighted context vector to decide the next word in the output sequence.\n",
    "\n",
    "\n",
    "\n",
    "# **ğŸ¤– Example: Machine Translation Using Cross-Attention**\n",
    "Imagine we are translating:  \n",
    "ğŸ“ **Input (English):** \"I love AI\"  \n",
    "ğŸŒ **Output (French):** \"J'aime l'IA\"  \n",
    "\n",
    "### **Encoder Process** (Self-Attention on Input)  \n",
    "```\n",
    "Input:  [\"I\", \"love\", \"AI\"]\n",
    "Embeddings â†’ Self-Attention â†’ Encoder Hidden States\n",
    "```\n",
    "\n",
    "The encoder outputs:  \n",
    "```\n",
    "[E1, E2, E3] (hidden representations for \"I\", \"love\", \"AI\")\n",
    "```\n",
    "\n",
    "### **Decoder Process (with Cross-Attention)**\n",
    "- **Step 1**: Decoder generates **Q (query) for \"J'\"**  \n",
    "  ```\n",
    "  Q1 = Decoder_hidden_state_1\n",
    "  ```\n",
    "  - Compute attention scores with encoder outputs `[E1, E2, E3]`.\n",
    "  - Get **context vector** and generate \"J'\".\n",
    "\n",
    "- **Step 2**: Decoder generates **Q (query) for \"aime\"**  \n",
    "  ```\n",
    "  Q2 = Decoder_hidden_state_2\n",
    "  ```\n",
    "  - Compute new attention scores with encoder outputs `[E1, E2, E3]`.\n",
    "  - Get **context vector** and generate \"aime\".\n",
    "\n",
    "- **Step 3**: Decoder generates **Q (query) for \"l'IA\"**  \n",
    "  ```\n",
    "  Q3 = Decoder_hidden_state_3\n",
    "  ```\n",
    "  - Compute attention scores again.\n",
    "  - Get **context vector** and generate \"l'IA\".\n",
    "\n",
    "Final Output:  \n",
    "âœ… **\"J'aime l'IA\"** ğŸ‰\n",
    "\n",
    "# **ğŸ†š Self-Attention vs. Cross-Attention**\n",
    "| Feature        | Self-Attention | Cross-Attention |\n",
    "|---------------|---------------|----------------|\n",
    "| **Where?**    | Encoder & Decoder | Decoder only |\n",
    "| **Query (Q)?** | From same sequence | From decoder hidden states |\n",
    "| **Key (K), Value (V)?** | From same sequence | From encoder outputs |\n",
    "| **Purpose?**  | Relate words within same sequence | Connect encoder & decoder |\n",
    "\n",
    "\n",
    "# **ğŸš€ Key Takeaways**\n",
    "âœ” **Cross-attention is essential** for sequence-to-sequence tasks like machine translation.  \n",
    "âœ” The **decoder uses cross-attention** to focus on relevant parts of the encoder's output.  \n",
    "âœ” It enables **better alignment** between input and output sequences.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cross-Attention in Simple Layman Terms**  \n",
    "\n",
    "Think of **cross-attention** like a **translator** who listens to one language (input) and speaks in another (output).  \n",
    "\n",
    "Letâ€™s say you have an **English teacher** and a **French student**:  \n",
    "- The **teacher (encoder)** speaks in **English**.  \n",
    "- The **student (decoder)** listens and translates into **French**.  \n",
    "- The student must **pay attention** to the right words from the teacher **before speaking**.  \n",
    "\n",
    "ğŸ’¡ **Cross-attention is how the student listens to the teacher!**  \n",
    "\n",
    "\n",
    "\n",
    "### **How It Works in Transformers**\n",
    "A Transformer has **two main parts**:  \n",
    "1. **Encoder** â†’ Reads and understands the input sentence.  \n",
    "2. **Decoder** â†’ Generates the output sentence, **paying attention to the encoderâ€™s words** using **cross-attention**.  \n",
    "\n",
    "ğŸ”¹ In **self-attention**, the decoder looks at **its own words**.  \n",
    "ğŸ”¹ In **cross-attention**, the decoder looks at **the encoderâ€™s words** before deciding what to say next.  \n",
    "\n",
    "\n",
    "\n",
    "### **Example: English to French Translation**\n",
    "Imagine the Transformer translating:  \n",
    "**\"I love apples\"** â†’ **\"J'aime les pommes\"**  \n",
    "\n",
    "ğŸ”¹ The **encoder** processes **\"I love apples\"** and stores its meaning.  \n",
    "ğŸ”¹ The **decoder** starts generating French words, but before picking the next word, it **looks at the most relevant parts of the English sentence**.  \n",
    "\n",
    "#### **Step-by-Step Process:**\n",
    "1ï¸âƒ£ The decoder starts with **\"J'\"**.  \n",
    "2ï¸âƒ£ It **attends to** (\"I love apples\") and decides the next word **\"aime\"**.  \n",
    "3ï¸âƒ£ It again checks (\"I love apples\") and picks **\"les\"**.  \n",
    "4ï¸âƒ£ Finally, it attends again and picks **\"pommes\"**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Analogy: Ordering Food at a Restaurant ğŸ”**  \n",
    "Imagine you're at a restaurant and **donâ€™t know what to order**.  \n",
    "- You look at the **menu (encoder)**, which has all options.  \n",
    "- You **cross-check** it with what you want.  \n",
    "- You then tell the waiter your choice (decoder).  \n",
    "\n",
    "The **menu = encoder**, and **your choice depends on looking at the menu first = cross-attention**!  \n",
    "\n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "âœ… **Self-attention** = Looking at your own notes to write a story.  \n",
    "âœ… **Cross-attention** = Looking at a book (encoder) to answer questions.  \n",
    "âœ… **Used in decoders** (like language translation & AI chatbots).  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
