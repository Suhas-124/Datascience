{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ **Understanding Transformer Decoder Architecture in Depth**  \n",
    "\n",
    "The **decoder** in a Transformer is responsible for **generating text step by step**, using the encoded input information. It is widely used in **machine translation, text generation, and other NLP tasks**.\n",
    "\n",
    "Letâ€™s break it down step by step and understand **how it works** in detail.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ— **Transformer Decoder Architecture Overview**  \n",
    "\n",
    "A **Transformer decoder** consists of multiple **decoder layers** (e.g., 6 in the original paper). Each layer has three main sub-components:  \n",
    "\n",
    "### ğŸ”¹ **1. Masked Multi-Head Self-Attention**  \n",
    "â¡ The decoder **attends to itself**, looking at previously generated tokens while ensuring it **doesnâ€™t peek ahead** (future tokens are masked).  \n",
    "\n",
    "### ğŸ”¹ **2. Cross-Attention (Encoder-Decoder Attention)**  \n",
    "â¡ The decoder **attends to the encoderâ€™s output**, focusing on the most relevant parts of the input sentence.  \n",
    "\n",
    "### ğŸ”¹ **3. Feed-Forward Network (FFN)**  \n",
    "â¡ A fully connected layer applied independently to each position to transform features.  \n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“Œ **Detailed Step-by-Step Flow**  \n",
    "\n",
    "Imagine we are **translating an English sentence to French**:\n",
    "\n",
    "ğŸ’¬ **Input (English):** `\"The cat sat on the mat.\"`  \n",
    "ğŸ“ **Output (French, step by step):** `\"Le chat est assis sur le tapis.\"`\n",
    "\n",
    "At each step, the decoder generates one word at a time while looking at the encoder's output.\n",
    "\n",
    "### ğŸ”¥ **Step 1: Token Embeddings & Positional Encoding**\n",
    "- The decoder **starts with an empty sequence**.\n",
    "- Each generated word (token) is converted into a vector using an **embedding layer**.\n",
    "- **Positional encoding** is added to retain **word order** information.\n",
    "\n",
    "ğŸ‘‰ Example:  \n",
    "```\n",
    "Step 1: [\"Le\"]\n",
    "Step 2: [\"Le\", \"chat\"]\n",
    "Step 3: [\"Le\", \"chat\", \"est\"]\n",
    "...\n",
    "```\n",
    "Each token is processed **one at a time**.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Step 2: Masked Multi-Head Self-Attention ğŸ›‘**  \n",
    "The decoder applies **self-attention**, but it must ensure **no future words are visible** (to prevent cheating!).  \n",
    "\n",
    "âœ… **Why is it masked?**  \n",
    "- If we are at **Step 2** generating `\"chat\"`, we should **not see** `\"est\", \"assis\", \"sur\", \"le tapis\"`.  \n",
    "- This prevents the model from accessing future tokens, ensuring **auto-regressive decoding**.  \n",
    "\n",
    "ğŸš€ **Self-Attention Formula:**  \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} + \\text{mask} \\right) V\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Step 3: Cross-Attention (Encoder-Decoder Attention)**  \n",
    "Now, the decoder needs to understand the **input sentence** to generate the correct translation.  \n",
    "\n",
    "âœ… **How does it work?**  \n",
    "- The decoder **attends to the encoder outputs**.\n",
    "- Each decoder token decides **which input words are most relevant**.  \n",
    "- This ensures **the correct meaning is captured**.\n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "For **\"chat\"**, the model attends strongly to **\"cat\"** in the encoderâ€™s output.  \n",
    "\n",
    "ğŸš€ **Cross-Attention Formula:**  \n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right) V\n",
    "$$\n",
    "where:  \n",
    "- **Query (Q)** comes from the decoder.  \n",
    "- **Key (K) and Value (V)** come from the encoder.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Step 4: Feed-Forward Network (FFN)**\n",
    "Each position is passed through a **fully connected network** to further process the information.  \n",
    "\n",
    "FFN is applied **independently to each position**:\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "ğŸš€ **Why is this needed?**  \n",
    "- Adds **non-linearity**, helping the model capture complex patterns.\n",
    "- Allows transformation of feature space for **better predictions**.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Step 5: Layer Normalization & Residual Connections**\n",
    "To **stabilize training**, we add:  \n",
    "âœ… **Residual connections** (skip connections) to allow information flow.  \n",
    "âœ… **Layer normalization** to normalize activations for faster convergence.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”¥ **Step 6: Softmax & Word Prediction**\n",
    "After passing through **multiple decoder layers**, the final output is a probability distribution over the vocabulary.\n",
    "\n",
    "$$\n",
    "\\text{P(word)} = \\text{softmax}(W_{\\text{out}} h_{\\text{final}})\n",
    "$$\n",
    "\n",
    "ğŸ‘‰ The highest probability word is chosen as the next word in the sequence.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Putting It All Together**\n",
    "At each decoding step:\n",
    "1ï¸âƒ£ **Masked Self-Attention** â†’ The decoder attends to past words only.  \n",
    "2ï¸âƒ£ **Cross-Attention** â†’ The decoder attends to the encoderâ€™s input.  \n",
    "3ï¸âƒ£ **FFN & Layer Norm** â†’ Helps learn patterns.  \n",
    "4ï¸âƒ£ **Softmax & Word Selection** â†’ Predicts the next word.  \n",
    "5ï¸âƒ£ **Repeat until END token is generated.**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Key Takeaways**\n",
    "âœ… The **decoder generates words step by step**, ensuring proper sentence structure.  \n",
    "âœ… **Masked self-attention prevents cheating** by hiding future words.  \n",
    "âœ… **Cross-attention helps align input and output sentences**.  \n",
    "âœ… **Layer normalization + residual connections stabilize training**.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually calculating how the **Transformer Decoder** processes a sentence is quite detailed, but Iâ€™ll break it down step by step with full calculations.  \n",
    "\n",
    "Weâ€™ll take a simple sentence:  \n",
    "\n",
    "**Sentence:** `\"I love AI\"`  \n",
    "\n",
    "### **Transformer Decoder Architecture Overview**  \n",
    "The Transformer Decoder consists of the following main components:  \n",
    "1. **Tokenization & Embedding** â€“ Convert words into numerical representations.  \n",
    "2. **Positional Encoding** â€“ Encode word positions into vectors.  \n",
    "3. **Masked Multi-Head Self-Attention** â€“ Prevent the decoder from seeing future words.  \n",
    "4. **Cross-Attention (Encoder-Decoder Attention)** â€“ Focus on relevant encoder outputs.  \n",
    "5. **Feedforward Neural Network** â€“ Enhance feature representations.  \n",
    "6. **Layer Normalization & Residual Connections** â€“ Stabilize and optimize learning.  \n",
    "7. **Final Softmax Layer** â€“ Generate probabilities for the next token.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 1: Tokenization & Embedding**  \n",
    "Each word is first converted into a token using a vocabulary mapping. Let's assume:  \n",
    "\n",
    "| Word  | Token ID |\n",
    "|--------|----------|\n",
    "| I      | 1        |\n",
    "| love   | 2        |\n",
    "| AI     | 3        |\n",
    "\n",
    "Using an embedding matrix (random values for illustration), letâ€™s assume a **3D embedding (d_model = 3) for simplicity**:  \n",
    "\n",
    "$$\n",
    "E = \\begin{bmatrix}  \n",
    "0.1 & 0.2 & 0.3 \\\\  \n",
    "0.4 & 0.5 & 0.6 \\\\  \n",
    "0.7 & 0.8 & 0.9  \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each token maps to an embedding row:  \n",
    "- `\"I\"` â†’ [0.1, 0.2, 0.3]  \n",
    "- `\"love\"` â†’ [0.4, 0.5, 0.6]  \n",
    "- `\"AI\"` â†’ [0.7, 0.8, 0.9]  \n",
    "\n",
    "### **Step 2: Positional Encoding**  \n",
    "Since Transformers donâ€™t have recurrence, we need to add position information using:  \n",
    "\n",
    "$$\n",
    "PE(pos, 2i) = \\sin(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "$$\n",
    "PE(pos, 2i+1) = \\cos(pos / 10000^{2i/d_{model}})\n",
    "$$\n",
    "\n",
    "For simplicity, letâ€™s assume **d_model = 3** and compute for each position manually:  \n",
    "\n",
    "#### **Position 0 (\"I\")**  \n",
    "$$\n",
    "PE_0 = [\\sin(0), \\cos(0), \\sin(0)] = [0, 1, 0]\n",
    "$$\n",
    "\n",
    "#### **Position 1 (\"love\")**  \n",
    "$$\n",
    "PE_1 = [\\sin(1/10000^{0}), \\cos(1/10000^{0}), \\sin(1/10000^{1/3})] \n",
    "$$\n",
    "$$\n",
    "PE_1 â‰ˆ [0.0001, 0.9999, 0.001]\n",
    "$$\n",
    "\n",
    "#### **Position 2 (\"AI\")**  \n",
    "$$\n",
    "PE_2 = [\\sin(2/10000^{0}), \\cos(2/10000^{0}), \\sin(2/10000^{1/3})]  \n",
    "$$\n",
    "$$\n",
    "PE_2 â‰ˆ [0.0002, 0.9998, 0.002]\n",
    "$$\n",
    "\n",
    "### **Step 3: Add Positional Encoding**  \n",
    "Now, we add PE to embeddings:  \n",
    "\n",
    "| Word  | Embedding | Positional Encoding | Sum |\n",
    "|--------|-----------|----------------------|-----|\n",
    "| `\"I\"` | [0.1, 0.2, 0.3] | [0, 1, 0] | [0.1, 1.2, 0.3] |\n",
    "| `\"love\"` | [0.4, 0.5, 0.6] | [0.0001, 0.9999, 0.001] | [0.4001, 1.4999, 0.601] |\n",
    "| `\"AI\"` | [0.7, 0.8, 0.9] | [0.0002, 0.9998, 0.002] | [0.7002, 1.7998, 0.902] |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Masked Multi-Head Self-Attention**  \n",
    "### **4.1 Compute Query (Q), Key (K), and Value (V) Matrices**  \n",
    "Assume weight matrices for Q, K, V:  \n",
    "\n",
    "$$\n",
    "W_Q = \\begin{bmatrix} 0.2 & 0.3 & 0.5 \\\\ 0.1 & 0.6 & 0.8 \\\\ 0.7 & 0.2 & 0.4 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_K = \\begin{bmatrix} 0.3 & 0.5 & 0.2 \\\\ 0.6 & 0.1 & 0.4 \\\\ 0.8 & 0.3 & 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_V = \\begin{bmatrix} 0.5 & 0.2 & 0.6 \\\\ 0.3 & 0.8 & 0.1 \\\\ 0.7 & 0.4 & 0.9 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute Q, K, V for **\"I\"** (first token):  \n",
    "\n",
    "$$\n",
    "Q = X W_Q = \\begin{bmatrix} 0.1 & 1.2 & 0.3 \\end{bmatrix} \\times W_Q\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [ (0.1*0.2 + 1.2*0.1 + 0.3*0.7), (0.1*0.3 + 1.2*0.6 + 0.3*0.2), (0.1*0.5 + 1.2*0.8 + 0.3*0.4)]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.02 + 0.12 + 0.21, 0.03 + 0.72 + 0.06, 0.05 + 0.96 + 0.12]\n",
    "$$\n",
    "\n",
    "$$\n",
    "= [0.35, 0.81, 1.13]\n",
    "$$\n",
    "\n",
    "Similarly, compute K and V.  \n",
    "\n",
    "### **4.2 Compute Attention Scores**  \n",
    "\n",
    "$$\n",
    "\\text{Attention} = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "Then apply **masking** (to prevent seeing future words) and **softmax** to get attention scores.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Cross-Attention (Encoder-Decoder Attention)**  \n",
    "- The decoder uses the **encoderâ€™s outputs** as Key (K) and Value (V).  \n",
    "- The decoderâ€™s own Query (Q) attends to the encoderâ€™s outputs.  \n",
    "- Follow the same attention formula.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 6: Feedforward Network (FFN)**  \n",
    "Each tokenâ€™s output is passed through:  \n",
    "\n",
    "$$\n",
    "FFN(x) = \\text{ReLU}(x W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "Assume:  \n",
    "\n",
    "$$\n",
    "W_1 = \\begin{bmatrix} 0.2 & 0.4 \\\\ 0.6 & 0.8 \\\\ 0.3 & 0.9 \\end{bmatrix}, \\quad b_1 = [0.1, 0.1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2 = \\begin{bmatrix} 0.5 & 0.7 \\\\ 0.2 & 0.6 \\end{bmatrix}, \\quad b_2 = [0.05, 0.05]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Step 7: Final Softmax Layer**  \n",
    "Finally, the output is passed through **softmax** to predict the next word.  \n",
    "\n",
    "\n",
    "\n",
    "### **Final Summary**  \n",
    "1. **Tokenization & Embedding**  \n",
    "2. **Positional Encoding**  \n",
    "3. **Masked Self-Attention**  \n",
    "4. **Cross-Attention (Encoder-Decoder Attention)**  \n",
    "5. **Feedforward Network**  \n",
    "6. **Final Softmax**  \n",
    "\n",
    "This gives probabilities for the next token prediction! ğŸ¯\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transformer Inference in Simple Layman Terms**  \n",
    "\n",
    "Think of a **transformer model** like a **smart storyteller** ğŸ¤–ğŸ“–. It has already learned a **huge book of patterns** during training, and now, during **inference**, it simply **predicts the next word** based on what youâ€™ve given it.  \n",
    "\n",
    "Letâ€™s break it down step by step using an analogy!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ­ Step 1: You Give an Input (Like Asking a Friend a Question)**\n",
    "Imagine you have a friend who is really good at guessing what comes next in a conversation. You say:  \n",
    "\n",
    "> **\"I love\"**  \n",
    "\n",
    "Now, your friend **thinks carefully** about what word might come next.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“– Step 2: Tokenization â€“ Breaking Words into Small Pieces**\n",
    "Before our transformer can understand the text, it **breaks it down into numbers** (because computers love numbers, not words!).  \n",
    "\n",
    "For example:  \n",
    "- **\"I\" â†’ Token 1**  \n",
    "- **\"love\" â†’ Token 2**  \n",
    "- **\"AI\" â†’ Token 3**  \n",
    "\n",
    "So, **\"I love AI\"** becomes **[1, 2, 3]** in a format the transformer understands.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“Œ Step 3: Positional Encoding â€“ Remembering Word Order**\n",
    "Unlike humans, computers donâ€™t naturally **remember order** (they see words as a bag of numbers). So, we add **positional encoding** to **tell the transformer where each word is in the sentence**.  \n",
    "\n",
    "Think of it like numbering words in a notebook:  \n",
    "- **\"I\" (1st word) â†’ Position 1**  \n",
    "- **\"love\" (2nd word) â†’ Position 2**  \n",
    "- **\"AI\" (3rd word) â†’ Position 3**  \n",
    "\n",
    "Now, the transformer knows both the **meaning of words** and **where they are** in the sentence!  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¤” Step 4: Understanding the Input (Encoder)**\n",
    "The **encoder** takes the input words and **figures out their relationships**. It does this using **self-attention**, which means:  \n",
    "\n",
    "ğŸ’¡ **Each word \"looks at\" every other word** in the sentence and decides which ones are important.  \n",
    "\n",
    "For example, in **\"I love AI\"**, the transformer might realize:  \n",
    "- \"I\" is not very important.  \n",
    "- \"love\" is strongly connected to \"AI\".  \n",
    "\n",
    "It creates a **mathematical score** for each wordâ€™s importance and stores this information.  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ“ Step 5: Decoding â€“ Predicting the Next Word**\n",
    "Now, letâ€™s say we want the transformer to complete the sentence **\"I love\" â†’ ???**.  \n",
    "\n",
    "ğŸ’¡ **The decoder now guesses the next word** using the information from the encoder.  \n",
    "\n",
    "ğŸš€ It starts with:  \n",
    "- **\"I love\"** â†’ **Looks at all the words it knows.**  \n",
    "- Checks past patterns it has learned.  \n",
    "- It predicts: **\"AI\"** (or another relevant word like \"coding\" or \"music\").  \n",
    "\n",
    "### **ğŸ¯ Step 6: Softmax â€“ Picking the Best Word**\n",
    "The decoder doesnâ€™t pick the next word randomly. Instead, it assigns a **probability score** to each possible word:  \n",
    "\n",
    "| Possible Next Word | Score (%) |\n",
    "|-------------------|----------|\n",
    "| AI               | 80%      |\n",
    "| coding           | 15%      |\n",
    "| music           | 5%       |\n",
    "\n",
    "Since **\"AI\" has the highest score (80%)**, the model selects it. ğŸ‰  \n",
    "\n",
    "\n",
    "### **ğŸ” Step 7: Repeating Until the Sentence is Complete**\n",
    "The decoder keeps generating one word at a time until it sees an **end-of-sentence token (`<EOS>`)**.  \n",
    "\n",
    "For example:  \n",
    "- \"I love\" â†’ **AI** (from decoder)  \n",
    "- \"I love AI\" â†’ **<EOS>** (End of sentence)  \n",
    "\n",
    "Final Output:  \n",
    "> **\"I love AI\"** âœ…  \n",
    "\n",
    "\n",
    "\n",
    "### **ğŸ¤– Summary (Think of Transformer as a Smart Storyteller)**\n",
    "1. **You give it words** â†’ \"I love\"  \n",
    "2. **It breaks them into numbers** â†’ [1, 2]  \n",
    "3. **It remembers word order** â†’ [1st, 2nd word]  \n",
    "4. **It understands the meaning** â†’ \"Love is related to AI\"  \n",
    "5. **It predicts the next word** â†’ \"AI\"  \n",
    "6. **It picks the best word based on probability**  \n",
    "7. **It stops when the sentence is complete**  \n",
    "\n",
    "Thatâ€™s how **transformer inference works!** ğŸ‰ğŸš€  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
