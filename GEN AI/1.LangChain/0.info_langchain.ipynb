{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **LangChain: The Ultimate Framework for LLM Applications!** ğŸš€  \n",
    "\n",
    "### **ğŸ”¹ What is LangChain?**  \n",
    "LangChain is a powerful **open-source framework** designed to help developers **build applications using Large Language Models (LLMs)** with ease! Whether you're working on **chatbots, AI agents, document processing, or search engines**, LangChain provides all the tools you need to integrate **LLMs with external data sources, memory, and reasoning capabilities**.  \n",
    "\n",
    "ğŸ’¡ **Think of it as a Swiss Army knife** for AI developmentâ€”helping you combine different AI components seamlessly to create **intelligent, interactive, and dynamic applications**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Why Use LangChain?**  \n",
    "\n",
    "LangChain makes it super easy to:  \n",
    "âœ… Connect with LLMs like **GPT-4, Claude, Gemini, and Llama** ğŸ¤–  \n",
    "âœ… Enhance responses with **memory and contextual awareness** ğŸ§   \n",
    "âœ… Retrieve information from **databases, APIs, and documents** ğŸ“„ğŸ”  \n",
    "âœ… Build **AI agents** that can perform complex tasks autonomously ğŸ¤¯  \n",
    "âœ… Simplify **prompt engineering & tuning** for better results ğŸ­  \n",
    "âœ… Deploy AI-powered applications **faster and more efficiently** âš¡  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Key Components of LangChain**  \n",
    "\n",
    "### **1ï¸âƒ£ Model I/O - Talking to LLMs** ğŸ’¬  \n",
    "LangChain provides an easy way to **interact with LLMs** like OpenAIâ€™s GPT, Googleâ€™s Gemini, or open-source models like Llama and Mistral.  \n",
    "âœ¨ **Features:**  \n",
    "ğŸ”¹ Simple API calls to generate text, complete prompts, or answer queries.  \n",
    "ğŸ”¹ Advanced **prompt engineering** to get better responses.  \n",
    "ğŸ”¹ Supports **multiple LLM providers** (OpenAI, Hugging Face, Cohere, etc.).  \n",
    "\n",
    "ğŸ“ **Example:**  \n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "response = llm(\"Tell me a joke about AI\")\n",
    "print(response)\n",
    "```\n",
    "ğŸ˜‚ **Output:** \"Why did the AI break up with its girlfriend? It lost interest!\"  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Memory - Making AI Remember!** ğŸ§   \n",
    "By default, LLMs donâ€™t **remember** past interactions. LangChain adds memory so that AI can **retain context** in conversations!  \n",
    "âœ¨ **Features:**  \n",
    "ğŸ”¹ Store and recall chat history.  \n",
    "ğŸ”¹ Enable AI to **continue conversations seamlessly**.  \n",
    "ğŸ”¹ Useful for chatbots, virtual assistants, and AI agents.  \n",
    "\n",
    "ğŸ“ **Example (Memory in Chatbots):**  \n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"user\": \"Hello\"}, {\"AI\": \"Hi! How can I help?\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "```\n",
    "ğŸ” **Output:** The AI remembers past conversations!\n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Chains - Connecting AI Workflows** ğŸ”—  \n",
    "LangChain lets you **combine multiple LLM calls and functions** into a **chain**, making complex AI applications possible!  \n",
    "âœ¨ **Features:**  \n",
    "ğŸ”¹ Process input â†’ generate responses â†’ refine output.  \n",
    "ğŸ”¹ Multi-step workflows for **question-answering, reasoning, and automation**.  \n",
    "ğŸ”¹ Can integrate with **APIs, databases, and tools**.  \n",
    "\n",
    "ğŸ“ **Example (LLM Chain for Question Answering):**  \n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(input_variables=[\"name\"], template=\"Tell me about {name}.\")\n",
    "chain = LLMChain(llm=OpenAI(), prompt=template)\n",
    "\n",
    "response = chain.run(\"Elon Musk\")\n",
    "print(response)\n",
    "```\n",
    "ğŸš€ **Output:** AI-generated biography of Elon Musk!\n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Agents - AI That Takes Action!** ğŸ¤–  \n",
    "LangChainâ€™s **Agents** allow AI to make decisions, use tools, and take actions **autonomously**!  \n",
    "âœ¨ **Features:**  \n",
    "ğŸ”¹ AI **decides** what to do next based on the userâ€™s query.  \n",
    "ğŸ”¹ Can use **search engines, APIs, calculators, and more!**  \n",
    "ğŸ”¹ Builds powerful **AI assistants and automation workflows**.  \n",
    "\n",
    "ğŸ“ **Example (AI Agent using Google Search):**  \n",
    "```python\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "\n",
    "tools = load_tools([\"serpapi\"])  # Using Google Search API\n",
    "agent = initialize_agent(tools=tools, llm=OpenAI(), agent=\"zero-shot-react-description\")\n",
    "\n",
    "response = agent.run(\"What is the latest news in AI?\")\n",
    "print(response)\n",
    "```\n",
    "ğŸŒ **Output:** AI fetches real-time AI news from the web!\n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ Retrieval - Supercharge AI with Knowledge!** ğŸ“š  \n",
    "LangChain allows AI to **search and retrieve data** from databases, PDFs, websites, and vector stores.  \n",
    "âœ¨ **Features:**  \n",
    "ğŸ”¹ Enhances AI responses with **real-world knowledge**.  \n",
    "ğŸ”¹ Works with **document search, vector databases (FAISS, Pinecone), and knowledge bases**.  \n",
    "ğŸ”¹ Ideal for **chatbots, legal/medical AI, and research tools**.  \n",
    "\n",
    "ğŸ“ **Example (Retrieving Info from PDFs):**  \n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content)\n",
    "```\n",
    "ğŸ“„ **Output:** AI retrieves text from a PDF file!\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Where is LangChain Used?**  \n",
    "ğŸ”¹ **Chatbots & Virtual Assistants** (e.g., AI-powered customer support)  \n",
    "ğŸ”¹ **AI-powered Search Engines** (e.g., Google-like AI search)  \n",
    "ğŸ”¹ **Autonomous AI Agents** (e.g., self-improving AI systems)  \n",
    "ğŸ”¹ **Business Intelligence & Data Analysis**  \n",
    "ğŸ”¹ **Healthcare, Legal, and Finance AI Apps**  \n",
    "ğŸ”¹ **Coding Assistants & AI Tutors**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Why Choose LangChain?**  \n",
    "âœ… **Easy to Use** â€“ Simplifies working with LLMs.  \n",
    "âœ… **Highly Modular** â€“ Works with multiple AI models & tools.  \n",
    "âœ… **Scalable** â€“ Ideal for both small and enterprise-level AI apps.  \n",
    "âœ… **Active Community** â€“ Constantly evolving with new features!  \n",
    "\n",
    "ğŸ”— **Learn More & Get Started:** ğŸ‘‰ [LangChain Official Docs](https://python.langchain.com/en/latest/)  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒˆ **Final Thoughts!**  \n",
    "LangChain is a **game-changer** for AI development, allowing you to **unlock the true power of LLMs**! Whether you're a beginner or an expert, LangChain helps you build **smart, scalable, and interactive AI applications** like never before.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ **LangChain Components Explained in Detail!** ğŸ¯  \n",
    "\n",
    "LangChain is made up of **several core components** that help you build **powerful AI applications** by connecting **LLMs (like GPT-4, Claude, and Gemini) with memory, data sources, APIs, and reasoning abilities**.  \n",
    "\n",
    "Let's dive deep into each **LangChain component** and see how they work! ğŸ§ğŸ‘‡  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **1. Models â€“ The Brain of Your AI** ğŸ§   \n",
    "At the heart of LangChain are **LLMs (Large Language Models)**, which generate text-based responses.  \n",
    "\n",
    "âœ¨ **Supported Models:**  \n",
    "- **OpenAIâ€™s GPT (GPT-3.5, GPT-4, etc.)**  \n",
    "- **Google Gemini, Anthropic Claude**  \n",
    "- **Hugging Face models (Falcon, LLaMA, Mistral, etc.)**  \n",
    "- **Local models (running on your own machine)**  \n",
    "\n",
    "ğŸ“ **Example: Connecting an OpenAI Model**  \n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")  # Choose any model\n",
    "response = llm(\"What are the benefits of AI?\")\n",
    "print(response)\n",
    "```\n",
    "ğŸ”¹ **Output:** AI-generated response with insights on AI benefits!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¬ **2. Prompt Templates â€“ Smart Prompt Engineering** âœï¸  \n",
    "LLMs respond based on the **prompts** we give them. LangChain provides **Prompt Templates** to structure and format these prompts efficiently.  \n",
    "\n",
    "âœ¨ **Why Use Prompt Templates?**  \n",
    "âœ… Create reusable prompts for different tasks.  \n",
    "âœ… Make prompts **dynamic** by inserting variables.  \n",
    "âœ… Improve **response quality** by giving **better context**.  \n",
    "\n",
    "ğŸ“ **Example: Using Prompt Templates**  \n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Explain {topic} in simple terms.\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic=\"Quantum Computing\")\n",
    "print(prompt)\n",
    "```\n",
    "ğŸ”¹ **Output:** `\"Explain Quantum Computing in simple terms.\"`  \n",
    "\n",
    "Now, you can use this formatted prompt with an LLM!\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”— **3. Chains â€“ Connecting Multiple AI Steps** ğŸ¤–  \n",
    "A **Chain** in LangChain is a sequence of operations, like:  \n",
    "1ï¸âƒ£ Accept user input  \n",
    "2ï¸âƒ£ Generate a response from LLM  \n",
    "3ï¸âƒ£ Process and refine the output  \n",
    "\n",
    "âœ¨ **Why Use Chains?**  \n",
    "âœ… **Combine** multiple models, prompts, and tools together.  \n",
    "âœ… Automate **multi-step AI workflows**.  \n",
    "âœ… Use **predefined templates** for common tasks (e.g., QA, summarization).  \n",
    "\n",
    "ğŸ“ **Example: Simple LLM Chain for Q&A**  \n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(input_variables=[\"question\"], template=\"Answer this: {question}\")\n",
    "chain = LLMChain(llm=OpenAI(), prompt=template)\n",
    "\n",
    "response = chain.run(\"What is deep learning?\")\n",
    "print(response)\n",
    "```\n",
    "ğŸ”¹ **Output:** AI-generated explanation of deep learning!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  **4. Memory â€“ Making AI Remember Conversations**  \n",
    "By default, LLMs **donâ€™t remember past interactions**. LangChain **Memory** helps AI retain information across multiple exchanges.  \n",
    "\n",
    "âœ¨ **Types of Memory in LangChain:**  \n",
    "âœ… **ConversationBufferMemory** â€“ Stores the entire chat history.  \n",
    "âœ… **ConversationSummaryMemory** â€“ Summarizes past messages to save space.  \n",
    "âœ… **Vector-based Memory** â€“ Stores conversations in a database for retrieval.  \n",
    "\n",
    "ğŸ“ **Example: Using Conversation Memory**  \n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"user\": \"Hello!\"}, {\"AI\": \"Hi there! How can I help?\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))  # Shows stored conversation\n",
    "```\n",
    "ğŸ”¹ **Output:** AI remembers and continues conversations naturally!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ” **5. Retrieval â€“ Accessing External Knowledge** ğŸ“š  \n",
    "LLMs **donâ€™t have real-time knowledge** beyond their training data. LangChainâ€™s **Retrieval** system allows AI to fetch **real-time and domain-specific data** from:  \n",
    "ğŸ“„ **PDFs, Word documents, and CSV files**  \n",
    "ğŸ“‚ **Databases & APIs**  \n",
    "ğŸ” **Web search (Google, Bing, etc.)**  \n",
    "\n",
    "âœ¨ **Why Use Retrieval?**  \n",
    "âœ… AI can answer based on **custom knowledge bases**.  \n",
    "âœ… Perfect for **legal, healthcare, and enterprise AI applications**.  \n",
    "\n",
    "ğŸ“ **Example: Retrieving Text from a PDF File**  \n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"example.pdf\")  # Load a PDF document\n",
    "docs = loader.load()\n",
    "print(docs[0].page_content)  # Extracts the text content\n",
    "```\n",
    "ğŸ”¹ **Output:** AI retrieves text from the document!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ­ **6. Agents â€“ AI That Thinks & Acts!** ğŸ¤¯  \n",
    "An **Agent** is an AI-powered assistant that can:  \n",
    "ğŸ›  **Use multiple tools** (search, APIs, calculators, etc.)  \n",
    "ğŸ§© **Decide what actions to take**  \n",
    "ğŸ”„ **Perform multi-step tasks autonomously**  \n",
    "\n",
    "âœ¨ **Why Use Agents?**  \n",
    "âœ… AI can dynamically decide which tool to use.  \n",
    "âœ… Ideal for **personal assistants, task automation, and AI chatbots**.  \n",
    "\n",
    "ğŸ“ **Example: Creating a Web Search Agent**  \n",
    "```python\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"serpapi\"])  # Use Google's search API\n",
    "agent = initialize_agent(tools=tools, llm=OpenAI(), agent=\"zero-shot-react-description\")\n",
    "\n",
    "response = agent.run(\"Find the latest advancements in AI.\")\n",
    "print(response)\n",
    "```\n",
    "ğŸ”¹ **Output:** AI fetches real-time AI news from the web!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“¦ **7. Toolkits â€“ Integrating with External Tools** ğŸ›   \n",
    "LangChain provides **toolkits** to connect AI with external services like:  \n",
    "ğŸ” **Search Engines** (Google, Bing, DuckDuckGo)  \n",
    "ğŸ“Š **Databases** (SQL, NoSQL, Pinecone, FAISS)  \n",
    "ğŸ”Œ **APIs** (Zapier, Twilio, Slack, GitHub)  \n",
    "\n",
    "âœ¨ **Why Use Toolkits?**  \n",
    "âœ… Extend AIâ€™s capabilities beyond text generation.  \n",
    "âœ… Enable **automation & API integration**.  \n",
    "\n",
    "ğŸ“ **Example: AI Answering Questions from a SQL Database**  \n",
    "```python\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from langchain.chains import SQLDatabaseChain\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///my_database.db\")  # Connect to database\n",
    "chain = SQLDatabaseChain(llm=OpenAI(), database=db, verbose=True)\n",
    "\n",
    "response = chain.run(\"How many users signed up last month?\")\n",
    "print(response)\n",
    "```\n",
    "ğŸ”¹ **Output:** AI queries the database and provides the answer!  \n",
    "\n",
    "## ğŸŒŸ **Final Summary: LangChain Components at a Glance!**  \n",
    "\n",
    "| **Component**  | **Purpose**  | **Example Use Case**  |\n",
    "|--------------|----------------|---------------------|\n",
    "| **Models** ğŸ§   | Connect with LLMs (GPT-4, Claude, etc.) | AI chatbots, text generation  |\n",
    "| **Prompt Templates** âœï¸  | Structure prompts effectively | Smart prompt engineering  |\n",
    "| **Chains** ğŸ”—  | Automate multi-step workflows | AI-driven content creation  |\n",
    "| **Memory** ğŸ§   | Store & recall past conversations | AI chatbots, virtual assistants  |\n",
    "| **Retrieval** ğŸ“š  | Fetch knowledge from external sources | AI-powered document search  |\n",
    "| **Agents** ğŸ¤–  | Make AI take decisions & actions | AI assistants, automation  |\n",
    "| **Toolkits** ğŸ›   | Connect AI to APIs, search engines, databases | AI + business applications  |\n",
    "\n",
    "\n",
    "## ğŸ¯ **Why Should You Learn LangChain?**  \n",
    "âœ… **Bridges the gap between AI and real-world applications.**  \n",
    "âœ… **Simplifies working with multiple AI models and data sources.**  \n",
    "âœ… **Empowers AI to take actions and perform reasoning tasks.**  \n",
    "âœ… **Used in cutting-edge AI applications (Chatbots, RAG, Agents).**  \n",
    "\n",
    "ğŸ’¡ **Want to get started?** Explore LangChainâ€™s official docs ğŸ‘‰ [LangChain Docs](https://python.langchain.com/en/latest/)  \n",
    "\n",
    "ğŸš€ **Now go build some amazing AI-powered apps!** ğŸ‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ **LangChain Prompts Explained (Full Guide!)** ğŸš€  \n",
    "\n",
    "LangChain is a **powerful framework** for building applications that use large language models (LLMs) like OpenAI's GPT. At the heart of LangChain are **prompts**, which guide the modelâ€™s responses. Let's break it all down in a **simple and colorful way!** ğŸ¨âœ¨  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ­ **What are Prompts in LangChain?**  \n",
    "A **prompt** is like a magic spell ğŸª„ that tells the LLM what to do. Itâ€™s just **text input** that guides the AI to generate meaningful responses. You can think of it as giving **instructions** to a super-intelligent assistant. ğŸ¤–ğŸ’¡  \n",
    "\n",
    "For example:  \n",
    "ğŸ‘‰ `\"Translate the following text into French: 'Hello, how are you?'\"`  \n",
    "ğŸ‘‰ `\"Summarize this article in three bullet points.\"`  \n",
    "\n",
    "A **well-crafted prompt** can significantly improve the **quality** of AI responses! ğŸš€  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ› ï¸ **Types of Prompts in LangChain**  \n",
    "\n",
    "LangChain provides several ways to structure your prompts:  \n",
    "\n",
    "### **1ï¸âƒ£ Prompt Templates** ğŸ“  \n",
    "Prompt templates allow you to **create reusable prompts** where certain parts are **dynamically filled** with values. Think of them as **mad-libs** for AI! ğŸ­  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"Write a product description for {product} in an engaging tone.\"\n",
    ")\n",
    "\n",
    "print(template.format(product=\"wireless earbuds\"))\n",
    "```\n",
    "ğŸ’¡ **Output:** `\"Write a product description for wireless earbuds in an engaging tone.\"`  \n",
    "\n",
    "This makes prompts **flexible and reusable!** ğŸ”„  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ Few-shot Prompts** ğŸ“š  \n",
    "Few-shot prompting means **giving the AI some examples** before the main question, so it learns the pattern! ğŸ§   \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```\n",
    "Translate the following English sentences into Spanish:\n",
    "- \"Good morning\" -> \"Buenos dÃ­as\"\n",
    "- \"How are you?\" -> \"Â¿CÃ³mo estÃ¡s?\"\n",
    "- \"Nice to meet you\" -> ???\n",
    "```\n",
    "ğŸ’¡ **Why use this?** It helps the AI **understand context** better! ğŸ“–  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Zero-shot Prompts** ğŸ¯  \n",
    "Zero-shot prompting **doesnâ€™t** provide examples. It just gives a **direct instruction**. ğŸš€  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```python\n",
    "prompt = \"Summarize this text in one sentence: {text}\"\n",
    "```\n",
    "ğŸ’¡ **Best for:** When the model **already knows** how to respond based on training!  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Chain-of-Thought (CoT) Prompting** ğŸ”—  \n",
    "This technique **guides the AIâ€™s reasoning** step-by-step, just like solving a math problem. ğŸ§®  \n",
    "\n",
    "ğŸ”¹ **Example:**  \n",
    "```\n",
    "Q: A car travels at 60 km/h for 2 hours. How far does it go?  \n",
    "A: Let's think step by step. The carâ€™s speed is 60 km/h. It travels for 2 hours.  \n",
    "Distance = Speed Ã— Time = 60 Ã— 2 = 120 km.\n",
    "```\n",
    "ğŸ”¥ **Why is this powerful?** It improves **logical accuracy!**  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒŸ **LangChain Prompt Engineering Best Practices**  \n",
    "\n",
    "âœ” **Be clear & specific** â€“ The model performs better with detailed instructions!  \n",
    "âœ” **Use examples when needed** â€“ Few-shot examples improve results.  \n",
    "âœ” **Test & iterate** â€“ Try different prompts to optimize responses.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Thoughts**  \n",
    "LangChain prompts are **super versatile** and help you control how AI **understands** and **responds** to queries. Whether you need **basic prompts, templates, few-shot learning, or advanced reasoning**, LangChain gives you the tools to **build powerful AI applications!** ğŸš€ğŸ‰  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¬ **Types of Messages in LangChain**  \n",
    "\n",
    "LangChain supports different types of messages that help structure conversations with AI. These messages are useful in **chat-based applications**.  \n",
    "\n",
    "### **1ï¸âƒ£ System Message** ğŸ› ï¸  \n",
    "Used to set the behavior of the AI **before interaction starts**.  \n",
    "\n",
    "âœ… **Example:**  \n",
    "```text\n",
    "\"You are a polite and helpful assistant.\"\n",
    "```\n",
    "âœ… **Use Case:** Sets the **tone, rules, or constraints** for AI responses.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ User Message** ğŸ§‘ğŸ’¬  \n",
    "This represents what the **user inputs** in a conversation.  \n",
    "\n",
    "âœ… **Example:**  \n",
    "```text\n",
    "\"What is the capital of France?\"\n",
    "```\n",
    "âœ… **Use Case:** Captures **user queries** or **commands**.  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ Assistant Message** ğŸ¤–ğŸ’¬  \n",
    "This is the **AIâ€™s response** to a user message.  \n",
    "\n",
    "âœ… **Example:**  \n",
    "```text\n",
    "\"The capital of France is Paris.\"\n",
    "```\n",
    "âœ… **Use Case:** Stores **AI-generated replies** in chat applications.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ Multi-Turn Conversation Messages** ğŸ”„  \n",
    "For **chatbots**, conversations involve multiple exchanges.  \n",
    "\n",
    "âœ… **Example:**  \n",
    "```\n",
    "User: \"Tell me a joke.\"  \n",
    "Assistant: \"Why did the AI break up with its partner? Because it lost its connection!\"  \n",
    "User: \"Haha, tell me another one.\"  \n",
    "```\n",
    "âœ… **Use Case:** Used in **chat history tracking** and **context-aware interactions**.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Final Thoughts**  \n",
    "\n",
    "- **Text-based prompts** in LangChain can be structured for **instructions, role-play, reasoning, or creativity.**  \n",
    "- **Message types** help manage AI interactions, making them **contextual** and **meaningful.**  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ **Understanding Structured Output: What, Why, and Its Importance** ğŸŒŸ  \n",
    "\n",
    "In the world of data processing, **structured output** plays a crucial role in organizing information in a well-defined format. Letâ€™s break it down!  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **What is Structured Output?**  \n",
    "\n",
    "**Structured output** refers to information that is arranged in a predefined format, making it easy to read, process, and analyze. It follows a clear pattern, such as tables, JSON, XML, or well-formatted reports.  \n",
    "\n",
    "âœ… **Example:**  \n",
    "Imagine you are building a chatbot that extracts flight details from user queries. Instead of giving raw text, structured output provides organized details like this:  \n",
    "\n",
    "**ğŸ“ Raw Output:**  \n",
    "*\"Your flight from New York to London departs at 7:30 PM.\"*  \n",
    "\n",
    "**ğŸ“Š Structured Output (JSON Format):**  \n",
    "```json\n",
    "{\n",
    "  \"origin\": \"New York\",\n",
    "  \"destination\": \"London\",\n",
    "  \"departure_time\": \"7:30 PM\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ **Why Do We Need Structured Output?**  \n",
    "\n",
    "Structured output is essential for making data useful and actionable. Hereâ€™s why it matters:  \n",
    "\n",
    "### ğŸ”¹ **1. Easy Data Processing & Automation**  \n",
    "- Machines can quickly read and process structured data.  \n",
    "- Useful in AI, NLP, and machine learning applications.  \n",
    "\n",
    "### ğŸ”¹ **2. Improved Readability**  \n",
    "- Humans can easily interpret structured information.  \n",
    "- Example: Well-formatted tables or JSON data in APIs.  \n",
    "\n",
    "### ğŸ”¹ **3. Efficient Storage & Retrieval**  \n",
    "- Databases store structured data more efficiently.  \n",
    "- Example: SQL databases store data in rows & columns.  \n",
    "\n",
    "### ğŸ”¹ **4. Interoperability Between Systems**  \n",
    "- Structured data formats (like JSON, XML) help different software systems communicate.  \n",
    "- Example: APIs return JSON responses for integration with apps.  \n",
    "\n",
    "### ğŸ”¹ **5. Better Decision-Making**  \n",
    "- Organized data helps businesses and data scientists derive meaningful insights.  \n",
    "- Example: Structured sales reports improve forecasting.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Where is Structured Output Used?**  \n",
    "\n",
    "ğŸ’» **APIs & Web Services:** APIs return structured JSON/XML responses.  \n",
    "ğŸ¤– **AI & Machine Learning:** NLP models generate structured responses.  \n",
    "ğŸ“Š **Data Analysis:** Data is stored in structured formats like CSV, SQL tables.  \n",
    "ğŸ’¬ **Chatbots:** Virtual assistants provide structured replies instead of free text.  \n",
    "\n",
    "\n",
    "## ğŸ¨ **Final Thoughts**  \n",
    "\n",
    "Structured output is like **organizing a messy room**â€”it makes everything easy to find and use! Whether in software development, AI, or business intelligence, it ensures clarity, efficiency, and automation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ **Understanding Output Parsers in LangChain** ğŸ¨  \n",
    "\n",
    "**Output parsers** in LangChain help **format, structure, and extract information** from the modelâ€™s response into a structured format like JSON, Pydantic objects, or specific string templates. This is extremely useful when you need structured data instead of free-text responses.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Why Use Output Parsers?**\n",
    "- **Consistency:** Ensures the model always returns structured output.\n",
    "- **Automation:** Makes it easy to use model responses in downstream applications.\n",
    "- **Error Handling:** Helps detect when the model produces unexpected results.\n",
    "- **Flexibility:** Supports multiple formats, including JSON, Pydantic, and custom templates.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Types of Output Parsers in LangChain**\n",
    "LangChain provides several built-in output parsers:\n",
    "\n",
    "### ğŸ“ **1. Simple String Output Parser**\n",
    "- Converts the response into a plain string.\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "  output_parser = StrOutputParser()\n",
    "  result = output_parser.parse(\"Hello, this is LangChain!\")\n",
    "  print(result)\n",
    "  ```\n",
    "  **ğŸ”¹ Output:**  \n",
    "  ```\n",
    "  Hello, this is LangChain!\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“œ **2. Structured Output Parser (JSON)**\n",
    "- Ensures the model returns a **valid JSON output**.\n",
    "- Useful when working with APIs or structured data.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain.output_parsers import JsonOutputParser\n",
    "  from langchain.schema import OutputParserException\n",
    "\n",
    "  output_parser = JsonOutputParser()\n",
    "  raw_response = '{\"name\": \"Suhas\", \"role\": \"Data Scientist\"}'\n",
    "\n",
    "  try:\n",
    "      result = output_parser.parse(raw_response)\n",
    "      print(result)\n",
    "  except OutputParserException as e:\n",
    "      print(\"Parsing Error:\", e)\n",
    "  ```\n",
    "  **ğŸ”¹ Output:**  \n",
    "  ```json\n",
    "  {\"name\": \"Suhas\", \"role\": \"Data Scientist\"}\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“Œ **3. Pydantic Output Parser (Strict Validation)**\n",
    "- Uses **Pydantic models** to enforce strict validation.\n",
    "- Ensures the response follows the expected format.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain.output_parsers import PydanticOutputParser\n",
    "  from pydantic import BaseModel\n",
    "\n",
    "  class Person(BaseModel):\n",
    "      name: str\n",
    "      age: int\n",
    "\n",
    "  output_parser = PydanticOutputParser(pydantic_object=Person)\n",
    "  raw_response = '{\"name\": \"Suhas\", \"age\": 30}'\n",
    "\n",
    "  result = output_parser.parse(raw_response)\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "  **ğŸ”¹ Output:**\n",
    "  ```\n",
    "  name='Suhas' age=30\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”„ **4. Comma-Separated List Output Parser**\n",
    "- Parses a response into a **list of items separated by commas**.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "  output_parser = CommaSeparatedListOutputParser()\n",
    "  raw_response = \"Apple, Banana, Orange, Mango\"\n",
    "\n",
    "  result = output_parser.parse(raw_response)\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "  **ğŸ”¹ Output:**\n",
    "  ```\n",
    "  ['Apple', 'Banana', 'Orange', 'Mango']\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ—ï¸ **5. Regex Output Parser**\n",
    "- Extracts specific parts of the model's output using **regular expressions (regex)**.\n",
    "\n",
    "- **Example:**\n",
    "  ```python\n",
    "  from langchain.output_parsers import RegexOutputParser\n",
    "\n",
    "  output_parser = RegexOutputParser(regex=r\"Name: (\\w+), Age: (\\d+)\")\n",
    "  raw_response = \"Name: Suhas, Age: 30\"\n",
    "\n",
    "  result = output_parser.parse(raw_response)\n",
    "  print(result)\n",
    "  ```\n",
    "\n",
    "  **ğŸ”¹ Output:**\n",
    "  ```\n",
    "  ('Suhas', '30')\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¨ **How to Use Output Parsers in a LangChain Pipeline?**\n",
    "Hereâ€™s how you can integrate an output parser with **LLM responses**:\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "# Define a structured output parser\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "# Define a prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract the key themes, summary, and sentiment from the following review:\\n\\n{review}\\n\\nReturn the result in JSON format.\",\n",
    "    input_variables=[\"review\"]\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "raw_response = llm.invoke(prompt.format(review=\"I love the new iPhone! The camera is great, but the battery life could be better.\"))\n",
    "\n",
    "# Parse the response into structured JSON\n",
    "result = output_parser.parse(raw_response)\n",
    "\n",
    "print(result)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒŸ **Key Takeaways**\n",
    "âœ… **Output parsers** ensure that model responses are formatted correctly.  \n",
    "âœ… **Different types of parsers** help in various scenarios (JSON, Pydantic, Lists, Regex, etc.).  \n",
    "âœ… **Using structured responses** improves automation and reliability in applications.  \n",
    "âœ… **Combining output parsers with prompts** gives more **predictable** results from LLMs.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ **When to Use Output Parsers?**\n",
    "ğŸ“Œ When you need **structured data** from a model response  \n",
    "ğŸ“Œ When you want **strict validation** of the response format  \n",
    "ğŸ“Œ When you want to **automate processing** of LLM outputs  \n",
    "ğŸ“Œ When integrating with **APIs, dashboards, or databases**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒŸ **LangChain Chains Explained with Types** ğŸŒŸ\n",
    "\n",
    "### ğŸ”— **What are Chains in LangChain?**\n",
    "Chains in **LangChain** ğŸ¦œğŸ”— refer to **sequences of operations** that process inputs and generate outputs. Instead of calling a **single LLM (Large Language Model)**, a chain **combines multiple steps** to achieve **complex** results. This allows developers to build **custom workflows** using different components like **prompt templates, memory, LLMs, and tools**.\n",
    "\n",
    "\n",
    "## ğŸ† **Types of Chains in LangChain**\n",
    "LangChain provides several types of chains, each designed for different tasks. Letâ€™s explore them one by one! ğŸš€\n",
    "\n",
    "### ğŸ¯ **1. LLMChain (Basic Chain)**\n",
    "- This is the **simplest** chain.\n",
    "- It takes **input** â†’ **formats a prompt** â†’ **sends it to an LLM** â†’ **returns output**.\n",
    "- Itâ€™s great for tasks like **text generation, summarization, and simple Q&A**.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "prompt = PromptTemplate(input_variables=[\"topic\"], template=\"Tell me a fact about {topic}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "result = chain.run(\"space\")\n",
    "print(result)\n",
    "```\n",
    "ğŸ“Œ **Use case:** Generate text based on a user query.\n",
    "\n",
    "\n",
    "### ğŸ”¥ **2. Sequential Chains**\n",
    "ğŸ”¹ Used when multiple steps **must be executed one after another**.\n",
    "\n",
    "#### ğŸ”„ **a) SimpleSequentialChain**\n",
    "- Each stepâ€™s output is **used as the input** for the next step.\n",
    "- Ideal for **text transformations** (e.g., summarization â†’ paraphrasing â†’ translation).\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[chain1, chain2])\n",
    "output = chain.run(\"Artificial Intelligence\")\n",
    "print(output)\n",
    "```\n",
    "ğŸ“Œ **Use case:** Processed step-by-step execution.\n",
    "\n",
    "#### ğŸ›  **b) SequentialChain**\n",
    "- **More flexible** than SimpleSequentialChain.\n",
    "- Allows **multiple inputs and outputs**.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[chain1, chain2],\n",
    "    input_variables=[\"topic\"],\n",
    "    output_variables=[\"summary\", \"analysis\"]\n",
    ")\n",
    "output = chain.run({\"topic\": \"climate change\"})\n",
    "```\n",
    "ğŸ“Œ **Use case:** Multi-step workflows like **data analysis pipelines**.\n",
    "\n",
    "\n",
    "### ğŸ” **3. Transform Chain**\n",
    "ğŸ”¹ Used to **apply a transformation** on data **before or after** processing.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "def capitalize_text(inputs):\n",
    "    return {\"output\": inputs[\"text\"].upper()}\n",
    "\n",
    "transform_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output\"], transform=capitalize_text)\n",
    "result = transform_chain.run({\"text\": \"hello langchain!\"})\n",
    "print(result)\n",
    "```\n",
    "ğŸ“Œ **Use case:** Data **preprocessing** or **postprocessing**.\n",
    "\n",
    "\n",
    "### ğŸ¤– **4. Router Chain**\n",
    "ğŸ”¹ Dynamically **routes** inputs to the correct chain based on the user query.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import LLMRouterChain\n",
    "\n",
    "router_chain = LLMRouterChain(llm=llm, default_chain=default_chain, destination_chains={\"finance\": finance_chain, \"health\": health_chain})\n",
    "```\n",
    "ğŸ“Œ **Use case:** **Chatbots** that switch responses based on the topic.\n",
    "\n",
    "\n",
    "### ğŸ§  **5. Memory-Enabled Chains**\n",
    "ğŸ”¹ Stores previous interactions for **better contextual understanding**.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "print(conversation.run(\"Hello!\"))\n",
    "print(conversation.run(\"What did I just say?\"))\n",
    "```\n",
    "ğŸ“Œ **Use case:** Conversational AI **remembers context**.\n",
    "\n",
    "\n",
    "### âš¡ **6. Parallel Chain**\n",
    "ğŸ”¹ Runs multiple chains **simultaneously** instead of sequentially.\n",
    "- Each chain processes the same or different input **in parallel**.\n",
    "- Reduces execution time when handling **independent tasks**.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import ParallelChain\n",
    "\n",
    "parallel_chain = ParallelChain(chains=[chain1, chain2, chain3])\n",
    "results = parallel_chain.run(\"input_data\")\n",
    "print(results)\n",
    "```\n",
    "ğŸ“Œ **Use case:** **Multi-task processing**, such as summarization + sentiment analysis at the same time.\n",
    "\n",
    "\n",
    "### ğŸ”„ **7. Conditional Chain**\n",
    "ğŸ”¹ Executes different chains **based on conditions**.\n",
    "- Helps in decision-making workflows.\n",
    "\n",
    "âœ… **Example:**\n",
    "```python\n",
    "from langchain.chains import ConditionalChain\n",
    "\n",
    "def condition_func(inputs):\n",
    "    return \"chain1\" if inputs[\"type\"] == \"simple\" else \"chain2\"\n",
    "\n",
    "conditional_chain = ConditionalChain(condition_func=condition_func, chains={\"chain1\": chain1, \"chain2\": chain2})\n",
    "result = conditional_chain.run({\"type\": \"simple\"})\n",
    "print(result)\n",
    "```\n",
    "ğŸ“Œ **Use case:** Chatbots that respond differently based on **user intent**.\n",
    "\n",
    "\n",
    "### ğŸš€ **Conclusion**\n",
    "LangChain Chains **boost the power of LLMs** by structuring complex workflows. From **basic LLM chains** to **sequential**, **transform**, **parallel**, and **conditional chains**, they offer **flexibility** and **efficiency** in AI-powered applications.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¥ **Runnables in LangChain: The Ultimate Guide** ğŸš€  \n",
    "\n",
    "LangChain introduced **Runnables** as a powerful abstraction to simplify and streamline the execution of AI-powered workflows. Whether youâ€™re chaining LLM calls, processing data, or integrating various tools, **Runnables** provide a modular, flexible, and scalable way to design your AI applications.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **What Are Runnables?**\n",
    "A **Runnable** in LangChain is an interface that represents a callable object. It is designed to standardize and compose various components, such as:  \n",
    "âœ… **LLMs (Language Models)** â€“ Calling models like GPT-4 or Claude.  \n",
    "âœ… **Chains** â€“ Sequences of operations like text transformations and prompt templating.  \n",
    "âœ… **Tools & Functions** â€“ APIs, databases, or external scripts that process data.  \n",
    "âœ… **Custom Python Functions** â€“ Your own logic wrapped inside a Runnable interface.  \n",
    "\n",
    "At its core, a Runnable is just something that **\"takes an input and produces an output\"**, but the real magic comes when you start combining them! ğŸŒŸ  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ›  **Key Features of Runnables**  \n",
    "ğŸ”¹ **Composable** â€“ Easily combine multiple Runnables into a **pipeline**.  \n",
    "ğŸ”¹ **Streaming Support** â€“ Process LLM outputs in real-time.  \n",
    "ğŸ”¹ **Parallel Execution** â€“ Run multiple tasks concurrently.  \n",
    "ğŸ”¹ **Logging & Tracing** â€“ Monitor execution flows.  \n",
    "ğŸ”¹ **Type Safety** â€“ Ensures expected inputs and outputs.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **How to Use Runnables? (With Code Examples!)**  \n",
    "\n",
    "### 1ï¸âƒ£ **Creating a Basic Runnable**\n",
    "Letâ€™s create a simple Runnable that converts a sentence to uppercase.\n",
    "\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Define a simple function\n",
    "def to_uppercase(text: str) -> str:\n",
    "    return text.upper()\n",
    "\n",
    "# Wrap it inside a Runnable\n",
    "uppercase_runnable = RunnableLambda(to_uppercase)\n",
    "\n",
    "# Run it\n",
    "print(uppercase_runnable.invoke(\"hello langchain!\"))  \n",
    "# Output: HELLO LANGCHAIN!\n",
    "```\n",
    "ğŸ”¹ Here, `RunnableLambda` allows us to **wrap any function** as a Runnable, making it more modular and reusable!  \n",
    "\n",
    "\n",
    "\n",
    "### 2ï¸âƒ£ **Chaining Multiple Runnables**\n",
    "Letâ€™s **combine** multiple transformations using Runnables.\n",
    "\n",
    "```python\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Create a lowercase transformer\n",
    "lowercase_runnable = RunnableLambda(lambda x: x.lower())\n",
    "\n",
    "# Chain multiple transformations\n",
    "pipeline = lowercase_runnable | uppercase_runnable  # Lowercase -> Uppercase\n",
    "\n",
    "print(pipeline.invoke(\"Hello LangChain!\"))  \n",
    "# Output: HELLO LANGCHAIN!\n",
    "```\n",
    "ğŸ”— The `|` **(pipe operator)** helps in chaining multiple Runnables seamlessly!  \n",
    "\n",
    "\n",
    "\n",
    "### 3ï¸âƒ£ **Using Runnables with LLMs (OpenAI GPT-4 Example)**\n",
    "Letâ€™s integrate a **language model** inside a Runnable.\n",
    "\n",
    "```python\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "# Define an LLM Runnable\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "\n",
    "# Create a pipeline to generate responses\n",
    "query_pipeline = RunnableMap({\n",
    "    \"query\": RunnablePassthrough(),\n",
    "    \"response\": llm\n",
    "})\n",
    "\n",
    "# Execute the pipeline\n",
    "output = query_pipeline.invoke(\"What is LangChain?\")\n",
    "print(output[\"response\"].content)\n",
    "```\n",
    "ğŸ’¡ **RunnableMap** helps structure inputs and outputs for more **complex pipelines**.\n",
    "\n",
    "\n",
    "\n",
    "### 4ï¸âƒ£ **Parallel Execution with RunnableParallel**\n",
    "What if you want to **run multiple Runnables at the same time**? Letâ€™s do that!\n",
    "\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "\n",
    "parallel_pipeline = RunnableParallel({\n",
    "    \"lowercase\": lowercase_runnable,\n",
    "    \"uppercase\": uppercase_runnable\n",
    "})\n",
    "\n",
    "output = parallel_pipeline.invoke(\"Hello LangChain!\")\n",
    "print(output)\n",
    "# Output: {'lowercase': 'hello langchain!', 'uppercase': 'HELLO LANGCHAIN!'}\n",
    "```\n",
    "âš¡ This executes both transformations **in parallel** and returns a dictionary of results.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Why Should You Use Runnables?**\n",
    "âœ… **Flexibility** â€“ Can be used with LLMs, tools, or any Python functions.  \n",
    "âœ… **Scalability** â€“ Supports streaming, parallel execution, and tracing.  \n",
    "âœ… **Simplicity** â€“ Reduces boilerplate code and enhances modularity.  \n",
    "âœ… **Seamless Integration** â€“ Works with all LangChain components.  \n",
    "\n",
    "\n",
    "## ğŸŒŸ **Final Thoughts**\n",
    "LangChainâ€™s **Runnables** are a game-changer in AI-powered applications, making it **super easy to build and scale complex workflows**. Whether you're working with **LLMs, APIs, or data transformations**, Runnables **bring clarity and structure** to your pipelines!  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Runnables in LangChain ğŸš€**  \n",
    "\n",
    "#### **ğŸ”¹ Runnable Primitives in LangChain ğŸš€**  \n",
    "\n",
    "LangChain introduces **Runnable Primitives** as the foundational building blocks for structuring AI workflows. These primitives allow developers to create modular, scalable, and reusable pipelines for AI tasks, making it easier to process data, interact with LLMs, and build sophisticated AI applications.  \n",
    "\n",
    "But why do we need **Runnable Primitives**? ğŸ¤”  \n",
    "\n",
    "âœ… **Flexibility** â€“ You can create reusable components that fit into various AI workflows.  \n",
    "âœ… **Modularity** â€“ Allows breaking down AI processes into independent, manageable parts.  \n",
    "âœ… **Composability** â€“ Easily chain, parallelize, or branch multiple runnables together.  \n",
    "âœ… **Scalability** â€“ Helps in handling large-scale AI applications efficiently.  \n",
    "\n",
    "\n",
    "## **ğŸ”¹ Types of Runnable Primitives**\n",
    "LangChain provides several **core Runnable Primitives** that define how data flows through the AI pipeline. These are the backbone of more complex task-specific components like LLMs, retrievers, and agents.\n",
    "\n",
    "### **1ï¸âƒ£ RunnableSequence (Chaining Multiple Runnables)**\n",
    "This type **connects multiple Runnables in a sequence**, where the output of one is passed to the next.\n",
    "\n",
    "#### âœ¨ **Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableSequence\n",
    "\n",
    "runnable_chain = RunnableSequence(\n",
    "    RunnableLambda(lambda x: x.lower()),  # Convert text to lowercase\n",
    "    RunnableLambda(lambda x: f\"Processed: {x}\")  # Add a prefix\n",
    ")\n",
    "\n",
    "result = runnable_chain.invoke(\"HELLO LANGCHAIN!\")\n",
    "print(result)  # Output: Processed: hello langchain!\n",
    "```\n",
    "âœ… **Use Case:** When you need **step-by-step processing**, similar to a pipeline.  \n",
    "\n",
    "\n",
    "\n",
    "### **2ï¸âƒ£ RunnableParallel (Executing Multiple Runnables Simultaneously)**\n",
    "Executes **multiple runnables in parallel**, helping to process different aspects of the same input at the same time.\n",
    "\n",
    "#### âœ¨ **Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "runnable_parallel = RunnableParallel({\n",
    "    \"original_text\": lambda x: x,  # Keeps original input\n",
    "    \"summary\": llm  # Uses LLM to generate a summary\n",
    "})\n",
    "\n",
    "result = runnable_parallel.invoke(\"LangChain makes AI development easier!\")\n",
    "print(result)\n",
    "```\n",
    "ğŸ¯ **Output:**  \n",
    "```json\n",
    "{\n",
    "  \"original_text\": \"LangChain makes AI development easier!\",\n",
    "  \"summary\": \"LangChain simplifies AI development by providing useful tools.\"\n",
    "}\n",
    "```\n",
    "âœ… **Use Case:** When you need to run **multiple independent tasks** on the same input **simultaneously**.  \n",
    "\n",
    "\n",
    "\n",
    "### **3ï¸âƒ£ RunnablePassthrough (No Modification, Just Pass Data)**\n",
    "This is the simplest runnableâ€”it just **forwards the input** without modifying it. Itâ€™s useful when you need to maintain data flow but don't require processing.\n",
    "\n",
    "#### âœ¨ **Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "result = passthrough.invoke(\"Just passing through! ğŸ˜ƒ\")\n",
    "print(result)  # Output: Just passing through! ğŸ˜ƒ\n",
    "```\n",
    "âœ… **Use Case:** When you need a **placeholder** in a workflow where processing might be added later.  \n",
    "\n",
    "\n",
    "\n",
    "### **4ï¸âƒ£ RunnableLambda (Custom Functions as Runnables)**\n",
    "This allows you to wrap any Python function and make it a **Runnable** component. It's useful when integrating custom processing logic into a LangChain workflow.\n",
    "\n",
    "#### âœ¨ **Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "def my_custom_function(text):\n",
    "    return text.upper()  # Simple transformation\n",
    "\n",
    "runnable = RunnableLambda(my_custom_function)\n",
    "\n",
    "result = runnable.invoke(\"hello langchain!\")\n",
    "print(result)  # Output: HELLO LANGCHAIN! ğŸš€\n",
    "```\n",
    "âœ… **Use Case:** When you need to integrate **custom logic** as a modular component.  \n",
    "\n",
    "\n",
    "\n",
    "### **5ï¸âƒ£ RunnableBranch (Conditional Execution)**\n",
    "Allows **conditional execution** of different paths based on input conditions.\n",
    "\n",
    "#### âœ¨ **Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "\n",
    "def check_length(text):\n",
    "    return len(text) > 50\n",
    "\n",
    "long_text_runnable = RunnableLambda(lambda x: \"Long text detected! ğŸ“\")\n",
    "short_text_runnable = RunnableLambda(lambda x: \"Short text detected! ğŸ”¹\")\n",
    "\n",
    "runnable_branch = RunnableBranch(\n",
    "    (check_length, long_text_runnable),\n",
    "    short_text_runnable  # Default case if no conditions match\n",
    ")\n",
    "\n",
    "result = runnable_branch.invoke(\"This is a short sentence.\")\n",
    "print(result)  # Output: Short text detected! ğŸ”¹\n",
    "```\n",
    "âœ… **Use Case:** When you need **dynamic execution** based on input conditions.  \n",
    "\n",
    "\n",
    "\n",
    "## **ğŸŒŸ LangChain Expression Language (LCEL) ğŸ”¥**\n",
    "LangChain Expression Language (LCEL) is a **declarative way** to define and structure AI workflows in LangChain. Instead of writing complex Python code to manage AI pipelines, LCEL enables developers to define their workflows using a **human-readable** and structured format.\n",
    "\n",
    "### **âœ¨ Key Benefits of LCEL:**\n",
    "âœ… **Simplicity:** Reduces boilerplate code by allowing concise, high-level workflow definitions.  \n",
    "âœ… **Readability:** Improves maintainability by making AI pipelines more understandable.  \n",
    "âœ… **Composability:** Makes it easy to reuse and modify workflows without deep Python expertise.  \n",
    "\n",
    "### **âœ¨ Example:**\n",
    "```python\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
    "\n",
    "def preprocess(text):\n",
    "    return text.lower()\n",
    "\n",
    "def format_output(text):\n",
    "    return f\"Processed: {text}\"\n",
    "\n",
    "workflow = (\n",
    "    RunnableLambda(preprocess) | RunnableLambda(format_output)\n",
    ")\n",
    "\n",
    "result = workflow.invoke(\"HELLO LANGCHAIN!\")\n",
    "print(result)  # Output: Processed: hello langchain!\n",
    "```\n",
    "âœ… **Use Case:** When you want to define workflows declaratively without writing complex imperative code.  \n",
    "\n",
    "## **ğŸŒˆ Conclusion: Choosing the Right Runnable**\n",
    "| Runnable Type        | Purpose |\n",
    "|----------------------|---------|\n",
    "| **RunnableSequence** | Chaining multiple Runnables together |\n",
    "| **RunnableParallel** | Running multiple Runnables in parallel |\n",
    "| **RunnablePassthrough** | Simply passing data without modification |\n",
    "| **RunnableLambda**   | Wrapping a custom function as a Runnable |\n",
    "| **RunnableBranch**   | Executing different logic based on conditions |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ **Document Loaders in LangChain** ğŸ“ğŸš€  \n",
    "\n",
    "### ğŸ”¹ **What are Document Loaders?**  \n",
    "In **LangChain**, **document loaders** are tools that help you import data from various sources (PDFs, web pages, databases, CSVs, etc.) and convert them into a format that can be processed by Language Models (LLMs).  \n",
    "\n",
    "They are **essential** in Retrieval-Augmented Generation (**RAG**) systems, as they enable your AI model to fetch real-time knowledge from external documents! ğŸ†  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ  **How Do They Work?**  \n",
    "A **document loader** typically:  \n",
    "1. **Reads the data** from a specific source (e.g., PDFs, CSVs, APIs).  \n",
    "2. **Parses and structures** the content into a format that an AI model understands.  \n",
    "3. **Returns the text** as `Document` objects, which can be used in further processing (like embeddings & vector searches).  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ¯ **Types of Document Loaders in LangChain**  \n",
    "LangChain provides **many** document loaders depending on the source of data. Letâ€™s explore some popular ones!  \n",
    "\n",
    "### ğŸ“œ **1. CSV Loaders (Structured Data Processing!)**  \n",
    "CSV files are commonly used for structured data storage. LangChain provides:  \n",
    "ğŸ“‚ `CSVLoader` â†’ Reads structured **CSV files** into rows and columns.  \n",
    "ğŸ“‚ `UnstructuredCSVLoader` â†’ Loads CSVs where column structures may vary.  \n",
    "\n",
    "> Example:  \n",
    "```python\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\"data.csv\")\n",
    "documents = loader.load()\n",
    "\n",
    "for doc in documents[:2]:  # Show first 2 rows\n",
    "    print(doc)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸŒŸ **2. Text-Based Loaders**  \n",
    "These loaders are for handling **simple text files**.  \n",
    "ğŸ“š `TextLoader` â†’ Loads plain `.txt` files.  \n",
    "ğŸ“š `JSONLoader` â†’ Extracts structured data from **JSON files**.  \n",
    "\n",
    "> Example:  \n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[:2])  # Print first 2 documents\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“„ **3. PDF Loaders (Extracting Data from PDFs!)**  \n",
    "LangChain makes it easy to read **PDF documents**, whether they contain **plain text** or **scanned images**!  \n",
    "ğŸ“€ `PyPDFLoader` â†’ Uses **PyPDF2** for extracting text.  \n",
    "ğŸ“€ `PDFMinerLoader` â†’ Uses **PDFMiner** (better for scanned docs).  \n",
    "ğŸ“€ `PDFPlumberLoader` â†’ Works well with **tables and structured PDFs**.  \n",
    "\n",
    "> Example:  \n",
    "```python\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"report.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "for doc in documents[:2]:  # Show first 2 pages\n",
    "    print(doc.page_content[:500])  # Print first 500 characters\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸŒ **4. Web Page Loaders (Scraping Websites!)**  \n",
    "Want to **scrape articles or blogs**? These loaders help:  \n",
    "ğŸŒ `WebBaseLoader` â†’ Fetches text from **web pages**.  \n",
    "ğŸ“° `NewsURLLoader` â†’ Scrapes **news articles**.  \n",
    "ğŸ“š `UnstructuredURLLoader` â†’ Extracts structured web content.  \n",
    "\n",
    "> Example (Scraping Wikipedia!):  \n",
    "```python\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[0].page_content[:500])  # Print first 500 characters\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ’¾ **5. Database & Cloud Storage Loaders**  \n",
    "If your data is stored in **databases or cloud services**, LangChain supports:  \n",
    "ğŸ“Š `SQLDatabaseLoader` â†’ Connects to **SQL databases** (MySQL, PostgreSQL).  \n",
    "ğŸ› ï¸ `GCSFileLoader` â†’ Loads files from **Google Cloud Storage**.  \n",
    "ğŸŒ `S3FileLoader` â†’ Reads files from **AWS S3 buckets**.  \n",
    "\n",
    "> Example (SQL Database Querying!):  \n",
    "```python\n",
    "from langchain.document_loaders import SQLDatabaseLoader\n",
    "from langchain.sql_database import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"sqlite:///my_database.db\")\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "loader = SQLDatabaseLoader(db, \"SELECT * FROM customers\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(documents[:2])  # Show first 2 rows\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”¥ **Why Use Document Loaders?**  \n",
    "âœ” **Automates** data ingestion from multiple sources.  \n",
    "âœ” **Works seamlessly** with LLMs like GPT, BERT, or Claude.  \n",
    "âœ” **Optimized for RAG** (Retrieval-Augmented Generation).  \n",
    "âœ” **Supports multiple formats** (text, PDFs, web, databases, CSVs).  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸš€ **Final Thoughts**  \n",
    "Document loaders are **the backbone** of RAG-based applications in LangChain. They **fetch, clean, and structure data**, making it accessible for AI models.  \n",
    "\n",
    "Whether you're building **chatbots, search engines, or summarization tools**, document loaders **supercharge** your AI applications! ğŸ’¡  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŒŸ What Are Text Splitters in LangChain?\n",
    "\n",
    "LangChain is all about connecting language models with your **custom data**â€”PDFs, articles, books, notes, you name it. But thereâ€™s a catchâ€¦\n",
    "\n",
    "### ğŸ§± Large Language Models (LLMs) Have Token Limits!\n",
    "\n",
    "Most LLMs (like GPT-4) can only handle a certain number of **tokens** (think of tokens as word-pieces). If your document is too long, you **can't feed it in one go**â€”itâ€™s like trying to fit a jumbo pizza into a mini microwave ğŸ•ğŸ’¥.\n",
    "\n",
    "Thatâ€™s where **Text Splitters** come in.\n",
    "\n",
    "\n",
    "\n",
    "## âœ‚ï¸ What Do Text Splitters Do?\n",
    "\n",
    "**Text splitters break down large texts into smaller, manageable chunks** so they can be fed into LLMs piece by piece, *without losing context or meaning*.\n",
    "\n",
    "Imagine turning a 200-page book into snack-sized knowledge bites for your AI buddy ğŸªğŸ¤–.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”§ How Do They Work?\n",
    "\n",
    "LangChain offers several types of text splitters. Here are the popular ones:\n",
    "\n",
    "### 1. **CharacterTextSplitter** â€“ ğŸ”¤\n",
    "Splits based on **character count**. You define:\n",
    "- `chunk_size`: how big each chunk should be (in characters)\n",
    "- `chunk_overlap`: how much content should *overlap* between chunks (helps preserve context)\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_text(long_text)\n",
    "```\n",
    "\n",
    "### 2. **RecursiveCharacterTextSplitter** â€“ ğŸ”„ğŸ§ \n",
    "Smart splitter! It tries to split on:\n",
    "- Paragraphs â†’ then\n",
    "- Sentences â†’ then\n",
    "- Words â†’ then\n",
    "- Characters\n",
    "\n",
    "This avoids breaking ideas mid-thought! Perfect for well-formatted documents.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_text(long_text)\n",
    "```\n",
    "\n",
    "### 3. **TokenTextSplitter** â€“ ğŸ§®\n",
    "Splits based on **token count** (great if you're hugging GPT models and want to control cost/token usage).\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "chunks = splitter.split_text(long_text)\n",
    "```\n",
    "\n",
    "> Pro Tip: Use this when youâ€™re working closely with model token limits (e.g., OpenAI/GPT-3.5/4).\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  Why Overlapping Chunks?\n",
    "\n",
    "Letâ€™s say the last sentence of Chunk A connects deeply with the first of Chunk B. Without overlap, the model might lose that connection.\n",
    "\n",
    "**Overlaps help LLMs â€œrememberâ€ the context better**, like connecting puzzle pieces ğŸ§©.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¼ Where Are Text Splitters Used in LangChain?\n",
    "\n",
    "Text splitters are often used in:\n",
    "\n",
    "- ğŸ” **Retrieval-based QA systems**\n",
    "- ğŸ“š **Document loaders** (PDFs, HTML, Markdown)\n",
    "- ğŸ’¾ **Vector databases** (like FAISS, Pinecone, etc.)\n",
    "- ğŸ¤– **Chatbots on custom data**\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“Œ Real-World Example:\n",
    "\n",
    "Say you have a 50-page company policy PDF.\n",
    "\n",
    "1. Load the PDF.\n",
    "2. Use a RecursiveCharacterTextSplitter to chunk the content.\n",
    "3. Store chunks in a vector DB (like FAISS).\n",
    "4. Let the AI search and answer queries based on **specific chunks**â€”super relevant, fast, and memory-efficient!\n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒˆ Final Thoughts\n",
    "\n",
    "Text splitters are like the **kitchen knives** in your AI toolkitâ€”cutting big, bulky information into easy-to-consume portions for your model chef ğŸ½ï¸ğŸ¤–.\n",
    "\n",
    "So next time your LLM complains about sizeâ€”donâ€™t panic. Just **split it, overlap it, and serve it smart!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ¨ Types of Text Splitters in LangChain\n",
    "\n",
    "LangChain doesn't just slice text randomlyâ€”it gives you **flavors** of splitters based on different **strategies and goals**. Hereâ€™s the extended menu:\n",
    "\n",
    "\n",
    "### 1. ğŸ”¢ **Length-Based Splitters**\n",
    "\n",
    "These splitters focus purely on the **length of text**â€”either characters, tokens, or words.\n",
    "\n",
    "#### ğŸ• Example: CharacterTextSplitter / TokenTextSplitter\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_text(long_text)\n",
    "```\n",
    "\n",
    "âœ… **Best for:** Simple, consistent splitting when format doesnâ€™t matter  \n",
    "ğŸš« **Downside:** Might split in the middle of a sentence or word\n",
    "\n",
    "\n",
    "\n",
    "### 2. ğŸ—ï¸ **Text-Structure Based Splitters**\n",
    "\n",
    "These are **smart splitters** like `RecursiveCharacterTextSplitter`, which try to preserve text structure: paragraphs â†’ sentences â†’ words â†’ characters.\n",
    "\n",
    "#### ğŸ° Example: RecursiveCharacterTextSplitter\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_text(long_text)\n",
    "```\n",
    "\n",
    "âœ… **Best for:** Articles, blogs, reports where formatting matters  \n",
    "ğŸ’¡ **Smart:** Maintains logical flow of ideas\n",
    "\n",
    "\n",
    "\n",
    "### 3. ğŸ“„ **Document-Structure Based Splitters**\n",
    "\n",
    "Tailored for structured formats like PDFs, HTML, Markdown, etc. These splitters use **headers, tags, or section markers** to split logically.\n",
    "\n",
    "#### ğŸ§¾ Example: MarkdownHeaderTextSplitter (splits by markdown headers like `#`, `##`, etc.)\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[('#', 'Header 1'), ('##', 'Header 2')])\n",
    "docs = markdown_splitter.split_text(markdown_text)\n",
    "```\n",
    "\n",
    "âœ… **Best for:** Markdown docs, PDFs with headings, structured reports  \n",
    "ğŸ“š **Great for:** Preserving document hierarchy (sections/subsections)\n",
    "\n",
    "\n",
    "\n",
    "### 4. ğŸ§  **Semantic Meaning-Based Splitters** (ğŸ’¥ Coming from embeddings)\n",
    "\n",
    "These split using the **meaning of the content**. It clusters or segments text based on **semantic similarity**, not just length or structure.\n",
    "\n",
    "> âš ï¸ These are *experimental* or built on top of LangChain using vector embeddings + clustering.\n",
    "\n",
    "#### ğŸ§¬ Example: (via external tools like sentence-transformers + clustering)\n",
    "\n",
    "```python\n",
    "# Pseudocode â€“ semantic split using embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "clusters = KMeans(n_clusters=5).fit_predict(embeddings)\n",
    "```\n",
    "\n",
    "âœ… **Best for:** Research papers, long blogs, where content shifts topically  \n",
    "ğŸ¤¯ **Powerful:** Groups text meaningfully even across different formats\n",
    "\n",
    "## ğŸ§ TL;DR Cheat Sheet:\n",
    "\n",
    "| Type                      | Splits Based On               | Best Use Case                      |\n",
    "|---------------------------|-------------------------------|-------------------------------------|\n",
    "| ğŸ”¢ Length-Based           | Character / token count       | Basic splits, control token limits  |\n",
    "| ğŸ—ï¸ Text-Structure Based   | Paragraphs, sentences, words  | Well-formatted natural text         |\n",
    "| ğŸ“„ Document-Structure Based| Headers, sections, tags       | Markdown, PDFs, HTML                |\n",
    "| ğŸ§  Semantic-Based          | Topic/meaning (via embeddings)| Smart clustering, deep documents    |\n",
    "\n",
    "\n",
    "\n",
    "## ğŸŒˆ Final Thoughts (Updated!)\n",
    "\n",
    "Text splitters are not just toolsâ€”theyâ€™re **story editors** for your AI. Whether you need:\n",
    "\n",
    "- Simple word counts ğŸ°  \n",
    "- Smart sentence breaks ğŸ§   \n",
    "- Heading-based sections ğŸ“„  \n",
    "- Or even idea-based grouping âœ¨  \n",
    "\n",
    "LangChainâ€™s got your back. So next time your document is too big for a modelâ€”just **split it smartly**, and let your AI shine! ğŸ’¡ğŸ¤–\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒŸ What are **Vector Stores** in LangChain?\n",
    "\n",
    "Imagine you're a **librarian**, but instead of sorting books by **title** or **author**, you're sorting **ideas**, **meanings**, or **concepts**. Sounds futuristic, right? ğŸš€  \n",
    "Thatâ€™s exactly what **Vector Stores** do in **LangChain**.\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ“¦ Real-Life Analogy:  \n",
    "Think of a **vector store** like a **magic filing cabinet** ğŸ“âœ¨.\n",
    "\n",
    "- Each **document**, **text chunk**, or **knowledge snippet** gets turned into a **vector** â€” a long list of numbers ğŸ§®.\n",
    "- These vectors donâ€™t just store text, they store **meaning**!\n",
    "- So when you ask a question, LangChain doesn't look for **exact matches** â€” it searches for **semantically similar** content ğŸ”ğŸ§ .\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ§± How it works step-by-step:\n",
    "\n",
    "1. **You add data** â†’ e.g., FAQs, articles, transcripts ğŸ“  \n",
    "2. LangChain uses an **embedding model** (like OpenAI or Hugging Face) to convert each piece into a vector ğŸ“Š  \n",
    "3. These vectors go into a **vector store** like:\n",
    "   - **FAISS** (fast & lightweight âš¡)\n",
    "   - **Pinecone** (cloud & scalable â˜ï¸)\n",
    "   - **Chroma**, **Weaviate**, **Qdrant**, and more! ğŸ§°\n",
    "4. When a user asks a question ğŸ¤”, LangChain:\n",
    "   - Converts that question into a vector too â¡ï¸ğŸ“ˆ\n",
    "   - Compares it to all the vectors in the store\n",
    "   - Returns the **most similar** ones â€” as if it found the most relevant \"pages\" ğŸ“šâœ¨\n",
    "\n",
    "### ğŸ§  Why Vector Stores Are Awesome\n",
    "\n",
    "| ğŸ’¥ Feature | ğŸŒˆ Why Itâ€™s Cool |\n",
    "|-----------|------------------|\n",
    "| ğŸ” Semantic Search | Finds **similar meaning** even if words differ |\n",
    "| ğŸš€ Speed | Vector search is optimized for **quick retrieval** |\n",
    "| ğŸ§  Context-Aware | Makes RAG (Retrieval Augmented Generation) smarter |\n",
    "| ğŸŒ Scalable | Works even with **millions** of docs! |\n",
    "\n",
    "\n",
    "### ğŸ§ª Example\n",
    "\n",
    "You have this document:\n",
    "\n",
    "> â€œThe sun is the star at the center of the solar system.â€\n",
    "\n",
    "You ask:\n",
    "\n",
    "> â€œWhat powers the solar system?â€\n",
    "\n",
    "Even though the question **never says \"sun\"**, the vector store knows the **meaning** matches â€” and gives you the right chunk. That's magic! ğŸŒğŸ’¡\n",
    "\n",
    "\n",
    "\n",
    "### ğŸ”— LangChain + Vector Stores = Smart RAG ğŸ§ ğŸ“š  \n",
    "Vector stores power LangChainâ€™s ability to **fetch relevant info**, give **contextual responses**, and build **intelligent chatbots** ğŸ¤–ğŸ’¬.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” What are Retrievers in LangChain?\n",
    "\n",
    "**Retrievers** are components in LangChain that help **fetch relevant information (documents or data chunks)** from a **knowledge base**, based on a user's query. \n",
    "\n",
    "> Think of retrievers as the \"search engine\" inside your AI application â€” they don't generate answers themselves but provide the **most relevant data** that the LLM can use to generate a response.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ’¡ Why Are Retrievers Needed?\n",
    "\n",
    "When you're building an app like:\n",
    "- A **chatbot for documentation**\n",
    "- A **support assistant with PDFs**\n",
    "- A **Q&A bot using internal data**\n",
    "\n",
    "You **canâ€™t fit all your documents into the prompt** because of token limits. Instead:\n",
    "1. Store all documents (as chunks) in a **vector store**.\n",
    "2. Use a **retriever** to fetch the top relevant chunks for any query.\n",
    "3. Pass only those to the LLM (like GPT) for generating answers.\n",
    "\n",
    "This is the essence of **Retrieval-Augmented Generation (RAG)**.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ”§ How Do Retrievers Work?\n",
    "\n",
    "### Step-by-step:\n",
    "\n",
    "1. **Ingest documents**: Break documents into smaller chunks using a **text splitter**.\n",
    "2. **Embed documents**: Use an **embedding model** (like OpenAI, HuggingFace, etc.) to convert text chunks into vectors.\n",
    "3. **Store embeddings**: Save them in a **vector store** like FAISS, Chroma, Pinecone, Weaviate, etc.\n",
    "4. **Retriever**: \n",
    "   - Converts the user query into an embedding.\n",
    "   - Finds the most **similar vector chunks** using similarity search or advanced retrieval logic.\n",
    "5. **LLM**: The relevant chunks are passed along with the user's query to generate a meaningful and accurate answer.\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§± Types of Retrievers in LangChain\n",
    "\n",
    "LangChain provides various retriever classes. Each has its own use case depending on your goal (accuracy, diversity, speed, etc.).\n",
    "\n",
    "\n",
    "\n",
    "### 1. **VectorStoreRetriever**\n",
    "- **Most common** and simple.\n",
    "- Uses similarity search to fetch the most relevant chunks from a vector store.\n",
    "- Ideal for standard RAG setups.\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embedding_model)\n",
    "retriever = vectorstore.as_retriever()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 2. **MMRRetriever (Max Marginal Relevance Retriever)**\n",
    "- **Smart retrieval** that balances:\n",
    "  - **Relevance to the query**\n",
    "  - **Diversity of results** (avoid redundancy)\n",
    "- Great for use cases where many chunks have similar content.\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"lambda_mult\": 0.5}\n",
    ")\n",
    "```\n",
    "\n",
    "- `k`: how many chunks to return.\n",
    "- `lambda_mult`: `0.0` = more diversity, `1.0` = more relevance.\n",
    "\n",
    "âœ… Use MMR if your current retriever is returning *too many similar-looking results*.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **MultiQueryRetriever**\n",
    "- Uses the LLM to generate **multiple variations** of the query.\n",
    "- Each variation is run through the retriever, and results are combined.\n",
    "- Useful for **query expansion** and getting a broader coverage of results.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=ChatOpenAI()\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 4. **ContextualCompressionRetriever**\n",
    "- Adds an **LLM-based filter** or **summarizer** over retrieved documents.\n",
    "- Helps reduce chunk size and keep only relevant parts â€” useful when close to **token limits**.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(ChatOpenAI())\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 5. **BM25Retriever**\n",
    "- **Traditional keyword-based search** using TF-IDF style scoring.\n",
    "- Doesnâ€™t use embeddings â€” fast and interpretable.\n",
    "- Works well when **keywords matter more than semantics**.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import BM25Retriever\n",
    "retriever = BM25Retriever.from_texts(texts)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 6. **EnsembleRetriever**\n",
    "- Combines multiple retrievers.\n",
    "  - For example, semantic search + keyword search.\n",
    "- You can assign weights to each retriever for balance.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "\n",
    "ensemble = EnsembleRetriever(\n",
    "    retrievers=[retriever1, retriever2],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§  Real-World Use Case: Chat with Your PDF\n",
    "\n",
    "Letâ€™s say you want to create a chatbot that answers questions based on a PDF manual.\n",
    "\n",
    "1. **Load** and **split** the PDF into chunks.\n",
    "2. **Embed** those chunks using a model like `OpenAIEmbeddings`.\n",
    "3. **Store** them in a vector store like `FAISS`.\n",
    "4. Use a **Retriever** (e.g., MMR or MultiQuery) to get top-matching chunks for the user query.\n",
    "5. Send those chunks + query to the LLM to generate a response.\n",
    "\n",
    "And boom ğŸ’¥ â€” youâ€™ve got your **RAG-powered PDF bot**!\n",
    "\n",
    "\n",
    "\n",
    "## ğŸ§ª Bonus Tip: Customize Retriever Settings\n",
    "\n",
    "You can customize how your retriever behaves:\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\n",
    "    \"k\": 4,  # number of chunks to retrieve\n",
    "    \"filter\": {\"source\": \"user_manual\"}  # filter using metadata\n",
    "})\n",
    "```\n",
    "\n",
    "## âœ… Summary Table\n",
    "\n",
    "| Retriever Type               | Use Case                                        | Uses Embeddings | Strength                  |\n",
    "|-----------------------------|--------------------------------------------------|------------------|---------------------------|\n",
    "| VectorStoreRetriever         | General RAG pipelines                           | âœ…               | Simple & effective        |\n",
    "| **MMRRetriever**             | Reduce redundancy in results                    | âœ…               | Relevance + Diversity     |\n",
    "| MultiQueryRetriever          | Broaden query coverage                          | âœ…               | Richer document retrieval |\n",
    "| ContextualCompressionRetriever | Stay within token limits, focus on relevance | âœ…               | Filtered concise results  |\n",
    "| BM25Retriever                | Exact keyword matching                          | âŒ               | Fast & interpretable      |\n",
    "| EnsembleRetriever            | Combine semantic + keyword search               | âœ… / âŒ           | Best of both worlds       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
