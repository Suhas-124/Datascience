{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy:\n",
    "\n",
    "**Accuracy** is one of the most commonly used **evaluation metrics** for classification models. It tells you how well your model is performing in terms of correctly predicting the labels (or classes) compared to all predictions made.\n",
    "\n",
    "Let me explain it step-by-step and in simple terms.\n",
    "\n",
    "\n",
    "\n",
    "## **What is Accuracy?**\n",
    "\n",
    "Accuracy is the **percentage of correct predictions** made by the model out of all the predictions. It is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\times 100\n",
    "$$\n",
    "\n",
    "In terms of **confusion matrix** (which we’ll explain shortly), it can also be written as:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **TP** = True Positive: Correctly predicted positive cases (e.g., predicted \"yes\" and the actual label was \"yes\").\n",
    "- **TN** = True Negative: Correctly predicted negative cases (e.g., predicted \"no\" and the actual label was \"no\").\n",
    "- **FP** = False Positive: Incorrectly predicted as positive (e.g., predicted \"yes\" but the actual label was \"no\").\n",
    "- **FN** = False Negative: Incorrectly predicted as negative (e.g., predicted \"no\" but the actual label was \"yes\").\n",
    "\n",
    "\n",
    "\n",
    "### **Simplified Example**\n",
    "\n",
    "Imagine you have a **binary classification problem** where you're trying to predict whether an email is **spam** or **not spam**.\n",
    "\n",
    "Let’s say you have 10 emails in your dataset, and the model makes predictions:\n",
    "\n",
    "| Email No. | Actual Label (True/False) | Predicted Label (True/False) |\n",
    "|-----------|---------------------------|-----------------------------|\n",
    "| 1         | True (Spam)                | True (Spam)                 |\n",
    "| 2         | False (Not Spam)           | False (Not Spam)            |\n",
    "| 3         | True (Spam)                | False (Not Spam)            |\n",
    "| 4         | False (Not Spam)           | True (Spam)                 |\n",
    "| 5         | True (Spam)                | True (Spam)                 |\n",
    "| 6         | False (Not Spam)           | False (Not Spam)            |\n",
    "| 7         | True (Spam)                | True (Spam)                 |\n",
    "| 8         | False (Not Spam)           | False (Not Spam)            |\n",
    "| 9         | True (Spam)                | False (Not Spam)            |\n",
    "| 10        | False (Not Spam)           | True (Spam)                 |\n",
    "\n",
    "\n",
    "Now let’s count the **True Positives (TP)**, **True Negatives (TN)**, **False Positives (FP)**, and **False Negatives (FN)**:\n",
    "\n",
    "- **True Positives (TP)**: 3 (Emails 1, 5, and 7 – correctly predicted as \"Spam\")\n",
    "- **True Negatives (TN)**: 3 (Emails 2, 6, and 8 – correctly predicted as \"Not Spam\")\n",
    "- **False Positives (FP)**: 2 (Emails 4 and 10 – incorrectly predicted as \"Spam\" but are \"Not Spam\")\n",
    "- **False Negatives (FN)**: 2 (Emails 3 and 9 – incorrectly predicted as \"Not Spam\" but are \"Spam\")\n",
    "\n",
    "### **Now, calculate accuracy:**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} = \\frac{3 + 3}{3 + 3 + 2 + 2} = \\frac{6}{10} = 0.6\n",
    "$$\n",
    "\n",
    "So, the accuracy of the model is **60%**.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations of Accuracy**\n",
    "\n",
    "While accuracy is easy to understand and widely used, it has some limitations, especially in imbalanced datasets:\n",
    "\n",
    "1. **Imbalanced Datasets**:  \n",
    "   If one class is much more frequent than the other (for example, in medical diagnoses, where you have many \"healthy\" samples and very few \"sick\" samples), the model can achieve a high accuracy by always predicting the majority class. This does not mean the model is performing well, especially for the minority class.\n",
    "\n",
    "   Example:  \n",
    "   If a model predicts 95% of the time that a patient is **healthy** (when the true label is **healthy**) and only 5% of the time predicts **sick** (when the true label is **sick**), then:\n",
    "   - If the dataset is mostly healthy people, the model may still have **high accuracy** but fail at detecting sick people.\n",
    "\n",
    "2. **Doesn't Reflect Model's Performance on Each Class**:  \n",
    "   Accuracy doesn’t tell you how well the model performs on each class (positive or negative). It just gives an overall measure.\n",
    "\n",
    "\n",
    "\n",
    "## **When is Accuracy Useful?**\n",
    "\n",
    "- **Balanced datasets**: Accuracy works well when the number of instances of each class is about the same.\n",
    "- **Quick benchmark**: Accuracy is often a quick and simple way to check how well your model is doing, especially for well-balanced datasets.\n",
    "\n",
    "\n",
    "\n",
    "## **Other Metrics to Consider**\n",
    "\n",
    "Because accuracy can be misleading in some cases, other classification metrics should also be considered, especially in imbalanced datasets. These include:\n",
    "\n",
    "1. **Precision**: The proportion of positive predictions that are actually correct (focuses on the positive class).\n",
    "2. **Recall** (Sensitivity): The proportion of actual positive cases that were correctly predicted (focuses on how well the model identifies the positive class).\n",
    "3. **F1 Score**: The harmonic mean of precision and recall, useful when you want a balance between precision and recall.\n",
    "4. **AUC-ROC**: Measures the trade-off between true positive rate (sensitivity) and false positive rate (1-specificity), particularly useful for binary classification.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Accuracy** = (Number of correct predictions) / (Total predictions).\n",
    "- Easy to compute and useful for balanced datasets.\n",
    "- It can be misleading in imbalanced datasets, so always consider other metrics like precision, recall, or F1 score, especially when one class is more important than the other.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix:\n",
    "\n",
    "A **confusion matrix** is a table that is used to evaluate the performance of a classification model. It shows how well the model's predictions match the actual results (or true labels). It helps you see where the model is making mistakes, and provides detailed insights into the types of errors it is making.\n",
    "\n",
    "\n",
    "\n",
    "### **Confusion Matrix Overview**\n",
    "\n",
    "A confusion matrix typically looks like this for a **binary classification** problem:\n",
    "\n",
    "|                | Predicted Positive (1) | Predicted Negative (0) |\n",
    "|----------------|------------------------|------------------------|\n",
    "| **Actual Positive (1)**  | True Positive (TP)   | False Negative (FN)     |\n",
    "| **Actual Negative (0)**  | False Positive (FP)  | True Negative (TN)      |\n",
    "\n",
    "Where:\n",
    "- **True Positive (TP)**: The number of correct predictions where the model predicted positive and the actual label was also positive.\n",
    "- **True Negative (TN)**: The number of correct predictions where the model predicted negative and the actual label was also negative.\n",
    "- **False Positive (FP)**: The number of incorrect predictions where the model predicted positive, but the actual label was negative.\n",
    "- **False Negative (FN)**: The number of incorrect predictions where the model predicted negative, but the actual label was positive.\n",
    "\n",
    "### **Understanding Each Term:**\n",
    "\n",
    "1. **True Positive (TP)**:  \n",
    "   These are the cases where the model correctly predicted the positive class. For example, if the task is to classify whether an email is \"spam\" or \"not spam,\" TP would be the number of times the model correctly classified spam emails as spam.\n",
    "\n",
    "2. **True Negative (TN)**:  \n",
    "   These are the cases where the model correctly predicted the negative class. Continuing with the spam example, TN would be the number of times the model correctly classified non-spam emails as non-spam.\n",
    "\n",
    "3. **False Positive (FP)**:  \n",
    "   These are the cases where the model incorrectly predicted the positive class. For example, if the model classified a non-spam email as spam, that would be a false positive.\n",
    "\n",
    "4. **False Negative (FN)**:  \n",
    "   These are the cases where the model incorrectly predicted the negative class. For example, if the model classified a spam email as non-spam, that would be a false negative.\n",
    "\n",
    "\n",
    "\n",
    "### **Confusion Matrix for Multiclass Classification:**\n",
    "\n",
    "For multiclass classification (more than two classes), the confusion matrix is larger. Here’s how it would look for a 3-class classification problem:\n",
    "\n",
    "|                | Predicted Class 1 | Predicted Class 2 | Predicted Class 3 |\n",
    "|----------------|-------------------|-------------------|-------------------|\n",
    "| **Actual Class 1** | TP1               | FP2               | FP3               |\n",
    "| **Actual Class 2** | FN1               | TP2               | FP3               |\n",
    "| **Actual Class 3** | FN1               | FN2               | TP3               |\n",
    "\n",
    "Where:\n",
    "- **TP1, TP2, TP3** represent true positives for each class.\n",
    "- **FP1, FP2, FP3** represent false positives for each class.\n",
    "- **FN1, FN2, FN3** represent false negatives for each class.\n",
    "\n",
    "Each row represents the actual class, and each column represents the predicted class. The diagonal elements (TP) show the correct classifications, while the off-diagonal elements (FP and FN) show the misclassifications.\n",
    "\n",
    "\n",
    "\n",
    "### **Metrics Derived from the Confusion Matrix**\n",
    "\n",
    "From the confusion matrix, we can calculate several important metrics to evaluate the performance of a classification model:\n",
    "\n",
    "1. **Accuracy**:  \n",
    "   Accuracy is the percentage of correct predictions made by the model. It can be calculated as:\n",
    "\n",
    "   $$\n",
    "   \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "   $$\n",
    "\n",
    "   This gives the overall correctness of the model's predictions.\n",
    "\n",
    "2. **Precision** (also called **Positive Predictive Value**):  \n",
    "   Precision is the proportion of true positives out of all predicted positives (how many of the predicted positives were actually positive).\n",
    "\n",
    "   $$\n",
    "   \\text{Precision} = \\frac{TP}{TP + FP}\n",
    "   $$\n",
    "\n",
    "   A higher precision means fewer false positives.\n",
    "\n",
    "3. **Recall** (also called **Sensitivity** or **True Positive Rate**):  \n",
    "   Recall is the proportion of true positives out of all actual positives (how many of the actual positives were correctly predicted).\n",
    "\n",
    "   $$\n",
    "   \\text{Recall} = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "\n",
    "   A higher recall means fewer false negatives.\n",
    "\n",
    "4. **F1 Score**:  \n",
    "   The F1 score is the harmonic mean of precision and recall. It balances both precision and recall, especially when the data is imbalanced.\n",
    "\n",
    "   $$\n",
    "   F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   $$\n",
    "\n",
    "   F1 score is useful when you need a balance between precision and recall.\n",
    "\n",
    "5. **Specificity** (also called **True Negative Rate**):  \n",
    "   Specificity measures how well the model can identify the negative class. It is the proportion of actual negatives that were correctly predicted.\n",
    "\n",
    "   $$\n",
    "   \\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "   $$\n",
    "\n",
    "6. **False Positive Rate (FPR)**:  \n",
    "   The false positive rate is the proportion of actual negatives that were incorrectly classified as positive.\n",
    "\n",
    "   $$\n",
    "   \\text{FPR} = \\frac{FP}{TN + FP}\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "### **Example: Confusion Matrix in Action**\n",
    "\n",
    "Let's say you're building a model to classify whether a patient has a disease (\"positive\" class) or not (\"negative\" class). After testing the model, you get the following confusion matrix:\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive**  | 50 (TP)           | 10 (FN)            |\n",
    "| **Actual Negative**  | 5 (FP)            | 100 (TN)           |\n",
    "\n",
    "Now you can calculate the metrics:\n",
    "\n",
    "- **Accuracy** = $\\frac{50 + 100}{50 + 100 + 5 + 10} = \\frac{150}{165} \\approx 0.91 \\text{ or } 91\\%$\n",
    "- **Precision** = $\\frac{50}{50 + 5} = \\frac{50}{55} \\approx 0.91$\n",
    "- **Recall** = $\\frac{50}{50 + 10} = \\frac{50}{60} \\approx 0.83$\n",
    "- **F1 Score** = $2 \\times \\frac{0.91 \\times 0.83}{0.91 + 0.83} \\approx 0.87$\n",
    "\n",
    "This shows that your model is **accurate** 91% of the time, but its **recall** is a bit lower (83%), meaning it's missing some positive cases (false negatives). The **precision** is high (91%), meaning most of the time when it predicts positive, it’s correct.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary:**\n",
    "- A **confusion matrix** is a table that shows the performance of a classification model.\n",
    "- It helps you see the number of true positives, true negatives, false positives, and false negatives.\n",
    "- From this matrix, you can calculate important metrics like accuracy, precision, recall, and F1 score to evaluate the model’s performance.\n",
    "- It gives you a clear view of the errors the model is making and helps you understand how well it is performing.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision:\n",
    "\n",
    "\n",
    "## **1. Confusion Matrix Overview**\n",
    "A confusion matrix is a table that describes the performance of a classification model by comparing predicted and actual outcomes.\n",
    "\n",
    "For a **binary classification problem**, the confusion matrix looks like this:\n",
    "\n",
    "|                        | **Predicted Positive** | **Predicted Negative** |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| **Actual Positive**    | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative**    | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "- **True Positive (TP):** Correctly predicted positive class.  \n",
    "- **True Negative (TN):** Correctly predicted negative class.  \n",
    "- **False Positive (FP):** Incorrectly predicted as positive (Type I error).  \n",
    "- **False Negative (FN):** Incorrectly predicted as negative (Type II error).\n",
    "\n",
    "\n",
    "\n",
    "## **2. Precision: Definition**\n",
    "**Precision** answers the question: *\"Out of all the predicted positive cases, how many are actually correct?\"*\n",
    "\n",
    "It focuses on the **quality** of positive predictions made by the model.\n",
    "\n",
    "The formula for **precision** is:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP)} + \\text{False Positives (FP)}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **3. How Precision Works (In Layman Terms)**\n",
    "\n",
    "- Imagine you are a doctor, and you **predict patients have a disease**.  \n",
    "- Precision measures **how many of your positive diagnoses were actually correct**.  \n",
    "- If you predict 10 patients have the disease, but only 7 really do, the precision is 7/10 = 0.7 (or 70%).\n",
    "\n",
    "\n",
    "\n",
    "## **4. Precision Example**\n",
    "Let’s say a model predicts 100 positive cases, but only 80 of those are correct.  \n",
    "\n",
    "| **TP** = 80 | **FP** = 20 |\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP} = \\frac{80}{80 + 20} = 0.8 \\, (80\\%)\n",
    "$$\n",
    "\n",
    "This means 80% of the time, when the model predicts \"positive,\" it is correct.\n",
    "\n",
    "\n",
    "\n",
    "## **5. Precision vs Recall**\n",
    "\n",
    "While precision measures the **accuracy of positive predictions**, **recall** focuses on the ability to **find all positive cases**.\n",
    "\n",
    "- **Precision**: Out of all the positive predictions, how many are correct?  \n",
    "- **Recall**: Out of all actual positives, how many did we correctly predict?  \n",
    "\n",
    "Example:  \n",
    "- If you predict a disease in 7 out of 10 patients, but there are **20 actual patients** with the disease, your precision might be high, but recall will be low.\n",
    "\n",
    "\n",
    "\n",
    "## **6. When to Use Precision?**\n",
    "- Precision is important when **False Positives** (incorrect positive predictions) are costly.  \n",
    "- Example scenarios:\n",
    "   - **Spam Detection**: You don’t want to label important emails as spam (high precision).  \n",
    "   - **Fraud Detection**: You don’t want to wrongly accuse innocent people of fraud.\n",
    "\n",
    "\n",
    "\n",
    "## **7. Relation to Confusion Matrix**\n",
    "From the confusion matrix:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP + FP}}\n",
    "$$\n",
    "\n",
    "You use **True Positives** (correct predictions) and **False Positives** (incorrect positive predictions) to calculate precision.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary Table**\n",
    "\n",
    "| **Metric**         | **Formula**                      | **Focus**                          | **Question Answered**                |\n",
    "|---------------------|----------------------------------|-----------------------------------|--------------------------------------|\n",
    "| **Precision**       | $ \\frac{TP}{TP + FP} $        | Quality of positive predictions    | \"How accurate are the positive predictions?\" |\n",
    "| **Recall**          | $ \\frac{TP}{TP + FN} $        | Finding all actual positives       | \"How many of the actual positives were found?\" |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall:\n",
    "\n",
    "\n",
    "\n",
    "## **What is Recall?**  \n",
    "**Recall** answers the question:  \n",
    "> *\"Out of all the actual positive cases, how many did the model correctly predict?\"*\n",
    "\n",
    "It focuses on **finding all the positive cases** and checks if the model missed any.\n",
    "\n",
    "\n",
    "\n",
    "## **Imagine This Scenario:**  \n",
    "You are a **doctor** who has to identify patients with a serious disease.  \n",
    "\n",
    "- Some patients truly have the disease (**actual positives**).  \n",
    "- Your job is to **correctly find as many of these patients as possible**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Layman Example**\n",
    "\n",
    "Suppose there are **10 patients** with the disease, but you test 10 people.  \n",
    "\n",
    "| **Actual Patient Status** | **Your Prediction** | **Result**        |\n",
    "|---------------------------|---------------------|-------------------|\n",
    "| Sick                      | Sick                | ✅ True Positive (TP) |\n",
    "| Sick                      | Sick                | ✅ TP             |\n",
    "| Sick                      | Not Sick            | ❌ False Negative (FN) |\n",
    "| Sick                      | Sick                | ✅ TP             |\n",
    "| Sick                      | Not Sick            | ❌ FN             |\n",
    "| Sick                      | Sick                | ✅ TP             |\n",
    "| Sick                      | Sick                | ✅ TP             |\n",
    "| Sick                      | Not Sick            | ❌ FN             |\n",
    "| Sick                      | Sick                | ✅ TP             |\n",
    "| Sick                      | Not Sick            | ❌ FN             |\n",
    "\n",
    "\n",
    "\n",
    "### **Counting the Results:**\n",
    "- **True Positives (TP):** You correctly predicted 6 patients as sick.  \n",
    "- **False Negatives (FN):** You missed 4 patients (incorrectly said they are “not sick”).  \n",
    "\n",
    "\n",
    "\n",
    "### **Recall Formula**  \n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives (TP) + False Negatives (FN)}}\n",
    "$$\n",
    "\n",
    "In our example:  \n",
    "$$\n",
    "\\text{Recall} = \\frac{6}{6 + 4} = \\frac{6}{10} = 0.6 \\, (or 60\\%).\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **What Does 60% Recall Mean?**\n",
    "It means you **correctly identified 60% of all the sick patients**. However, you missed 40% of the actual positives (false negatives).  \n",
    "\n",
    "\n",
    "\n",
    "## **Key Analogy for Recall**  \n",
    "Imagine you are a **fire alarm** system:  \n",
    "- **Actual Fire** = A fire is happening (**positive case**).  \n",
    "- **Alarm Rings** = You predict \"fire\" (**positive prediction**).  \n",
    "\n",
    "**Recall** measures how many **actual fires** you detected.  \n",
    "\n",
    "- If the fire alarm **fails to ring** when there’s fire (false negatives), your **recall** goes down.  \n",
    "- High recall means you **catch almost all fires**, even if the alarm occasionally rings for no fire (false positives).\n",
    "\n",
    "\n",
    "\n",
    "## **Relation to Confusion Matrix**\n",
    "\n",
    "From the confusion matrix:\n",
    "\n",
    "|                        | **Predicted Positive** | **Predicted Negative** |\n",
    "|------------------------|------------------------|------------------------|\n",
    "| **Actual Positive**    | True Positive (TP)     | False Negative (FN)    |\n",
    "| **Actual Negative**    | False Positive (FP)    | True Negative (TN)     |\n",
    "\n",
    "**Recall** focuses on the row of **Actual Positives** and checks:  \n",
    "*\"Out of all actual positives, how many did we correctly predict?\"*\n",
    "\n",
    "\n",
    "\n",
    "## **When to Use Recall?**  \n",
    "**Recall** is very important when missing positive cases (false negatives) is costly.  \n",
    "\n",
    "Examples:  \n",
    "1. **Medical Tests:** You want to catch **all sick patients**. Missing someone (false negative) could be life-threatening.  \n",
    "2. **Fraud Detection:** Better to flag **all possible fraud cases** rather than miss any.  \n",
    "3. **Search Engines:** You want to show **all relevant results**. Missing results frustrates users.\n",
    "\n",
    "\n",
    "\n",
    "## **Precision vs Recall**  \n",
    "\n",
    "| **Metric**       | **Focus**                       | **Formula**                       | **Question Answered**              |\n",
    "|-------------------|---------------------------------|-----------------------------------|------------------------------------|\n",
    "| **Precision**     | Quality of positive predictions | $ \\frac{TP}{TP + FP} $          | \"How accurate are my positive predictions?\" |\n",
    "| **Recall**        | Finding all actual positives    | $ \\frac{TP}{TP + FN} $          | \"Did I miss any actual positives?\" |\n",
    "\n",
    "\n",
    "\n",
    "## **Summary (Simplified)**  \n",
    "- **Precision** = Out of all predicted positives, how many are correct?  \n",
    "- **Recall** = Out of all actual positives, how many did we find?  \n",
    "\n",
    "\n",
    "\n",
    "### **Analogy Recap**:  \n",
    "- Precision: \"How many apples in my basket are real apples?\"  \n",
    "- Recall: \"Did I pick up **all the apples** from the tree?\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision vs Recall:\n",
    "\n",
    "Sure! Let’s clarify **Recall** and **Precision** using very simple examples that anyone can relate to.\n",
    "\n",
    "\n",
    "\n",
    "### **Big Picture**  \n",
    "- **Precision** is about being *careful* and *accurate* when you say something is positive.  \n",
    "- **Recall** is about **not missing anything important** and finding all positive cases.\n",
    "\n",
    "\n",
    "\n",
    "## **The Story of a Doctor (Medical Test Analogy)**  \n",
    "Imagine you are a doctor who tests patients for a **disease** (positive cases).\n",
    "\n",
    "- **Precision** → Focuses on how accurate you are **when you say someone has the disease**.  \n",
    "- **Recall** → Focuses on how well you find **all patients with the disease**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Precision = Quality of Positive Predictions**  \n",
    "Imagine you test **100 people** for a disease.  \n",
    "- You say **10 people** have the disease.  \n",
    "- But out of those 10, **only 6 really have the disease**.  \n",
    "\n",
    "**Precision** asks:  \n",
    "*\"Out of the people I said are sick, how many did I get right?\"*\n",
    "\n",
    "\\[\n",
    "\\text{Precision} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives + False Positives}}\n",
    "\\]  \n",
    "\n",
    "Here:  \n",
    "- **True Positives (TP):** 6 people who actually have the disease and were predicted correctly.  \n",
    "- **False Positives (FP):** 4 people who don’t have the disease but you said they do.  \n",
    "\n",
    "**Precision = 6 / (6 + 4) = 60%.**  \n",
    "It means 60% of the time, you were correct when predicting someone as sick.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Recall = Ability to Find All Positives**  \n",
    "Now imagine there are actually **20 people** who have the disease.  \n",
    "- You correctly identified **6** people as sick.  \n",
    "- But you **missed 14 sick people**.  \n",
    "\n",
    "**Recall** asks:  \n",
    "*\"Out of all the sick people, how many did I find?\"*  \n",
    "\n",
    "\\[\n",
    "\\text{Recall} = \\frac{\\text{True Positives (TP)}}{\\text{True Positives + False Negatives}}\n",
    "\\]  \n",
    "\n",
    "Here:  \n",
    "- **True Positives (TP):** 6 people you predicted correctly as sick.  \n",
    "- **False Negatives (FN):** 14 people who were sick but you missed them.\n",
    "\n",
    "**Recall = 6 / (6 + 14) = 30%.**  \n",
    "It means you only found **30%** of the actual sick people.\n",
    "\n",
    "\n",
    "\n",
    "## **Layman-Friendly Analogy: Fishing**\n",
    "\n",
    "Imagine you’re fishing for **golden fish** (positive cases) in a big lake.\n",
    "\n",
    "### **Precision** (Quality of Caught Fish):\n",
    "- Precision = *How many of the fish you caught are actually golden fish?*  \n",
    "- If you only catch golden fish and no regular fish, you have **high precision**.\n",
    "\n",
    "Example:  \n",
    "- You caught **10 fish**, and **8 are golden fish**. Precision = 8/10 = 80%.  \n",
    "- **Mistake:** If you catch too many **wrong fish** (regular fish), precision goes down.\n",
    "\n",
    "\n",
    "\n",
    "### **Recall** (Catching All the Golden Fish):\n",
    "- Recall = *Did you catch **all the golden fish** in the lake?*  \n",
    "- If you miss a lot of golden fish, you have **low recall**.\n",
    "\n",
    "Example:  \n",
    "- There are **20 golden fish** in the lake, but you only caught **8**. Recall = 8/20 = 40%.  \n",
    "- **Mistake:** Missing golden fish lowers recall, even if the ones you caught are correct.\n",
    "\n",
    "## **Precision vs Recall Summary**\n",
    "\n",
    "| **Metric**    | **What It Measures**                      | **Layman Explanation**                              |\n",
    "|---------------|-------------------------------------------|----------------------------------------------------|\n",
    "| **Precision** | Quality of positive predictions           | \"Of the fish I caught, how many are golden fish?\"  |\n",
    "| **Recall**    | Ability to find all actual positives      | \"Did I catch all the golden fish in the lake?\"     |\n",
    "\n",
    "\n",
    "\n",
    "### **When to Focus on Precision or Recall?**\n",
    "\n",
    "- **Precision Matters** when **false positives** are costly.  \n",
    "  - Example: Spam emails → You don’t want important emails to go to spam.  \n",
    "\n",
    "- **Recall Matters** when **missing positives** is costly.  \n",
    "  - Example: Medical tests → You don’t want to miss sick patients.\n",
    "\n",
    "\n",
    "\n",
    "## **Quick Example Recap**\n",
    "- Precision: “How accurate was I in catching golden fish?”  \n",
    "- Recall: “Did I catch all the golden fish, or did I miss some?”\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score:\n",
    "\n",
    "### **F1 Score Explained in Simple Layman Terms**\n",
    "\n",
    "The **F1 score** is a metric used to evaluate a classification model by combining **Precision** and **Recall** into a single value. It is especially useful when you want to find a balance between these two metrics.\n",
    "\n",
    "\n",
    "\n",
    "## **Why Do We Need F1 Score?**\n",
    "\n",
    "- Precision and Recall sometimes **conflict**:  \n",
    "  - If Precision is high, Recall might be low, and vice versa.  \n",
    "- The F1 score helps you get a **single score** to measure overall performance.  \n",
    "- It’s very helpful when **classes are imbalanced** (e.g., one class has very few samples compared to the other).\n",
    "\n",
    "\n",
    "\n",
    "## **F1 Score Formula**\n",
    "\n",
    "The F1 score is the **harmonic mean** of Precision and Recall.  \n",
    "It gives equal importance to both.\n",
    "\n",
    "\\[\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "## **Layman Analogy: Balancing Cooking Time and Taste**  \n",
    "\n",
    "Imagine you are a **chef** preparing a dish:  \n",
    "- **Precision** → Focuses on getting the **taste perfect**.  \n",
    "- **Recall** → Focuses on **cooking the food completely**.  \n",
    "\n",
    "If you:  \n",
    "- Focus too much on taste (**Precision**), you might not cook everything properly.  \n",
    "- Focus too much on cooking everything thoroughly (**Recall**), you might ruin the taste.\n",
    "\n",
    "The **F1 score** finds a **balance** between cooking the food completely and ensuring it tastes great.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step Example**\n",
    "\n",
    "Let’s say you are a **spam email classifier**:  \n",
    "- **True Positives (TP):** Correctly identified spam emails.  \n",
    "- **False Positives (FP):** Emails incorrectly marked as spam (but they are not spam).  \n",
    "- **False Negatives (FN):** Spam emails that were missed (not detected as spam).  \n",
    "\n",
    "### 1. **Precision**  \n",
    "“How accurate are you when you say an email is spam?”  \n",
    "\\[\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "\n",
    "### 2. **Recall**  \n",
    "“How well did you find all the spam emails?”  \n",
    "\\[\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "\n",
    "### 3. **F1 Score Calculation**  \n",
    "The F1 score combines both Precision and Recall:  \n",
    "\\[\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "\n",
    "\n",
    "### **Example Calculation**  \n",
    "\n",
    "Suppose:  \n",
    "- **Precision = 0.8** (80%) → Out of all emails flagged as spam, 80% were correct.  \n",
    "- **Recall = 0.6** (60%) → Out of all actual spam emails, you detected 60%.  \n",
    "\n",
    "The F1 score is:  \n",
    "\\[\n",
    "F1 = 2 \\times \\frac{0.8 \\times 0.6}{0.8 + 0.6} = 2 \\times \\frac{0.48}{1.4} = 0.685 \\, (68.5\\%)\n",
    "\\]\n",
    "\n",
    "This means the F1 score balances Precision and Recall to give a combined performance measure.\n",
    "\n",
    "\n",
    "\n",
    "## **Key Points of F1 Score**  \n",
    "\n",
    "1. **When is it useful?**  \n",
    "   - When **false positives (Precision)** and **false negatives (Recall)** are equally important.  \n",
    "   - When the dataset is **imbalanced** (e.g., very few spam emails in a large inbox).  \n",
    "\n",
    "2. **Why is it a harmonic mean?**  \n",
    "   - The harmonic mean penalizes extreme values.  \n",
    "   - If Precision or Recall is very low, the F1 score will also be low.  \n",
    "   - For example:  \n",
    "     - If Precision = 100% but Recall = 0%, F1 score = 0%.  \n",
    "\n",
    "3. **Balanced Performance:**  \n",
    "   - F1 score lies between 0 and 1.  \n",
    "   - Higher F1 score = Better balance between Precision and Recall.\n",
    "\n",
    "## **Precision vs Recall vs F1 Score**\n",
    "\n",
    "| **Metric**      | **Focus**                                  | **When to Use**                      |  \n",
    "|------------------|-------------------------------------------|-------------------------------------|  \n",
    "| **Precision**   | Being accurate when predicting positives  | When false positives are costly.    |  \n",
    "| **Recall**      | Finding all actual positive cases         | When missing positives is costly.   |  \n",
    "| **F1 Score**    | Balance between Precision and Recall      | When you need an overall metric.    |\n",
    "\n",
    "\n",
    "\n",
    "### **Quick Example Recap**  \n",
    "Imagine a spam filter:  \n",
    "- **Precision:** Of all flagged emails, how many are actually spam?  \n",
    "- **Recall:** Did I find all the spam emails?  \n",
    "- **F1 Score:** How well did I balance finding all spam emails and avoiding mistakes?  \n",
    "\n",
    "\n",
    "\n",
    "## **Final Words**  \n",
    "The F1 score is like a **balanced report card** for a model. It considers both how accurate you are (Precision) and how many positives you found (Recall). When these two metrics are equally important, the F1 score is your best friend!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of F1 Score:\n",
    "\n",
    "Let me simplify the **F1 score** even further for you in **super simple layman terms**:\n",
    "\n",
    "\n",
    "\n",
    "### **What is F1 Score?**  \n",
    "The **F1 score** is like a **balance scale** that combines two important things:  \n",
    "\n",
    "1. **Precision** → How accurate are you when you say \"this is correct\"?  \n",
    "2. **Recall** → How good are you at finding *all the correct answers*?  \n",
    "\n",
    "**F1 score** gives you a single score to see how well your model is doing **overall**.\n",
    "\n",
    "\n",
    "\n",
    "### **Super Simple Analogy**  \n",
    "Imagine you are **fishing** in a pond full of fish (positive cases) and some trash (negative cases).  \n",
    "\n",
    "- **Precision**: Out of all the things you caught, how many are actually fish?  \n",
    "   - If you catch 10 things, and 8 are fish → Precision = 8/10 = 80%.  \n",
    "- **Recall**: Out of all the fish in the pond, how many did you catch?  \n",
    "   - If there are 20 fish and you caught 8 → Recall = 8/20 = 40%.  \n",
    "\n",
    "Now:  \n",
    "- If you focus **only** on catching fish accurately (high Precision), you might **miss many fish**.  \n",
    "- If you focus **only** on catching all the fish (high Recall), you might **catch a lot of trash too**.  \n",
    "\n",
    "The **F1 score** is the **balance** between these two.  \n",
    "It’s like saying, “How good am I at catching fish accurately **while also catching as many fish as possible**?”\n",
    "\n",
    "\n",
    "\n",
    "### **Example with Numbers**  \n",
    "\n",
    "- **Precision** = 80% → Out of what I said are fish, 80% were correct.  \n",
    "- **Recall** = 40% → Out of all fish, I only caught 40%.  \n",
    "\n",
    "To balance them, we calculate the **F1 score**:  \n",
    "$$\n",
    "F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "For our example:  \n",
    "$$\n",
    "F1 = 2 \\times \\frac{0.8 \\times 0.4}{0.8 + 0.4} = 2 \\times \\frac{0.32}{1.2} = 0.53 \\, (53\\%)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Key Point:**  \n",
    "If either **Precision** or **Recall** is low, the F1 score will also be low.  \n",
    "- You need **both to be good** to have a high F1 score.\n",
    "\n",
    "\n",
    "\n",
    "### **Why is F1 Score Important?**  \n",
    "It helps answer:  \n",
    "- Am I **accurate** (Precision) when predicting?  \n",
    "- Did I **miss anything important** (Recall)?  \n",
    "- How well am I doing overall?  \n",
    "\n",
    "F1 score balances these two and gives you a **single performance number** to judge your model.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Precision, Recall, and F1-Score for Multi-Class Classification**\n",
    "\n",
    "In **multi-class classification**, instead of just two classes (like spam vs. not spam), you have **more than two categories**. For example:  \n",
    "- Predicting a fruit as **apple, orange, or banana**.  \n",
    "- Classifying emails as **spam, promotions, or primary**.\n",
    "\n",
    "The challenge is to calculate **Precision**, **Recall**, and **F1-Score** for **each class**, and then combine the results.\n",
    "\n",
    "\n",
    "\n",
    "## **Concept Recap**\n",
    "\n",
    "1. **Precision** → How many predicted \"X\" are actually \"X\"?  \n",
    "2. **Recall** → How many actual \"X\" did we correctly predict?  \n",
    "3. **F1-Score** → Balance between Precision and Recall.  \n",
    "\n",
    "In **multi-class classification**, we calculate these metrics **for each class** and combine them in one of two main ways:  \n",
    "1. **Macro-averaging**  \n",
    "2. **Weighted-averaging**\n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step Explanation**\n",
    "\n",
    "### **1. Class-Wise Metrics (One-vs-Rest Approach)**  \n",
    "For each class, treat it as **positive** and the rest of the classes as **negative**. Then calculate:  \n",
    "\n",
    "- **True Positives (TP):** Correctly predicted for this class.  \n",
    "- **False Positives (FP):** Predicted as this class but incorrect.  \n",
    "- **False Negatives (FN):** Missed predictions for this class.  \n",
    "\n",
    "Repeat this for **every class**.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Combining Metrics**\n",
    "\n",
    "#### **Macro-Averaging**  \n",
    "- Calculate **Precision**, **Recall**, and **F1-Score** **separately for each class**.  \n",
    "- Take the **average** of these scores across all classes.  \n",
    "\n",
    "**Example**:  \n",
    "If you have 3 classes:  \n",
    "- Precision for Class 1 = 0.8  \n",
    "- Precision for Class 2 = 0.6  \n",
    "- Precision for Class 3 = 0.9  \n",
    "\n",
    "**Macro Precision** = $ \\frac{0.8 + 0.6 + 0.9}{3} = 0.77 $  \n",
    "\n",
    "This gives **equal importance to all classes**, regardless of their size.\n",
    "\n",
    "\n",
    "\n",
    "#### **Weighted-Averaging**  \n",
    "- Calculate Precision, Recall, and F1-Score **separately for each class**.  \n",
    "- Weight each class's score by the **number of samples** in that class.  \n",
    "\n",
    "**Example**:  \n",
    "Suppose you have:  \n",
    "- Class 1: Precision = 0.8, 50 samples  \n",
    "- Class 2: Precision = 0.6, 30 samples  \n",
    "- Class 3: Precision = 0.9, 20 samples  \n",
    "\n",
    "**Weighted Precision**:  \n",
    "$$\n",
    "\\text{Weighted Precision} = \\frac{(0.8 \\times 50) + (0.6 \\times 30) + (0.9 \\times 20)}{50 + 30 + 20}\n",
    "$$\n",
    "$$\n",
    "= \\frac{40 + 18 + 18}{100} = 0.76\n",
    "$$\n",
    "\n",
    "This gives **more importance to larger classes**.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Micro-Averaging**  \n",
    "- Combine all **TPs, FPs, and FNs** across classes first.  \n",
    "- Then calculate Precision, Recall, and F1-Score using the combined counts.  \n",
    "\n",
    "**Example**:  \n",
    "- Class 1: TP = 40, FP = 10, FN = 5  \n",
    "- Class 2: TP = 30, FP = 20, FN = 15  \n",
    "- Class 3: TP = 20, FP = 5, FN = 10  \n",
    "\n",
    "**Total TP** = $ 40 + 30 + 20 = 90 $  \n",
    "**Total FP** = $ 10 + 20 + 5 = 35 $  \n",
    "**Total FN** = $ 5 + 15 + 10 = 30 $  \n",
    "\n",
    "**Micro Precision**:  \n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{Total TP}}{\\text{Total TP} + \\text{Total FP}} = \\frac{90}{90 + 35} = 0.72\n",
    "$$\n",
    "\n",
    "**Micro Recall**:  \n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{Total TP}}{\\text{Total TP} + \\text{Total FN}} = \\frac{90}{90 + 30} = 0.75\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Choosing Between Macro, Weighted, and Micro**\n",
    "\n",
    "- **Macro-Averaging**:  \n",
    "   Use when all classes are equally important, even if some are small.  \n",
    "\n",
    "- **Weighted-Averaging**:  \n",
    "   Use when larger classes should have more impact on the overall score.  \n",
    "\n",
    "- **Micro-Averaging**:  \n",
    "   Use when you care about overall performance **across all classes**, especially with imbalanced data.  \n",
    "\n",
    "## **Summary Table**\n",
    "\n",
    "| Metric               | Explanation                                 | Use Case                           |  \n",
    "|-----------------------|---------------------------------------------|------------------------------------|  \n",
    "| **Macro-Averaging**   | Average Precision/Recall for all classes    | All classes are equally important. |  \n",
    "| **Weighted-Averaging**| Weighted average (based on class size)      | Classes have different sample sizes. |  \n",
    "| **Micro-Averaging**   | Global Precision/Recall across all classes  | Focus on overall performance.      |\n",
    "\n",
    "\n",
    "\n",
    "### **Simple Example Recap**  \n",
    "\n",
    "Imagine you have 3 classes of animals:  \n",
    "1. **Cats**  \n",
    "2. **Dogs**  \n",
    "3. **Birds**  \n",
    "\n",
    "For **each class**, calculate:  \n",
    "- Precision → How many predicted “cats” are actually cats?  \n",
    "- Recall → How many real “cats” did you find?  \n",
    "- F1-Score → Balance between these two.  \n",
    "\n",
    "Then combine these results using **Macro**, **Weighted**, or **Micro averaging** to get the overall score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Multi Class Classification:\n",
    "\n",
    "### **Multi-Class Classification in Simple Layman Terms**\n",
    "\n",
    "Imagine you are a **teacher** grading a test where students must pick one fruit from:  \n",
    "1. **Apple**  \n",
    "2. **Orange**  \n",
    "3. **Banana**  \n",
    "\n",
    "Now, when you check their answers:  \n",
    "- Some students correctly picked the fruit (✅).  \n",
    "- Some picked the wrong fruit (❌).  \n",
    "\n",
    "This is **multi-class classification** — you’re trying to **predict one category** (Apple, Orange, or Banana) out of **three or more possible categories**.\n",
    "\n",
    "\n",
    "\n",
    "### **How It Works**  \n",
    "\n",
    "1. **The Problem**:  \n",
    "   Instead of predicting **yes/no** (like spam vs. not spam), you now have to choose from **multiple classes**.  \n",
    "\n",
    "   Example: Predict the fruit — **Apple, Orange, or Banana**.\n",
    "\n",
    "2. **Model Prediction**:  \n",
    "   A machine learning model will look at features (like **color, size, or shape**) and assign a **probability** for each class.  \n",
    "\n",
    "   For example:  \n",
    "   - Apple → 80% chance  \n",
    "   - Orange → 15% chance  \n",
    "   - Banana → 5% chance  \n",
    "\n",
    "   The model predicts **Apple** because it has the highest probability.\n",
    "\n",
    "3. **Confusion Matrix**:  \n",
    "   For multi-class problems, the confusion matrix will show results for all classes:  \n",
    "   - How many times the model **correctly** predicted each class.  \n",
    "   - How many times it **confused** one class with another.  \n",
    "\n",
    "\n",
    "\n",
    "### **Metrics for Multi-Class Classification**  \n",
    "\n",
    "To evaluate how good the predictions are, we use metrics like:  \n",
    "\n",
    "1. **Precision** → Of all the times the model said \"Apple,\" how many were actually \"Apple\"?  \n",
    "2. **Recall** → Of all the actual Apples, how many did the model correctly predict as \"Apple\"?  \n",
    "3. **F1-Score** → A balance between Precision and Recall.  \n",
    "\n",
    "These metrics are calculated for **each class** (Apple, Orange, and Banana) **separately**.\n",
    "\n",
    "\n",
    "\n",
    "### **How Results Are Combined**  \n",
    "\n",
    "We combine the results for all classes in three ways:\n",
    "\n",
    "1. **Macro-Average** → Treat all classes equally. Average the metrics for Apple, Orange, and Banana.  \n",
    "\n",
    "2. **Weighted-Average** → Give more importance to classes with more examples. For example, if you have **100 Apples** and only **10 Bananas**, Apples will impact the result more.  \n",
    "\n",
    "3. **Micro-Average** → Add up all correct predictions and errors across all classes, then calculate overall Precision and Recall.\n",
    "\n",
    "\n",
    "\n",
    "### **Simple Example**  \n",
    "\n",
    "**Task**: Predict the animal from 3 classes — **Cat, Dog, Bird**.  \n",
    "\n",
    "**Results**:  \n",
    "- Cats → Predicted correctly 90 times, 10 times wrong.  \n",
    "- Dogs → Predicted correctly 70 times, 30 times wrong.  \n",
    "- Birds → Predicted correctly 50 times, 50 times wrong.  \n",
    "\n",
    "You calculate:  \n",
    "- Precision, Recall, and F1 for **Cats**, **Dogs**, and **Birds**.  \n",
    "- Combine the results using **Macro, Weighted, or Micro averaging** to get one final score.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Point to Remember**  \n",
    "\n",
    "- Multi-class classification is just like grading a test where answers can belong to **more than two categories**.  \n",
    "- The model picks the class with the **highest probability**.  \n",
    "- We calculate metrics **for each class** and combine them for the overall performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
