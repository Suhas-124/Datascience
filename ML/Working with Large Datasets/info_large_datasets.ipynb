{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 **Out-of-Core Learning in Machine Learning (Full Explanation)**\n",
    "\n",
    "Out-of-core learning is a technique used to train machine learning models on **large datasets that cannot fit into memory (RAM)**. Instead of loading the entire dataset at once, the data is processed in smaller **batches** or **chunks**.\n",
    "\n",
    "Think of it like **streaming a video online** rather than downloading the entire file first. The model learns from data in chunks, processes each batch, and updates itself without keeping all the data in memory.\n",
    "\n",
    "\n",
    "\n",
    "### 📌 **Why Use Out-of-Core Learning?**\n",
    "\n",
    "1. **When datasets are too large to fit in RAM.**  \n",
    "   For example, if you have a **500GB dataset** and your system has **16GB of RAM**, you need out-of-core learning to process the data efficiently.\n",
    "\n",
    "2. **When working with real-time data streams.**  \n",
    "   For instance, processing live data from sensors, social media feeds, or stock prices.\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 **How Does Out-of-Core Learning Work?**\n",
    "\n",
    "- The dataset is **split into small chunks** (batches).\n",
    "- Each chunk is **loaded into memory**, processed, and then discarded.\n",
    "- The model **updates its parameters** after processing each batch.\n",
    "\n",
    "💡 This technique ensures that only a **small part of the data** is in memory at any time.\n",
    "\n",
    "\n",
    "\n",
    "### 📚 **Libraries Supporting Out-of-Core Learning**\n",
    "\n",
    "1. **Scikit-learn**  \n",
    "   - Supports out-of-core learning using the `partial_fit()` method.\n",
    "   - Works for algorithms like **SGDClassifier**, **SGDRegressor**, etc.\n",
    "\n",
    "2. **Dask**  \n",
    "   - Helps in handling large datasets by breaking them into smaller, manageable chunks.\n",
    "   \n",
    "3. **TensorFlow and PyTorch**  \n",
    "   - Supports data generators for feeding large datasets in batches.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ **Example Code: Out-of-Core Learning Using `partial_fit()` in Scikit-Learn**\n",
    "\n",
    "Let's build a simple **out-of-core learning example** using `SGDClassifier` with the famous **MNIST dataset**.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# 📥 Load the MNIST dataset (large dataset)\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X, y = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "# 🧩 Split data into chunks (out-of-core learning)\n",
    "chunk_size = 10000\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "for start in range(0, len(X), chunk_size):\n",
    "    end = start + chunk_size\n",
    "    sgd_clf.partial_fit(X[start:end], y[start:end], classes=np.unique(y))\n",
    "\n",
    "# ✅ Test the model\n",
    "accuracy = sgd_clf.score(X[:10000], y[:10000])\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 📌 **How `partial_fit()` Works:**\n",
    "\n",
    "- **`partial_fit()`** allows the model to **incrementally update** itself using small batches of data.\n",
    "- Unlike **`fit()`**, it doesn't require the entire dataset at once.\n",
    "  \n",
    "\n",
    "\n",
    "### ⚙️ **Example: Out-of-Core Learning with Dask**\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load a large CSV file in chunks using Dask\n",
    "df = dd.read_csv('large_dataset.csv')\n",
    "\n",
    "# Split features and target\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X.compute(), y.compute())  # Dask computes the data in chunks\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🧪 **Real-Life Use Cases of Out-of-Core Learning**\n",
    "\n",
    "1. **Fraud Detection**:  \n",
    "   - Detect fraud in **real-time payment systems**.\n",
    "  \n",
    "2. **Recommendation Systems**:  \n",
    "   - Train on large user interaction datasets for **personalized recommendations**.\n",
    "  \n",
    "3. **IoT Data Processing**:  \n",
    "   - Process continuous data from **sensors** without storing all of it.\n",
    "\n",
    "### 🛠 **Advantages of Out-of-Core Learning:**\n",
    "\n",
    "| Advantage              | Description                                  |\n",
    "|------------------------|----------------------------------------------|\n",
    "| Memory Efficiency       | Handles datasets that are too large to fit in RAM. |\n",
    "| Incremental Updates     | Can be used with **real-time data streams**. |\n",
    "| Scalable                | Works on large datasets without crashing the system. |\n",
    "\n",
    "\n",
    "\n",
    "### ⚠️ **Challenges with Out-of-Core Learning:**\n",
    "\n",
    "| Challenge              | Description                                  |\n",
    "|------------------------|----------------------------------------------|\n",
    "| Slower Training         | Processing in batches can take more time than in-memory training. |\n",
    "| Limited Algorithms      | Not all machine learning algorithms support out-of-core learning. |\n",
    "| Requires Data Generators| You need to create **data generators** or chunk-based loaders. |\n",
    "\n",
    "\n",
    "\n",
    "### 💡 **Out-of-Core Algorithms in Scikit-Learn:**\n",
    "\n",
    "| Algorithm              | Supports Out-of-Core? | Method          |\n",
    "|------------------------|-----------------------|-----------------|\n",
    "| SGDClassifier           | ✅ Yes                | `partial_fit()` |\n",
    "| SGDRegressor            | ✅ Yes                | `partial_fit()` |\n",
    "| PassiveAggressiveClassifier | ✅ Yes            | `partial_fit()` |\n",
    "| GaussianNB              | ✅ Yes                | `partial_fit()` |\n",
    "| RandomForestClassifier  | ❌ No                 | -               |\n",
    "\n",
    "\n",
    "### 🔍 **Summary in Simple Terms:**\n",
    "\n",
    "- **Out-of-Core Learning** = Training large datasets without loading everything into memory.\n",
    "- The model learns from **small chunks** and updates itself.\n",
    "- It is useful for **large datasets** and **real-time data streams**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 **Out-of-Core Learning with Vaex (Simple Explanation)**\n",
    "\n",
    "Vaex is a **fast, memory-efficient library** for handling **large datasets**. Unlike pandas, which loads the entire dataset into memory, **Vaex reads the data directly from disk**, making it perfect for **out-of-core learning**.\n",
    "\n",
    "In simple terms, **Vaex** helps you:\n",
    "\n",
    "✅ Handle **huge datasets** (even terabytes) without running out of memory.  \n",
    "✅ Perform **data preprocessing** like filtering, aggregations, and transformations efficiently.  \n",
    "✅ Use **lazy evaluation**, meaning it processes data **only when needed**.\n",
    "\n",
    "\n",
    "\n",
    "### 🔧 **How Vaex Handles Large Datasets**\n",
    "\n",
    "- Vaex **doesn't load the entire dataset into RAM**. Instead, it reads data from disk in **chunks** and processes only what you need.\n",
    "- It works with formats like **CSV**, **HDF5**, **Apache Arrow**, and more.\n",
    "\n",
    "💡 Think of Vaex as **Netflix streaming a video**. Instead of downloading the whole movie, it streams parts of it when needed. Similarly, Vaex reads data in chunks.\n",
    "\n",
    "### 📚 **Vaex vs Pandas: Why Vaex?**\n",
    "\n",
    "| Feature              | Pandas                 | Vaex                     |\n",
    "|----------------------|------------------------|--------------------------|\n",
    "| Data Loading         | Entire dataset in memory | On-demand (out-of-core)  |\n",
    "| Performance          | Slower on large datasets | Faster and memory-efficient |\n",
    "| Data Formats         | CSV, Excel, etc.        | CSV, HDF5, Arrow, etc.   |\n",
    "| Lazy Evaluation      | ❌ No                   | ✅ Yes                   |\n",
    "\n",
    "\n",
    "### 🏋️ **Example: Out-of-Core Learning with Vaex**\n",
    "\n",
    "Let's go step by step and see how to use Vaex for large datasets.\n",
    "\n",
    "#### 📥 **1. Load a Large Dataset with Vaex**\n",
    "\n",
    "```python\n",
    "import vaex\n",
    "\n",
    "# Load a large dataset (HDF5 format is efficient for Vaex)\n",
    "df = vaex.open('large_dataset.hdf5')\n",
    "\n",
    "# Check the first few rows\n",
    "df.head(5)\n",
    "\n",
    "# Check the dataset size\n",
    "print(f\"Number of rows: {len(df)}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 🔧 **2. Basic Data Exploration**\n",
    "\n",
    "```python\n",
    "# Describe the dataset (out-of-core, so fast even for large data)\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 🧪 **3. Filter and Transform Data**\n",
    "\n",
    "```python\n",
    "# Filter rows where age is greater than 30\n",
    "filtered_df = df[df['age'] > 30]\n",
    "\n",
    "# Add a new column with lazy evaluation\n",
    "df['income_in_thousands'] = df['income'] / 1000\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 🏋️ **4. Training a Model with Vaex and Scikit-Learn**\n",
    "\n",
    "Since Vaex handles data efficiently, we can combine it with **out-of-core learning** using `partial_fit()` in **Scikit-Learn**.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Convert Vaex DataFrame to NumPy arrays in chunks\n",
    "chunk_size = 10000\n",
    "model = SGDClassifier(random_state=42)\n",
    "\n",
    "for i in range(0, len(df), chunk_size):\n",
    "    # Load chunk of data\n",
    "    chunk = df[i:i+chunk_size].to_pandas_df()\n",
    "    \n",
    "    # Split features and target\n",
    "    X = chunk.drop('target', axis=1)\n",
    "    y = chunk['target']\n",
    "    \n",
    "    # Update the model with each chunk\n",
    "    model.partial_fit(X, y, classes=[0, 1])\n",
    "\n",
    "# Print model accuracy\n",
    "accuracy = model.score(X, y)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 🎨 **5. Visualize the Data with Vaex**\n",
    "\n",
    "```python\n",
    "# Plot a heatmap (out-of-core)\n",
    "df.plot_binned('age', 'income', colormap='viridis')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ✅ **Advantages of Using Vaex for Out-of-Core Learning**\n",
    "\n",
    "1. **Fast and memory-efficient**  \n",
    "   Works seamlessly with **huge datasets**.\n",
    "\n",
    "2. **Lazy evaluation**  \n",
    "   Only processes data when required, saving time and memory.\n",
    "\n",
    "3. **Easy to integrate**  \n",
    "   Works well with **Scikit-Learn**, **XGBoost**, **LightGBM**, etc.\n",
    "\n",
    "\n",
    "\n",
    "### 🔍 **Summary in Simple Terms:**\n",
    "\n",
    "- Vaex handles **large datasets efficiently** without loading them into memory.\n",
    "- It supports **out-of-core learning** by processing data in **chunks**.\n",
    "- You can use **Scikit-Learn’s partial_fit()** with Vaex for training models incrementally.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Methods:\n",
    "\n",
    "Yes, there are **better alternatives** to Vaex for handling **large datasets** in a more **efficient and scalable** way, especially when dealing with **out-of-core learning**. Let’s explore these tools, their pros/cons, and when to use them.\n",
    "\n",
    "\n",
    "\n",
    "## ✅ **Alternatives to Vaex for Handling Large Datasets**\n",
    "\n",
    "| Tool               | Type                        | Best Use Case                 | Key Features                               |\n",
    "|--------------------|-----------------------------|--------------------------------|--------------------------------------------|\n",
    "| **Dask**           | Parallel computing library   | Out-of-core machine learning  | Scales pandas-like operations to clusters  |\n",
    "| **Modin**          | Pandas alternative           | Faster pandas operations       | Automatically parallelizes pandas code     |\n",
    "| **PySpark**        | Distributed computing        | Big data (GBs to TBs)          | Works with clusters and Spark ecosystem    |\n",
    "| **Polars**         | DataFrame library            | Fast in-memory processing      | Faster than pandas with arrow-based engine |\n",
    "| **H2O.ai**         | Automated ML platform        | Distributed ML                | Handles large datasets automatically       |\n",
    "\n",
    "\n",
    "\n",
    "### 🎯 **Recommended Approach: Dask + Scikit-learn**\n",
    "\n",
    "**Dask** is a popular tool for **out-of-core processing** that integrates **seamlessly with Scikit-learn**.\n",
    "\n",
    "\n",
    "\n",
    "### ✅ **Example Code Using Dask + Scikit-learn for Large Datasets:**\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "from dask_ml.model_selection import train_test_split\n",
    "from dask_ml.linear_model import SGDClassifier\n",
    "from dask_ml.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset with Dask\n",
    "df = dd.read_csv('large_dataset.csv')\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X = df.drop(['gender', 'category'], axis=1)\n",
    "y = df['gender'].apply(lambda x: 1 if x == 'Male' else 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = SGDClassifier(random_state=42)\n",
    "\n",
    "# Fit the model incrementally (out-of-core learning)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and calculate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 📊 **Why Use Dask?**\n",
    "- **Efficient handling** of large datasets without loading everything into memory.\n",
    "- **Seamless integration** with pandas and Scikit-learn.\n",
    "- Works **locally or distributed across a cluster**.\n",
    "\n",
    "\n",
    "### ✅ **When to Use Each Tool:**\n",
    "| Tool        | When to Use                                               |\n",
    "|-------------|-----------------------------------------------------------|\n",
    "| **Vaex**    | When you need fast DataFrame operations on disk-based files. |\n",
    "| **Dask**    | When working with distributed data or out-of-core learning. |\n",
    "| **Modin**   | When you want a faster drop-in replacement for pandas.     |\n",
    "| **PySpark** | When dealing with big data (GBs to TBs) in distributed environments. |\n",
    "| **Polars**  | When you need super-fast in-memory operations.             |\n",
    "\n",
    "\n",
    "\n",
    "### 🔍 **Which Tool to Choose?**\n",
    "\n",
    "| If Your Dataset is...    | Best Tool        |\n",
    "|--------------------------|------------------|\n",
    "| 1 GB - 10 GB             | Dask             |\n",
    "| 10 GB - 100 GB           | Dask or PySpark  |\n",
    "| 100 GB+                  | PySpark or H2O.ai|\n",
    "\n",
    "\n",
    "\n",
    "**🔧 Pro Tip:**  \n",
    "If you're comfortable with pandas, start with **Dask**. It’s the most similar in syntax and will scale your existing workflows efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
