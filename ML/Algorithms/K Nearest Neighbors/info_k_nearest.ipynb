{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN):\n",
    "\n",
    "\n",
    "K-Nearest Neighbors (KNN) is one of the simplest machine learning algorithms used for **classification** and **regression**. It works by comparing a new data point to its nearest neighbors in the dataset and assigning a label or value based on their properties.\n",
    "\n",
    "\n",
    "### **How Does KNN Work?**\n",
    "\n",
    "1. **Understanding the Data:**\n",
    "   - Imagine your data points are plotted in a space (e.g., on a 2D graph).\n",
    "   - Each data point belongs to a specific class (for classification) or has a specific value (for regression).\n",
    "\n",
    "2. **Making a Prediction:**\n",
    "   - When a new point is introduced, the algorithm looks at the **k nearest neighbors** to that point.\n",
    "   - These neighbors are identified based on their distance to the new point (closer points are considered more relevant).\n",
    "\n",
    "3. **Decision:**\n",
    "   - **For Classification:**\n",
    "     - The algorithm looks at the majority class among the k neighbors and assigns that class to the new point.\n",
    "     - Example: If 3 out of 5 neighbors belong to Class A, the new point is classified as Class A.\n",
    "   - **For Regression:**\n",
    "     - The algorithm calculates the average value (or sometimes the median) of the k neighbors and assigns that value to the new point.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Components of KNN**\n",
    "\n",
    "#### **1. Distance Metrics**\n",
    "To find the nearest neighbors, the algorithm uses a distance metric. Common ones include:\n",
    "- **Euclidean Distance:** Straight-line distance between two points.\n",
    "- **Manhattan Distance:** Sum of the absolute differences between coordinates.\n",
    "- **Minkowski Distance:** A generalization of both Euclidean and Manhattan distances.\n",
    "- **Cosine Similarity:** Measures the angle between two vectors (used when data is sparse).\n",
    "\n",
    "#### **2. The Hyperparameter $ k $**\n",
    "- $ k $ determines how many neighbors to consider.\n",
    "- **Small $ k $:** Can be noisy and may overfit the data.\n",
    "- **Large $ k $:** More stable but may ignore finer details and patterns.\n",
    "- Choosing $ k $: Use cross-validation to find the optimal value.\n",
    "\n",
    "#### **3. Weighting Neighbors**\n",
    "- Sometimes, closer neighbors are given more importance.\n",
    "- **Uniform Weighting:** All neighbors are treated equally.\n",
    "- **Distance Weighting:** Closer neighbors have more influence than farther ones.\n",
    "\n",
    "\n",
    "\n",
    "### **Steps of the KNN Algorithm**\n",
    "1. **Load the Data:** Prepare the dataset for training and testing.\n",
    "2. **Choose $ k $:** Decide on the number of neighbors.\n",
    "3. **Calculate Distances:** Compute the distance between the new point and all existing points.\n",
    "4. **Find Nearest Neighbors:** Identify the $ k $ closest points.\n",
    "5. **Predict:**\n",
    "   - For classification: Assign the majority class.\n",
    "   - For regression: Assign the average value.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of KNN**\n",
    "1. **Simple and Intuitive:** Easy to understand and implement.\n",
    "2. **Non-Parametric:** Makes no assumption about the underlying data distribution.\n",
    "3. **Versatile:** Works for both classification and regression tasks.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of KNN**\n",
    "1. **Computationally Expensive:**\n",
    "   - For large datasets, calculating distances for all points can be slow.\n",
    "2. **Sensitive to Noise:**\n",
    "   - Outliers or irrelevant features can affect results.\n",
    "3. **Requires Scaling:**\n",
    "   - Distance-based calculations are affected by feature scaling (e.g., use StandardScaler or MinMaxScaler).\n",
    "4. **Curse of Dimensionality:**\n",
    "   - Performance can degrade with high-dimensional data.\n",
    "\n",
    "\n",
    "\n",
    "### **Applications of KNN**\n",
    "1. **Classification Problems:**\n",
    "   - Handwritten digit recognition (e.g., MNIST dataset).\n",
    "   - Sentiment analysis.\n",
    "2. **Regression Problems:**\n",
    "   - Predicting house prices based on nearby properties.\n",
    "3. **Recommendation Systems:**\n",
    "   - Suggesting movies, songs, or products based on user preferences.\n",
    "4. **Anomaly Detection:**\n",
    "   - Identifying unusual patterns in data.\n",
    "\n",
    "\n",
    "\n",
    "### **Example in Python**\n",
    "Here’s a basic implementation using Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the KNN model\n",
    "k = 5\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Tips for Using KNN**\n",
    "1. **Scale Your Features:**\n",
    "   - Always scale data to ensure fair distance calculations.\n",
    "2. **Choose $ k $ Wisely:**\n",
    "   - Use grid search or cross-validation to determine the best $ k $.\n",
    "3. **Reduce Dimensions:**\n",
    "   - Use techniques like PCA for high-dimensional data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **K-Nearest Neighbors (KNN) Examples**\n",
    "\n",
    "Imagine you move to a new neighborhood and want to find out what kind of food people like around you. You don’t know yet, but you can ask your **neighbors** to get an idea.\n",
    "\n",
    "\n",
    "\n",
    "### **Step-by-Step Example:**\n",
    "\n",
    "1. **You Have a Question:**\n",
    "   - You ask, \"What type of food is popular here?\"\n",
    "\n",
    "2. **Your Nearest Neighbors:**\n",
    "   - To answer, you ask the **k nearest neighbors** (e.g., 3 or 5 people living closest to you) what their favorite food is.\n",
    "\n",
    "3. **Majority Wins:**\n",
    "   - If most of them say \"Pizza,\" you assume pizza is the most popular food in your area.\n",
    "   - This is **classification**: labeling your neighborhood’s food preference based on what the majority of nearby people like.\n",
    "\n",
    "4. **If It's About Numbers:**\n",
    "   - Let’s say you want to know the average monthly food bill in your area. You ask your 3 or 5 closest neighbors for their monthly bills, then calculate the **average**.\n",
    "   - This is **regression**: predicting a number by averaging the values of nearby neighbors.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Points:**\n",
    "\n",
    "- **The \"k\" in KNN**:\n",
    "   - You decide how many neighbors to ask. A small \\( k \\) (e.g., 3) gives quick, local insights but can be noisy. A larger \\( k \\) (e.g., 10) considers more opinions but might miss local trends.\n",
    "\n",
    "- **\"Nearest\" Means Closest:**\n",
    "   - The neighbors are chosen based on how **close** they are. Closeness could mean physical distance, similarity in preferences, or other ways of measuring similarity.\n",
    "\n",
    "\n",
    "\n",
    "### **Real-Life Analogy:**\n",
    "\n",
    "- Imagine you’re shopping for clothes and want advice on style. \n",
    "   - You look at 3 or 5 people nearby who dress like you. Based on their preferences, you make your choice. \n",
    "   - This is how KNN works—it uses **neighbors' input** to decide.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Use KNN?**\n",
    "\n",
    "- It’s **simple**: Just ask the neighbors.\n",
    "- It’s **flexible**: Works for predicting labels (pizza) or numbers (monthly bills).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
