{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression:\n",
    "\n",
    "Linear regression is one of the most fundamental and widely used algorithms in statistics and machine learning for modeling relationships between variables. Hereâ€™s a full explanation:\n",
    "\n",
    "\n",
    "\n",
    "### **1. What is Linear Regression?**\n",
    "Linear regression is a supervised learning algorithm used to predict a continuous target variable (**y**) based on one or more predictor variables (**x**). It assumes a linear relationship between the predictor(s) and the target variable.\n",
    "\n",
    "- **Simple Linear Regression**: Deals with one predictor variable.\n",
    "- **Multiple Linear Regression**: Deals with two or more predictor variables.\n",
    "- **Polynomial Linear Regression**: Deals with when data is not linear.\n",
    "\n",
    "\n",
    "\n",
    "### **2. The Equation of Linear Regression**\n",
    "\n",
    "For **Simple Linear Regression**:\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "The model assumes the relationship can be described using the equation of a straight line:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_nx_n + \\epsilon\n",
    "$$\n",
    "\n",
    "- **$y$**: The target variable (dependent variable).\n",
    "- **$x_i$**: Predictor variables (independent variables).\n",
    "- **$\\beta_0$**: Intercept (value of $y$ when all $x_i$ are 0).\n",
    "- **$\\beta_i$**: Coefficients (weights) for each predictor, determining their contribution to $y$.\n",
    "- **$\\epsilon$**: Error term accounting for variability not explained by the model.\n",
    "\n",
    "For **simple linear regression**, the equation simplifies to:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x + \\epsilon\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **3. Goals of Linear Regression**\n",
    "1. **Fit the Line**: Find the best-fit line through the data points.\n",
    "2. **Minimize the Error**: Use a method like **Ordinary Least Squares (OLS)** to minimize the sum of squared residuals:\n",
    "   $$\n",
    "   \\text{Residual} = \\text{Actual Value} - \\text{Predicted Value}\n",
    "   $$\n",
    "   The objective is to minimize:\n",
    "   $$\n",
    "   \\text{Sum of Squared Errors (SSE)} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "### **4. Assumptions of Linear Regression**\n",
    "For linear regression to work effectively, the following assumptions should hold:\n",
    "1. **Linearity**: The relationship between $x$ and $y$ is linear.\n",
    "2. **Independence**: Observations are independent of each other.\n",
    "3. **Homoscedasticity**: Constant variance of residuals.\n",
    "4. **Normality**: Residuals are normally distributed.\n",
    "5. **No Multicollinearity** (for multiple regression): Predictor variables should not be highly correlated with each other.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Steps to Perform Linear Regression**\n",
    "1. **Understand the Problem**:\n",
    "   - Identify the target variable and predictors.\n",
    "2. **Prepare the Data**:\n",
    "   - Handle missing values, normalize/scale predictors if needed, and split into training/testing sets.\n",
    "3. **Fit the Model**:\n",
    "   - Use tools like **Scikit-learn**, **statsmodels**, or others to fit the regression model.\n",
    "4. **Evaluate the Model**:\n",
    "   - Metrics:\n",
    "     - **R-squared ($R^2$)**: Proportion of variance explained by the model.\n",
    "     - **Mean Squared Error (MSE)**: Average squared difference between actual and predicted values.\n",
    "     - **Mean Absolute Error (MAE)**: Average absolute difference between actual and predicted values.\n",
    "5. **Interpret the Results**:\n",
    "   - Analyze the coefficients to understand how predictors influence the target.\n",
    "\n",
    "\n",
    "\n",
    "### **6. Example in Python (Using Scikit-learn)**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Hours_Studied': [1, 2, 3, 4, 5],\n",
    "    'Scores': [20, 40, 60, 80, 100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Independent and dependent variables\n",
    "X = df[['Hours_Studied']]\n",
    "y = df['Scores']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n",
    "# Model parameters\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficient:\", model.coef_)\n",
    "```\n",
    "\n",
    "\n",
    "### **7. Applications of Linear Regression**\n",
    "- Predicting sales, prices, or trends.\n",
    "- Understanding relationships between variables in domains like economics, biology, and marketing.\n",
    "- Analyzing the impact of independent variables on a dependent variable.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression:\n",
    "\n",
    "### What is Polynomial Regression?\n",
    "\n",
    "Polynomial Regression is a type of regression analysis where the relationship between the independent variable ($x$) and the dependent variable ($y$) is modeled as an $n$-degree polynomial. It is an extension of linear regression that captures non-linear relationships in data.\n",
    "\n",
    "\n",
    "\n",
    "### Formula of Polynomial Regression:\n",
    "\n",
    "The general equation for a polynomial regression model is:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\ldots + \\beta_nx^n + \\epsilon\n",
    "$$\n",
    "\n",
    "- $y$: Dependent variable (target)\n",
    "- $x$: Independent variable (feature)\n",
    "- $\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n$: Coefficients of the polynomial\n",
    "- $n$: Degree of the polynomial\n",
    "- $\\epsilon$: Error term (captures the noise in the data)\n",
    "\n",
    "\n",
    "\n",
    "### Key Characteristics:\n",
    "\n",
    "1. **Captures Non-linearity**: Polynomial regression is suitable for data that cannot be fit well with a straight line.\n",
    "2. **Higher Degrees**: Higher-degree polynomials can fit more complex patterns, but they risk overfitting.\n",
    "3. **Transformed Features**: It converts the input feature $x$ into polynomial features ($x^2, x^3, \\ldots, x^n$) to model the non-linearity.\n",
    "\n",
    "\n",
    "\n",
    "### Steps to Perform Polynomial Regression:\n",
    "\n",
    "1. **Prepare the Data**:\n",
    "   - Collect and preprocess your dataset.\n",
    "   - Identify the independent ($x$) and dependent ($y$) variables.\n",
    "\n",
    "2. **Generate Polynomial Features**:\n",
    "   - Use polynomial transformations to create higher-degree terms ($x^2, x^3, \\ldots$).\n",
    "   - Tools like `PolynomialFeatures` from Scikit-learn in Python are helpful.\n",
    "\n",
    "3. **Train the Model**:\n",
    "   - Use linear regression to fit the transformed features to the target $y$.\n",
    "   - Even though the model is \"polynomial,\" the regression algorithm remains linear since it optimizes the coefficients linearly.\n",
    "\n",
    "4. **Evaluate the Model**:\n",
    "   - Calculate metrics such as R-squared ($R^2$) and Mean Squared Error (MSE) to check the model's performance.\n",
    "   - Use visualization to see how well the curve fits the data.\n",
    "\n",
    "5. **Adjust the Degree of Polynomial**:\n",
    "   - Increase or decrease the degree of the polynomial based on underfitting or overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### Python Implementation:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample data\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)\n",
    "y = np.array([1.5, 3.2, 5.8, 8.4, 11.1, 15.3, 20.1, 26.5])\n",
    "\n",
    "# Transform to polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_poly = poly.fit_transform(x)\n",
    "\n",
    "# Train the linear regression model on polynomial features\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(x_poly)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y, y_pred))\n",
    "print(\"R-Squared:\", r2_score(y, y_pred))\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(x, y, color='blue', label='Actual Data')\n",
    "plt.plot(x, y_pred, color='red', label='Polynomial Regression Curve')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Polynomial Regression?\n",
    "\n",
    "1. **Non-linear Relationships**:\n",
    "   Use it when data shows a clear non-linear trend that cannot be captured by a straight line.\n",
    "\n",
    "2. **Avoid Overfitting**:\n",
    "   Avoid very high-degree polynomials unless thereâ€™s a good reason, as they may fit noise rather than the actual pattern.\n",
    "\n",
    "3. **Explainable Complexity**:\n",
    "   Keep the degree of the polynomial low to make the model interpretable.\n",
    "\n",
    "\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "1. **Flexible**: Can fit a wide range of curves.\n",
    "2. **Simple Extension**: Builds on linear regression, making implementation straightforward.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages:\n",
    "\n",
    "1. **Overfitting**: Higher-degree polynomials may fit the noise rather than the trend.\n",
    "2. **Extrapolation Issues**: Predicting values outside the range of training data can lead to unreliable results.\n",
    "3. **Increased Complexity**: As the degree increases, the model becomes harder to interpret and computationally expensive.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression:\n",
    "\n",
    "Sure! Linear regression has some key assumptions that need to hold true for it to work effectively. Letâ€™s go over them in a simple way:  \n",
    "\n",
    "\n",
    "\n",
    "### **1. Linearity**  \n",
    "- The relationship between the input variables (X) and the output variable (Y) should be linear.  \n",
    "- This means if you plot your data, it should look like a straight line (or close to one).  \n",
    "\n",
    "ðŸ’¡ **Example**: If youâ€™re predicting house prices based on square footage, the price should increase or decrease consistently as square footage changes.  \n",
    "\n",
    "\n",
    "\n",
    "### **2. Independence of Errors (No Autocorrelation)**  \n",
    "- The errors (or residuals) in your predictions shouldnâ€™t be related to each other.  \n",
    "- In simple terms, the error for one data point shouldnâ€™t depend on the error for another.  \n",
    "\n",
    "ðŸ’¡ **Example**: If youâ€™re predicting stock prices, todayâ€™s error shouldnâ€™t depend on yesterdayâ€™s error.  \n",
    "\n",
    "\n",
    "\n",
    "### **3. Homoscedasticity (Constant Variance of Errors)**  \n",
    "- The spread of the errors should remain constant across all values of X.  \n",
    "- If the errors are too big for some values of X and too small for others, this assumption is violated.  \n",
    "\n",
    "ðŸ’¡ **Example**: Imagine youâ€™re predicting exam scores based on study hours. The error should be similar whether the student studies 2 hours or 10 hours.  \n",
    "\n",
    "\n",
    "\n",
    "### **4. No Multicollinearity (for Multiple Variables)**  \n",
    "- If youâ€™re using multiple input variables, they shouldnâ€™t be strongly correlated with each other.  \n",
    "- Multicollinearity makes it hard to figure out which variable is really affecting Y.  \n",
    "\n",
    "ðŸ’¡ **Example**: Predicting sales using both \"ad spend on TV\" and \"ad spend online\" might be tricky if these two are closely related.  \n",
    "\n",
    "\n",
    "\n",
    "### **5. Normality of Errors**  \n",
    "- The errors (differences between actual and predicted values) should follow a normal distribution (a bell-shaped curve).  \n",
    "- This assumption ensures accurate confidence intervals and hypothesis testing.  \n",
    "\n",
    "ðŸ’¡ **Example**: If you predict house prices, most errors should cluster around zero, with fewer big errors.  \n",
    "\n",
    "### **Summary of Assumptions**  \n",
    "| **Assumption**            | **What It Ensures**                                           |  \n",
    "|---------------------------|-------------------------------------------------------------|  \n",
    "| **Linearity**              | Predicting a straight-line relationship.                   |  \n",
    "| **Independence of Errors** | Errors donâ€™t influence each other.                         |  \n",
    "| **Homoscedasticity**       | Errors have equal variance across all values of X.         |  \n",
    "| **No Multicollinearity**   | Input variables donâ€™t duplicate information.               |  \n",
    "| **Normality of Errors**    | Errors are symmetrically distributed for better inference. |  \n",
    "\n",
    "\n",
    "\n",
    "### **How to Check These Assumptions?**  \n",
    "1. **Linearity**: Plot the actual vs. predicted values to see if they form a straight line.  \n",
    "2. **Independence of Errors**: Use the Durbin-Watson test for autocorrelation.  \n",
    "3. **Homoscedasticity**: Create a scatter plot of residuals vs. predicted values; the spread should be consistent.  \n",
    "4. **No Multicollinearity**: Check the Variance Inflation Factor (VIF); values >10 indicate problems.  \n",
    "5. **Normality of Errors**: Use a histogram or Q-Q plot to see if the errors form a bell curve.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
