{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Random Forest?**\n",
    "\n",
    "> **Random Forest is an ensemble learning algorithm that combines multiple decision trees to improve prediction accuracy and prevent overfitting. It works by averaging predictions for regression tasks or using majority voting for classification tasks.**\n",
    "\n",
    "Random Forest is a popular machine learning algorithm used for both **classification** and **regression** tasks. It is an ensemble learning method, meaning it combines the predictions of multiple models (in this case, decision trees) to improve performance and reduce the risk of overfitting. \n",
    "\n",
    "### **Key Concept**\n",
    "- **Random Forest = Multiple Decision Trees + Voting/Averaging**\n",
    "  - For **classification**, it uses the majority voting approach: the class that gets the most votes from individual trees is the final prediction.\n",
    "  - For **regression**, it takes the average of the predictions made by the individual trees.\n",
    "\n",
    "\n",
    "\n",
    "### **How Random Forest Works**\n",
    "1. **Data Sampling:**\n",
    "   - It uses a technique called **bootstrap aggregation (bagging)**.\n",
    "   - Multiple subsets of the original dataset are randomly sampled **with replacement** (some data points may appear multiple times in one subset).\n",
    "\n",
    "2. **Tree Building:**\n",
    "   - For each subset, a decision tree is built.\n",
    "   - During the construction of each tree, the algorithm selects a random subset of features to split on (not all features are considered). This helps reduce correlation between trees.\n",
    "\n",
    "3. **Prediction:**\n",
    "   - Each tree in the forest makes its own prediction.\n",
    "   - For classification, the majority class across all trees is selected.\n",
    "   - For regression, the average of all tree predictions is taken.\n",
    "\n",
    "4. **Final Output:**\n",
    "   - Combines predictions from all the trees to make a more accurate and robust prediction.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Random Forest**\n",
    "1. **Accuracy:** High accuracy due to multiple trees reducing variance.\n",
    "2. **Robustness:** Handles missing values and noisy data well.\n",
    "3. **Non-Linear Relationships:** Can capture complex patterns and relationships in data.\n",
    "4. **Overfitting Reduction:** Random feature selection and averaging reduce overfitting compared to individual decision trees.\n",
    "5. **Feature Importance:** Provides a ranking of feature importance, helping in feature selection.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "1. **Computational Cost:** Training multiple decision trees can be computationally intensive for large datasets.\n",
    "2. **Interpretability:** Harder to interpret than a single decision tree.\n",
    "3. **Memory Usage:** Requires more memory since multiple trees are stored.\n",
    "\n",
    "\n",
    "\n",
    "### **Applications of Random Forest**\n",
    "- **Classification:** \n",
    "  - Spam detection\n",
    "  - Image classification\n",
    "  - Sentiment analysis\n",
    "- **Regression:** \n",
    "  - Stock price prediction\n",
    "  - House price estimation\n",
    "- **Feature Selection:** Identifying important features in datasets.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Hyperparameters**\n",
    "- **n_estimators:** Number of decision trees in the forest.\n",
    "- **max_depth:** Maximum depth of each tree.\n",
    "- **max_features:** Number of features to consider for each split.\n",
    "- **min_samples_split:** Minimum number of samples required to split a node.\n",
    "- **min_samples_leaf:** Minimum number of samples required at a leaf node.\n",
    "\n",
    "\n",
    "\n",
    "### **Why is it called \"Random\"?**\n",
    "1. Random **data sampling**: Bootstrapped subsets are created randomly.\n",
    "2. Random **feature selection**: For each split, a random subset of features is considered.\n",
    "\n",
    "\n",
    "\n",
    "### **Example in Python**\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Random Forest Classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "\n",
    "In this example, the Random Forest uses 100 decision trees to classify the Iris dataset.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Random Forest:\n",
    "\n",
    "Sure! Let me break it down in very simple terms. ðŸ˜Š\n",
    "\n",
    "\n",
    "\n",
    "Imagine you're trying to decide which restaurant to go to. Instead of deciding on your own, you ask **10 of your friends** for their opinions. Each friend gives their suggestion based on their own experience.\n",
    "\n",
    "Now:\n",
    "- If most of your friends suggest the same restaurant (say 7 out of 10 say \"Pizza Place\"), youâ€™re more likely to trust their choice.\n",
    "- This is like **Random Forest**â€”it takes opinions from multiple \"friends\" (decision trees) and goes with the majority vote (for classification) or averages their opinions (for regression).\n",
    "\n",
    "\n",
    "\n",
    "### How does Random Forest work?\n",
    "\n",
    "1. **Many Decision Makers (Trees):**\n",
    "   - Imagine each \"friend\" in your group as a **decision tree**.\n",
    "   - Each tree gives a prediction (or vote) based on the information it sees.\n",
    "\n",
    "2. **Random Choices:**\n",
    "   - Each tree gets **different random data** to work with (just like giving your friends slightly different info).\n",
    "   - Each tree also looks at **random parts of the data** (like telling one friend to only consider the menu and another to look at the location).\n",
    "\n",
    "3. **Combining Opinions:**\n",
    "   - Once all trees have made their predictions:\n",
    "     - If it's a classification problem: The \"majority vote\" wins (e.g., most trees say \"Pizza Place\").\n",
    "     - If it's a regression problem: You take the **average** of all predictions (e.g., they give different price estimates, and you average them).\n",
    "\n",
    "\n",
    "\n",
    "### Why is Random Forest awesome?\n",
    "\n",
    "1. **Better Decisions:**\n",
    "   - Instead of relying on just one friend (tree), youâ€™re taking the collective wisdom of many.\n",
    "2. **Handles Different Perspectives:**\n",
    "   - By giving each friend different info, they can focus on various aspects, leading to a more balanced decision.\n",
    "3. **Mistakes Get Balanced Out:**\n",
    "   - Even if one friend/tree makes a bad suggestion, the others can outvote it.\n",
    "\n",
    "\n",
    "\n",
    "### Example in Real Life\n",
    "Letâ€™s say you're trying to predict whether a movie will be a hit or flop. \n",
    "- You give different parts of the data (genre, cast, director) to several decision trees.\n",
    "- Each tree makes its prediction (hit/flop).\n",
    "- The Random Forest combines all these predictions and gives you the final answer (e.g., \"Hit\" if most trees agree).\n",
    "\n",
    "\n",
    "\n",
    "### In Short:\n",
    "- **Random Forest** is like asking multiple \"friends\" (decision trees) for advice.\n",
    "- Each friend gives their answer based on random pieces of the data.\n",
    "- The final decision is made by combining all their suggestions.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Random Forest Performs Well Compared to other algorithms:\n",
    "\n",
    "Random Forest outperforms many algorithms in various scenarios because of its unique design. Here's why:\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Combining Multiple Trees (Ensemble Learning)**\n",
    "- Instead of relying on a single decision tree, which can overfit or underfit the data, Random Forest builds multiple trees and combines their predictions. \n",
    "- This combination makes the model more **stable** and **accurate**, as it reduces errors caused by individual trees.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Randomization**\n",
    "- **Random Sampling:** Each tree is trained on a random subset of the data (bootstrap sampling). This ensures the trees are diverse.\n",
    "- **Random Feature Selection:** Each tree splits nodes using only a random subset of features. This prevents all trees from focusing on the same dominant features, reducing overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Resilience to Overfitting**\n",
    "- Individual decision trees often **overfit**â€”they perform well on training data but poorly on new data.\n",
    "- Random Forest reduces overfitting by averaging the predictions of multiple trees, smoothing out their extreme behaviors.\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Bias-Variance Tradeoff in Random Forest**\n",
    "To understand why Random Forest is effective, we need to discuss the **bias-variance tradeoff**:\n",
    "\n",
    "#### **Bias** \n",
    "- Bias is the error due to **simplifying assumptions** in the model (e.g., assuming data is linear when itâ€™s not).\n",
    "- High bias = underfitting.\n",
    "\n",
    "#### **Variance**\n",
    "- Variance is the error due to the model being too **sensitive to small changes** in the data (e.g., overfitting to noise).\n",
    "- High variance = overfitting.\n",
    "\n",
    "#### **How Random Forest Handles Bias and Variance**\n",
    "- **Low Variance:** Random Forest averages the predictions of multiple trees. Since individual trees might overfit (high variance), averaging them reduces variance.\n",
    "- **Moderate Bias:** Decision trees have low bias but can overfit. By using multiple trees and randomization, Random Forest slightly increases bias but ensures itâ€™s not too high.\n",
    "\n",
    "This balance is why Random Forest achieves **high accuracy** without overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Robustness to Noise and Missing Data**\n",
    "- Because of averaging, Random Forest is less affected by noisy data or outliers compared to algorithms like a single decision tree or linear models.\n",
    "- It can handle missing values effectively by averaging predictions from trees trained on different subsets of the data.\n",
    "\n",
    "### **Comparison to Other Algorithms**\n",
    "| **Algorithm**              | **Strengths**                                                                 | **Weaknesses**                                                       |\n",
    "|-----------------------------|-------------------------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Random Forest**           | Handles overfitting well, works on both classification and regression, robust | Slower for large datasets, harder to interpret                       |\n",
    "| **Decision Tree**           | Simple, interpretable                                                        | Prone to overfitting, sensitive to data variations                   |\n",
    "| **Logistic Regression**     | Great for linear problems, interpretable                                     | Poor for non-linear relationships                                    |\n",
    "| **SVM (Support Vector Machines)** | Good for complex boundaries, works on small datasets                       | Slow for large datasets, sensitive to noise                         |\n",
    "| **Neural Networks**         | Excellent for large datasets, complex problems                              | Needs lots of data, computationally expensive         \n",
    "\n",
    "               |\n",
    "\n",
    "\n",
    "\n",
    "### **Practical Example of Bias-Variance in Random Forest**\n",
    "#### Scenario: Predicting house prices\n",
    "- **Single Decision Tree:**\n",
    "  - Might learn every detail of the training data, overfitting (high variance).\n",
    "- **Random Forest:**\n",
    "  - Each tree sees different data and uses different features. Combining them creates a model that generalizes better (low variance, moderate bias).\n",
    "\n",
    "\n",
    "\n",
    "### **Summary: Why Random Forest Performs Well**\n",
    "1. **Reduces Overfitting:** Averages multiple trees to smooth predictions.\n",
    "2. **Balances Bias and Variance:** Achieves a good tradeoff by introducing randomization.\n",
    "3. **Handles Noise and Missing Data:** Makes it robust and reliable.\n",
    "4. **Works Well on Complex Data:** Captures non-linear patterns effectively.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest vs Bagging:\n",
    "\n",
    "### **Random Forest vs Bagging: Key Differences**\n",
    "\n",
    "Both Random Forest and Bagging are ensemble learning methods that aim to improve the performance of machine learning models by combining multiple weaker models (like decision trees). However, there are distinct differences between the two. Here's a breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Core Idea**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Bagging (Bootstrap Aggregating) is a general ensemble method where multiple models (often decision trees) are trained on **different random subsets** of the data. | Random Forest is a **specialized form of bagging** that builds decision trees but also introduces additional randomness by selecting **random subsets of features** at each split. |\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Feature Selection**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| All features are considered when splitting nodes in decision trees. | Only a **random subset of features** is considered for each split. This further reduces correlation between trees and improves generalization. |\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model Type**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Bagging can be used with any base model (e.g., decision trees, SVMs, or neural networks). | Random Forest specifically uses **decision trees** as the base model. |\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Reduction of Overfitting**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Reduces overfitting primarily by combining predictions from models trained on bootstrapped datasets. | Reduces overfitting **even further** by introducing randomness in feature selection in addition to bootstrapped datasets. |\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Bias-Variance Tradeoff**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Decreases variance by averaging predictions but does not significantly alter the bias of the base model. | Decreases variance and slightly increases bias due to random feature selection, achieving a better **bias-variance tradeoff**. |\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Performance**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Works well but can struggle with overfitting if the base models are too complex (e.g., deep decision trees). | Performs better in most cases because random feature selection reduces correlation between trees. |\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Interpretability**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Slightly easier to interpret since it doesnâ€™t introduce additional randomness at the feature level. | Harder to interpret because of random feature selection, but provides **feature importance scores**. |\n",
    "\n",
    "---\n",
    "\n",
    "### **8. When to Use**\n",
    "| **Bagging**                                      | **Random Forest**                                  |\n",
    "|--------------------------------------------------|--------------------------------------------------|\n",
    "| Use bagging when you want to generalize ensemble learning to various base models or when you donâ€™t need random feature selection. | Use Random Forest when working with tabular data and decision trees, and you want a more robust model that handles overfitting and feature correlation well. |\n",
    "\n",
    "---\n",
    "\n",
    "### **Similarities**\n",
    "1. Both use **bootstrap sampling** (training models on random subsets of data with replacement).\n",
    "2. Both aggregate predictions (majority vote for classification, averaging for regression).\n",
    "3. Both reduce variance by combining multiple models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example in Python**\n",
    "#### **Bagging**\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Bagging with Decision Tree as the base model\n",
    "bagging_model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Bagging Accuracy:\", bagging_model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "#### **Random Forest**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Accuracy:\", rf_model.score(X_test, y_test))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- **Bagging**: General framework, any base model, all features are used for splits.\n",
    "- **Random Forest**: A specific bagging implementation using decision trees and random feature selection for better performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Out-of-Bag (OOB) Score in Random Forest**\n",
    "\n",
    "The **Out-of-Bag (OOB) score** is a performance metric used in Random Forest to estimate the accuracy of the model without needing a separate validation or test set. It leverages the bootstrap sampling technique used during training.\n",
    "\n",
    "\n",
    "\n",
    "### **How Bootstrap Sampling Works in Random Forest**\n",
    "1. When building each decision tree in the Random Forest, a **random subset** of the training data is selected **with replacement** (bootstrap sampling). \n",
    "   - This means some data points are used multiple times in the same subset.\n",
    "   - On average, about **63% of the original data** is included in each subset.\n",
    "   - The remaining **37% of the data** is not included and is called the **Out-of-Bag (OOB) data** for that tree.\n",
    "\n",
    "\n",
    "\n",
    "### **What Is the OOB Score?**\n",
    "The **OOB score** is the accuracy of the Random Forest model on the **OOB data**:\n",
    "1. Each data point in the training set serves as OOB data for approximately 37% of the trees in the forest.\n",
    "2. The model predicts the output for these OOB data points using only the trees that did not see these points during training.\n",
    "3. The OOB score is calculated as the accuracy of these predictions compared to the true labels.\n",
    "\n",
    "\n",
    "\n",
    "### **Steps to Calculate the OOB Score**\n",
    "1. Train the Random Forest model using bootstrap sampling.\n",
    "2. For each data point:\n",
    "   - Collect predictions from the trees where the data point was OOB.\n",
    "   - Use majority voting (classification) or averaging (regression) to combine these predictions.\n",
    "3. Compare the combined predictions to the true values to compute accuracy or another performance metric.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of OOB Score**\n",
    "1. **No Need for Separate Validation Data:**\n",
    "   - The OOB score provides a reliable estimate of model performance without requiring a dedicated validation set.\n",
    "   - This is especially useful when the dataset is small.\n",
    "2. **Efficient Use of Data:**\n",
    "   - All training data points contribute to both model training and validation in some trees.\n",
    "3. **Reduces Overfitting:**\n",
    "   - Provides a built-in mechanism to evaluate model performance without relying on test data.\n",
    "\n",
    "\n",
    "\n",
    "### **How to Enable OOB Score in Python**\n",
    "In `scikit-learn`, you can compute the OOB score while training the Random Forest by setting `oob_score=True`:\n",
    "\n",
    "#### Example Code:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Random Forest with OOB score enabled\n",
    "rf_model = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get OOB Score\n",
    "print(\"OOB Score:\", rf_model.oob_score_)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **OOB Score vs Test Score**\n",
    "- The **OOB score** is often close to the accuracy on a separate test set.\n",
    "- However, it may be slightly optimistic or pessimistic depending on the dataset's characteristics.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations of OOB Score**\n",
    "1. **Not Suitable for Highly Imbalanced Data:**\n",
    "   - OOB accuracy may not reflect performance well when classes are imbalanced.\n",
    "2. **Dependent on the Number of Trees:**\n",
    "   - OOB estimates become more stable with more trees in the forest.\n",
    "3. **Slower Training:**\n",
    "   - OOB requires additional computations for predictions, slightly increasing training time.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "- The OOB score is an **internal cross-validation method** used in Random Forest.\n",
    "- It uses the **37% of unseen data** for each tree to evaluate the modelâ€™s performance.\n",
    "- Itâ€™s a great way to assess accuracy **without needing a separate validation set**.\n",
    "- To enable it, set `oob_score=True` when training your Random Forest in scikit-learn.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of OOB Score:\n",
    "\n",
    "Imagine youâ€™re building a Random Forest model with a group of decision trees. To train each tree, you randomly pick data points from the training data **with replacement** (some data points get picked multiple times). \n",
    "\n",
    "Now, hereâ€™s the trick:\n",
    "- Not all data points get picked for every tree. \n",
    "- On average, about **37% of the data points are left out** when training a particular tree. These are called **Out-of-Bag (OOB) data points**.\n",
    "\n",
    "\n",
    "\n",
    "### **How Does the OOB Score Work?**\n",
    "\n",
    "1. **Each tree is trained only on a part of the data.**\n",
    "   - The 37% of data points not seen by the tree are like a **hidden test set** for that tree.\n",
    "\n",
    "2. **Make predictions for the OOB data.**\n",
    "   - Once the tree is trained, it tries to predict the outputs (labels) for the OOB data points it didnâ€™t see.\n",
    "\n",
    "3. **Combine the predictions.**\n",
    "   - For each data point in the training set, collect predictions from all the trees where it was OOB.\n",
    "   - Use **majority vote** (for classification) or **average** (for regression) to make a final prediction.\n",
    "\n",
    "4. **Calculate accuracy.**\n",
    "   - Compare these predictions to the actual labels of the data points.\n",
    "   - The overall accuracy is called the **OOB Score**.\n",
    "\n",
    "\n",
    "\n",
    "### **Why is OOB Score Useful?**\n",
    "- Itâ€™s like having a built-in test set.\n",
    "- You donâ€™t need to set aside extra data for validation, which means you can use all your data for training.\n",
    "\n",
    "\n",
    "\n",
    "### **Analogy**\n",
    "Think of training the Random Forest as a group project:\n",
    "- Each tree (person) works on **different parts of the project**.\n",
    "- After working, each person is asked to review the parts they didnâ€™t work on (OOB data).\n",
    "- The teamâ€™s final performance is measured by how well they did on the parts they didnâ€™t initially see.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Points in Layman Terms**\n",
    "1. **OOB data is \"leftover\" data:** Itâ€™s the part of the training data that each tree didnâ€™t use during training.\n",
    "2. **OOB score measures performance on this leftover data.** It tells you how well the Random Forest can predict unseen data.\n",
    "3. **You save time and data:** You donâ€™t need to set aside a separate test set to check accuracy because the OOB score does it for you.\n",
    "\n",
    "\n",
    "\n",
    "### **Example in Everyday Life**\n",
    "Imagine youâ€™re training multiple chefs (trees) to cook using a cookbook (training data). Each chef randomly selects recipes to practice (with replacement). The recipes they didnâ€™t practice (OOB data) are tested to see how well they learned. The test results from all chefs are combined to evaluate how good the overall team is.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technicality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s dive into the technical details of **Random Forest** and understand its working step by step. Random Forest is based on two core concepts: **Bootstrap Aggregation (Bagging)** and **Random Feature Selection**.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Bootstrap Aggregation (Bagging)**\n",
    "\n",
    "Random Forest creates multiple **decision trees**, and each tree is trained on a **different bootstrap sample**. \n",
    "\n",
    "#### Bootstrap Sampling:\n",
    "- From the training dataset of size $ N $, Random Forest takes $ N $ samples **with replacement** to create a new dataset for each tree. \n",
    "- Since sampling is with replacement, some data points may appear multiple times in the same sample, while others may be left out.\n",
    "- On average, about $ 63.2\\% $ of the data is used in each bootstrap sample, and the remaining $ 36.8\\% $ of the data (called **Out-Of-Bag (OOB)** data) can be used for performance evaluation.\n",
    "\n",
    "This helps in creating diverse datasets, which is key to reducing variance in predictions.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Random Feature Selection**\n",
    "\n",
    "Unlike traditional decision trees, Random Forest introduces an additional layer of randomness: **random feature selection**. \n",
    "\n",
    "#### At each split in a tree:\n",
    "- Instead of considering all the features to determine the best split, Random Forest selects a **random subset of features** (denoted as $ m $).\n",
    "- This ensures diversity among the trees because different trees will focus on different features.\n",
    "\n",
    "#### Typical values for $ m $:\n",
    "- For **classification tasks**: $ m = \\sqrt{p} $, where $ p $ is the total number of features.\n",
    "- For **regression tasks**: $ m = p/3 $.\n",
    "\n",
    "This randomness further decorrelates the trees and prevents overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Building Decision Trees**\n",
    "\n",
    "Each decision tree in the forest is constructed as follows:\n",
    "- Use the bootstrap sample as the training data.\n",
    "- At each node, consider only the random subset of features ($ m $).\n",
    "- Split the node on the feature and threshold that minimizes a specific criterion:\n",
    "  - **Gini Impurity** (for classification):\n",
    "    \\[\n",
    "    Gini = 1 - \\sum_{i=1}^k P_i^2\n",
    "    \\]\n",
    "    where $ P_i $ is the proportion of samples belonging to class $ i $ at a node.\n",
    "  - **Mean Squared Error (MSE)** (for regression):\n",
    "    \\[\n",
    "    MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\bar{y})^2\n",
    "    \\]\n",
    "    where $ \\bar{y} $ is the mean of the target variable at the node.\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Aggregation of Predictions**\n",
    "\n",
    "Once all the trees are trained, the forest makes predictions by aggregating the outputs of individual trees:\n",
    "\n",
    "- **For classification**: Perform majority voting. Each tree predicts a class label, and the class with the highest votes is the final prediction.\n",
    "- **For regression**: Compute the mean of the predictions from all the trees.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Out-of-Bag (OOB) Error**\n",
    "\n",
    "Since each tree is trained on a bootstrap sample, the remaining data (OOB data) can be used to evaluate the modelâ€™s performance:\n",
    "- Predict the OOB data using only the trees that did not see those samples during training.\n",
    "- Compute the **OOB error** as an estimate of the modelâ€™s generalization error.\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Feature Importance**\n",
    "\n",
    "Random Forest calculates feature importance based on two metrics:\n",
    "1. **Gini Importance (Mean Decrease in Impurity)**:\n",
    "   - Measures how much a feature contributes to reducing impurity across all trees.\n",
    "2. **Permutation Importance**:\n",
    "   - Measures the decrease in model performance when the values of a feature are randomly shuffled.\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Hyperparameters in Random Forest**\n",
    "\n",
    "Random Forest has several hyperparameters that you can tune to control its behavior:\n",
    "1. **Number of Trees** ($ n\\_estimators $): Number of trees in the forest.\n",
    "   - More trees generally improve performance but increase computational cost.\n",
    "2. **Maximum Depth** ($ max\\_depth $): Maximum depth of each tree.\n",
    "   - Controls overfitting by limiting the growth of trees.\n",
    "3. **Number of Features** ($ max\\_features $): Number of features considered at each split.\n",
    "4. **Minimum Samples per Split** ($ min\\_samples\\_split $): Minimum number of samples required to split a node.\n",
    "5. **Minimum Samples per Leaf** ($ min\\_samples\\_leaf $): Minimum number of samples required at a leaf node.\n",
    "6. **Bootstrap**: Whether bootstrap samples are used (default is True).\n",
    "\n",
    "\n",
    "\n",
    "### Random Forest Algorithm Summary:\n",
    "1. Draw $ n $ bootstrap samples from the training dataset.\n",
    "2. For each bootstrap sample:\n",
    "   - Train a decision tree, considering a random subset of features at each split.\n",
    "3. Repeat for $ n\\_estimators $ trees.\n",
    "4. For predictions:\n",
    "   - Aggregate results (majority vote for classification or average for regression).\n",
    "5. Optionally, calculate OOB error for evaluation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s take an example classification problem and go through how Random Forest works step-by-step using the technical concepts explained above.\n",
    "\n",
    "\n",
    "\n",
    "### Problem: Predicting if a customer will churn or not\n",
    "\n",
    "#### Dataset:\n",
    "Assume a dataset called `CustomerChurn` with the following features:\n",
    "1. **Age** (numeric)\n",
    "2. **MonthlyCharges** (numeric)\n",
    "3. **Tenure** (numeric)\n",
    "4. **ContractType** (categorical)\n",
    "5. **Churn** (target: binary classification with values `Yes` or `No`)\n",
    "\n",
    "Our task is to predict if a customer will churn (`Yes`) or not (`No`).\n",
    "\n",
    "\n",
    "\n",
    "### Step-by-Step Working of Random Forest\n",
    "\n",
    "#### **1. Bootstrap Sampling**\n",
    "Random Forest creates multiple bootstrap samples from the dataset. Letâ€™s say we have $ N = 1000 $ rows in the dataset, and we decide to build $ n\\_estimators = 5 $ trees.\n",
    "\n",
    "- For each tree:\n",
    "  - Randomly select $ N = 1000 $ rows **with replacement**.\n",
    "  - Each sample will contain duplicates of some rows and leave out others.\n",
    "  - For example:\n",
    "    - Tree 1: Uses rows [3, 7, 9, 3, 45, 18, ...].\n",
    "    - Tree 2: Uses rows [5, 23, 9, 45, 9, 12, ...].\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Random Feature Selection**\n",
    "At each split of each tree:\n",
    "- Instead of considering **all features**, only a random subset of features is considered.\n",
    "- Suppose the dataset has $ p = 4 $ features (`Age`, `MonthlyCharges`, `Tenure`, and `ContractType`), and we set $ max\\_features = \\sqrt{p} = 2 $.\n",
    "- For each split:\n",
    "  - Randomly choose 2 features to evaluate (e.g., `Age` and `MonthlyCharges`).\n",
    "  - Split on the feature that minimizes the **Gini Impurity**.\n",
    "\n",
    "\n",
    "\n",
    "#### **3. Decision Tree Building**\n",
    "Each tree grows using the bootstrap sample and splits nodes based on the best feature from the random subset. For example:\n",
    "- Tree 1:\n",
    "  - Root split: `MonthlyCharges > 70` (based on Gini Impurity).\n",
    "  - Second level: `Tenure > 12` on the left side, `ContractType == \"Month-to-Month\"` on the right side.\n",
    "  - Continue splitting until either the maximum depth is reached or there are fewer than `min_samples_split` samples.\n",
    "\n",
    "- Tree 2:\n",
    "  - Root split: `Age > 40`.\n",
    "  - Second level: `MonthlyCharges > 50` on the left side, `ContractType == \"One Year\"` on the right side.\n",
    "\n",
    "Each tree will grow differently due to randomness in both **bootstrap sampling** and **feature selection**.\n",
    "\n",
    "\n",
    "\n",
    "#### **4. Prediction Aggregation**\n",
    "Once the trees are built, we predict for a new customer. Suppose the input is:\n",
    "- `Age = 35`, `MonthlyCharges = 60`, `Tenure = 10`, `ContractType = \"Month-to-Month\"`\n",
    "\n",
    "- Each tree makes a prediction:\n",
    "  - Tree 1 predicts `Yes`.\n",
    "  - Tree 2 predicts `No`.\n",
    "  - Tree 3 predicts `Yes`.\n",
    "  - Tree 4 predicts `No`.\n",
    "  - Tree 5 predicts `Yes`.\n",
    "\n",
    "- Final Prediction:\n",
    "  - Random Forest performs a **majority vote**:\n",
    "    - $ \\text{Yes: 3 votes}, \\text{No: 2 votes} $\n",
    "    - Final prediction: `Yes`.\n",
    "\n",
    "\n",
    "\n",
    "#### **5. Out-of-Bag (OOB) Error**\n",
    "- Each tree leaves out about $ 36.8\\% $ of the data during its bootstrap sampling.\n",
    "- These OOB samples are used to estimate the model's performance without needing a separate validation set.\n",
    "\n",
    "For example:\n",
    "- Tree 1â€™s OOB samples: Rows [1, 4, 6, 10, ...].\n",
    "- Use Tree 1 to predict these rows and compute the OOB error.\n",
    "\n",
    "OOB error is calculated as:\n",
    "$$\n",
    "\\text{OOB Error} = \\frac{\\text{Number of Incorrect Predictions on OOB Samples}}{\\text{Total Number of OOB Samples}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### **6. Feature Importance**\n",
    "- Random Forest calculates feature importance by measuring:\n",
    "  1. **Decrease in Gini Impurity**:\n",
    "     - Track how much splitting on a feature reduces impurity across all trees.\n",
    "     - Example:\n",
    "       - Splitting on `MonthlyCharges` reduces impurity significantly, so its importance is high.\n",
    "  2. **Permutation Importance**:\n",
    "     - Randomly shuffle the values of each feature and measure how much the modelâ€™s performance drops.\n",
    "     - Example:\n",
    "       - If shuffling `Age` causes no performance drop, its importance is low.\n",
    "\n",
    "Feature Importance Example:\n",
    "| Feature           | Importance Score |\n",
    "|--------------------|------------------|\n",
    "| MonthlyCharges     | 0.45            |\n",
    "| Tenure             | 0.30            |\n",
    "| ContractType       | 0.20            |\n",
    "| Age                | 0.05            |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "For this problem, some common hyperparameters to tune include:\n",
    "1. `n_estimators = 5`: Number of trees in the forest.\n",
    "2. `max_features = 2`: Number of features considered at each split.\n",
    "3. `max_depth = 10`: Maximum depth of each tree.\n",
    "4. `min_samples_split = 5`: Minimum samples required to split a node.\n",
    "5. `bootstrap = True`: Use bootstrap sampling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scikit-learn Syntax for Random Forest**\n",
    "Below is the syntax for initializing and training a **Random Forest** in Scikit-learn, along with detailed explanations of its key parameters:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,         # Number of trees in the forest\n",
    "    criterion='gini',         # Criterion to measure quality of splits ('gini' or 'entropy')\n",
    "    max_depth=None,           # Maximum depth of trees\n",
    "    min_samples_split=2,      # Minimum samples required to split a node\n",
    "    min_samples_leaf=1,       # Minimum samples required in a leaf node\n",
    "    min_weight_fraction_leaf=0.0, # Minimum weighted fraction of total samples in a leaf node\n",
    "    max_features='sqrt',      # Number of features considered for the best split\n",
    "    max_leaf_nodes=None,      # Maximum number of leaf nodes\n",
    "    bootstrap=True,           # Whether bootstrap samples are used when building trees\n",
    "    oob_score=False,          # Whether to use out-of-bag samples to estimate generalization error\n",
    "    n_jobs=None,              # Number of parallel jobs to run\n",
    "    random_state=None,        # Random seed\n",
    "    verbose=0,                # Controls verbosity of tree building\n",
    "    warm_start=False,         # Reuse solution of previous fit for faster fitting\n",
    "    class_weight=None         # Weights associated with classes ('balanced' or dict)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Explanation of Parameters**\n",
    "\n",
    "#### 1. **`n_estimators` (Default: 100)**\n",
    "   - **Description**: Number of trees in the forest.\n",
    "   - **Impact**: Increasing this value reduces variance but increases computation time. More trees usually improve model performance until a point of diminishing returns.\n",
    "\n",
    "#### 2. **`criterion` (Default: `'gini'`)**\n",
    "   - **Options**:\n",
    "     - `'gini'`: Uses Gini Impurity to measure the quality of a split.\n",
    "     - `'entropy'`: Uses Information Gain (from entropy) to measure split quality.\n",
    "   - **Impact**: Both are effective; try both to see which performs better for your data.\n",
    "\n",
    "#### 3. **`max_depth` (Default: `None`)**\n",
    "   - **Description**: Maximum depth of each tree.\n",
    "   - **Impact**: Limits the depth of trees to prevent overfitting. A smaller value may underfit; a large value or `None` allows full growth.\n",
    "\n",
    "#### 4. **`min_samples_split` (Default: 2)**\n",
    "   - **Description**: Minimum number of samples required to split an internal node.\n",
    "   - **Impact**: Controls tree growth. Increasing this value restricts splitting and reduces overfitting.\n",
    "\n",
    "#### 5. **`min_samples_leaf` (Default: 1)**\n",
    "   - **Description**: Minimum number of samples required to be in a leaf node.\n",
    "   - **Impact**: Larger values create smoother, more generalized trees.\n",
    "\n",
    "#### 6. **`min_weight_fraction_leaf` (Default: 0.0)**\n",
    "   - **Description**: Minimum weighted fraction of total samples required to be in a leaf node.\n",
    "   - **Impact**: Useful when working with unbalanced datasets or sample weights.\n",
    "\n",
    "#### 7. **`max_features` (Default: `'sqrt'` for classification)**\n",
    "   - **Options**:\n",
    "     - `'sqrt'`: Uses the square root of total features.\n",
    "     - `'log2'`: Uses the base-2 logarithm of total features.\n",
    "     - `None`: Considers all features.\n",
    "   - **Impact**: Smaller values reduce variance but increase bias.\n",
    "\n",
    "#### 8. **`max_leaf_nodes` (Default: `None`)**\n",
    "   - **Description**: Maximum number of leaf nodes in the tree.\n",
    "   - **Impact**: Limits the growth of the tree.\n",
    "\n",
    "#### 9. **`bootstrap` (Default: `True`)**\n",
    "   - **Description**: Whether bootstrap samples are used when building trees.\n",
    "   - **Impact**: Enables **out-of-bag (OOB) score** estimation.\n",
    "\n",
    "#### 10. **`oob_score` (Default: `False`)**\n",
    "   - **Description**: Whether to use out-of-bag samples to estimate generalization error.\n",
    "   - **Impact**: If `True`, you can calculate the **OOB score**, which estimates test performance without needing a separate validation set.\n",
    "\n",
    "#### 11. **`n_jobs` (Default: `None`)**\n",
    "   - **Description**: Number of jobs to run in parallel for fitting and prediction.\n",
    "   - **Options**:\n",
    "     - `-1`: Uses all available processors.\n",
    "     - `None`: Runs sequentially.\n",
    "\n",
    "#### 12. **`random_state` (Default: `None`)**\n",
    "   - **Description**: Seed for reproducibility.\n",
    "\n",
    "#### 13. **`verbose` (Default: 0)**\n",
    "   - **Description**: Controls verbosity of the output.\n",
    "   - **Options**:\n",
    "     - `0`: No output.\n",
    "     - `1`: Progress output during training.\n",
    "\n",
    "#### 14. **`warm_start` (Default: `False`)**\n",
    "   - **Description**: If `True`, adds more trees to the existing model instead of starting from scratch.\n",
    "\n",
    "#### 15. **`class_weight` (Default: `None`)**\n",
    "   - **Description**: Weights associated with classes to handle imbalanced datasets.\n",
    "   - **Options**:\n",
    "     - `'balanced'`: Automatically adjusts weights inversely proportional to class frequencies.\n",
    "     - `dict`: You can manually specify weights.\n",
    "\n",
    "\n",
    "\n",
    "### **Feature Importance**\n",
    "\n",
    "Feature importance in a Random Forest is calculated based on how much each feature reduces impurity (e.g., Gini Impurity or Entropy) across all trees in the forest. Features with higher importance values contribute more to the modelâ€™s predictions.\n",
    "\n",
    "#### **Code to Display Feature Importance**\n",
    "\n",
    "```python\n",
    "# Extract feature importance\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display features from most to least important\n",
    "print(\"\\nFeature Importance (Descending Order):\\n\", importance_df)\n",
    "\n",
    "# Plot the feature importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.gca().invert_yaxis()  # Invert axis to show the most important at the top\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Most Important Features Output**\n",
    "For the example dataset:\n",
    "\n",
    "```\n",
    "Feature Importance (Descending Order):\n",
    "            Feature  Importance\n",
    "0              Age      0.6361\n",
    "1  MonthlyCharges      0.1775\n",
    "2           Tenure      0.1130\n",
    "3     ContractType      0.0733\n",
    "```\n",
    "\n",
    "The **Age** feature has the highest importance, meaning it plays the most significant role in predicting the target (`Churn`).\n",
    "\n",
    "\n",
    "\n",
    "### **OOB Score**\n",
    "If `bootstrap=True` and `oob_score=True`, the OOB samples (out-of-bag samples) are those not included in the bootstrap sample for each tree. The OOB score estimates the modelâ€™s test accuracy without needing a validation set.\n",
    "\n",
    "#### **Key Points**:\n",
    "- OOB samples are effectively a form of cross-validation.\n",
    "- OOB error decreases as the number of trees increases.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "```python\n",
    "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"OOB Score:\", rf.oob_score_)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full syntax of the `RandomForestRegressor` in Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    criterion='squared_error', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    oob_score=False, \n",
    "    n_jobs=None, \n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Full Explanation of Parameters (Ranked High to Low Priority):\n",
    "\n",
    "#### **1. `n_estimators`**\n",
    "- **Description**: Number of trees in the forest.\n",
    "- **Default**: `100`\n",
    "- **Impact**: A higher number increases stability and accuracy but also computational cost.\n",
    "- **Recommendation**: Start with 100 and increase if more accuracy is needed.\n",
    "\n",
    "\n",
    "\n",
    "#### **2. `max_depth`**\n",
    "- **Description**: The maximum depth of each decision tree.\n",
    "- **Default**: `None` (trees grow until leaves are pure or minimum samples are reached).\n",
    "- **Impact**: Controls overfitting (too deep) and underfitting (too shallow).\n",
    "- **Recommendation**: Use a value to limit depth for large datasets to reduce overfitting.\n",
    "\n",
    "\n",
    "\n",
    "#### **3. `max_features`**\n",
    "- **Description**: The number of features to consider for splitting at each node.\n",
    "- **Default**: `'sqrt'` (square root of the total features).\n",
    "- **Impact**: Balances the diversity of trees (low value) with accuracy (high value).\n",
    "- **Options**:\n",
    "  - `'auto'` or `'sqrt'`: âˆštotal features (default).\n",
    "  - `'log2'`: logâ‚‚(total features).\n",
    "  - `None`: Use all features.\n",
    "  - An integer or float: Specifies the exact number or fraction of features.\n",
    "\n",
    "\n",
    "\n",
    "#### **4. `min_samples_split`**\n",
    "- **Description**: The minimum number of samples required to split a node.\n",
    "- **Default**: `2`\n",
    "- **Impact**: Higher values reduce overfitting by limiting tree growth.\n",
    "- **Recommendation**: Use larger values (e.g., 5â€“10) for noisy datasets.\n",
    "\n",
    "\n",
    "\n",
    "#### **5. `min_samples_leaf`**\n",
    "- **Description**: The minimum number of samples required to be a leaf node.\n",
    "- **Default**: `1`\n",
    "- **Impact**: Prevents trees from learning overly specific patterns.\n",
    "- **Recommendation**: Increase to reduce overfitting (e.g., `5` or `10`).\n",
    "\n",
    "\n",
    "\n",
    "#### **6. `bootstrap`**\n",
    "- **Description**: Whether to use bootstrapped samples when building trees.\n",
    "- **Default**: `True`\n",
    "- **Impact**: Ensures randomness and prevents overfitting.\n",
    "- **Recommendation**: Keep `True` unless you want to use all data for every tree.\n",
    "\n",
    "\n",
    "\n",
    "#### **7. `criterion`**\n",
    "- **Description**: Function to measure the quality of a split.\n",
    "- **Default**: `'squared_error'` (mean squared error for regression).\n",
    "- **Impact**: Determines how splits are made.\n",
    "- **Options**:\n",
    "  - `'squared_error'`: Default, minimizes variance.\n",
    "  - `'absolute_error'`: Minimizes absolute error (less sensitive to outliers).\n",
    "  - `'poisson'`: Suitable for count data (non-negative targets).\n",
    "\n",
    "\n",
    "\n",
    "#### **8. `oob_score`**\n",
    "- **Description**: Whether to use out-of-bag samples to estimate accuracy.\n",
    "- **Default**: `False`\n",
    "- **Impact**: Provides an unbiased estimate of model performance.\n",
    "- **Recommendation**: Use `True` for evaluating models without a validation set.\n",
    "\n",
    "\n",
    "\n",
    "#### **9. `n_jobs`**\n",
    "- **Description**: Number of parallel jobs to run for training.\n",
    "- **Default**: `None` (uses 1 core).\n",
    "- **Impact**: Reduces training time.\n",
    "- **Recommendation**: Use `-1` to utilize all available cores.\n",
    "\n",
    "\n",
    "\n",
    "#### **10. `random_state`**\n",
    "- **Description**: Seed for random number generation.\n",
    "- **Default**: `None`\n",
    "- **Impact**: Ensures reproducibility of results.\n",
    "- **Recommendation**: Use an integer (e.g., `42`) for consistent results.\n",
    "\n",
    "\n",
    "\n",
    "#### **11. `max_leaf_nodes`**\n",
    "- **Description**: Maximum number of leaf nodes in a tree.\n",
    "- **Default**: `None`\n",
    "- **Impact**: Limits tree complexity to prevent overfitting.\n",
    "- **Recommendation**: Set a reasonable limit for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "#### **12. `min_weight_fraction_leaf`**\n",
    "- **Description**: Minimum weighted fraction of the input samples required to be a leaf node.\n",
    "- **Default**: `0.0`\n",
    "- **Impact**: Useful when dealing with imbalanced datasets.\n",
    "- **Recommendation**: Keep default unless working with weighted datasets.\n",
    "\n",
    "\n",
    "\n",
    "#### **13. `min_impurity_decrease`**\n",
    "- **Description**: Splits a node only if impurity reduction is greater than this value.\n",
    "- **Default**: `0.0`\n",
    "- **Impact**: Controls growth of trees and reduces overfitting.\n",
    "- **Recommendation**: Use small positive values for fine control.\n",
    "\n",
    "\n",
    "\n",
    "#### **14. `warm_start`**\n",
    "- **Description**: Reuse previous solution to add more trees.\n",
    "- **Default**: `False`\n",
    "- **Impact**: Speeds up training when adding estimators incrementally.\n",
    "- **Recommendation**: Use for iterative improvements.\n",
    "\n",
    "\n",
    "\n",
    "#### **15. `ccp_alpha`**\n",
    "- **Description**: Complexity parameter used for pruning trees.\n",
    "- **Default**: `0.0`\n",
    "- **Impact**: Removes nodes with insufficient complexity improvement.\n",
    "- **Recommendation**: Tune for balancing underfitting and overfitting.\n",
    "\n",
    "\n",
    "\n",
    "#### **16. `max_samples`**\n",
    "- **Description**: Number (or fraction) of samples to draw for training each tree.\n",
    "- **Default**: `None` (uses all samples).\n",
    "- **Impact**: Provides additional randomness and reduces overfitting.\n",
    "- **Recommendation**: Use a fraction (e.g., `0.8`) for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "### Example Usage:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=200, \n",
    "    max_depth=10, \n",
    "    min_samples_split=5, \n",
    "    min_samples_leaf=2, \n",
    "    max_features='sqrt', \n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
