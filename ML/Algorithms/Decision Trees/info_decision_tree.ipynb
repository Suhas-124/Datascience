{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree:\n",
    "\n",
    "A **Decision Tree** is a supervised learning algorithm used for classification and regression tasks. It works by splitting the dataset into subsets based on feature values, forming a tree-like structure. Each node in the tree represents a decision based on a feature, and the leaves represent the final output or prediction.\n",
    "\n",
    "\n",
    "\n",
    "### How Does a Decision Tree Work?\n",
    "\n",
    "1. **Root Node**: The starting point of the tree, representing the entire dataset. A feature is selected to split the data into two or more subsets.\n",
    "  \n",
    "2. **Splitting**: The process of dividing the dataset into subsets based on feature values. The splits aim to maximize the separation of data into distinct classes or values.\n",
    "\n",
    "3. **Decision Nodes**: These are intermediate nodes where further splitting occurs based on other features.\n",
    "\n",
    "4. **Leaf Nodes**: These are terminal nodes that contain the final output or prediction. For classification, it represents the class label, and for regression, it represents a numerical value.\n",
    "\n",
    "5. **Path**: A sequence of decisions (from the root to a leaf) that leads to a prediction.\n",
    "\n",
    "\n",
    "\n",
    "### Key Concepts in Decision Trees\n",
    "\n",
    "#### 1. **Splitting Criteria**\n",
    "The algorithm chooses the best feature to split the data at each node using a mathematical criterion:\n",
    "- **Gini Impurity**: Measures the probability of incorrect classification.\n",
    "- **Entropy** (Information Gain): Measures the amount of information gained from a split.\n",
    "- **Variance Reduction**: Used in regression tasks to minimize the variance in the target variable.\n",
    "\n",
    "#### 2. **Recursive Partitioning**\n",
    "The tree grows by recursively splitting the dataset until a stopping criterion is met:\n",
    "- Maximum tree depth is reached.\n",
    "- Minimum number of samples per node is satisfied.\n",
    "- Further splitting does not improve the model.\n",
    "\n",
    "#### 3. **Overfitting**\n",
    "Decision trees can grow too complex, fitting the training data perfectly but performing poorly on new data. This is called overfitting.\n",
    "\n",
    "\n",
    "\n",
    "### Types of Decision Trees\n",
    "\n",
    "1. **Classification Tree**: Used when the target variable is categorical. For example:\n",
    "   - Predicting whether an email is \"Spam\" or \"Not Spam.\"\n",
    "   \n",
    "2. **Regression Tree**: Used when the target variable is continuous. For example:\n",
    "   - Predicting house prices based on location, size, and age.\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Decision Trees\n",
    "1. **Easy to Understand**: The tree structure is interpretable and can be visualized.\n",
    "2. **No Feature Scaling Required**: Works well with raw, unscaled data.\n",
    "3. **Handles Both Numerical and Categorical Data**.\n",
    "4. **Non-Parametric**: No assumptions about the underlying data distribution.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of Decision Trees\n",
    "1. **Overfitting**: Trees can become overly complex without proper regularization.\n",
    "2. **Unstable**: Small changes in the data can lead to a completely different tree.\n",
    "3. **Bias Towards Dominant Classes**: In imbalanced datasets, the tree may favor the majority class.\n",
    "\n",
    "\n",
    "\n",
    "### Regularization Techniques\n",
    "To control overfitting, decision trees use regularization parameters:\n",
    "1. **Maximum Depth**: Limits the number of levels in the tree.\n",
    "2. **Minimum Samples per Leaf**: Ensures each leaf has a minimum number of samples.\n",
    "3. **Pruning**: Trims branches of the tree that do not contribute to predictive power.\n",
    "\n",
    "\n",
    "\n",
    "### Applications of Decision Trees\n",
    "1. **Healthcare**: Predicting diseases based on symptoms.\n",
    "2. **Finance**: Credit risk assessment.\n",
    "3. **Marketing**: Customer segmentation.\n",
    "4. **Retail**: Predicting customer churn.\n",
    "\n",
    "\n",
    "\n",
    "### Visualization Example\n",
    "\n",
    "Suppose we want to classify whether a person will buy a car based on:\n",
    "- **Income** (High, Medium, Low)\n",
    "- **Age** (Young, Middle-aged, Old)\n",
    "\n",
    "A decision tree might look like this:\n",
    "\n",
    "```\n",
    "             [Income?]\n",
    "            /          \\\n",
    "        High            Low\n",
    "        /                 \\\n",
    "    [Age?]              No\n",
    "   /      \\\n",
    "Young    Old\n",
    "Yes       No\n",
    "```\n",
    "\n",
    "**Interpretation**:\n",
    "1. If income is high and age is young, the person buys the car.\n",
    "2. If income is low, the person does not buy the car.\n",
    "\n",
    "\n",
    "\n",
    "### Algorithms for Decision Trees\n",
    "- **CART (Classification and Regression Trees)**: The most common algorithm.\n",
    "- **ID3 (Iterative Dichotomiser 3)**: Uses information gain.\n",
    "- **C4.5**: An extension of ID3, handles both categorical and continuous data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of decision Tree:\n",
    "\n",
    "Imagine you're trying to decide **what to eat for dinner**. You ask yourself a series of yes/no questions, like:\n",
    "\n",
    "1. Do I feel like eating something healthy?  \n",
    "   - If **yes**, move to the next question.  \n",
    "   - If **no**, eat pizza.\n",
    "\n",
    "2. Do I want a vegetarian option?  \n",
    "   - If **yes**, eat a salad.  \n",
    "   - If **no**, eat grilled chicken.\n",
    "\n",
    "By answering these questions step-by-step, you \"decide\" what to eat. This step-by-step process is what a **decision tree** does in machine learning.\n",
    "\n",
    "\n",
    "\n",
    "### How Decision Trees Work\n",
    "A decision tree is just a set of rules organized like a flowchart. Here's how it works:\n",
    "1. **Start at the top (root)**: The first question is the \"root\" of the tree.\n",
    "2. **Follow the branches**: Each question (node) has branches that split the data based on the answers (conditions).\n",
    "3. **End at the leaves**: The final decision (leaf) gives you the result or prediction.\n",
    "\n",
    "\n",
    "\n",
    "### Example in a Real Scenario\n",
    "\n",
    "**Task**: Predict whether a student will pass an exam based on study hours and sleep.\n",
    "\n",
    "1. **Question 1**: Did the student study more than 2 hours?  \n",
    "   - If **yes**, go to the next question.  \n",
    "   - If **no**, predict: **Fail**.\n",
    "\n",
    "2. **Question 2**: Did the student sleep at least 6 hours?  \n",
    "   - If **yes**, predict: **Pass**.  \n",
    "   - If **no**, predict: **Fail**.\n",
    "\n",
    "This \"tree\" helps us decide whether the student will pass or fail.\n",
    "\n",
    "\n",
    "\n",
    "### Key Parameters (Tree Tuning Options)\n",
    "\n",
    "Decision trees need some rules to decide how to split and stop growing. These rules are called **parameters**:\n",
    "\n",
    "1. **Max Depth (How Tall the Tree Is)**  \n",
    "   - Limits how many questions the tree can ask.\n",
    "   - Example: If max depth is 2, the tree will only ask 2 questions, even if it could ask more.\n",
    "\n",
    "2. **Min Samples Split (When to Stop Splitting)**  \n",
    "   - The tree won’t split a branch if it has fewer than this number of data points.\n",
    "   - Example: If min samples split is 5, the tree won’t ask more questions in branches with less than 5 data points.\n",
    "\n",
    "3. **Min Samples Leaf (How Many Data Points Per Answer)**  \n",
    "   - The leaf (final decision) must have at least this many data points.\n",
    "   - Example: If this is 2, a branch won’t end with just 1 data point.\n",
    "\n",
    "4. **Criterion (How to Decide the Best Question)**  \n",
    "   - **Gini Index**: Chooses questions that make the data as \"pure\" as possible (like grouping similar items together).  \n",
    "   - **Entropy**: Measures how much \"disorder\" is reduced by a question.  \n",
    "   Think of these as methods to pick the smartest question to ask at each step.\n",
    "\n",
    "\n",
    "\n",
    "### Why Use a Decision Tree?\n",
    "- **Easy to Understand**: Like asking questions to solve a problem.\n",
    "- **Flexible**: Works with numbers (study hours) and categories (healthy vs unhealthy food).\n",
    "- **No Need for Fancy Math**: No need to scale or modify the data.\n",
    "\n",
    "\n",
    "\n",
    "### Problems with Decision Trees\n",
    "- **Overfitting**: If you let the tree ask too many questions, it can memorize the data instead of learning general rules.  \n",
    "  (Imagine a tree that has 100 levels and knows every possible dinner choice but can't handle new situations.)  \n",
    "  Solution: Limit the depth or number of splits.\n",
    "\n",
    "\n",
    "\n",
    "### A Real-Life Analogy\n",
    "\n",
    "Think of a decision tree as a **quiz in a magazine**:\n",
    "- You start at the top with a question, like \"Do you enjoy outdoor activities?\"  \n",
    "- Each answer takes you to a different question or result (like \"You should try hiking!\").\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees Hyperparameters, Overfitting, and Underfitting\n",
    "\n",
    "\n",
    "\n",
    "### **1. Hyperparameters of Decision Trees**\n",
    "\n",
    "Hyperparameters are settings we can tweak in a decision tree to control how it grows. These settings impact the performance of the tree and how well it generalizes to new data.\n",
    "\n",
    "Here are the **key hyperparameters**:\n",
    "\n",
    "#### **1.1 Max Depth**\n",
    "- **What it does**: Limits the maximum number of levels in the tree.\n",
    "- **Effect**:\n",
    "  - **Small max depth**: The tree is shallow, making simple decisions. Risk of **underfitting** (missing patterns in the data).\n",
    "  - **Large max depth**: The tree is deep, capturing every detail. Risk of **overfitting** (memorizing the data instead of generalizing).\n",
    "\n",
    "#### **1.2 Min Samples Split**\n",
    "- **What it does**: The minimum number of data points required to split a node into branches.\n",
    "- **Effect**:\n",
    "  - **High value**: Stops splitting early, resulting in a simpler tree (helps prevent overfitting).\n",
    "  - **Low value**: Allows the tree to keep splitting, creating complex structures (risk of overfitting).\n",
    "\n",
    "#### **1.3 Min Samples Leaf**\n",
    "- **What it does**: The minimum number of data points that must be present in a leaf node (final decision point).\n",
    "- **Effect**:\n",
    "  - **High value**: Ensures each leaf represents more data, simplifying the tree (prevents overfitting).\n",
    "  - **Low value**: Allows the tree to create very small groups (risk of overfitting).\n",
    "\n",
    "#### **1.4 Max Features**\n",
    "- **What it does**: Limits the number of features (variables) to consider at each split.\n",
    "- **Effect**:\n",
    "  - **Low value**: Forces the tree to pick the most important features, reducing complexity (helps avoid overfitting).\n",
    "  - **High value**: Uses more features, increasing the risk of overfitting.\n",
    "\n",
    "#### **1.5 Criterion**\n",
    "- **What it does**: Determines how the tree decides the best split at each node.\n",
    "  - **Gini Impurity**: Focuses on creating \"pure\" groups (e.g., most of the data in one class).\n",
    "  - **Entropy**: Measures how much information is gained by a split.\n",
    "- **Effect**:\n",
    "  - Different criteria might result in slightly different splits, but the overall behavior of the tree remains similar.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Overfitting and Underfitting in Decision Trees**\n",
    "\n",
    "#### **2.1 Overfitting**\n",
    "- **What it is**: When the decision tree becomes too complex, capturing noise and irrelevant details in the training data.\n",
    "- **Symptoms**:\n",
    "  - High accuracy on the training data.\n",
    "  - Poor performance on unseen (test) data.\n",
    "- **Cause**:\n",
    "  - Deep trees with many splits.\n",
    "  - Small `min_samples_leaf` or `min_samples_split`.\n",
    "- **Solution**:\n",
    "  - Limit the depth (`max_depth`).\n",
    "  - Set a higher `min_samples_split` or `min_samples_leaf`.\n",
    "  - Use **pruning**: Remove unnecessary branches after the tree is built.\n",
    "  - Use ensembles like **Random Forest** or **Boosting** to average out overfitted trees.\n",
    "\n",
    "\n",
    "\n",
    "#### **2.2 Underfitting**\n",
    "- **What it is**: When the decision tree is too simple to capture the patterns in the data.\n",
    "- **Symptoms**:\n",
    "  - Low accuracy on both training and test data.\n",
    "- **Cause**:\n",
    "  - Tree is too shallow (low `max_depth`).\n",
    "  - Early stopping by high `min_samples_split` or `min_samples_leaf`.\n",
    "- **Solution**:\n",
    "  - Allow a deeper tree (`max_depth`).\n",
    "  - Lower `min_samples_split` or `min_samples_leaf`.\n",
    "  - Add more relevant features to the dataset.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Balancing Overfitting and Underfitting**\n",
    "\n",
    "To create a tree that generalizes well (neither overfits nor underfits), you need to balance its complexity using hyperparameters:\n",
    "1. **Choose a reasonable max depth**:\n",
    "   - Small datasets: Use a deeper tree (`max_depth`).\n",
    "   - Large datasets: Limit depth to prevent overfitting.\n",
    "2. **Use minimum samples for split and leaf**:\n",
    "   - Avoid very small splits or leaves to reduce overfitting.\n",
    "3. **Cross-validation**:\n",
    "   - Use techniques like k-fold cross-validation to evaluate your tree's performance and tune the hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "### Example in Simple Terms\n",
    "Imagine you're teaching a child to distinguish between fruits:\n",
    "- **Underfitting**: You teach them, \"If it's round, it's an apple.\" (Too simple, misses important patterns like color and size.)\n",
    "- **Overfitting**: You teach them, \"If it's red, round, weighs exactly 150 grams, and has a small stem, it's an apple.\" (Too detailed, won’t work for new apples that don't fit this exact description.)\n",
    "- **Balanced**: You teach them, \"If it's red, round, and medium-sized, it’s probably an apple.\" (Captures the general pattern without being too rigid.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees:\n",
    "\n",
    "A **Regression Tree** is a type of decision tree used to predict continuous (numerical) values instead of categories. It splits the data into smaller and smaller subsets based on feature values, creating a tree-like structure. The final predictions at the leaves are the **average of target values** (for those data points in the leaf).\n",
    "\n",
    "\n",
    "\n",
    "### **How Regression Trees Work**\n",
    "\n",
    "1. **Splitting the Data**:\n",
    "   - The tree chooses a feature and a value to split the data into two groups.\n",
    "   - The goal is to minimize the difference (error) between the predicted and actual values after the split.\n",
    "   - A common metric for this is the **Mean Squared Error (MSE)**:\n",
    "     $$\n",
    "     MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y})^2\n",
    "     $$\n",
    "     - $ y_i $: Actual target value.\n",
    "     - $ \\hat{y} $: Predicted value (mean of target values in the group).\n",
    "     - $ n $: Number of samples in the group.\n",
    "\n",
    "2. **Recursive Partitioning**:\n",
    "   - The tree keeps splitting the data at each node until it meets a stopping criterion (e.g., minimum samples in a leaf, maximum depth).\n",
    "\n",
    "3. **Making Predictions**:\n",
    "   - Once the tree is built, predictions are made by traversing the tree and arriving at a leaf node.\n",
    "   - The prediction for a new data point is the **mean target value** of the training data points in that leaf.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Parameters of Regression Trees**\n",
    "\n",
    "1. **Max Depth**:\n",
    "   - Limits how deep the tree can grow.\n",
    "   - Prevents overfitting by restricting the number of splits.\n",
    "\n",
    "2. **Min Samples Split**:\n",
    "   - Minimum number of samples required to split a node.\n",
    "   - Larger values prevent unnecessary splits.\n",
    "\n",
    "3. **Min Samples Leaf**:\n",
    "   - Minimum number of samples required in a leaf.\n",
    "   - Ensures the tree doesn’t create small, irrelevant leaves.\n",
    "\n",
    "4. **Criterion**:\n",
    "   - The metric used to decide the quality of splits.\n",
    "   - For regression trees, it is usually **MSE** or **MAE (Mean Absolute Error)**.\n",
    "\n",
    "5. **Max Features**:\n",
    "   - Limits the number of features to consider at each split.\n",
    "   - Reduces overfitting by forcing the tree to focus on the most important features.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Regression Trees**\n",
    "\n",
    "1. **Simple to Understand**:\n",
    "   - The tree structure is easy to interpret and explain.\n",
    "2. **No Need for Data Scaling**:\n",
    "   - Works directly with raw numerical data.\n",
    "3. **Handles Non-linear Relationships**:\n",
    "   - Can model complex relationships between features and the target variable.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of Regression Trees**\n",
    "\n",
    "1. **Prone to Overfitting**:\n",
    "   - Without regularization, the tree can grow too complex, capturing noise in the data.\n",
    "2. **Unstable**:\n",
    "   - Small changes in the training data can result in a completely different tree.\n",
    "3. **Not as Accurate Alone**:\n",
    "   - Regression trees are often combined in ensembles like **Random Forest** or **Gradient Boosting** for better performance.\n",
    "\n",
    "\n",
    "\n",
    "### **Overfitting and Underfitting in Regression Trees**\n",
    "\n",
    "#### Overfitting\n",
    "- **What it is**: The tree is too deep and complex, perfectly fitting the training data but performing poorly on new data.\n",
    "- **Solution**:\n",
    "  - Limit the depth (`max_depth`).\n",
    "  - Use a higher `min_samples_leaf`.\n",
    "\n",
    "#### Underfitting\n",
    "- **What it is**: The tree is too simple and doesn’t capture the patterns in the data.\n",
    "- **Solution**:\n",
    "  - Allow a deeper tree.\n",
    "  - Reduce `min_samples_split` or `min_samples_leaf`.\n",
    "\n",
    "\n",
    "### **Example**\n",
    "\n",
    "Let’s say you’re predicting house prices based on **number of rooms** and **location quality**.\n",
    "\n",
    "#### Data:\n",
    "| Rooms | Location Quality | Price ($) |\n",
    "|-------|------------------|-----------|\n",
    "| 2     | Good             | 300,000   |\n",
    "| 3     | Average          | 200,000   |\n",
    "| 4     | Good             | 400,000   |\n",
    "| 2     | Average          | 150,000   |\n",
    "\n",
    "\n",
    "#### Splitting:\n",
    "1. First Split:\n",
    "   - Split on **Location Quality**.\n",
    "   - Two groups: \"Good\" and \"Average\".\n",
    "\n",
    "2. Second Split (if allowed):\n",
    "   - Split \"Good\" group further based on **Rooms**.\n",
    "\n",
    "#### Predictions:\n",
    "- For any new house:\n",
    "  - If **Location Quality = Good** and **Rooms = 4**, predict $ 400,000 $ (mean of \"Good, 4 rooms\").\n",
    "  - If **Location Quality = Average**, predict $ 175,000 $ (mean of all houses in the \"Average\" group).\n",
    "\n",
    "\n",
    "\n",
    "### **Python Implementation**\n",
    "\n",
    "Here’s how to build a regression tree in Python using `sklearn`:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.DataFrame({\n",
    "    'Rooms': [2, 3, 4, 2],\n",
    "    'LocationQuality': [1, 0, 1, 0],  # Good=1, Average=0\n",
    "    'Price': [300000, 200000, 400000, 150000]\n",
    "})\n",
    "\n",
    "X = data[['Rooms', 'LocationQuality']]\n",
    "y = data['Price']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train regression tree\n",
    "regressor = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Visualization of the Tree**\n",
    "You can visualize the tree using `plot_tree`:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(regressor, feature_names=['Rooms', 'LocationQuality'], filled=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This visualization helps understand how the regression tree splits the data and makes predictions.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
