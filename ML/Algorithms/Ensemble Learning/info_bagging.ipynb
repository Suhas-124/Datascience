{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging:\n",
    "\n",
    "**Bagging**, short for **Bootstrap Aggregating**, is an ensemble learning technique used to improve the performance of machine learning models by combining multiple models (often called base models) trained on different subsets of the training data. It reduces **variance** and improves model stability and accuracy, especially for high-variance models like decision trees.\n",
    "\n",
    "\n",
    "\n",
    "### Key Steps in Bagging\n",
    "\n",
    "1. **Bootstrapping (Data Sampling)**:\n",
    "   - Randomly sample data **with replacement** from the training dataset.\n",
    "   - Each sample (called a \"bootstrap sample\") has the same size as the original dataset but may have duplicates due to replacement.\n",
    "   - Each base model is trained on a different bootstrap sample.\n",
    "\n",
    "2. **Training Base Models**:\n",
    "   - Multiple models (e.g., decision trees, SVMs) are trained on the different bootstrap samples independently.\n",
    "   - These models can be of the same type but are trained on varied subsets of data.\n",
    "\n",
    "3. **Aggregation of Predictions**:\n",
    "   - For classification tasks: Use **majority voting** (hard voting) or **average class probabilities** (soft voting) to combine predictions from all base models.\n",
    "   - For regression tasks: Take the **average** of the predictions from all base models.\n",
    "\n",
    "\n",
    "\n",
    "### Why Use Bagging?\n",
    "\n",
    "1. **Reduces Overfitting**:\n",
    "   - By averaging multiple models, bagging reduces the risk of overfitting on the training data.\n",
    "\n",
    "2. **Decreases Variance**:\n",
    "   - A single model may be sensitive to small changes in the training data. Bagging mitigates this by aggregating multiple models trained on different data subsets.\n",
    "\n",
    "3. **Improves Stability**:\n",
    "   - Especially effective for algorithms prone to high variance, like decision trees.\n",
    "\n",
    "\n",
    "\n",
    "### Example: Bagging with Decision Trees\n",
    "\n",
    "1. **Without Bagging**:\n",
    "   - A single decision tree may overfit the training data and perform poorly on unseen data.\n",
    "\n",
    "2. **With Bagging (e.g., Random Forest)**:\n",
    "   - Multiple decision trees are trained on bootstrapped datasets.\n",
    "   - The predictions are aggregated, leading to a more stable and accurate model.\n",
    "\n",
    "\n",
    "\n",
    "### Bagging Algorithm (Steps in Pseudocode)\n",
    "\n",
    "1. **Input**:\n",
    "   - Training dataset \\( D = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} \\)\n",
    "   - Number of base models \\( k \\)\n",
    "\n",
    "2. **For \\( i = 1 \\) to \\( k \\):**\n",
    "   - Draw a bootstrap sample \\( D_i \\) from \\( D \\).\n",
    "   - Train a base model \\( M_i \\) on \\( D_i \\).\n",
    "\n",
    "3. **Aggregate Predictions**:\n",
    "   - For a new input \\( x \\):\n",
    "     - If classification: Take the majority vote or average probabilities.\n",
    "     - If regression: Take the mean of the predictions.\n",
    "\n",
    "4. **Output**:\n",
    "   - Final prediction based on aggregated results.\n",
    "\n",
    "\n",
    "\n",
    "### Real-World Example: Random Forest\n",
    "- **Random Forest** is a popular bagging algorithm that builds multiple decision trees.\n",
    "- Each tree is trained on a bootstrap sample of the data.\n",
    "- During training, Random Forest also randomly selects a subset of features for splitting, adding an extra layer of randomness.\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Bagging\n",
    "1. Works well with high-variance models.\n",
    "2. Reduces overfitting (especially with decision trees).\n",
    "3. Easy to implement.\n",
    "4. Improves prediction accuracy.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of Bagging\n",
    "1. **Computationally Intensive**:\n",
    "   - Training multiple models can be time-consuming.\n",
    "2. **Less Effective on Low-Variance Models**:\n",
    "   - Algorithms like linear regression or SVMs may not benefit much from bagging.\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Bagging?\n",
    "- When your model has **high variance** and is prone to overfitting (e.g., decision trees).\n",
    "- When you have enough computational power to train multiple models.\n",
    "- When you want to improve model stability and prediction accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Bagging:\n",
    "\n",
    "Absolutely! Let’s break **bagging** into very simple terms with an easy-to-follow example.\n",
    "\n",
    "\n",
    "\n",
    "### Imagine this Situation:\n",
    "You want to make a big decision, like buying a new phone. Instead of trusting just **one person’s advice**, you ask **10 of your friends**. Each friend gives you their opinion, and then you decide based on the majority's recommendation (if it’s a yes/no question) or average their suggestions (if it’s a rating out of 10).\n",
    "\n",
    "**Bagging works like this!**\n",
    "\n",
    "\n",
    "\n",
    "### What is Bagging in Layman Terms?\n",
    "- **Bagging (Bootstrap Aggregating)** is like asking many \"advisors\" (models) for their opinions (predictions) and combining them to make the final decision.\n",
    "- Instead of relying on **one model**, it trains **multiple models** on slightly different versions of the data and then combines their outputs to make a stronger, more reliable prediction.\n",
    "\n",
    "\n",
    "\n",
    "### How Does Bagging Work?\n",
    "\n",
    "1. **Split the Work**: \n",
    "   - Take your original dataset and create **multiple random samples** from it. (These are like different questions you ask your friends.)\n",
    "   - Each random sample may have some overlap because sampling is done **with replacement** (like picking marbles from a bag and putting them back).\n",
    "\n",
    "2. **Train Multiple Models**:\n",
    "   - Train a separate model (or friend) on each sample.\n",
    "   - Each model learns slightly differently because it’s looking at a slightly different version of the data.\n",
    "\n",
    "3. **Combine the Results**:\n",
    "   - For classification (yes/no): Use **majority voting** — if most models say \"yes,\" the final answer is \"yes.\"\n",
    "   - For regression (numbers): Use **averaging** — take the average of all the model predictions.\n",
    "\n",
    "\n",
    "\n",
    "### Why Does Bagging Work?\n",
    "1. **Reduces Overfitting**:\n",
    "   - A single model might overfit (memorize the training data and perform poorly on new data). Bagging spreads the \"work\" across multiple models, reducing overfitting.\n",
    "\n",
    "2. **Increases Stability**:\n",
    "   - A single model might make random mistakes. Combining many models reduces the impact of those mistakes.\n",
    "\n",
    "\n",
    "\n",
    "### Easy Example\n",
    "Let’s say you’re trying to predict the weather (rain or no rain):\n",
    "\n",
    "1. **Original Dataset**: You have 1000 weather reports.\n",
    "2. **Sampling**: You create 10 different samples, each with 100 random reports (with replacement).\n",
    "3. **Train Models**: Train 10 different weather-predicting models, one on each sample.\n",
    "4. **Combine Predictions**: Use majority voting to decide if it will rain or not.\n",
    "\n",
    "Now, instead of trusting a single model, you trust the **\"group decision\"** — which is almost always more reliable!\n",
    "\n",
    "\n",
    "\n",
    "### Real-World Example: Random Forest\n",
    "- Random Forest is a **bagging algorithm** that uses decision trees as the \"friends.\"\n",
    "- Each tree is trained on a random subset of data and random features.\n",
    "- The final prediction is made by combining the results of all the trees.\n",
    "\n",
    "\n",
    "\n",
    "### Key Benefits of Bagging:\n",
    "1. **Better Predictions**: More reliable and accurate than a single model.\n",
    "2. **Resilience to Overfitting**: Handles noisy data better.\n",
    "3. **Versatility**: Works for both classification (yes/no) and regression (numbers).\n",
    "\n",
    "\n",
    "\n",
    "### Analogy Summary:\n",
    "Bagging is like asking a **group of advisors** instead of relying on **one person’s opinion**. By combining everyone’s advice, you get a smarter, more balanced decision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Bagging:\n",
    "\n",
    "Bagging can be categorized into different types based on how the technique is implemented or used for specific tasks. Below are the primary types of bagging:\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Bagging for Classification**\n",
    "   - **Purpose**: Used to improve the accuracy of classification models by aggregating predictions through **voting**.\n",
    "   - **How it works**:\n",
    "     - Multiple base classifiers are trained on bootstrap samples of the data.\n",
    "     - Each classifier gives a class prediction.\n",
    "     - The final prediction is made using **majority voting** (hard voting) or **averaging probabilities** (soft voting).\n",
    "   - **Example Algorithms**:\n",
    "     - Random Forest for classification tasks.\n",
    "   - **Use Case**: \n",
    "     - Used when the target variable is categorical, such as \"spam\" vs. \"not spam.\"\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Bagging for Regression**\n",
    "   - **Purpose**: Reduces variance in regression tasks by aggregating predictions through **averaging**.\n",
    "   - **How it works**:\n",
    "     - Multiple base regressors are trained on bootstrap samples.\n",
    "     - Each regressor outputs a numerical value (continuous).\n",
    "     - The final prediction is the **average** of all predictions.\n",
    "   - **Example Algorithms**:\n",
    "     - Random Forest for regression tasks.\n",
    "   - **Use Case**:\n",
    "     - Predicting numerical values like house prices or stock prices.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Pasting**\n",
    "   - **Purpose**: Similar to bagging but without replacement.\n",
    "   - **How it works**:\n",
    "     - Instead of sampling data **with replacement**, subsets of the data are sampled **without replacement**.\n",
    "     - Base models are trained on these non-overlapping subsets.\n",
    "   - **Advantage**: Can work well when the dataset is large and redundant samples are not needed.\n",
    "   - **Use Case**:\n",
    "     - Large datasets with little variability between rows.\n",
    "\n",
    "\n",
    "\n",
    "### 4. **Random Subspace Method**\n",
    "   - **Purpose**: Focuses on feature sampling instead of row sampling.\n",
    "   - **How it works**:\n",
    "     - Instead of selecting subsets of data instances (rows), subsets of features (columns) are selected.\n",
    "     - Base models are trained on different subsets of features.\n",
    "   - **Example**:\n",
    "     - Random Forest uses a form of this by randomly selecting features for splitting at each node.\n",
    "   - **Use Case**:\n",
    "     - Effective when the dataset has a large number of features and high dimensionality.\n",
    "\n",
    "\n",
    "\n",
    "### 5. **Random Patches**\n",
    "   - **Purpose**: Combines row sampling (as in bagging) and feature sampling (as in random subspace).\n",
    "   - **How it works**:\n",
    "     - Both rows and features are sampled randomly to create bootstrap samples.\n",
    "     - Base models are trained on these samples.\n",
    "   - **Use Case**:\n",
    "     - Highly effective in handling datasets with both large feature sets and instances.\n",
    "\n",
    "\n",
    "\n",
    "### 6. **Bootstrap Aggregation with Weighted Voting**\n",
    "   - **Purpose**: Adds weights to predictions from base models.\n",
    "   - **How it works**:\n",
    "     - After training, some models may perform better on validation data.\n",
    "     - These models are given higher weights during the voting or averaging process.\n",
    "   - **Use Case**:\n",
    "     - When some models are more reliable or accurate than others.\n",
    "\n",
    "\n",
    "\n",
    "### 7. **Parallel Bagging**\n",
    "   - **Purpose**: Base models are trained independently and in parallel.\n",
    "   - **How it works**:\n",
    "     - Each model trains on a different bootstrap sample independently.\n",
    "     - Aggregation of results is done after all models are trained.\n",
    "   - **Use Case**:\n",
    "     - Used in distributed systems or when computational resources allow parallel processing.\n",
    "\n",
    "\n",
    "\n",
    "### 8. **Bagging with Pruning**\n",
    "   - **Purpose**: Improves model performance by pruning weak base models.\n",
    "   - **How it works**:\n",
    "     - After training multiple base models, models with poor performance on validation data are removed.\n",
    "     - Aggregation is done using only the top-performing models.\n",
    "   - **Use Case**:\n",
    "     - To reduce computational cost or noise caused by weak models.\n",
    "\n",
    "### Summary Table of Bagging Types\n",
    "\n",
    "| **Type**                  | **Sampling**              | **Aggregation Method**     | **Use Case**                     |\n",
    "|---------------------------|---------------------------|----------------------------|-----------------------------------|\n",
    "| Bagging (Standard)        | Rows (with replacement)   | Voting (classification) / Averaging (regression) | General-purpose variance reduction |\n",
    "| Pasting                   | Rows (without replacement)| Voting / Averaging         | Large datasets                   |\n",
    "| Random Subspace Method    | Features (columns only)   | Voting / Averaging         | High-dimensional data            |\n",
    "| Random Patches            | Rows and Features         | Voting / Averaging         | Large and high-dimensional data  |\n",
    "| Weighted Voting Bagging   | Rows (with replacement)   | Weighted Voting            | When some models are more reliable |\n",
    "| Parallel Bagging          | Rows (with replacement)   | Voting / Averaging         | Distributed or parallel systems  |\n",
    "| Bagging with Pruning      | Rows (with replacement)   | Voting / Averaging         | Improve efficiency and robustness|\n",
    "\n",
    "\n",
    "\n",
    "### Practical Applications of Bagging\n",
    "\n",
    "1. **Random Forests**:\n",
    "   - An extension of bagging that adds feature randomness.\n",
    "2. **Bagging Regressor/Classifier** in scikit-learn:\n",
    "   - Easily implemented using `BaggingClassifier` or `BaggingRegressor`.\n",
    "3. **Custom Bagging**:\n",
    "   - For scenarios where base models are highly specialized, bagging can be implemented manually.\n",
    "\n",
    "Bagging is highly versatile and is particularly useful when individual models are prone to overfitting or high variance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Bagging Examples:\n",
    "\n",
    "Sure! Let’s simplify the **types of bagging** (like **pasting**, **random subspace**, and **random patches**) in an easy-to-understand way, using analogies.\n",
    "\n",
    "\n",
    "\n",
    "### Imagine You’re Hiring for a Job\n",
    "You have **100 resumes** to review, and you want to choose the **best candidate**. But instead of reading all the resumes yourself, you decide to split the work among a team of evaluators. Depending on how you divide the work, you get different types of bagging.\n",
    "\n",
    "\n",
    "\n",
    "### 1. **Standard Bagging (Bootstrap Aggregating)**  \n",
    "- **How it works**: Each evaluator gets a random **sample** of resumes, but the same resume can appear in multiple samples. (This is because sampling is done **with replacement**, like drawing names out of a hat and putting them back after each draw.)\n",
    "- **Key Idea**: Overlapping data (duplicates allowed) helps evaluators come to a more balanced decision.\n",
    "- **Analogy**: \n",
    "  - Suppose you randomly give each evaluator 30 resumes. Some evaluators might review the same resumes, while others may not.  \n",
    "  - You combine their decisions for the final result (e.g., majority vote).\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Pasting**\n",
    "- **How it works**: Like bagging, but sampling is done **without replacement**.  \n",
    "- **Key Idea**: No duplicates in the samples, so each resume is assigned to only one evaluator.  \n",
    "- **Analogy**:\n",
    "  - If you give 30 resumes to each evaluator, none of them overlap. Every resume is reviewed exactly once.  \n",
    "  - This approach ensures that no evaluator reviews the same resume twice, which may lead to faster evaluation and more diverse results.\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Random Subspace**\n",
    "- **How it works**: Instead of splitting the **resumes**, you divide the **features** (columns or attributes) in the resumes. Each evaluator sees a random selection of features.  \n",
    "- **Key Idea**: Models are trained on only a few features instead of the entire dataset.  \n",
    "- **Analogy**:\n",
    "  - Imagine resumes have details like **name, age, education, skills, and experience.**\n",
    "  - One evaluator might get only **education and skills**, another might see only **age and experience**. No one gets the full resume!\n",
    "  - The final decision comes from combining everyone's partial evaluations.  \n",
    "\n",
    "\n",
    "\n",
    "### 4. **Random Patches**\n",
    "- **How it works**: A mix of **pasting** and **random subspace**. You randomly select both **rows (samples)** and **columns (features)** for each evaluator.  \n",
    "- **Key Idea**: Evaluators get a smaller, random portion of the resumes and their details.  \n",
    "- **Analogy**:\n",
    "  - One evaluator might get 20 resumes but only see their **education and skills**.\n",
    "  - Another evaluator might get a different 20 resumes and focus on **age and experience**.  \n",
    "  - It’s a highly randomized approach to encourage diversity in decision-making.\n",
    "\n",
    "### Summary Table of Differences\n",
    "\n",
    "| **Type**             | **What’s Randomized?**           | **Duplicates Allowed?** | **Key Use Case**                              |\n",
    "|-----------------------|-----------------------------------|--------------------------|-----------------------------------------------|\n",
    "| **Bagging**           | Rows (samples)                  | Yes (with replacement)  | General-purpose improvement in accuracy       |\n",
    "| **Pasting**           | Rows (samples)                  | No (without replacement) | Faster, simpler than bagging                  |\n",
    "| **Random Subspace**   | Columns (features)              | N/A                      | Works well when features dominate over rows   |\n",
    "| **Random Patches**    | Both rows (samples) and columns | No (without replacement) | Highly diverse models for complex datasets    |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### How to Remember:\n",
    "- **Bagging**: Standard approach with duplicates in data samples.\n",
    "- **Pasting**: Same as bagging, but no duplicates (samples are unique).\n",
    "- **Random Subspace**: Focuses on **features**, not rows.\n",
    "- **Random Patches**: Combines **random rows** and **random features** for maximum diversity.\n",
    "\n",
    "\n",
    "\n",
    "### Why Use These Variants?\n",
    "- Each method introduces randomness, which helps **reduce overfitting** and makes the models more **robust**.  \n",
    "- The choice depends on your data: \n",
    "  - If you have too many rows, try **pasting**.\n",
    "  - If you have too many features, try **random subspace**.\n",
    "  - For very large datasets, try **random patches**. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
