{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes:\n",
    "\n",
    "The **Naive Bayes Algorithm** is a simple and powerful machine learning algorithm often used for classification tasks. It's based on **Bayes' Theorem** with a strong (and \"naive\") assumption: all features in the dataset are **independent** of each other given the output label.\n",
    "\n",
    "\n",
    "\n",
    "### Key Concepts in Naive Bayes:\n",
    "1. **Bayes' Theorem**:\n",
    "   $$\n",
    "   P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "   $$\n",
    "   Here:\n",
    "   - $P(A|B)$: Probability of $A$ happening given $B$ (posterior probability).\n",
    "   - $P(B|A)$: Probability of $B$ happening given $A$ (likelihood).\n",
    "   - $P(A)$: Probability of $A$ (prior probability).\n",
    "   - $P(B)$: Total probability of $B$ (evidence).\n",
    "\n",
    "2. **Naive Assumption**:\n",
    "   - The algorithm assumes that each feature is **independent** of the others.\n",
    "   - This is rarely true in real-world data, but the algorithm works surprisingly well even when this assumption is violated.\n",
    "\n",
    "3. **Goal**:\n",
    "   - Given some input features, predict the **class label** with the highest posterior probability.\n",
    "\n",
    "\n",
    "\n",
    "### How Naive Bayes Works:\n",
    "1. **Training**:\n",
    "   - Calculate the **prior probabilities** of each class ($P(Class)$).\n",
    "   - For each feature, calculate the **likelihood** ($P(Feature | Class)$) for all class labels.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - For a new input, calculate the **posterior probability** for each class using Bayes' Theorem:\n",
    "     $$\n",
    "     P(Class|Features) \\propto P(Class) \\prod_{i=1}^{n} P(Feature_i|Class)\n",
    "     $$\n",
    "   - Choose the class with the **highest posterior probability**.\n",
    "\n",
    "\n",
    "\n",
    "### Types of Naive Bayes:\n",
    "1. **Gaussian Naive Bayes**:\n",
    "   - Assumes features follow a **Gaussian (Normal) distribution**.\n",
    "   - Suitable for continuous data.\n",
    "   - Likelihood:\n",
    "     $$\n",
    "     P(Feature|Class) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "     $$\n",
    "     - $\\mu$: Mean of the feature values for the class.\n",
    "     - $\\sigma$: Standard deviation of the feature values for the class.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Used for discrete data (e.g., text classification).\n",
    "   - Assumes features represent counts or frequencies (e.g., word counts in documents).\n",
    "\n",
    "3. **Bernoulli Naive Bayes**:\n",
    "   - Used for binary features (e.g., whether a word exists in a document or not).\n",
    "   - Assumes features follow a **Bernoulli distribution** (0s and 1s).\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Naive Bayes:\n",
    "1. **Fast**: Training and prediction are computationally efficient.\n",
    "2. **Simple**: Easy to implement and interpret.\n",
    "3. **Effective**: Performs well on small datasets and text classification tasks.\n",
    "4. **Robust**: Works well even when the independence assumption is not strictly true.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of Naive Bayes:\n",
    "1. **Independence Assumption**: Assumes all features are independent, which is rarely true in practice.\n",
    "2. **Zero Probability Problem**: If a feature value never appears for a class in the training data, the probability becomes zero (solved using **smoothing** techniques like Laplace smoothing).\n",
    "\n",
    "\n",
    "\n",
    "### Example of Naive Bayes in Action:\n",
    "#### Problem:\n",
    "You want to classify whether an email is **spam** or **not spam** based on the words it contains.\n",
    "\n",
    "#### Steps:\n",
    "1. **Training**:\n",
    "   - Count the frequency of each word in spam and non-spam emails.\n",
    "   - Calculate the likelihood of each word given spam or non-spam.\n",
    "   - Compute the prior probabilities of spam and non-spam emails.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - For a new email, calculate the posterior probability for both spam and non-spam using the formula:\n",
    "     $$\n",
    "     P(Class|Features) \\propto P(Class) \\prod_{i=1}^{n} P(Feature_i|Class)\n",
    "     $$\n",
    "   - Classify the email as spam if $P(Spam|Features)$ is greater than $P(NotSpam|Features)$.\n",
    "\n",
    "\n",
    "\n",
    "### Python Code Example:\n",
    "```python\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data\n",
    "emails = [\n",
    "    \"Win a free car now\",\n",
    "    \"Call this number to claim your prize\",\n",
    "    \"Meeting at 3 PM today\",\n",
    "    \"Reminder: Submit the report by tomorrow\",\n",
    "    \"Congratulations! You won a lottery\"\n",
    "]\n",
    "labels = [1, 1, 0, 0, 1]  # 1 = spam, 0 = not spam\n",
    "\n",
    "# Convert text to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(emails)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "predictions = nb.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "```\n",
    "\n",
    "### Summary:\n",
    "Naive Bayes is a simple yet powerful algorithm that works well for many problems, especially text classification (e.g., spam detection). It uses probabilities to make decisions but assumes that all features are independent—a limitation that often doesn’t matter in practice.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Naive Bayes:\n",
    "\n",
    "Sure! Let me explain **Naive Bayes** in **simple layman terms**:\n",
    "\n",
    "\n",
    "\n",
    "### Imagine This Scenario:\n",
    "You’re a **detective** trying to figure out if a person is likely to be a **criminal** or not. To make your decision, you look at some clues, like:\n",
    "- Does the person have a **scar**?  \n",
    "- Are they wearing **dark glasses**?  \n",
    "- Do they carry a **suspicious bag**?\n",
    "\n",
    "Now, based on past cases, you know:\n",
    "- **How many criminals** had these traits.\n",
    "- **How many innocent people** also had these traits.\n",
    "\n",
    "You use this knowledge to calculate the **probability** of someone being a criminal given the clues they have.\n",
    "\n",
    "\n",
    "\n",
    "### Naive Bayes in This Context:\n",
    "1. **Bayes’ Theorem**:\n",
    "   It’s like a formula to figure out:  \n",
    "   **Given the clues**, what’s the chance this person is a criminal?  \n",
    "\n",
    "   - Prior Knowledge:  \n",
    "     If 60% of all people you’ve investigated were criminals, there’s already a **60% chance** (prior probability) anyone could be a criminal.\n",
    "   - Clues (Features):  \n",
    "     For example, if wearing dark glasses is a clue, you ask:\n",
    "     - Among criminals, how many wore dark glasses?\n",
    "     - Among innocent people, how many wore dark glasses?\n",
    "\n",
    "2. **Naive Assumption**:  \n",
    "   You assume all clues are **independent**.  \n",
    "   For example:\n",
    "   - Whether someone has a scar **does not depend** on whether they carry a suspicious bag.\n",
    "   - This assumption might not always be true but makes calculations much easier.\n",
    "\n",
    "3. **Final Decision**:  \n",
    "   You calculate the overall probability of being a criminal (or not) based on all the clues, then pick the **most likely option**.\n",
    "\n",
    "\n",
    "\n",
    "### Key Idea:\n",
    "- If a person has traits that criminals **often** have, the algorithm will predict they’re a criminal.  \n",
    "- If they have traits innocent people **often** have, it predicts they’re innocent.\n",
    "\n",
    "\n",
    "\n",
    "### Why is it Called “Naive”?  \n",
    "Because it **naively assumes** that all clues are independent, even if they might not be. For example:\n",
    "- Having a scar **might** be linked to carrying a suspicious bag, but Naive Bayes ignores such relationships.\n",
    "\n",
    "\n",
    "\n",
    "### Real-Life Example: Spam Emails\n",
    "Think of spam detection for emails:\n",
    "- **Clues**: Words in the email, like “win,” “free,” “lottery.”\n",
    "- **Learning**: Based on past emails, the algorithm learns:\n",
    "  - Emails with “win” and “free” are usually spam.\n",
    "  - Emails with “meeting” and “agenda” are usually not spam.\n",
    "\n",
    "For a new email:\n",
    "- If it has many spammy words, the algorithm predicts **spam**.\n",
    "- If it has words from normal emails, it predicts **not spam**.\n",
    "\n",
    "\n",
    "\n",
    "### Why Does Naive Bayes Work?  \n",
    "Even though the clues aren’t truly independent, Naive Bayes is very good at **quickly making accurate guesses** when:\n",
    "- The data has clear patterns (e.g., spammy vs. non-spammy words).\n",
    "- You don’t need perfection, just a good-enough decision.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
