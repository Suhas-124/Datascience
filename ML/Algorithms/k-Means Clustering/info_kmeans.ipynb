{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering:\n",
    "\n",
    "K-means clustering is a popular unsupervised machine learning algorithm used to group data into clusters based on their similarity. It is primarily used for **data segmentation**, such as customer segmentation, image compression, and anomaly detection.\n",
    "\n",
    "\n",
    "\n",
    "### **Steps of K-Means Clustering**\n",
    "\n",
    "1. **Choose the Number of Clusters (k):**\n",
    "   Decide the number of clusters (k) you want to divide your data into. This value is often chosen manually or determined using methods like the **Elbow Method**.\n",
    "\n",
    "2. **Initialize Centroids:**\n",
    "   - Randomly select $k$ data points from the dataset as the initial **centroids** (cluster centers).\n",
    "   - These centroids will act as the starting points for the clustering process.\n",
    "\n",
    "3. **Assign Data Points to the Nearest Centroid:**\n",
    "   - For each data point, calculate the **distance** to each centroid (commonly using Euclidean distance).\n",
    "   - Assign the data point to the cluster with the nearest centroid.\n",
    "\n",
    "4. **Update Centroids:**\n",
    "   - Calculate the new centroid of each cluster by taking the **mean** of all data points in that cluster.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Steps 3 and 4 are repeated until:\n",
    "     - The centroids do not change significantly (convergence).\n",
    "     - Or, a predefined number of iterations is reached.\n",
    "\n",
    "6. **Final Clusters:**\n",
    "   - Once the algorithm converges, the data points are grouped into $k$ clusters, with each cluster represented by its centroid.\n",
    "\n",
    "\n",
    "\n",
    "### **Mathematics Behind K-Means**\n",
    "\n",
    "- The algorithm minimizes the **inertia** or **within-cluster sum of squares (WCSS)**:\n",
    "  $$\n",
    "  \\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
    "  $$\n",
    "  Where:\n",
    "  - $C_i$: Cluster $i$\n",
    "  - $\\mu_i$: Centroid of cluster $i$\n",
    "  - $x$: Data point in cluster $i$\n",
    "\n",
    "- It iteratively adjusts the centroids to reduce WCSS.\n",
    "\n",
    "\n",
    "\n",
    "### **Strengths of K-Means**\n",
    "1. **Simplicity:** Easy to understand and implement.\n",
    "2. **Efficiency:** Computationally efficient, especially for small datasets.\n",
    "3. **Scalability:** Works well with large datasets.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations of K-Means**\n",
    "1. **Predefined $k$:** The user must specify $k$, which can be challenging without prior knowledge.\n",
    "2. **Sensitive to Initialization:** Random initialization can lead to different results.\n",
    "3. **Outlier Sensitivity:** Outliers can significantly affect centroids.\n",
    "4. **Assumes Spherical Clusters:** Works best when clusters are roughly circular and similar in size.\n",
    "\n",
    "\n",
    "\n",
    "### **Choosing $k$:**\n",
    "- **Elbow Method:**\n",
    "  - Plot WCSS against different values of $k$.\n",
    "  - The \"elbow point\" indicates the optimal number of clusters.\n",
    "- **Silhouette Score:**\n",
    "  - Measures how well-separated the clusters are.\n",
    "\n",
    "\n",
    "\n",
    "### **Applications of K-Means**\n",
    "1. **Customer Segmentation:** Group customers based on purchasing behavior.\n",
    "2. **Image Compression:** Reduce the number of colors in an image.\n",
    "3. **Document Clustering:** Organize text documents by topic.\n",
    "4. **Anomaly Detection:** Identify outliers in data.\n",
    "\n",
    "\n",
    "\n",
    "### **Python Implementation Example**\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# K-Means clustering\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(data)\n",
    "\n",
    "# Cluster centers\n",
    "print(\"Cluster Centers:\", kmeans.cluster_centers_)\n",
    "\n",
    "# Labels for each point\n",
    "print(\"Cluster Labels:\", kmeans.labels_)\n",
    "```\n",
    "\n",
    "This will create two clusters and output the centroids and the labels for each data point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of K-Means Clustering:\n",
    "\n",
    "Of course! Let me break it down in the simplest terms:  \n",
    "\n",
    "\n",
    "\n",
    "### **Imagine You Have a Box of Mixed Items**\n",
    "You have a box filled with random items like pens, pencils, erasers, and markers. But they’re all mixed together, and you don’t know which is which. You want to organize them into separate groups (clusters) based on their similarities, such as **shape** or **size**.  \n",
    "\n",
    "Now, you decide to use the **K-Means clustering method** to do this.\n",
    "\n",
    "\n",
    "\n",
    "### **How K-Means Works in Layman Terms**\n",
    "1. **Pick the Number of Groups (k):**\n",
    "   You decide how many groups (clusters) you want. Let’s say you want 3 groups: pens, pencils, and erasers. So, \\( k = 3 \\).  \n",
    "\n",
    "2. **Start by Guessing:**\n",
    "   Randomly pick 3 items (one from each group) to act as the **center** or **leader** of each group. These are called **centroids**.  \n",
    "\n",
    "3. **Assign Each Item to a Group:**\n",
    "   Look at each item in the box and decide which leader (centroid) it is closest to. For example:  \n",
    "   - A thin and long item might go to the \"pencil\" group.  \n",
    "   - A short and small item might go to the \"eraser\" group.  \n",
    "\n",
    "4. **Adjust the Leaders:**\n",
    "   After assigning all the items to groups, you calculate the **average position** of each group and move the leader (centroid) to the center of its group.  \n",
    "\n",
    "5. **Repeat Until It Looks Perfect:**\n",
    "   You keep repeating steps 3 and 4:  \n",
    "   - Reassign items to the closest leader.  \n",
    "   - Adjust the leaders to the new center.  \n",
    "   This continues until the groups stop changing.\n",
    "\n",
    "6. **Done!**\n",
    "   At the end, each item belongs to one of the 3 groups, and the groups are clearly separated.\n",
    "\n",
    "\n",
    "\n",
    "### **Example**\n",
    "Think of a group of kids at a park:  \n",
    "- Some are playing football, some are skipping, and some are drawing.  \n",
    "If you were to group them, you’d look for patterns:  \n",
    "- Kids with a football are one group.  \n",
    "- Kids with skipping ropes are another group.  \n",
    "- Kids with drawing books are the third group.  \n",
    "\n",
    "That’s clustering in action!\n",
    "\n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **K** is the number of groups you want (you decide this).  \n",
    "- **Means** refers to calculating the average (center) of each group.  \n",
    "- K-Means is all about organizing things into groups based on their similarities.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
