{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hierarchical Clustering:**\n",
    "\n",
    "Hierarchical clustering is a method of clustering data points into groups (clusters) based on their similarity. Unlike methods like k-means, hierarchical clustering builds a **hierarchy** of clusters that can be visualized as a **tree-like structure** called a **dendrogram**.\n",
    "\n",
    "\n",
    "\n",
    "### **Types of Hierarchical Clustering**\n",
    "There are two main types of hierarchical clustering:\n",
    "\n",
    "1. **Agglomerative Clustering (Bottom-Up):**\n",
    "   - Starts with each data point as its own cluster.\n",
    "   - Gradually merges the closest clusters step by step.\n",
    "   - Continues until all points form a single cluster or a desired number of clusters is achieved.\n",
    "   - **Analogy:** Imagine assembling small puzzle pieces into bigger sections until you complete the puzzle.\n",
    "\n",
    "2. **Divisive Clustering (Top-Down):**\n",
    "   - Starts with all data points in a single cluster.\n",
    "   - Gradually splits the cluster into smaller clusters.\n",
    "   - Continues until each data point is its own cluster.\n",
    "   - **Analogy:** Imagine breaking a big piece of bread into smaller and smaller crumbs.\n",
    "\n",
    "\n",
    "\n",
    "### **How Does It Work?**\n",
    "Hierarchical clustering requires a way to measure **distance (similarity)** between data points and clusters. Common steps include:\n",
    "\n",
    "#### **1. Measure Similarity (Distance Metrics):**\n",
    "   - **Euclidean Distance:** Straight-line distance between points.\n",
    "   - **Manhattan Distance:** Distance measured along grid lines (like city blocks).\n",
    "   - **Cosine Similarity:** Measures the angle between vectors.\n",
    "   - **Others:** Correlation distance, Mahalanobis distance, etc.\n",
    "\n",
    "#### **2. Linkage Criteria (How to Combine Clusters):**\n",
    "   - **Single Linkage:** Distance between the closest points in two clusters.\n",
    "   - **Complete Linkage:** Distance between the farthest points in two clusters.\n",
    "   - **Average Linkage:** Average distance between all points in two clusters.\n",
    "   - **Ward’s Method:** Minimizes the increase in variance within clusters when merging.\n",
    "\n",
    "#### **3. Build the Hierarchy:**\n",
    "   - In agglomerative clustering:\n",
    "     - Start with individual data points.\n",
    "     - Merge the closest clusters at each step.\n",
    "     - Continue until all points are part of a single cluster.\n",
    "   - In divisive clustering:\n",
    "     - Start with one big cluster.\n",
    "     - Split clusters iteratively based on distance.\n",
    "\n",
    "#### **4. Visualize with a Dendrogram:**\n",
    "   - A dendrogram shows how clusters are merged or split at each step.\n",
    "   - You can \"cut\" the dendrogram at a desired level to get the final clusters.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Hierarchical Clustering**\n",
    "1. **Doesn’t Require Predefined Number of Clusters:** Unlike k-means, you don’t need to specify the number of clusters upfront.\n",
    "2. **Captures Hierarchical Relationships:** Useful when you want to explore how clusters are nested.\n",
    "3. **Works with Different Distance Metrics and Linkage Criteria:** Flexible for different types of data.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages**\n",
    "1. **Scalability:** Computationally expensive for large datasets (\\(O(n^3)\\)).\n",
    "2. **Sensitive to Noise and Outliers:** Can create biased clusters.\n",
    "3. **Rigid Assignments:** Once clusters are merged or split, they cannot be undone.\n",
    "\n",
    "\n",
    "\n",
    "### **Applications**\n",
    "- **Biology:** Taxonomy of species.\n",
    "- **Marketing:** Customer segmentation.\n",
    "- **Social Networks:** Community detection.\n",
    "- **Image Processing:** Grouping similar pixels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering:\n",
    "\n",
    "Agglomerative Clustering is a hierarchical clustering technique used to group data points into clusters based on their similarity. It's called \"agglomerative\" because it starts with each data point as its own cluster and **iteratively merges the closest clusters** until all points belong to a single cluster or a desired number of clusters is reached.\n",
    "\n",
    "\n",
    "\n",
    "### **How Agglomerative Clustering Works**\n",
    "1. **Start with Each Data Point as a Cluster:**\n",
    "   - Suppose you have \\( n \\) data points. Initially, each data point is its own cluster, so there are \\( n \\) clusters.\n",
    "\n",
    "2. **Compute Similarity (or Distance):**\n",
    "   - Measure the distance between every pair of clusters. Common distance metrics include:\n",
    "     - **Euclidean Distance**: Straight-line distance.\n",
    "     - **Manhattan Distance**: Sum of absolute differences.\n",
    "     - **Cosine Similarity**: Measures the angle between vectors.\n",
    "\n",
    "3. **Merge the Closest Clusters:**\n",
    "   - Identify the two clusters that are closest and merge them into a single cluster.\n",
    "\n",
    "4. **Update Distances:**\n",
    "   - Recalculate the distance between the new cluster and all remaining clusters. This step depends on the **linkage criterion** (explained below).\n",
    "\n",
    "5. **Repeat Until Stopping Criteria:**\n",
    "   - Continue merging clusters until:\n",
    "     - A single cluster contains all data points (complete hierarchical tree).\n",
    "     - A predefined number of clusters is reached.\n",
    "\n",
    "\n",
    "\n",
    "### **Linkage Criteria**\n",
    "The choice of how to measure the distance between clusters affects the results. Common linkage methods are:\n",
    "\n",
    "1. **Single Linkage:**\n",
    "   - Distance between the two closest points in each cluster.\n",
    "   - Tends to create \"chain-like\" clusters.\n",
    "\n",
    "2. **Complete Linkage:**\n",
    "   - Distance between the two farthest points in each cluster.\n",
    "   - Tends to create compact, spherical clusters.\n",
    "\n",
    "3. **Average Linkage:**\n",
    "   - Average distance between all pairs of points in the two clusters.\n",
    "   - Balances between compactness and chaining.\n",
    "\n",
    "4. **Ward's Method:**\n",
    "   - Minimizes the increase in the total within-cluster variance when two clusters are merged.\n",
    "   - Often produces more balanced clusters.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Agglomerative Clustering**\n",
    "1. **Hierarchical Structure:**\n",
    "   - Produces a dendrogram (a tree-like structure) that shows the relationships between clusters at different levels.\n",
    "\n",
    "2. **No Predefined Cluster Count:**\n",
    "   - You don't need to specify the number of clusters beforehand.\n",
    "\n",
    "3. **Works with Different Distance Metrics:**\n",
    "   - Flexible in handling various types of data.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of Agglomerative Clustering**\n",
    "1. **Scalability:**\n",
    "   - Computationally expensive for large datasets (\\( O(n^3) \\)).\n",
    "\n",
    "2. **Sensitivity to Noise:**\n",
    "   - Can be affected by outliers.\n",
    "\n",
    "3. **No Automatic Cluster Number:**\n",
    "   - You must manually decide the number of clusters by \"cutting\" the dendrogram.\n",
    "\n",
    "\n",
    "\n",
    "### **Practical Example Using Python**\n",
    "Here’s an example of agglomerative clustering using Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "X, _ = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.2)\n",
    "\n",
    "# Create the Agglomerative Clustering model\n",
    "model = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
    "\n",
    "# Fit the model and predict cluster labels\n",
    "y_pred = model.fit_predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap='viridis', s=30)\n",
    "plt.title(\"Agglomerative Clustering\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Visualizing the Hierarchy (Dendrogram)**\n",
    "To see the hierarchical structure, you can use a dendrogram:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Dendrogram\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Applications**\n",
    "1. **Biology:**\n",
    "   - Organizing species into taxonomies.\n",
    "2. **Marketing:**\n",
    "   - Customer segmentation.\n",
    "3. **Image Segmentation:**\n",
    "   - Grouping similar pixels in images.\n",
    "4. **Social Networks:**\n",
    "   - Detecting communities or similar groups.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Agglomerative Clustering:\n",
    "\n",
    "Think of Agglomerative Clustering as a way of grouping similar things together step by step. Let’s use a simple analogy to explain:\n",
    "\n",
    "\n",
    "\n",
    "### **Imagine a Party**\n",
    "You’re at a party with 20 people. Initially:\n",
    "- Everyone is standing alone (each person is their own \"cluster\").\n",
    "\n",
    "\n",
    "\n",
    "### **Step 1: Pair Up**\n",
    "Look around and find the person **most similar** to you (maybe someone with the same interests, like music or sports). You form a pair (a cluster of 2 people).\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Small Groups**\n",
    "Each pair now looks for another pair or individual nearby who is similar to join their group. Maybe they all like the same kind of food or movies. This creates slightly bigger groups.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Merge Groups**\n",
    "The small groups keep merging with other similar groups, step by step. Over time:\n",
    "- Small groups grow into larger groups.\n",
    "- The process continues until everyone at the party is part of one big group.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Decisions**\n",
    "When deciding how to merge groups, you can use different rules:\n",
    "1. **Single Linkage:** Connect groups based on the two closest people.\n",
    "2. **Complete Linkage:** Connect groups based on the farthest distance between any two people.\n",
    "3. **Average Linkage:** Consider the average similarity between everyone in two groups.\n",
    "4. **Ward’s Method:** Minimize overall differences when merging groups.\n",
    "\n",
    "\n",
    "\n",
    "### **Final Output**\n",
    "When you're done, you have a **hierarchy of groups**—small groups inside bigger groups. If you don’t want one big group at the end, you can stop merging at some point and have multiple smaller groups instead.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Is This Useful?**\n",
    "Agglomerative Clustering helps us:\n",
    "- Group similar things (e.g., customers with similar buying habits or animals with similar traits).\n",
    "- See the relationships between groups (like a family tree).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
