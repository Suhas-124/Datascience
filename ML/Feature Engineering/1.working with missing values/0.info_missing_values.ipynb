{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ **Handling Missing Values Based on Missing Percentage**\n",
    "\n",
    "| Missing %      | Action to Take           |\n",
    "|----------------|--------------------------|\n",
    "| **0-5%**       | Fill missing values (impute) |\n",
    "| **5-30%**      | Impute or drop depending on importance |\n",
    "| **Above 50%**  | Consider dropping the column (too much missing data) |\n",
    "\n",
    "\n",
    "\n",
    "### ðŸŽ¯ **B. Choose the Right Imputation Method**\n",
    "| Data Type   | Missing %       | Best Technique            | Code Example                          |\n",
    "|-------------|-----------------|---------------------------|---------------------------------------|\n",
    "| Numerical   | 0-5%            | Mean/Median Imputation     | `df['col'].fillna(df['col'].mean())`  |\n",
    "| Numerical   | 5-30%           | Mean/Median Imputation     | `df['col'].fillna(df['col'].median())`|\n",
    "| Numerical   | Above 50%       | Drop the Column            | `df.drop('col', axis=1, inplace=True)`|\n",
    "| Categorical | 0-5%            | Mode Imputation            | `df['col'].fillna(df['col'].mode()[0])` |\n",
    "| Categorical | 5-30%           | Mode/Unknown Imputation    | `df['col'].fillna('Unknown')`         |\n",
    "| Categorical | Above 50%       | Drop the Column            | `df.drop('col', axis=1, inplace=True)`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCA (Complete Case Analysis):\n",
    "\n",
    "Complete Case Analysis (CCA), also known as **listwise deletion**, is a straightforward and commonly used approach to handle missing data in a dataset. Here's a comprehensive explanation:\n",
    "\n",
    "\n",
    "\n",
    "### **What is Complete Case Analysis?**\n",
    "In CCA, **only the rows (cases) of data that have no missing values in any variable of interest are retained for analysis**. Any row with even a single missing value is excluded entirely.\n",
    "\n",
    "\n",
    "\n",
    "### **Steps in Complete Case Analysis**\n",
    "1. **Identify Missing Values:**\n",
    "   - Missing values in the dataset are typically represented as `NaN`, blanks, or placeholders (e.g., -999).\n",
    "   \n",
    "2. **Filter Out Incomplete Rows:**\n",
    "   - Remove all rows where at least one variable of interest contains a missing value.\n",
    "\n",
    "3. **Perform Analysis:**\n",
    "   - Use the remaining data (complete cases) to carry out the intended analysis (e.g., regression, hypothesis testing).\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of CCA**\n",
    "1. **Simplicity:**\n",
    "   - Easy to implement and interpret without requiring advanced statistical techniques.\n",
    "\n",
    "2. **Unbiased Results (under MCAR):**\n",
    "   - If data is Missing Completely at Random (MCAR), CCA provides unbiased estimates because the missingness does not depend on observed or unobserved data.\n",
    "\n",
    "3. **Compatibility with Most Models:**\n",
    "   - Works well with most statistical methods and machine learning models as they often assume complete datasets.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of CCA**\n",
    "1. **Loss of Data:**\n",
    "   - Excluding rows with missing values can lead to a significant reduction in the sample size, potentially lowering the statistical power and reliability of the results.\n",
    "\n",
    "2. **Biased Results (if not MCAR):**\n",
    "   - If the data is **Missing at Random (MAR)** or **Not Missing at Random (NMAR)**, CCA can lead to biased estimates as the missingness might relate to the observed or unobserved data.\n",
    "\n",
    "3. **Not Always Feasible:**\n",
    "   - If a large portion of the data is missing, CCA might leave too few rows for meaningful analysis.\n",
    "\n",
    "\n",
    "\n",
    "### **When to Use Complete Case Analysis**\n",
    "CCA is suitable when:\n",
    "- The amount of missing data is **small** (e.g., less than 5-10% of the total data).\n",
    "- Missingness is **MCAR**, meaning the probability of missingness is the same for all observations and unrelated to the data itself.\n",
    "- Data reduction does not significantly impact the analysis.\n",
    "\n",
    "### **Example**\n",
    "#### **Dataset with Missing Values:**\n",
    "| ID  | Age | Income | Education |  \n",
    "|------|-----|--------|-----------|  \n",
    "| 1    | 25  | 50K    | Bachelor's|  \n",
    "| 2    | NaN | 60K    | Master's  |  \n",
    "| 3    | 30  | NaN    | High School |  \n",
    "| 4    | 35  | 70K    | PhD       |  \n",
    "\n",
    "#### **After Applying CCA:**\n",
    "| ID  | Age | Income | Education |  \n",
    "|------|-----|--------|-----------|  \n",
    "| 1    | 25  | 50K    | Bachelor's|  \n",
    "| 4    | 35  | 70K    | PhD       |  \n",
    "\n",
    "Here, rows 2 and 3 are removed because they contain missing values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Alternatives to CCA**\n",
    "If the drawbacks of CCA are significant for your data:\n",
    "1. **Imputation Techniques:**\n",
    "   - Fill missing values with the mean, median, mode, or predicted values from models.\n",
    "2. **Maximum Likelihood Estimation:**\n",
    "   - Use statistical methods to estimate parameters directly from incomplete data.\n",
    "3. **Multiple Imputation:**\n",
    "   - Create multiple complete datasets by imputing missing values and combine results.\n",
    "4. **Model-based Methods:**\n",
    "   - Use algorithms like tree-based models that can handle missing values inherently.\n",
    "\n",
    "\n",
    "\n",
    "In summary, Complete Case Analysis is simple and works well when data is MCAR and missingness is minimal. However, if a substantial amount of data is missing or the missingness mechanism is MAR/NMAR, alternative techniques may be more appropriate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Data Imputation:\n",
    "\n",
    "Imputing numerical data refers to the process of replacing missing values in a dataset with substituted values to allow analysis without losing rows of data. Imputation helps preserve as much data as possible while reducing biases and errors introduced by missingness.\n",
    "\n",
    "Hereâ€™s a detailed explanation:\n",
    "\n",
    "\n",
    "\n",
    "### **Why Impute Numerical Data?**\n",
    "- Missing values can cause problems for many machine learning models and statistical analyses that require complete data.\n",
    "- Imputation maintains dataset size and helps preserve statistical properties.\n",
    "\n",
    "\n",
    "\n",
    "### **Common Techniques for Imputing Numerical Data**\n",
    "\n",
    "#### 1. **Mean Imputation**\n",
    "   - **Definition:** Replace missing values with the **mean** of the non-missing values for the column.\n",
    "   - **When to Use:** When the data is symmetrically distributed without extreme outliers.\n",
    "   - **Advantages:**\n",
    "     - Easy to calculate and implement.\n",
    "     - Preserves the overall average of the data.\n",
    "   - **Disadvantages:**\n",
    "     - Reduces variability in the data, which can bias results.\n",
    "     - Not ideal for skewed distributions.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   data = [10, 20, NaN, 40, 50]\n",
    "   mean = (10 + 20 + 40 + 50) / 4 = 30\n",
    "   data_imputed = [10, 20, 30, 40, 50]\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "#### 2. **Median Imputation**\n",
    "   - **Definition:** Replace missing values with the **median** of the non-missing values for the column.\n",
    "   - **When to Use:** For skewed distributions or data with outliers.\n",
    "   - **Advantages:**\n",
    "     - Less sensitive to outliers than mean imputation.\n",
    "   - **Disadvantages:**\n",
    "     - Like mean imputation, it reduces variability.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   data = [10, 20, NaN, 40, 100]\n",
    "   median = 40\n",
    "   data_imputed = [10, 20, 40, 40, 100]\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "#### 3. **Mode Imputation**\n",
    "   - **Definition:** Replace missing values with the **mode** (most frequent value) of the non-missing values.\n",
    "   - **When to Use:** Useful for numerical data with repeated values (e.g., age groups, counts).\n",
    "   - **Advantages:**\n",
    "     - Easy to implement and preserves common values in the dataset.\n",
    "   - **Disadvantages:**\n",
    "     - Not effective for continuous data with few repeated values.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. **K-Nearest Neighbors (KNN) Imputation**\n",
    "   - **Definition:** Replace missing values with the average (or median) of the **k nearest neighbors** based on feature similarity.\n",
    "   - **When to Use:** When the dataset has multiple features with correlations among them.\n",
    "   - **Advantages:**\n",
    "     - Considers relationships between features.\n",
    "     - Preserves variability in the data.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally expensive for large datasets.\n",
    "     - Sensitive to the choice of `k` and scaling of features.\n",
    "\n",
    "   **Example:** A missing age value might be imputed using the average age of people with similar incomes or education levels.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. **Regression Imputation**\n",
    "   - **Definition:** Use regression models to predict the missing value based on other features in the dataset.\n",
    "   - **When to Use:** When a strong relationship exists between the feature with missing values and other features.\n",
    "   - **Advantages:**\n",
    "     - Captures relationships in the data.\n",
    "   - **Disadvantages:**\n",
    "     - Introduces model dependency.\n",
    "     - Can propagate errors if the regression model is inaccurate.\n",
    "\n",
    "\n",
    "\n",
    "#### 6. **Multiple Imputation**\n",
    "   - **Definition:** Create multiple complete datasets by imputing missing values multiple times using statistical models, then combine the results for analysis.\n",
    "   - **When to Use:** For robust statistical analysis, especially when missingness is not random.\n",
    "   - **Advantages:**\n",
    "     - Reflects uncertainty in imputations.\n",
    "     - Suitable for MAR (Missing at Random) data.\n",
    "   - **Disadvantages:**\n",
    "     - Complex and computationally intensive.\n",
    "\n",
    "\n",
    "\n",
    "#### 7. **Random Sample Imputation**\n",
    "   - **Definition:** Replace missing values with a random value sampled from the observed data.\n",
    "   - **When to Use:** To preserve variability in the data.\n",
    "   - **Advantages:**\n",
    "     - Retains data distribution.\n",
    "   - **Disadvantages:**\n",
    "     - Can introduce randomness, which might reduce reproducibility.\n",
    "\n",
    "\n",
    "\n",
    "#### 8. **Advanced Techniques (ML-based Imputation)**\n",
    "   - **Definition:** Use machine learning models (e.g., Random Forests, Gradient Boosting) to predict missing values.\n",
    "   - **When to Use:** For complex datasets with nonlinear relationships.\n",
    "   - **Advantages:**\n",
    "     - Handles complex patterns in data.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally expensive.\n",
    "     - Requires careful tuning and validation.\n",
    "\n",
    "\n",
    "\n",
    "### **Choosing the Right Imputation Method**\n",
    "1. **Nature of the Data:**\n",
    "   - Symmetrical? Use mean imputation.\n",
    "   - Skewed? Use median imputation.\n",
    "   - Repeated values? Use mode imputation.\n",
    "2. **Amount of Missingness:**\n",
    "   - Low missingness (e.g., <5%): Simpler techniques (mean/median).\n",
    "   - High missingness: Advanced techniques (KNN, Regression, or ML-based).\n",
    "3. **Relationship Between Variables:**\n",
    "   - Strong relationships? Use KNN or regression.\n",
    "4. **Purpose of Analysis:**\n",
    "   - Statistical robustness? Consider multiple imputation.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Considerations**\n",
    "- **Impact on Variability:**\n",
    "  - Simpler techniques (mean/median) reduce variability and may bias results.\n",
    "- **Computational Complexity:**\n",
    "  - Advanced methods (KNN, ML) are resource-intensive but preserve patterns.\n",
    "- **Mechanism of Missingness:**\n",
    "  - MCAR (Missing Completely at Random): Any method is suitable.\n",
    "  - MAR (Missing at Random): Advanced methods preferred.\n",
    "  - NMAR (Not Missing at Random): Requires domain knowledge or specialized models.\n",
    "\n",
    "\n",
    "\n",
    "Imputation helps ensure that missing data does not overly distort results, but it is crucial to understand the limitations and assumptions of each technique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical data Imputation:\n",
    "\n",
    "Handling missing values for **categorical data** is crucial because missing values can introduce biases, reduce the quality of models, and affect analysis outcomes. Below is a detailed explanation of techniques for managing missing values in categorical variables.\n",
    "\n",
    "\n",
    "\n",
    "### **Why Handle Missing Values in Categorical Data?**\n",
    "1. **Bias Prevention:** Missing values might not be random and could indicate a meaningful pattern.\n",
    "2. **Algorithm Requirements:** Many machine learning models (e.g., decision trees, neural networks) cannot handle missing values directly.\n",
    "3. **Preserving Data:** Proper handling ensures minimal information loss.\n",
    "\n",
    "\n",
    "\n",
    "### **Techniques for Handling Missing Categorical Data**\n",
    "\n",
    "#### 1. **Mode Imputation**\n",
    "   - **Definition:** Replace missing values with the **most frequent category** in the column.\n",
    "   - **When to Use:** When a category is overwhelmingly dominant (e.g., most customers belong to a single class).\n",
    "   - **Advantages:**\n",
    "     - Simple and quick to implement.\n",
    "     - Works well if the missing values are random.\n",
    "   - **Disadvantages:**\n",
    "     - Reduces variability, potentially leading to bias.\n",
    "     - Less effective if the data is not Missing Completely at Random (MCAR).\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   data = ['Red', 'Blue', NaN, 'Red', 'Red']\n",
    "   mode = 'Red'\n",
    "   data_imputed = ['Red', 'Blue', 'Red', 'Red', 'Red']\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "#### 2. **Fill with a New Category**\n",
    "   - **Definition:** Create a new category (e.g., \"Missing\", \"Unknown\") to replace missing values.\n",
    "   - **When to Use:** When missingness might have a unique meaning or convey useful information.\n",
    "   - **Advantages:**\n",
    "     - Preserves all data.\n",
    "     - Highlights missingness as a distinct feature.\n",
    "   - **Disadvantages:**\n",
    "     - Increases the number of categories, which might complicate analysis.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   data = ['Red', 'Blue', NaN, 'Red', 'Green']\n",
    "   data_imputed = ['Red', 'Blue', 'Missing', 'Red', 'Green']\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "#### 3. **Frequency-based Imputation**\n",
    "   - **Definition:** Replace missing values with a category based on its **probability distribution** in the data (e.g., fill with a random category weighted by frequency).\n",
    "   - **When to Use:** When preserving the original distribution of categories is essential.\n",
    "   - **Advantages:**\n",
    "     - Retains variability.\n",
    "     - More realistic than mode imputation.\n",
    "   - **Disadvantages:**\n",
    "     - Adds randomness, potentially reducing reproducibility.\n",
    "\n",
    "   **Example:**\n",
    "   ```python\n",
    "   Categories: ['Red': 60%, 'Blue': 30%, 'Green': 10%]\n",
    "   Missing values are replaced probabilistically.\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "#### 4. **Imputation Based on Correlation with Other Features**\n",
    "   - **Definition:** Use relationships between features to predict and impute missing values (e.g., infer missing gender based on a person's name or occupation).\n",
    "   - **When to Use:** When there is a clear relationship between features.\n",
    "   - **Advantages:**\n",
    "     - Leverages patterns in the data for accurate imputation.\n",
    "   - **Disadvantages:**\n",
    "     - Requires domain knowledge or statistical modeling.\n",
    "     - Computationally intensive for large datasets.\n",
    "\n",
    "   **Example:**\n",
    "   - Missing product categories might be filled based on customer age or region.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. **K-Nearest Neighbors (KNN) Imputation**\n",
    "   - **Definition:** Identify the `k` nearest neighbors of a row with a missing value and impute the missing value with the most frequent category among its neighbors.\n",
    "   - **When to Use:** When other features provide strong clues about the missing values.\n",
    "   - **Advantages:**\n",
    "     - Preserves relationships between features.\n",
    "     - Effective for small datasets.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally expensive for large datasets.\n",
    "     - Sensitive to the choice of `k` and feature scaling.\n",
    "\n",
    "\n",
    "\n",
    "#### 6. **Machine Learning Models for Imputation**\n",
    "   - **Definition:** Train a classification model (e.g., decision trees, random forest) to predict the missing categories based on other features.\n",
    "   - **When to Use:** For complex datasets with nonlinear relationships.\n",
    "   - **Advantages:**\n",
    "     - Captures intricate relationships in data.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally expensive.\n",
    "     - Requires a separate model and validation.\n",
    "\n",
    "\n",
    "\n",
    "#### 7. **Multiple Imputation**\n",
    "   - **Definition:** Create several imputed datasets by replacing missing values using different plausible values, then combine the results for analysis.\n",
    "   - **When to Use:** For robust statistical analysis where uncertainty in missing values matters.\n",
    "   - **Advantages:**\n",
    "     - Accounts for imputation uncertainty.\n",
    "     - Suitable for MAR (Missing at Random) data.\n",
    "   - **Disadvantages:**\n",
    "     - Computationally intensive.\n",
    "     - More complex to implement and interpret.\n",
    "\n",
    "\n",
    "\n",
    "### **Choosing the Right Method**\n",
    "1. **Nature of Missingness:**\n",
    "   - **MCAR (Missing Completely at Random):** Simple methods like mode imputation or filling with a new category work well.\n",
    "   - **MAR (Missing at Random):** Advanced techniques like KNN or ML models are preferred.\n",
    "   - **NMAR (Not Missing at Random):** Requires domain expertise and careful modeling.\n",
    "\n",
    "2. **Size of Missing Data:**\n",
    "   - Small proportion (<5%): Mode or new category imputation.\n",
    "   - Large proportion (>10%): Advanced methods like KNN or ML-based imputation.\n",
    "\n",
    "3. **Complexity of Dataset:**\n",
    "   - Fewer features: Use simpler methods.\n",
    "   - Many features: Use correlation-based or ML-based methods.\n",
    "\n",
    "\n",
    "\n",
    "### **Example in Python**\n",
    "\n",
    "Hereâ€™s how to handle categorical missing values with different techniques:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Color': ['Red', 'Blue', None, 'Green', 'Red'],\n",
    "    'Shape': ['Circle', None, 'Square', 'Circle', 'Square']\n",
    "})\n",
    "\n",
    "# 1. Mode Imputation\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data['Color_Mode'] = imputer.fit_transform(data[['Color']])\n",
    "\n",
    "# 2. Fill with a New Category\n",
    "data['Color_NewCategory'] = data['Color'].fillna('Missing')\n",
    "\n",
    "# 3. Frequency-based Imputation\n",
    "import random\n",
    "frequencies = data['Color'].value_counts(normalize=True)\n",
    "data['Color_Probabilistic'] = data['Color'].apply(\n",
    "    lambda x: x if pd.notnull(x) else random.choices(frequencies.index, frequencies.values)[0]\n",
    ")\n",
    "\n",
    "print(data)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Key Considerations**\n",
    "- **Impact on Variability:** Imputation methods like mode or new category can distort the true distribution.\n",
    "- **Data Integrity:** Advanced techniques like KNN or ML-based imputation better preserve relationships but require more resources.\n",
    "- **Interpretability:** Adding a \"Missing\" category might make analysis more interpretable by highlighting gaps explicitly.\n",
    "\n",
    "By carefully selecting an imputation method, you can ensure your categorical data is ready for meaningful analysis or modeling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Imputation and Missing Indicator:\n",
    "\n",
    "### **Random Imputation**  \n",
    "Random imputation is a technique used to handle missing data by replacing the missing values with a randomly selected value from the available data in the same column. The main goal is to preserve the overall distribution and variability of the data while filling in missing entries.\n",
    "\n",
    "#### **Steps for Random Imputation:**\n",
    "1. **Identify Missing Values:** Locate the missing values in the dataset.\n",
    "2. **Select Non-Missing Values:** Identify the non-missing values in the same column where the missing value exists.\n",
    "3. **Random Selection:** Randomly pick one of the available non-missing values and use it to replace the missing value.\n",
    "4. **Repeat:** Continue this process for all missing values in the dataset.\n",
    "\n",
    "#### **Advantages:**\n",
    "- Maintains the statistical properties of the dataset (e.g., mean, variance).\n",
    "- Simple to implement and computationally inexpensive.\n",
    "\n",
    "#### **Disadvantages:**\n",
    "- Introduces randomness, which might not reflect the actual missing data pattern.\n",
    "- Can distort relationships in the data if the missing values are not missing completely at random (MCAR).\n",
    "\n",
    "\n",
    "\n",
    "### **Missing Indicator**\n",
    "The missing indicator technique involves creating an additional feature (column) that flags whether a value in a particular column is missing or not. It does not fill the missing values but rather helps the model learn patterns related to missingness.\n",
    "\n",
    "#### **Steps for Missing Indicator:**\n",
    "1. **Identify Missing Values:** Detect where the missing values are in the dataset.\n",
    "2. **Create Indicator Column:** For each column with missing values, create a new binary column.  \n",
    "   - Assign `1` if the value is missing.\n",
    "   - Assign `0` if the value is present.\n",
    "3. **Keep Original Data:** The missing indicator column is used alongside the original dataset.\n",
    "\n",
    "#### **Advantages:**\n",
    "- Retains information about the missingness itself, which can sometimes be informative for predictions.\n",
    "- Does not alter the original dataset.\n",
    "\n",
    "#### **Disadvantages:**\n",
    "- Increases the dimensionality of the dataset, which may lead to overfitting if the number of indicators grows significantly.\n",
    "- Requires careful handling to ensure the model interprets the indicator appropriately.\n",
    "\n",
    "\n",
    "\n",
    "### **Use in Combination:**\n",
    "Random imputation and missing indicators can be used together. For instance:\n",
    "- Random imputation can replace missing values to allow for a complete dataset.\n",
    "- A missing indicator column can simultaneously capture the information that the value was missing, which might carry predictive importance.\n",
    "\n",
    "#### **Example:**\n",
    "Suppose you have a dataset with a column `Age` where some values are missing.\n",
    "\n",
    "1. Use **random imputation**:\n",
    "   - Replace missing `Age` values with randomly selected non-missing `Age` values (e.g., 25, 30, 35).\n",
    "\n",
    "2. Create a **missing indicator column**:\n",
    "   - Add a new column `Age_missing`:\n",
    "     - `1` if `Age` was missing.\n",
    "     - `0` if `Age` was present.\n",
    "\n",
    "This ensures that the imputed value doesnâ€™t hide the fact that the value was originally missing, enabling the model to leverage this additional information.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Imputer\n",
    "\n",
    "The **KNN Imputer (K-Nearest Neighbors Imputer)** is a method to handle missing values in a dataset by filling in (or imputing) the missing values using the values of the nearest neighbors. It is an advanced imputation method that leverages the structure of the data, making it particularly useful for numerical and categorical data.\n",
    "\n",
    "\n",
    "\n",
    "### How KNN Imputer Works:\n",
    "\n",
    "1. **Identify Missing Values**:\n",
    "   - First, identify the missing values in the dataset. For instance, if you have a dataset with some cells having `NaN`, these are the missing values.\n",
    "\n",
    "2. **Compute Distance**:\n",
    "   - For each data point with missing values, calculate the distance between this data point and all other data points that do not have missing values. This is done using a distance metric (e.g., Euclidean distance for numerical data).\n",
    "\n",
    "3. **Find Nearest Neighbors**:\n",
    "   - Identify the `k` nearest neighbors (the closest `k` data points) based on the computed distances.\n",
    "\n",
    "4. **Impute the Missing Value**:\n",
    "   - Replace the missing value with a value derived from the `k` nearest neighbors:\n",
    "     - For **numerical data**: Use the **mean** or **median** of the neighbors.\n",
    "     - For **categorical data**: Use the **mode** (most frequent value) of the neighbors.\n",
    "\n",
    "5. **Repeat**:\n",
    "   - Repeat this process for all data points with missing values.\n",
    "\n",
    "\n",
    "\n",
    "### Key Parameters in KNN Imputer:\n",
    "The KNN Imputer implementation in libraries like **scikit-learn** provides configurable options:\n",
    "\n",
    "- **`n_neighbors`**:\n",
    "  - Specifies the number of nearest neighbors (`k`) to consider for imputing the missing value.\n",
    "  - A higher value of `k` considers more neighbors but can dilute the influence of the nearest points.\n",
    "\n",
    "- **`weights`**:\n",
    "  - Specifies the weight function used in prediction. Options include:\n",
    "    - `'uniform'`: All neighbors are equally weighted.\n",
    "    - `'distance'`: Neighbors closer to the missing point are given higher weights.\n",
    "\n",
    "- **`metric`**:\n",
    "  - Determines how the distance is computed. For example, `Euclidean`, `Manhattan`, etc.\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of KNN Imputer:\n",
    "1. **Uses the Data's Structure**:\n",
    "   - Imputation is based on similar records, which often leads to more accurate imputations.\n",
    "   \n",
    "2. **Handles Both Numerical and Categorical Data**:\n",
    "   - Can be used effectively for mixed data types.\n",
    "\n",
    "3. **Non-Parametric**:\n",
    "   - Does not assume any underlying distribution of the data.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of KNN Imputer:\n",
    "1. **Computationally Expensive**:\n",
    "   - Computing distances for every missing value can be slow for large datasets.\n",
    "\n",
    "2. **Sensitive to Outliers**:\n",
    "   - The presence of outliers can significantly affect the imputed values.\n",
    "\n",
    "3. **Scaling Issues**:\n",
    "   - Requires scaling of features to avoid domination by features with larger ranges.\n",
    "\n",
    "\n",
    "\n",
    "### Steps to Use KNN Imputer in Python:\n",
    "Hereâ€™s an example of using KNN Imputer from scikit-learn:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = np.array([[1, 2, np.nan],\n",
    "                 [3, np.nan, 6],\n",
    "                 [7, 8, 9],\n",
    "                 [np.nan, 5, 3]])\n",
    "\n",
    "# Initialize the KNN Imputer with 2 neighbors\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "\n",
    "# Perform the imputation\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nImputed Data:\")\n",
    "print(imputed_data)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Output:\n",
    "If you run the code above, the output will look something like this:\n",
    "\n",
    "**Original Data:**\n",
    "```\n",
    "[[ 1.  2. nan]\n",
    " [ 3. nan  6.]\n",
    " [ 7.  8.  9.]\n",
    " [nan  5.  3.]]\n",
    "```\n",
    "\n",
    "**Imputed Data:**\n",
    "```\n",
    "[[ 1.   2.   4.5]\n",
    " [ 3.   5.5  6. ]\n",
    " [ 7.   8.   9. ]\n",
    " [ 5.   5.   3. ]]\n",
    "```\n",
    "\n",
    "Here, the missing values have been filled by the average of the neighbors' values.\n",
    "\n",
    "\n",
    "\n",
    "### Tips for Using KNN Imputer:\n",
    "1. **Scale the Data**:\n",
    "   - Always scale the dataset before applying the KNN Imputer. This ensures no feature disproportionately affects the distance calculations.\n",
    "\n",
    "2. **Choose `k` Wisely**:\n",
    "   - Experiment with different values of `k` to find the optimal number of neighbors for your dataset.\n",
    "\n",
    "3. **Handle Large Datasets**:\n",
    "   - For very large datasets, consider using a subset or preprocessing to reduce dimensionality.\n",
    "\n",
    "\n",
    "\n",
    "KNN Imputer is an effective tool for imputing missing values, especially when the dataset has meaningful patterns or correlations between features.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MICE (Multiple Imputation by Chained Equations)\n",
    "\n",
    "### **MICE (Multiple Imputation by Chained Equations)**\n",
    "\n",
    "MICE is a sophisticated technique for imputing missing values in a dataset. Unlike simpler methods (e.g., mean, median, or mode imputation), MICE performs **multiple imputations** to create several plausible datasets, analyzes them separately, and combines the results to account for the uncertainty introduced by missing values.\n",
    "\n",
    "\n",
    "\n",
    "### **How MICE Works**\n",
    "\n",
    "The MICE process involves the following steps:\n",
    "\n",
    "#### 1. **Initialization (Impute Initial Values)**:\n",
    "   - Replace missing values with an initial guess, typically the mean, median, or a random value for the variable.\n",
    "   - This initial guess serves as a starting point for the iterative imputation process.\n",
    "\n",
    "#### 2. **Iterative Imputation**:\n",
    "   - MICE imputes each variable with missing values one at a time while treating other variables as predictors.\n",
    "   - For each variable:\n",
    "     1. Set the variable with missing values as the **target**.\n",
    "     2. Use the remaining variables (both observed and imputed) as **predictors** to fit a regression model.\n",
    "     3. Predict and replace the missing values with the model's output.\n",
    "\n",
    "   - Repeat this process for all variables with missing values in a cyclic manner.\n",
    "\n",
    "#### 3. **Multiple Imputations**:\n",
    "   - Perform the iterative imputation process multiple times (e.g., 5-10 times) to generate multiple datasets.\n",
    "   - Each dataset contains slightly different imputed values to reflect the uncertainty in missing data.\n",
    "\n",
    "#### 4. **Analysis and Pooling**:\n",
    "   - Analyze each imputed dataset separately (e.g., by training a machine learning model).\n",
    "   - Combine the results using Rubinâ€™s rules to produce final estimates that incorporate variability due to imputation.\n",
    "\n",
    "\n",
    "\n",
    "### **Example of MICE**\n",
    "\n",
    "Hereâ€™s an example in Python using the `fancyimpute` or `statsmodels` library:\n",
    "\n",
    "#### Code Example:\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = pd.DataFrame({\n",
    "    \"A\": [1, 2, np.nan, 4, 5],\n",
    "    \"B\": [np.nan, 2, 3, 4, 5],\n",
    "    \"C\": [1, 2, 3, np.nan, 5]\n",
    "})\n",
    "\n",
    "# Initialize the MICE imputer (Iterative Imputer in scikit-learn)\n",
    "imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "\n",
    "# Convert back to a DataFrame\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=data.columns)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "\n",
    "print(\"\\nImputed Data:\")\n",
    "print(imputed_df)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Output**:\n",
    "**Original Data:**\n",
    "```\n",
    "     A    B    C\n",
    "0  1.0  NaN  1.0\n",
    "1  2.0  2.0  2.0\n",
    "2  NaN  3.0  3.0\n",
    "3  4.0  4.0  NaN\n",
    "4  5.0  5.0  5.0\n",
    "```\n",
    "\n",
    "**Imputed Data:**\n",
    "```\n",
    "          A         B         C\n",
    "0  1.000000  1.666667  1.000000\n",
    "1  2.000000  2.000000  2.000000\n",
    "2  2.999981  3.000000  3.000000\n",
    "3  4.000000  4.000000  3.999963\n",
    "4  5.000000  5.000000  5.000000\n",
    "```\n",
    "\n",
    "Here, missing values are imputed iteratively using other variables as predictors.\n",
    "\n",
    "\n",
    "\n",
    "### **Key Features of MICE**:\n",
    "1. **Model Flexibility**:\n",
    "   - MICE can use different models for imputation (e.g., linear regression, logistic regression for categorical variables).\n",
    "\n",
    "2. **Handles Complex Relationships**:\n",
    "   - Accounts for correlations and relationships between variables.\n",
    "\n",
    "3. **Multiple Imputation**:\n",
    "   - Provides multiple plausible imputed datasets to account for the variability and uncertainty in missing data.\n",
    "\n",
    "4. **Iterative Process**:\n",
    "   - Each variable is imputed iteratively, using updated values for better accuracy.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of MICE**:\n",
    "1. **Preserves Relationships**:\n",
    "   - By using other variables as predictors, MICE maintains the relationships in the dataset.\n",
    "   \n",
    "2. **Flexibility Across Data Types**:\n",
    "   - Suitable for continuous, categorical, or mixed data.\n",
    "\n",
    "3. **Accounts for Uncertainty**:\n",
    "   - Multiple imputations capture variability in the missing data, avoiding overconfidence in a single imputed value.\n",
    "\n",
    "4. **No Data Reduction**:\n",
    "   - All observations are retained, unlike listwise or pairwise deletion methods.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of MICE**:\n",
    "1. **Computational Cost**:\n",
    "   - Iterative and multiple imputations can be computationally intensive for large datasets.\n",
    "\n",
    "2. **Model Dependency**:\n",
    "   - Results depend on the choice of the imputation model, which might lead to biased results if the model is misspecified.\n",
    "\n",
    "3. **Complexity**:\n",
    "   - Requires expertise to handle multiple datasets and pool the results correctly.\n",
    "\n",
    "\n",
    "\n",
    "### **When to Use MICE**:\n",
    "- **Data with Strong Feature Relationships**:\n",
    "  - When variables are highly correlated and the relationship between features is important.\n",
    "- **Mixed Data Types**:\n",
    "  - MICE is versatile and can handle numerical and categorical variables together.\n",
    "- **Accounting for Variability**:\n",
    "  - Use when you want to acknowledge uncertainty in missing value estimation.\n",
    "\n",
    "\n",
    "### **Comparison with Other Imputation Techniques**:\n",
    "| Technique        | Single/Multiple | Handles Relationships | Computational Cost | Simplicity |\n",
    "|------------------|-----------------|------------------------|--------------------|------------|\n",
    "| Mean/Median      | Single          | No                     | Low                | Very Simple|\n",
    "| KNN Imputer      | Single          | Yes (local)            | Medium             | Simple     |\n",
    "| MICE             | Multiple        | Yes (global)           | High               | Moderate   |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Best Practices**:\n",
    "1. **Scale Your Data**:\n",
    "   - If using distance-based metrics (e.g., linear regression), scale features to avoid bias from feature magnitude.\n",
    "\n",
    "2. **Choose Imputation Models Carefully**:\n",
    "   - Use models appropriate for the type of variable (e.g., logistic regression for binary variables).\n",
    "\n",
    "3. **Assess the Impact of Missing Data**:\n",
    "   - Always check if imputing missing values significantly alters your analysis.\n",
    "\n",
    "\n",
    "\n",
    "MICE is one of the most robust imputation methods, suitable for datasets where relationships between variables are important and where you want to minimize bias from missing data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best and most commonly used technique For Missing data:\n",
    "\n",
    "The **best and most commonly used technique** for handling missing data depends on several factors, including the **nature of your data**, **amount of missingness**, and **resources available (e.g., time, computational power)**. Here's a simplified guide to help you make decisions based on common scenarios:\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Techniques Based on Scenarios**\n",
    "\n",
    "| **Scenario**                                         | **Best Technique**                               | **Reason**                                                                                   |\n",
    "|------------------------------------------------------|-------------------------------------------------|---------------------------------------------------------------------------------------------|\n",
    "| **Small percentage of missing data (<5%)**           | - **Complete Case Analysis**                    | - Simple and effective when missingness is negligible and does not introduce bias.          |\n",
    "| **Numerical data with low missingness**              | - **Mean/Median Imputation**                    | - Quick, easy to implement, and works well when missingness is random.                      |\n",
    "| **Categorical data with low missingness**            | - **Mode Imputation** or **Missing Category**   | - Retains simplicity; \"Missing\" category preserves the signal if missingness has meaning.   |\n",
    "| **Data has relationships between features**          | - **KNN Imputer** or **MICE**                   | - Uses correlations and patterns in data to impute values more accurately.                  |\n",
    "| **Missingness as a potential signal**                | - **Missing Indicator** (with another imputer)  | - Adds additional predictive power when missingness itself is informative.                  |\n",
    "| **High percentage of missing data (>20%)**           | - **MICE**                                      | - Best for preserving data relationships and variability, though computationally expensive.  |\n",
    "| **Small datasets with missing data**                 | - **KNN Imputer**                               | - Effective in small datasets where relationships between features are strong.              |\n",
    "| **Large datasets with missing data**                 | - **MICE (with sampling)** or **Missing Indicator** | - MICE ensures accurate imputations; indicators help models capture missingness patterns.  |\n",
    "\n",
    "---\n",
    "\n",
    "### **Most Commonly Used Techniques**\n",
    "\n",
    "1. **Complete Case Analysis**  \n",
    "   - Popular due to simplicity.\n",
    "   - Best when missing data is small (<5%) and random.\n",
    "\n",
    "2. **Mean/Median/Mode Imputation**  \n",
    "   - Common for quick fixes in preprocessing pipelines.\n",
    "   - Used extensively for numerical (mean/median) or categorical (mode) data with low missingness.\n",
    "\n",
    "3. **KNN Imputer**  \n",
    "   - Widely used for datasets where relationships between features are crucial.\n",
    "   - Easy to implement using tools like `scikit-learn`.\n",
    "\n",
    "4. **MICE (Multiple Imputation by Chained Equations)**  \n",
    "   - Increasingly popular in research and production for preserving relationships.\n",
    "   - Computationally intensive but provides robust results.\n",
    "\n",
    "5. **Missing Indicator**  \n",
    "   - Frequently combined with other imputers to capture missingness as a signal for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "### **If Still Confused: Default Approach**\n",
    "\n",
    "If you are unsure, a good **default approach** is:  \n",
    "1. Use **KNN Imputer** for numerical and categorical data.  \n",
    "   - Ensure data is scaled if numerical.  \n",
    "2. Add a **Missing Indicator** column to capture missingness.  \n",
    "3. For large or complex datasets, consider **MICE** if computational resources permit.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a more specific recommendation based on a dataset you're working with? Let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereâ€™s a table summarizing the techniques youâ€™ve learned for handling missing data, their usage scenarios, and the best practices for each:\n",
    "\n",
    "| **Technique**             | **When to Use**                                                                                                                                             | **Best Practices**                                                                                                                                                              |\n",
    "|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Complete Case Analysis** | - When the percentage of missing data is very low (<5%).                                                                                                  | - Ensure missing data is random (MCAR). <br> - Avoid if missing data introduces significant bias or reduces the dataset size drastically.                                      |\n",
    "|                            | - When removing rows/columns won't harm the analysis.                                                                                                    | - Apply only when the dataset is large enough to handle reduced data size without losing statistical power.                                                                    |\n",
    "| **Handling Missing Numerical Data** | - When numerical features have missing values that need imputation.                                                                                 | - For small gaps: Use mean or median.<br> - For large gaps: Use KNN or MICE to account for correlations.<br> - Scale data if needed (for KNN or advanced models).            |\n",
    "| **Handling Missing Categorical Data** | - When categorical features have missing values that cannot be ignored.                                                                             | - Use mode (most frequent) for simple cases.<br> - Use a separate \"Missing\" category to preserve missing data as a signal.<br> - Consider MICE for sophisticated imputations. |\n",
    "| **Missing Indicator**      | - When missing data itself might contain useful information (e.g., missing values correlate with a target variable).                                       | - Add a binary column (1 = missing, 0 = not missing) for affected features. <br> - Use alongside imputation techniques to improve model performance.                         |\n",
    "| **KNN Imputer**            | - When missing values can be imputed based on similar data points (numerical or categorical).                                                             | - Choose the right number of neighbors (`k`).<br> - Scale numerical features to avoid biases in distance calculations.<br> - Be cautious with large datasets (computational cost). |\n",
    "|                            | - Works well when data has strong correlations between features.                                                                                         |                                                                                                                                                                                |\n",
    "| **MICE (Multiple Imputation)** | - When missing data is complex, occurs in multiple variables, or has patterns based on relationships between variables.                                   | - Use for datasets with significant feature correlations.<br> - Perform multiple imputations and combine results to account for variability.<br> - Computationally intensive; consider smaller datasets if necessary. |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Tips:\n",
    "1. **Understand the Missingness Type**:\n",
    "   - **MCAR (Missing Completely at Random)**: Any technique can work; simpler ones (e.g., mean/median) are fine.\n",
    "   - **MAR (Missing at Random)**: Use methods like KNN or MICE that consider relationships between variables.\n",
    "   - **MNAR (Missing Not at Random)**: Use techniques like adding missing indicators to capture missingness as a feature.\n",
    "\n",
    "2. **Experiment with Different Techniques**:\n",
    "   - Use cross-validation to test which technique performs best for your dataset and predictive task.\n",
    "\n",
    "3. **Combine Techniques**:\n",
    "   - Sometimes, combining methods (e.g., MICE with a missing indicator) yields better results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntax Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleImputer Explanation:\n",
    "\n",
    "The `SimpleImputer` class in scikit-learn is used to handle missing values by imputing them with specified strategies. Here's a detailed explanation of its syntax and parameters:\n",
    "\n",
    "\n",
    "\n",
    "### **Syntax**\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean', fill_value=None, add_indicator=False, copy=True, keep_empty_features=False)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Parameters**\n",
    "\n",
    "#### **1. `missing_values`**\n",
    "- **Definition**: Specifies the placeholder for missing values in your dataset.\n",
    "- **Default**: `np.nan` (NumPy's representation of missing values).\n",
    "- **Other Options**: You can set a specific value (e.g., `0`, `-1`, or a custom marker) if missing data is not represented by `np.nan`.\n",
    "- **Example**: If your dataset uses `-1` to indicate missing values, set `missing_values=-1`.\n",
    "\n",
    "#### **2. `strategy`**\n",
    "- **Definition**: Determines the imputation strategy for replacing missing values.\n",
    "- **Options**:\n",
    "  - `'mean'`: Replaces missing values with the mean of the column (numerical only).\n",
    "  - `'median'`: Replaces missing values with the median of the column (numerical only).\n",
    "  - `'most_frequent'`: Replaces missing values with the most frequently occurring value in the column (can be used for both numerical and categorical data).\n",
    "  - `'constant'`: Replaces missing values with a constant value specified in the `fill_value` parameter.\n",
    "- **Default**: `'mean'`.\n",
    "\n",
    "#### **3. `fill_value`**\n",
    "- **Definition**: Specifies the constant value to use when `strategy='constant'`.\n",
    "- **Default**: `None`.\n",
    "- **Example**: If you want to replace missing values with `0` or `'missing'` for a categorical column, set `fill_value=0` or `fill_value='missing'`.\n",
    "\n",
    "#### **4. `add_indicator`**\n",
    "- **Definition**: Whether to add an additional binary indicator column for each feature with missing values.\n",
    "- **Default**: `False`.\n",
    "- **Effect**: If `True`, the imputer will add a binary column for each feature, indicating whether a value was imputed (1 for missing, 0 otherwise).\n",
    "\n",
    "#### **5. `copy`**\n",
    "- **Definition**: Determines whether the input data should be copied or modified in place.\n",
    "- **Default**: `True`.\n",
    "- **Effect**: \n",
    "  - If `True`, the input data is copied, and the original dataset remains unchanged.\n",
    "  - If `False`, the imputation modifies the input dataset directly.\n",
    "\n",
    "#### **6. `keep_empty_features`**\n",
    "- **Definition**: Whether to keep features that are entirely missing during imputation.\n",
    "- **Default**: `False`.\n",
    "- **Effect**: \n",
    "  - If `False`, features with all values missing are dropped after imputation.\n",
    "  - If `True`, such features are retained and filled with the imputed values.\n",
    "\n",
    "\n",
    "\n",
    "### **Usage Examples**\n",
    "\n",
    "#### **Example 1: Mean Imputation for Numerical Data**\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Example data with missing values\n",
    "data = np.array([[1, 2, np.nan], [3, np.nan, 6], [7, 8, 9]])\n",
    "\n",
    "# Create an imputer instance\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform the data\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "print(imputed_data)\n",
    "```\n",
    "**Output**:\n",
    "```\n",
    "[[1.  2.  7.5]\n",
    " [3.  5.  6. ]\n",
    " [7.  8.  9. ]]\n",
    "```\n",
    "- Missing values in the 3rd column are replaced by the column's mean (`(6 + 9) / 2 = 7.5`).\n",
    "\n",
    "\n",
    "\n",
    "#### **Example 2: Median Imputation**\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "print(imputed_data)\n",
    "```\n",
    "- Replaces missing values with the median of each column.\n",
    "\n",
    "\n",
    "\n",
    "#### **Example 3: Most Frequent Imputation for Categorical Data**\n",
    "```python\n",
    "data = np.array([['cat', 'dog', np.nan], ['cat', np.nan, 'mouse'], [np.nan, 'dog', 'mouse']])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "print(imputed_data)\n",
    "```\n",
    "**Output**:\n",
    "```\n",
    "[['cat' 'dog' 'mouse']\n",
    " ['cat' 'dog' 'mouse']\n",
    " ['cat' 'dog' 'mouse']]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### **Example 4: Constant Imputation**\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "print(imputed_data)\n",
    "```\n",
    "- Missing values are replaced with the constant value `'missing'`.\n",
    "\n",
    "\n",
    "\n",
    "#### **Example 5: Adding Indicators for Missing Values**\n",
    "```python\n",
    "imputer = SimpleImputer(strategy='mean', add_indicator=True)\n",
    "imputed_data = imputer.fit_transform(data)\n",
    "print(imputed_data)\n",
    "```\n",
    "- Adds binary columns indicating whether a value was missing in the original data.\n",
    "\n",
    "\n",
    "\n",
    "### **Attributes**\n",
    "\n",
    "1. **`statistics_`**:\n",
    "   - Stores the values used for imputation (e.g., mean, median, etc.).\n",
    "   - Example: After fitting the imputer, `imputer.statistics_` will show the calculated means or medians.\n",
    "\n",
    "2. **`indicator_`**:\n",
    "   - Stores the binary indicator for missing values if `add_indicator=True`.\n",
    "\n",
    "\n",
    "\n",
    "### **Best Practices**\n",
    "- Always analyze the nature of your data before deciding on an imputation strategy.\n",
    "- Use `add_indicator=True` if missingness itself carries predictive information.\n",
    "- For categorical features, consider `most_frequent` or `constant` strategies.\n",
    "- For numerical features, use `mean` or `median` based on the distribution of the data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
