{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding:\n",
    "\n",
    "Feature encoding is a technique in machine learning used to convert categorical data into a numerical format that can be understood by machine learning algorithms. Many machine learning algorithms work with numerical data and cannot process non-numeric, textual, or categorical data directly. Feature encoding bridges this gap by transforming these features into numerical representations.\n",
    "\n",
    "### Types of Feature Encoding\n",
    "\n",
    "There are several methods for feature encoding, each suited for different types of data and machine learning tasks:\n",
    "\n",
    "\n",
    "\n",
    "#### 1. **Label Encoding**\n",
    "- **Description**: Converts each category into a unique integer value.\n",
    "- **Example**:\n",
    "  - Input: `[\"Red\", \"Green\", \"Blue\"]`\n",
    "  - Encoded Output: `[0, 1, 2]`\n",
    "- **Advantages**: Simple and space-efficient.\n",
    "- **Disadvantages**: May introduce a false ordinal relationship between categories, which might not make sense for non-ordered categories.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. **One-Hot Encoding**\n",
    "- **Description**: Converts each category into a binary vector, where only one element is `1` (hot) and the rest are `0`.\n",
    "- **Example**:\n",
    "  - Input: `[\"Red\", \"Green\", \"Blue\"]`\n",
    "  - Encoded Output:\n",
    "    ```\n",
    "    Red:   [1, 0, 0]\n",
    "    Green: [0, 1, 0]\n",
    "    Blue:  [0, 0, 1]\n",
    "    ```\n",
    "- **Advantages**:\n",
    "  - No ordinal relationship introduced.\n",
    "  - Works well with nominal (non-ordered) data.\n",
    "- **Disadvantages**:\n",
    "  - High-dimensionality for datasets with many categories, leading to the \"curse of dimensionality.\"\n",
    "\n",
    "\n",
    "\n",
    "#### 3. **Ordinal Encoding**\n",
    "- **Description**: Maps categories to integers based on an order or ranking.\n",
    "- **Example**:\n",
    "  - Input: `[\"Low\", \"Medium\", \"High\"]`\n",
    "  - Encoded Output: `[0, 1, 2]`\n",
    "- **Advantages**: Preserves the inherent order of data.\n",
    "- **Disadvantages**: Only suitable for ordered categories.\n",
    "\n",
    "\n",
    "\n",
    "#### 4. **Binary Encoding**\n",
    "- **Description**: Combines aspects of label and one-hot encoding. The category is first label-encoded and then converted into binary form.\n",
    "- **Example**:\n",
    "  - Input: `[\"A\", \"B\", \"C\", \"D\"]`\n",
    "  - Label Encoding: `[0, 1, 2, 3]`\n",
    "  - Binary Encoding:\n",
    "    ```\n",
    "    A: [0, 0]\n",
    "    B: [0, 1]\n",
    "    C: [1, 0]\n",
    "    D: [1, 1]\n",
    "    ```\n",
    "- **Advantages**:\n",
    "  - Reduces dimensionality compared to one-hot encoding.\n",
    "  - Handles large categorical datasets efficiently.\n",
    "- **Disadvantages**: May be less interpretable.\n",
    "\n",
    "\n",
    "\n",
    "#### 5. **Frequency Encoding**\n",
    "- **Description**: Encodes categories based on their frequency or count in the dataset.\n",
    "- **Example**:\n",
    "  - Input: `[\"A\", \"A\", \"B\", \"B\", \"B\", \"C\"]`\n",
    "  - Encoded Output: `[2, 2, 3, 3, 3, 1]` (frequencies of `A`, `B`, and `C`).\n",
    "- **Advantages**:\n",
    "  - Retains some statistical information about the data.\n",
    "- **Disadvantages**: May introduce bias if frequencies are not representative.\n",
    "\n",
    "\n",
    "\n",
    "#### 6. **Target Encoding**\n",
    "- **Description**: Replaces each category with the mean of the target variable for that category.\n",
    "- **Example** (For regression):\n",
    "  - Input Categories: `[\"A\", \"B\", \"C\"]`\n",
    "  - Target Values: `[10, 15, 20, 10, 15, 25]`\n",
    "  - Encoded Output: Mean Target Value per Category (e.g., `A = 10`, `B = 15`, `C = 25`).\n",
    "- **Advantages**:\n",
    "  - Preserves information about the relationship between the feature and the target.\n",
    "- **Disadvantages**: Can cause overfitting if not regularized.\n",
    "\n",
    "\n",
    "\n",
    "### When to Use Different Encoding Methods\n",
    "- **Nominal Data (No order)**: Use one-hot encoding or binary encoding.\n",
    "- **Ordinal Data (Ordered categories)**: Use ordinal encoding.\n",
    "- **High Cardinality Data (Many unique categories)**: Use binary encoding, frequency encoding, or target encoding.\n",
    "\n",
    "### Challenges of Feature Encoding\n",
    "1. **Dimensionality Explosion**: One-hot encoding can create many new features for datasets with numerous categories.\n",
    "2. **Overfitting**: Target encoding might overfit if categories have few samples.\n",
    "3. **Interpretability**: Encoded values might lose human interpretability, especially in complex methods like binary or target encoding.\n",
    "\n",
    "### Practical Example in Python\n",
    "Here’s an example using `pandas` for one-hot and label encoding:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df['Color_Label'] = label_encoder.fit_transform(df['Color'])\n",
    "\n",
    "# One-Hot Encoding\n",
    "one_hot_encoder = pd.get_dummies(df['Color'], prefix='Color')\n",
    "df = pd.concat([df, one_hot_encoder], axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "   Color  Color_Label  Color_Blue  Color_Green  Color_Red\n",
    "0   Red            2           0            0          1\n",
    "1  Green           1           0            1          0\n",
    "2   Blue           0           1            0          0\n",
    "3  Green           1           0            1          0\n",
    "4   Red            2           0            0          1\n",
    "```\n",
    "\n",
    "### Summary\n",
    "Feature encoding is an essential preprocessing step that transforms categorical data into a format suitable for machine learning models. The choice of encoding method depends on the type of data, the machine learning algorithm, and the specific problem being addressed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding:\n",
    "\n",
    "\n",
    "\n",
    "**Ordinal encoding** is a feature encoding technique where categorical values are mapped to integer values based on their order or rank. It is specifically used when the categories have a logical order or hierarchy, making it suitable for **ordinal data** (data with an inherent order).\n",
    "\n",
    "\n",
    "\n",
    "### How It Works\n",
    "1. **Identify the categories**: Define all the unique categories in the data.\n",
    "2. **Assign a rank**: Assign each category a unique integer based on its position in the order.\n",
    "3. **Replace the values**: Replace the original category values with their corresponding integer ranks.\n",
    "\n",
    "\n",
    "\n",
    "### Characteristics of Ordinal Encoding\n",
    "- **Order is preserved**: The encoded integers reflect the relative position of categories.\n",
    "- **No assumption of distance**: While the order is respected, the difference between the encoded integers does not represent a meaningful metric (e.g., the difference between `1` and `2` is not the same as between `2` and `3` unless explicitly designed to do so).\n",
    "- **Suitable for ordinal data**: This method is ideal when categories have a natural order (e.g., ratings, educational levels).\n",
    "\n",
    "\n",
    "\n",
    "### Example of Ordinal Encoding\n",
    "\n",
    "#### Input Data\n",
    "Consider a dataset with the following feature: `Education Level`.\n",
    "\n",
    "| Education Level |\n",
    "|------------------|\n",
    "| High School      |\n",
    "| Bachelor's       |\n",
    "| Master's         |\n",
    "| PhD              |\n",
    "\n",
    "#### Encoding Process\n",
    "1. **Determine the order**:\n",
    "   ```\n",
    "   High School < Bachelor's < Master's < PhD\n",
    "   ```\n",
    "2. **Assign integer values**:\n",
    "   - High School → `0`\n",
    "   - Bachelor's → `1`\n",
    "   - Master's → `2`\n",
    "   - PhD → `3`\n",
    "\n",
    "#### Encoded Data\n",
    "| Education Level | Encoded Value |\n",
    "|------------------|---------------|\n",
    "| High School      | 0             |\n",
    "| Bachelor's       | 1             |\n",
    "| Master's         | 2             |\n",
    "| PhD              | 3             |\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of Ordinal Encoding\n",
    "1. **Simple and efficient**: Easy to implement and uses minimal memory compared to other methods like one-hot encoding.\n",
    "2. **Preserves order**: The inherent order of the categories is retained, which is important for ordinal data.\n",
    "3. **Compact representation**: No increase in dimensionality, unlike one-hot encoding.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of Ordinal Encoding\n",
    "1. **Misinterpretation of distance**: Models might interpret the numerical differences as meaningful distances, even though the gap between categories (e.g., \"High School\" and \"Bachelor's\") may not be equivalent.\n",
    "2. **Not suitable for nominal data**: If the data has no inherent order, ordinal encoding can introduce spurious relationships.\n",
    "\n",
    "\n",
    "\n",
    "### Python Example\n",
    "\n",
    "Here’s an example using `pandas`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Education Level': ['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the order of categories\n",
    "categories = [['High School', 'Bachelor\\'s', 'Master\\'s', 'PhD']]\n",
    "\n",
    "# Apply Ordinal Encoding\n",
    "ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "df['Encoded Level'] = ordinal_encoder.fit_transform(df[['Education Level']])\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "#### Output:\n",
    "```\n",
    "  Education Level  Encoded Level\n",
    "0    High School            0.0\n",
    "1      Bachelor's           1.0\n",
    "2        Master's           2.0\n",
    "3            PhD            3.0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Use Case Scenarios\n",
    "Ordinal encoding is particularly useful in scenarios where the relationship between categories is meaningful. For example:\n",
    "1. **Customer satisfaction levels**: \"Very Unsatisfied,\" \"Unsatisfied,\" \"Neutral,\" \"Satisfied,\" \"Very Satisfied.\"\n",
    "2. **Educational qualifications**: \"High School,\" \"Bachelor's,\" \"Master's,\" \"PhD.\"\n",
    "3. **Severity levels**: \"Low,\" \"Medium,\" \"High,\" \"Critical.\"\n",
    "\n",
    "\n",
    "\n",
    "### When Not to Use Ordinal Encoding\n",
    "- **For nominal data**: If the data does not have a natural order (e.g., colors like \"Red,\" \"Green,\" \"Blue\"), ordinal encoding can mislead machine learning models.\n",
    "- **When distances matter**: If the encoded values are used in algorithms sensitive to distances (e.g., k-NN, SVM, or linear regression), ordinal encoding might introduce bias.\n",
    "\n",
    "\n",
    "\n",
    "### Key Considerations\n",
    "1. **Order Assumptions**: Ensure the categories genuinely have a logical order before applying ordinal encoding.\n",
    "2. **Impact on Model**: Evaluate whether the model being used is sensitive to the numerical values of the encoding.\n",
    "3. **Alternative Methods**: For nominal data or cases where distance misinterpretation is problematic, consider one-hot encoding or target encoding instead.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "Ordinal encoding is a compact and effective method to encode ordinal categorical features, preserving their inherent order. However, it should be applied cautiously, as its numerical representation might inadvertently mislead some machine learning models. Always evaluate the nature of the data and the requirements of the model before choosing this encoding technique.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding:\n",
    "\n",
    "\n",
    "**One-Hot Encoding** is a feature encoding technique used in machine learning to transform categorical data into a numerical format that algorithms can process. It represents each unique category as a binary vector, where only one position is marked with a `1` (hot), and the others are marked with `0` (cold). This encoding is particularly useful for **nominal data** (categories without any inherent order).\n",
    "\n",
    "\n",
    "\n",
    "### Why One-Hot Encoding?\n",
    "\n",
    "Machine learning models work with numbers and often struggle with non-numeric, categorical data. However, directly mapping categories to integers (e.g., \"Red\" → 0, \"Green\" → 1, \"Blue\" → 2) can introduce unintended ordinal relationships where none exist. One-hot encoding solves this issue by representing each category as a separate binary feature, ensuring the model treats them as independent and unrelated.\n",
    "\n",
    "\n",
    "\n",
    "### How It Works\n",
    "\n",
    "#### Input Data\n",
    "Suppose you have a feature `Color` with three unique categories: `[\"Red\", \"Green\", \"Blue\"]`.\n",
    "\n",
    "#### One-Hot Encoding Process\n",
    "1. **Identify unique categories**:\n",
    "   ```\n",
    "   Categories = [\"Red\", \"Green\", \"Blue\"]\n",
    "   ```\n",
    "2. **Create binary vectors**:\n",
    "   Assign a separate binary feature for each category.\n",
    "   - \"Red\" → `[1, 0, 0]`\n",
    "   - \"Green\" → `[0, 1, 0]`\n",
    "   - \"Blue\" → `[0, 0, 1]`\n",
    "3. **Replace the original values**:\n",
    "   Replace the categorical values in the dataset with their respective binary vectors.\n",
    "\n",
    "#### Encoded Data\n",
    "| Color      | Red | Green | Blue |\n",
    "|------------|-----|-------|------|\n",
    "| Red        | 1   | 0     | 0    |\n",
    "| Green      | 0   | 1     | 0    |\n",
    "| Blue       | 0   | 0     | 1    |\n",
    "| Green      | 0   | 1     | 0    |\n",
    "| Red        | 1   | 0     | 0    |\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of One-Hot Encoding\n",
    "\n",
    "1. **No Ordinal Relationship Introduced**:\n",
    "   - Ensures the categories are treated as distinct and unrelated, avoiding unintended assumptions about their order.\n",
    "2. **Works Well with Many Algorithms**:\n",
    "   - Most machine learning models can handle binary inputs effectively.\n",
    "3. **Simplicity**:\n",
    "   - Easy to understand and implement.\n",
    "\n",
    "\n",
    "\n",
    "### Disadvantages of One-Hot Encoding\n",
    "\n",
    "1. **High Dimensionality**:\n",
    "   - For features with many unique categories, one-hot encoding can create a large number of new features, leading to the \"curse of dimensionality.\"\n",
    "   - Example: A column with 1,000 unique categories results in 1,000 binary features.\n",
    "2. **Sparsity**:\n",
    "   - The resulting matrix is sparse (contains many `0`s), which can lead to inefficient memory and computation.\n",
    "3. **Potential Overfitting**:\n",
    "   - Models can overfit when working with datasets containing high-cardinality categorical features, especially if the dataset is small.\n",
    "\n",
    "\n",
    "\n",
    "### When to Use One-Hot Encoding\n",
    "\n",
    "- **Nominal Data**:\n",
    "  - Categories have no inherent order (e.g., colors, countries, product names).\n",
    "- **Small to Medium Cardinality**:\n",
    "  - The feature has a manageable number of unique categories.\n",
    "\n",
    "\n",
    "\n",
    "### When Not to Use One-Hot Encoding\n",
    "\n",
    "- **High Cardinality**:\n",
    "  - If a feature has too many unique categories, consider alternatives like binary encoding, frequency encoding, or embedding techniques.\n",
    "- **Sparse Features**:\n",
    "  - If you expect the matrix to be highly sparse, use dimensionality reduction or hashing techniques.\n",
    "\n",
    "\n",
    "\n",
    "### Python Implementation\n",
    "\n",
    "Here’s how to implement one-hot encoding in Python using `pandas` and `sklearn`.\n",
    "\n",
    "#### Using Pandas\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-Hot Encoding using pandas\n",
    "one_hot = pd.get_dummies(df['Color'], prefix='Color')\n",
    "df = pd.concat([df, one_hot], axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "   Color  Color_Blue  Color_Green  Color_Red\n",
    "0    Red           0            0          1\n",
    "1  Green           0            1          0\n",
    "2   Blue           1            0          0\n",
    "3  Green           0            1          0\n",
    "4    Red           0            0          1\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Using Scikit-Learn\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Color': ['Red', 'Green', 'Blue', 'Green', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)  # Set sparse=False to get a dense array\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded = encoder.fit_transform(df[['Color']])\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(['Color']))\n",
    "\n",
    "# Combine with original data\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "print(df)\n",
    "```\n",
    "\n",
    "**Output**:\n",
    "```\n",
    "   Color  Color_Blue  Color_Green  Color_Red\n",
    "0    Red         0.0          0.0        1.0\n",
    "1  Green         0.0          1.0        0.0\n",
    "2   Blue         1.0          0.0        0.0\n",
    "3  Green         0.0          1.0        0.0\n",
    "4    Red         0.0          0.0        1.0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Alternatives to One-Hot Encoding\n",
    "\n",
    "1. **Binary Encoding**:\n",
    "   - Combines label and binary encoding to reduce dimensionality.\n",
    "2. **Target Encoding**:\n",
    "   - Encodes categories based on the target variable (mean or proportion).\n",
    "3. **Frequency Encoding**:\n",
    "   - Uses the frequency of each category for encoding.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "**One-hot encoding** is an effective method for converting categorical data into a machine-readable format while preserving the independence of categories. It is particularly suitable for nominal data but can lead to high-dimensional datasets when dealing with high-cardinality features. Understanding its advantages, limitations, and alternatives is crucial for selecting the appropriate encoding method for your machine learning problem.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformer:\n",
    "\n",
    "In **machine learning pipelines**, datasets often contain features of different types that require different preprocessing techniques. For example, some columns might be categorical and require encoding, while others are numerical and need scaling or imputation.\n",
    "\n",
    "The **`ColumnTransformer`** in **scikit-learn** is a powerful tool that allows you to apply different preprocessing transformations to specific columns of a dataset in a single, efficient step.\n",
    "\n",
    "\n",
    "\n",
    "### What is a ColumnTransformer?\n",
    "\n",
    "A **`ColumnTransformer`** applies **different preprocessing steps** to **different columns** of a dataset. It streamlines the preprocessing process by allowing you to specify transformations for individual columns or groups of columns, making it highly efficient for handling mixed data types.\n",
    "\n",
    "\n",
    "\n",
    "### Why Use ColumnTransformer?\n",
    "\n",
    "1. **Ease of Use**:\n",
    "   - Handles multiple preprocessing steps in a single unified framework.\n",
    "   - Avoids the need for manually separating and processing subsets of data.\n",
    "\n",
    "2. **Consistency**:\n",
    "   - Ensures that the same preprocessing steps are applied consistently during training and testing.\n",
    "\n",
    "3. **Integration**:\n",
    "   - Fits seamlessly into scikit-learn pipelines, enabling smooth end-to-end machine learning workflows.\n",
    "\n",
    "\n",
    "\n",
    "### How Does ColumnTransformer Work?\n",
    "\n",
    "#### Key Steps:\n",
    "1. **Define Transformers**:\n",
    "   - Specify the preprocessing steps for specific columns (e.g., scaling, encoding).\n",
    "   - Each transformer is a tuple of:\n",
    "     - A name for the transformer (e.g., `\"num_scaler\"`).\n",
    "     - The preprocessing object (e.g., `StandardScaler()`).\n",
    "     - The list of columns to apply the transformer to.\n",
    "\n",
    "2. **Instantiate the ColumnTransformer**:\n",
    "   - Combine the transformers into a single `ColumnTransformer`.\n",
    "\n",
    "3. **Fit and Transform**:\n",
    "   - Use `fit_transform` on training data and `transform` on testing data.\n",
    "\n",
    "\n",
    "\n",
    "### Example Use Case\n",
    "\n",
    "#### Dataset\n",
    "A dataset with mixed feature types:\n",
    "| Age   | Salary     | Gender   | City       |\n",
    "|-------|------------|----------|------------|\n",
    "| 25    | 50000      | Male     | New York   |\n",
    "| 30    | 60000      | Female   | San Diego  |\n",
    "| 35    | 55000      | Female   | Chicago    |\n",
    "\n",
    "#### Goal\n",
    "- Scale numerical features (`Age` and `Salary`) using `StandardScaler`.\n",
    "- Encode categorical features (`Gender` and `City`) using `OneHotEncoder`.\n",
    "\n",
    "\n",
    "\n",
    "#### Implementation\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    'Age': [25, 30, 35],\n",
    "    'Salary': [50000, 60000, 55000],\n",
    "    'Gender': ['Male', 'Female', 'Female'],\n",
    "    'City': ['New York', 'San Diego', 'Chicago']\n",
    "})\n",
    "\n",
    "# Define transformers\n",
    "numeric_transformer = ('num_scaler', StandardScaler(), ['Age', 'Salary'])\n",
    "categorical_transformer = ('cat_encoder', OneHotEncoder(), ['Gender', 'City'])\n",
    "\n",
    "# Combine transformers in a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        numeric_transformer,\n",
    "        categorical_transformer\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit and transform the data\n",
    "transformed_data = preprocessor.fit_transform(data)\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "transformed_columns = (\n",
    "    ['Age_scaled', 'Salary_scaled'] +\n",
    "    preprocessor.named_transformers_['cat_encoder'].get_feature_names_out(['Gender', 'City']).tolist()\n",
    ")\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=transformed_columns)\n",
    "\n",
    "print(transformed_df)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Output\n",
    "The transformed dataset:\n",
    "| Age_scaled | Salary_scaled | Gender_Female | Gender_Male | City_Chicago | City_New York | City_San Diego |\n",
    "|------------|---------------|---------------|-------------|--------------|---------------|----------------|\n",
    "| -1.22474   | -1.22474      | 0.0           | 1.0         | 0.0          | 1.0           | 0.0            |\n",
    "| 0.0        | 1.22474       | 1.0           | 0.0         | 0.0          | 0.0           | 1.0            |\n",
    "| 1.22474    | 0.0           | 1.0           | 0.0         | 1.0          | 0.0           | 0.0            |\n",
    "\n",
    "\n",
    "\n",
    "### Parameters of ColumnTransformer\n",
    "\n",
    "1. **`transformers`**:\n",
    "   - List of tuples specifying the transformations.\n",
    "   - Each tuple contains:\n",
    "     - Name of the transformer.\n",
    "     - The transformation object.\n",
    "     - List of column names or indices.\n",
    "\n",
    "2. **`remainder`**:\n",
    "   - Specifies what to do with columns not explicitly listed in `transformers`.\n",
    "   - Options:\n",
    "     - `'drop'` (default): Drops unprocessed columns.\n",
    "     - `'passthrough'`: Keeps unprocessed columns.\n",
    "\n",
    "3. **`sparse_threshold`**:\n",
    "   - Specifies the density of the output matrix to decide whether it should be a sparse matrix or dense.\n",
    "\n",
    "\n",
    "\n",
    "### Advanced Usage\n",
    "\n",
    "#### Custom Transformers\n",
    "You can define and use custom transformers within `ColumnTransformer` to handle specific preprocessing tasks.\n",
    "\n",
    "#### Integration with Pipelines\n",
    "`ColumnTransformer` integrates well with scikit-learn's `Pipeline`. For example:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on new data\n",
    "predictions = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Advantages of ColumnTransformer\n",
    "\n",
    "1. **Handles Mixed Data**:\n",
    "   - Process numerical and categorical data simultaneously.\n",
    "\n",
    "2. **Modularity**:\n",
    "   - Each preprocessing step is isolated and can be easily modified.\n",
    "\n",
    "3. **Improved Readability**:\n",
    "   - Centralizes preprocessing logic in one place.\n",
    "\n",
    "4. **Efficiency**:\n",
    "   - Reduces manual preprocessing steps and automates repetitive tasks.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "`ColumnTransformer` is a versatile and efficient tool for preprocessing datasets with mixed data types. It simplifies workflows by combining multiple preprocessing steps into a single operation, ensuring consistent and efficient transformations. Combined with `Pipeline`, it helps create clean, maintainable, and reusable machine learning workflows.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines:\n",
    "\n",
    "A **machine learning (ML) pipeline** is a systematic, automated workflow that helps streamline the end-to-end process of building and deploying ML models. It integrates various steps, from data preprocessing to model training and evaluation, ensuring consistency and reproducibility. Pipelines are particularly useful in production environments, where automation and scalability are essential.\n",
    "\n",
    "Here’s a **step-by-step explanation** of the typical stages in an ML pipeline:\n",
    "\n",
    "\n",
    "\n",
    "### **1. Data Collection**\n",
    "- **Purpose**: Gather data from multiple sources like databases, APIs, or flat files.\n",
    "- **Examples**: User activity logs, sales data, sensor readings.\n",
    "- **Tools**: Python libraries (e.g., `pandas`, `requests`), ETL tools.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Data Preprocessing**\n",
    "- **Purpose**: Clean, format, and prepare raw data for modeling.\n",
    "- **Steps**:\n",
    "  - **Cleaning**: Handle missing values, remove duplicates, and correct errors.\n",
    "  - **Transformation**: Convert data into suitable formats (e.g., one-hot encoding for categorical variables).\n",
    "  - **Scaling**: Normalize or standardize features.\n",
    "- **Tools**: `pandas`, `scikit-learn`, `numpy`.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Feature Engineering**\n",
    "- **Purpose**: Enhance the predictive power of data by creating or selecting relevant features.\n",
    "- **Techniques**:\n",
    "  - Feature selection: Identify the most important features.\n",
    "  - Feature extraction: Derive new features (e.g., using PCA).\n",
    "  - Domain knowledge: Add new features based on expertise.\n",
    "- **Tools**: Scikit-learn, Featuretools, domain-specific tools.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Model Selection**\n",
    "- **Purpose**: Choose the right algorithm based on the problem type (classification, regression, clustering, etc.) and data.\n",
    "- **Examples**:\n",
    "  - Classification: Logistic regression, decision trees.\n",
    "  - Regression: Linear regression, random forest.\n",
    "  - Clustering: K-means, DBSCAN.\n",
    "- **Tools**: Scikit-learn, TensorFlow, PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Model Training**\n",
    "- **Purpose**: Train the chosen model on the processed data.\n",
    "- **Key Concepts**:\n",
    "  - Splitting data into training, validation, and test sets.\n",
    "  - Hyperparameter tuning (e.g., grid search, random search).\n",
    "  - Cross-validation to avoid overfitting.\n",
    "- **Tools**: Scikit-learn, TensorFlow, Keras, PyTorch.\n",
    "\n",
    "\n",
    "\n",
    "### **6. Model Evaluation**\n",
    "- **Purpose**: Assess the model's performance using metrics relevant to the problem.\n",
    "- **Metrics**:\n",
    "  - Classification: Accuracy, precision, recall, F1 score.\n",
    "  - Regression: Mean Squared Error (MSE), R².\n",
    "  - Clustering: Silhouette score, Davies–Bouldin index.\n",
    "- **Tools**: Scikit-learn, Matplotlib, Seaborn.\n",
    "\n",
    "\n",
    "\n",
    "### **7. Model Deployment**\n",
    "- **Purpose**: Integrate the trained model into a production environment for real-world use.\n",
    "- **Steps**:\n",
    "  - Export the model (e.g., as a `.pkl` or `.h5` file).\n",
    "  - Serve the model via APIs (e.g., using Flask, FastAPI, or Django).\n",
    "  - Monitor performance and update if needed.\n",
    "- **Tools**: Flask, FastAPI, Docker, Kubernetes.\n",
    "\n",
    "\n",
    "\n",
    "### **8. Model Monitoring and Maintenance**\n",
    "- **Purpose**: Track the model’s performance in production and retrain if needed.\n",
    "- **Key Aspects**:\n",
    "  - Monitor accuracy and drift in data.\n",
    "  - Log predictions and errors.\n",
    "  - Schedule retraining with updated data.\n",
    "- **Tools**: Prometheus, Grafana, Airflow.\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of ML Pipelines**\n",
    "1. **Automation**: Reduces manual effort by automating repetitive tasks.\n",
    "2. **Reproducibility**: Ensures consistent results with a fixed workflow.\n",
    "3. **Scalability**: Allows processing large datasets and deploying models at scale.\n",
    "4. **Modularity**: Easy to update or replace specific steps.\n",
    "\n",
    "### **Example: Scikit-learn Pipeline**\n",
    "In Python, libraries like Scikit-learn provide built-in support for ML pipelines.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Step 1: Scaling\n",
    "    ('classifier', RandomForestClassifier())  # Step 2: Model\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the pipeline\n",
    "y_pred = pipeline.predict(X_test)\n",
    "```\n",
    "\n",
    "### **Conclusion**\n",
    "ML pipelines are essential for streamlining workflows, making them efficient and reliable. By automating processes and ensuring consistency, pipelines enable faster development and deployment of machine learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding:\n",
    "\n",
    "The **`OrdinalEncoder`** is a part of the `sklearn.preprocessing` module and is used to convert categorical data into integer labels. This is particularly useful when the categorical values have an inherent order, such as low, medium, high, or rating scales.\n",
    "\n",
    "### **Syntax:**\n",
    "```python\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories='auto', dtype=<data type>)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = ordinal_encoder.fit_transform(X)\n",
    "```\n",
    "\n",
    "### **Parameters Explanation:**\n",
    "\n",
    "1. **`categories`** (`'auto'`, list of lists, or 'manual'):\n",
    "   - Determines the category values for each feature.\n",
    "   - **`'auto'`** (default): The encoder automatically determines the category ordering based on the data. This is useful when the data is already ordered.\n",
    "   - **list of lists**: You can manually specify the order of categories for each feature.\n",
    "     ```python\n",
    "     categories=[['Low', 'Medium', 'High'], ['Male', 'Female']]\n",
    "     ```\n",
    "   - **'manual'**: The user can specify a list of categories that will override the default order in each column.\n",
    "\n",
    "2. **`dtype`** (`data type`):\n",
    "   - The data type of the output encoded array.\n",
    "   - The default is `np.int64`. You can change it to other types like `np.float64` if needed.\n",
    "     ```python\n",
    "     dtype='float64'\n",
    "     ```\n",
    "\n",
    "3. **`handle_unknown`** (`'error'`, `'use_encoded_value'`):\n",
    "   - Specifies how to handle categories in the test data that were not seen during training:\n",
    "     - **`'error'`** (default): If an unknown category is encountered during `transform`, an error is raised.\n",
    "     - **`'use_encoded_value'`**: Assigns a custom value (defined in `unknown_value` below) for unknown categories during `transform`.\n",
    "     - **`'use_encoded_value'`** is useful when you're dealing with unseen categories during inference.\n",
    "     \n",
    "4. **`unknown_value`** (`int` or `float`):\n",
    "   - The integer or float value assigned to unknown categories when `handle_unknown='use_encoded_value'`.\n",
    "   - Default is `-1`.\n",
    "   \n",
    "5. **`encoded_missing_value`** (`'use_encoded_value'`):\n",
    "   - Whether to treat missing values (`NaN`) as an unknown value.\n",
    "   \n",
    "### **Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'review': ['Poor', 'Average', 'Good', 'Good', 'Average'],\n",
    "    'education': ['School', 'UG', 'PG', 'PG', 'UG']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the order of categories\n",
    "categories = [['Poor', 'Average', 'Good'], ['School', 'UG', 'PG']]\n",
    "\n",
    "# Initialize the OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=categories)\n",
    "\n",
    "# Fit and transform the data\n",
    "df_encoded = df.copy()\n",
    "df_encoded[['review_encoded', 'education_encoded']] = ordinal_encoder.fit_transform(df[['review', 'education']])\n",
    "\n",
    "print(df_encoded)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Output:**\n",
    "```\n",
    "    review education  review_encoded  education_encoded\n",
    "0     Poor    School             0.0               0.0\n",
    "1  Average        UG             1.0               1.0\n",
    "2     Good        PG             2.0               2.0\n",
    "3     Good        PG             2.0               2.0\n",
    "4  Average        UG             1.0               1.0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Explanation of the Example:**\n",
    "\n",
    "- **`categories`**: \n",
    "  - For the `review` column, the categories are ordered as `['Poor', 'Average', 'Good']`, so `Poor` is encoded as `0`, `Average` as `1`, and `Good` as `2`.\n",
    "  - For the `education` column, the categories are ordered as `['School', 'UG', 'PG']`, so `School` is encoded as `0`, `UG` as `1`, and `PG` as `2`.\n",
    "  \n",
    "- **`fit_transform`**:\n",
    "  - This method first learns the encoding (using `fit`) based on the categories you specify, then applies this encoding to the input data (using `transform`).\n",
    "\n",
    "\n",
    "\n",
    "### **Key Notes:**\n",
    "- **`OrdinalEncoder`** is useful when the categorical features have an inherent order (ordinal data), such as ratings (e.g., \"low\", \"medium\", \"high\").\n",
    "- If your categories don't have a meaningful order, consider using **`OneHotEncoder`** instead, which creates binary columns for each category.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoder:\n",
    "\n",
    "**Label Encoding** is a technique used to convert categorical labels into numeric labels. It is especially useful when the categorical variable is ordinal (has an inherent order) or when it's necessary to convert categorical data to a numeric format for machine learning algorithms.\n",
    "\n",
    "In **scikit-learn**, the **`LabelEncoder`** class is used to perform label encoding.\n",
    "\n",
    "### **Syntax of LabelEncoder**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_labels = le.fit_transform(y)\n",
    "```\n",
    "\n",
    "### **Parameters Explanation:**\n",
    "\n",
    "1. **`classes_`** (attribute):\n",
    "   - **Type**: `numpy.ndarray`\n",
    "   - **Description**: After fitting the encoder, `classes_` stores the unique classes in the order they were encountered.\n",
    "   - **Note**: This is an attribute and not a parameter.\n",
    "   - **Example**: If you have a column with labels `['Low', 'Medium', 'High']`, then `classes_` will hold `['Low', 'Medium', 'High']`.\n",
    "\n",
    "2. **`fit_transform(y)`**:\n",
    "   - **Parameters**:\n",
    "     - **`y`**: This is the target data that needs to be encoded (1D array, list, or pandas Series).\n",
    "       - **Description**: It is the categorical data you want to encode into integers.\n",
    "       - **Shape**: The input data should be a 1D array.\n",
    "   \n",
    "   - **Returns**: The transformed data (encoded labels) as a numpy array.\n",
    "   \n",
    "   **Usage**: \n",
    "   ```python\n",
    "   y_encoded = le.fit_transform(['low', 'medium', 'high', 'high', 'medium'])\n",
    "   ```\n",
    "   This will return the corresponding integer-encoded labels.\n",
    "\n",
    "3. **`inverse_transform(y)`**:\n",
    "   - **Parameters**:\n",
    "     - **`y`**: The encoded labels (numeric labels) that you want to convert back to the original categories.\n",
    "   - **Returns**: The original labels (categories).\n",
    "   \n",
    "   **Usage**:\n",
    "   ```python\n",
    "   original_labels = le.inverse_transform([0, 1, 2])\n",
    "   ```\n",
    "\n",
    "4. **`handle_unknown`**:\n",
    "   - **Type**: `'error'` or `'use_encoded_value'`\n",
    "   - **Description**: This is an optional parameter introduced in version `0.24` of scikit-learn, which is used when you want to handle unknown labels during transformation.\n",
    "     - **`'error'`** (default): If an unknown label is encountered during transformation, it raises an error.\n",
    "     - **`'use_encoded_value'`**: When set to this value, the encoder assigns an encoded value (defined by `unknown_value`) to unknown labels during transformation.\n",
    "   \n",
    "5. **`unknown_value`**:\n",
    "   - **Type**: int or str\n",
    "   - **Description**: This is used only when `handle_unknown='use_encoded_value'`. It specifies what value should be assigned to unknown labels during transformation.\n",
    "   - **Default**: `-1`\n",
    "   \n",
    "   Example:\n",
    "   ```python\n",
    "   le = LabelEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "### **Example Using LabelEncoder**\n",
    "\n",
    "Let's walk through an example to demonstrate how `LabelEncoder` works:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Review': ['Good', 'Bad', 'Average', 'Good', 'Bad']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'Review' column\n",
    "df['Review_Encoded'] = le.fit_transform(df['Review'])\n",
    "\n",
    "# Display the original and encoded values\n",
    "print(df)\n",
    "```\n",
    "\n",
    "### **Output**\n",
    "```\n",
    "    Review  Review_Encoded\n",
    "0     Good               1\n",
    "1      Bad               0\n",
    "2  Average               2\n",
    "3     Good               1\n",
    "4      Bad               0\n",
    "```\n",
    "\n",
    "### **Explanation**:\n",
    "1. **Fitting and Transforming**:\n",
    "   - The `fit_transform()` method first learns the unique categories in the `Review` column (`'Good'`, `'Bad'`, `'Average'`) and assigns each category a unique numeric label:\n",
    "     - `Bad` -> `0`\n",
    "     - `Good` -> `1`\n",
    "     - `Average` -> `2`\n",
    "   \n",
    "2. **Inverse Transformation**:\n",
    "   - You can convert the encoded labels back to the original labels using the `inverse_transform()` method.\n",
    "   \n",
    "   ```python\n",
    "   original_labels = le.inverse_transform([0, 1, 2])\n",
    "   print(original_labels)  # Output: ['Bad' 'Good' 'Average']\n",
    "   ```\n",
    "\n",
    "### **Key Takeaways**:\n",
    "- **LabelEncoder** is generally used for **target encoding** (i.e., the dependent variable in supervised learning).\n",
    "- It works by assigning a unique integer to each category in a column.\n",
    "- The output of `fit_transform()` is a numeric representation of the original labels.\n",
    "- If you have new, unseen labels during transformation, you can use the `handle_unknown` parameter to control how to handle those.\n",
    "\n",
    "### **Important Notes**:\n",
    "- Label encoding should only be used when there is an inherent order in the categories (i.e., the categories are ordinal).\n",
    "- If the categorical data is nominal (i.e., no inherent order), consider using **OneHotEncoder** or other encoding methods instead.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoder:\n",
    "\n",
    "Here is the full syntax of `OneHotEncoder` in scikit-learn along with a detailed explanation of each parameter and attribute:\n",
    "\n",
    "### **Full Syntax of `OneHotEncoder`**:\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.OneHotEncoder(\n",
    "    categories='auto',\n",
    "    drop=None,\n",
    "    sparse=True,\n",
    "    dtype=<class 'numpy.float64'>,\n",
    "    handle_unknown='error',\n",
    "    min_frequency=None,\n",
    "    max_categories=None,\n",
    "    n_values='deprecated',\n",
    "    encoding='onehot',\n",
    "    dtype_out=None\n",
    ")\n",
    "```\n",
    "\n",
    "### **Parameters Explanation**:\n",
    "\n",
    "1. **`categories`** (`'auto'`, list of lists, or `None`):\n",
    "   - **Type**: `str` or `list of lists` or `None`\n",
    "   - **Description**: Defines the categories for each feature. This can be:\n",
    "     - **`'auto'`** (default): The categories are inferred from the training data.\n",
    "     - **List of lists**: You can specify the categories for each feature manually as a list of lists.\n",
    "     ```python\n",
    "     categories=[['Low', 'Medium', 'High'], ['Male', 'Female']]\n",
    "     ```\n",
    "     - **`None`**: This will allow the encoder to automatically infer categories from the dataset.\n",
    "\n",
    "2. **`drop`** (`'first'`, `'if_binary'`, or `None`):\n",
    "   - **Type**: `str` or `None`\n",
    "   - **Description**: Controls how to drop categories:\n",
    "     - **`'first'`**: Drop the first category in each feature (useful for avoiding multicollinearity).\n",
    "     - **`'if_binary'`**: Drops one category if the feature is binary.\n",
    "     - **`None`**: No category is dropped (default).\n",
    "     \n",
    "3. **`sparse`** (`True`, `False`):\n",
    "   - **Type**: `bool`\n",
    "   - **Description**: Whether to return a sparse matrix or a dense matrix:\n",
    "     - **`True`** (default): Returns a sparse matrix in CSR format.\n",
    "     - **`False`**: Returns a dense matrix (numpy array).\n",
    "   \n",
    "4. **`dtype`** (`np.float64` or other numeric types):\n",
    "   - **Type**: `dtype`\n",
    "   - **Description**: The data type for the output matrix.\n",
    "     - **Default**: `np.float64`.\n",
    "     - Example: You can set `dtype=np.float32` to get a 32-bit floating point output.\n",
    "   \n",
    "5. **`handle_unknown`** (`'error'`, `'ignore'`):\n",
    "   - **Type**: `str`\n",
    "   - **Description**: Controls what happens when unknown categories are encountered during transformation:\n",
    "     - **`'error'`** (default): Raises an error if unknown categories are encountered during transformation.\n",
    "     - **`'ignore'`**: Ignores unknown categories and returns zero vectors for them during transformation.\n",
    "     \n",
    "6. **`min_frequency`** (`int` or `float`, optional):\n",
    "   - **Type**: `int` or `float`\n",
    "   - **Description**: Specifies the minimum frequency for categories to be considered. Categories that appear less frequently than this value will be ignored.\n",
    "   - **Default**: `None`.\n",
    "\n",
    "7. **`max_categories`** (`int`, optional):\n",
    "   - **Type**: `int`\n",
    "   - **Description**: Specifies the maximum number of categories to encode. Features with more categories than this will be treated differently.\n",
    "   - **Default**: `None`.\n",
    "\n",
    "8. **`n_values`** (`'auto'`, list of integers, or `deprecated`):\n",
    "   - **Type**: `str`, `list of int`\n",
    "   - **Description**: Deprecated. Use `categories` instead.\n",
    "   \n",
    "9. **`encoding`** (`'onehot'`, `'ordinal'`):\n",
    "   - **Type**: `str`\n",
    "   - **Description**: The encoding scheme to use for the categorical data.\n",
    "     - **`'onehot'`** (default): Standard one-hot encoding.\n",
    "     - **`'ordinal'`**: Use ordinal encoding.\n",
    "   \n",
    "10. **`dtype_out`** (`dtype`, optional):\n",
    "    - **Type**: `dtype`\n",
    "    - **Description**: The output data type, similar to `dtype`, but more specific for the transformation.\n",
    "\n",
    "\n",
    "\n",
    "### **Attributes Explanation**:\n",
    "\n",
    "1. **`categories_`** (read-only):\n",
    "   - **Type**: `list of arrays`\n",
    "   - **Description**: This attribute contains the categories for each feature after fitting the encoder. Each array in the list corresponds to the unique categories in the feature.\n",
    "\n",
    "2. **`n_values_`** (deprecated):\n",
    "   - **Type**: `int` or `list of ints`\n",
    "   - **Description**: This is a deprecated attribute and represents the number of categories for each feature. Use `categories_` instead.\n",
    "\n",
    "3. **`feature_indices_`** (read-only):\n",
    "   - **Type**: `list of ints`\n",
    "   - **Description**: Contains the indices of the original features in the transformed matrix. Useful when using sparse formats.\n",
    "\n",
    "4. **`n_features_in_`** (read-only):\n",
    "   - **Type**: `int`\n",
    "   - **Description**: The number of features in the input data, which corresponds to the number of columns in the original data.\n",
    "\n",
    "\n",
    "\n",
    "### **Example**:\n",
    "\n",
    "Here’s an example of using `OneHotEncoder`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', 'Female', 'Male', 'Female'],\n",
    "    'Education': ['UG', 'PG', 'PG', 'UG', 'UG']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize OneHotEncoder with custom parameters\n",
    "encoder = OneHotEncoder(categories='auto', drop='first', sparse=False, dtype=int)\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df[['Gender', 'Education']])\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "print(encoded_df)\n",
    "```\n",
    "\n",
    "### **Output**:\n",
    "\n",
    "```\n",
    "   Gender_Male  Education_UG  Education_PG\n",
    "0            1             1             0\n",
    "1            0             0             1\n",
    "2            0             0             1\n",
    "3            1             1             0\n",
    "4            0             1             0\n",
    "```\n",
    "\n",
    "### **Explanation**:\n",
    "\n",
    "- **`drop='first'`**: This parameter drops the first category from both `Gender` and `Education` columns to avoid the dummy variable trap.\n",
    "  - For `Gender`, it dropped \"Female\" (so we have `Gender_Male`).\n",
    "  - For `Education`, it dropped \"UG\" (so we have `Education_PG`).\n",
    "  \n",
    "- **`sparse=False`**: The output is a dense matrix.\n",
    "  \n",
    "- **`dtype=int`**: The data type of the resulting encoded matrix is `int`.\n",
    "\n",
    "### **When to Use OneHotEncoder**:\n",
    "- When you have **categorical** features with **no inherent order** (nominal data) such as colors, brands, etc.\n",
    "- When working with **machine learning algorithms** that require numeric input.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
