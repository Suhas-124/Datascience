{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Transformers in Machine Learning\n",
    "\n",
    "A **Function Transformer** applies mathematical or custom functions to your data as part of preprocessing, enabling you to handle various types of transformations like log, reciprocal, square root, and square transformations. These transformations are particularly useful for normalizing, stabilizing variance, or reshaping the data distribution to better suit machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "### **1. Log Transformation**\n",
    "The **log transformation** applies the logarithm function to each element in your data. This is useful for:\n",
    "- Handling **skewed data** by compressing large values and stretching small ones.\n",
    "- Converting **multiplicative relationships** into additive ones.\n",
    "- Stabilizing **variance**.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "x_{\\text{transformed}} = \\log(x + c)\n",
    "$$\n",
    "- $c$: A constant (usually 1) added to avoid undefined behavior for $x \\leq 0$.\n",
    "\n",
    "#### Example in Python:\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Log Transformation\n",
    "log_transformer = FunctionTransformer(func=np.log1p)  # log(1 + x)\n",
    "X = np.array([[1, 10], [100, 1000]])\n",
    "X_transformed = log_transformer.transform(X)\n",
    "\n",
    "print(X_transformed)\n",
    "```\n",
    "\n",
    "#### Use Cases:\n",
    "- Handling data with **positive skewness**.\n",
    "- Transforming variables like income, population, or sales data.\n",
    "\n",
    "#### Caveats:\n",
    "- Requires all values to be **positive** (or shifted to positive using a constant).\n",
    "- May lose interpretability of transformed values.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Reciprocal Transformation**\n",
    "The **reciprocal transformation** applies the reciprocal of each value. It is effective in:\n",
    "- Decreasing the impact of large values.\n",
    "- Spreading out small values.\n",
    "- Handling cases where larger values have less influence.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "x_{\\text{transformed}} = \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "#### Example in Python:\n",
    "```python\n",
    "def reciprocal_transform(X):\n",
    "    return 1 / X\n",
    "\n",
    "reciprocal_transformer = FunctionTransformer(func=reciprocal_transform)\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "X_transformed = reciprocal_transformer.transform(X)\n",
    "\n",
    "print(X_transformed)\n",
    "```\n",
    "\n",
    "#### Use Cases:\n",
    "- Addressing **inverse relationships** (e.g., speed vs. time).\n",
    "- Handling **extremely large values**.\n",
    "\n",
    "#### Caveats:\n",
    "- Cannot handle **zero or negative values**.\n",
    "- Requires careful interpretation as values approach infinity.\n",
    "\n",
    "\n",
    "\n",
    "### **3. Square Root and Square Transformations**\n",
    "\n",
    "#### **a. Square Root Transformation**\n",
    "The **square root transformation** applies the square root function to compress large values while retaining the relative scale of the data. Itâ€™s commonly used for:\n",
    "- Stabilizing **variance**.\n",
    "- Handling **moderate skewness**.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "x_{\\text{transformed}} = \\sqrt{x}\n",
    "$$\n",
    "\n",
    "#### Example in Python:\n",
    "```python\n",
    "def sqrt_transform(X):\n",
    "    return np.sqrt(X)\n",
    "\n",
    "sqrt_transformer = FunctionTransformer(func=sqrt_transform)\n",
    "X = np.array([[1, 4], [9, 16]])\n",
    "X_transformed = sqrt_transformer.transform(X)\n",
    "\n",
    "print(X_transformed)\n",
    "```\n",
    "\n",
    "#### Use Cases:\n",
    "- Count data or non-negative distributions (e.g., population, clicks).\n",
    "\n",
    "#### Caveats:\n",
    "- Input values must be **non-negative**.\n",
    "\n",
    "\n",
    "\n",
    "#### **b. Square Transformation**\n",
    "The **square transformation** squares each value, which emphasizes larger values and spreads out the data. It is useful when:\n",
    "- Smaller values need to have **less impact**.\n",
    "- Relationships are **exponential** or quadratic.\n",
    "\n",
    "#### Formula:\n",
    "$$\n",
    "x_{\\text{transformed}} = x^2\n",
    "$$\n",
    "\n",
    "#### Example in Python:\n",
    "```python\n",
    "def square_transform(X):\n",
    "    return X ** 2\n",
    "\n",
    "square_transformer = FunctionTransformer(func=square_transform)\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "X_transformed = square_transformer.transform(X)\n",
    "\n",
    "print(X_transformed)\n",
    "```\n",
    "\n",
    "#### Use Cases:\n",
    "- Amplifying **larger values**.\n",
    "- Creating quadratic features for polynomial regression.\n",
    "\n",
    "#### Caveats:\n",
    "- **Exaggerates outliers**.\n",
    "- Can make values too large, requiring additional scaling.\n",
    "\n",
    "\n",
    "\n",
    "### Summary Table of Transformations\n",
    "\n",
    "| Transformation       | Formula                | Key Use Case                      | Limitations                |\n",
    "|-----------------------|------------------------|------------------------------------|----------------------------|\n",
    "| **Log**              | $\\log(x + c)$       | Handle skewed data; stabilize variance | Requires positive values    |\n",
    "| **Reciprocal**       | $1 / x$             | Reduce large values' impact       | Cannot handle zero/negative |\n",
    "| **Square Root**      | $\\sqrt{x}$          | Stabilize variance for counts     | Non-negative input only     |\n",
    "| **Square**           | $x^2$               | Amplify larger values             | Exaggerates outliers        |\n",
    "\n",
    "By combining these transformations, you can preprocess your data effectively and improve the performance and interpretability of your machine learning models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Transformers (Box-Cox:Yeo Johnson):\n",
    "\n",
    "Power transformers in machine learning are a type of **data preprocessing technique** used to stabilize variance, make data more Gaussian-like, and improve the performance of models sensitive to data distribution, such as linear regression, logistic regression, and neural networks. \n",
    "\n",
    "Power transformers apply **mathematical functions** to transform numerical data, especially for variables with a skewed distribution, into a form that is closer to a **normal distribution** (Gaussian distribution). \n",
    "\n",
    "\n",
    "\n",
    "### **Why Use Power Transformers?**\n",
    "1. **Stabilize Variance**: Reduce the impact of heteroscedasticity (unequal variance across the data).\n",
    "2. **Symmetry Correction**: Transform skewed data (positively or negatively skewed) into a symmetric distribution.\n",
    "3. **Model Improvement**: Models that assume normality (e.g., linear models) can perform better with transformed data.\n",
    "4. **Reduce Outlier Influence**: Compress the range of extreme values to mitigate their effect.\n",
    "\n",
    "\n",
    "\n",
    "### **Types of Power Transformers**\n",
    "1. **Box-Cox Transformation**\n",
    "2. **Yeo-Johnson Transformation**\n",
    "\n",
    "\n",
    "\n",
    "#### **1. Box-Cox Transformation**\n",
    "- **Definition**: A parametric transformation that applies a power-law function to the data, defined only for **positive values**.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  y' =\n",
    "  \\begin{cases} \n",
    "  \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n",
    "  \\ln(y) & \\text{if } \\lambda = 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "  Where $ y $ is the input data and $ \\lambda $ is a transformation parameter that determines the power.\n",
    "\n",
    "- **Key Features**:\n",
    "  - Requires input data to be strictly **positive**.\n",
    "  - Automatically determines the optimal value of $ \\lambda $ to minimize skewness.\n",
    "  - Common values of $ \\lambda $:\n",
    "    - $ \\lambda = 1 $: No transformation (identity transformation).\n",
    "    - $ \\lambda = 0 $: Logarithmic transformation.\n",
    "    - $ \\lambda = -1 $: Reciprocal transformation.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Transforming variables like income, population, or area size that are positive and skewed.\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Yeo-Johnson Transformation**\n",
    "- **Definition**: A generalization of the Box-Cox transformation that works for both **positive and negative values**.\n",
    "- **Formula**:\n",
    "  $$\n",
    "  y' =\n",
    "  \\begin{cases} \n",
    "  \\frac{((y + 1)^\\lambda - 1)}{\\lambda} & \\text{if } \\lambda \\neq 0 \\text{ and } y \\geq 0 \\\\\n",
    "  \\ln(y + 1) & \\text{if } \\lambda = 0 \\text{ and } y \\geq 0 \\\\\n",
    "  \\frac{(-(1 - y)^{2 - \\lambda} - 1)}{2 - \\lambda} & \\text{if } \\lambda \\neq 2 \\text{ and } y < 0 \\\\\n",
    "  -\\ln(1 - y) & \\text{if } \\lambda = 2 \\text{ and } y < 0\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "- **Key Features**:\n",
    "  - Handles **negative, zero, and positive values**.\n",
    "  - Automatically optimizes $ \\lambda $ to reduce skewness.\n",
    "  - Suitable for data that may contain zeros or negative values.\n",
    "\n",
    "- **Use Case**:\n",
    "  - Transforming variables like temperature changes (positive and negative), profit and loss data, or any dataset with mixed signs.\n",
    "\n",
    "\n",
    "### **Choosing Between Box-Cox and Yeo-Johnson**\n",
    "| Feature                  | Box-Cox                  | Yeo-Johnson              |\n",
    "|--------------------------|--------------------------|--------------------------|\n",
    "| Data type               | Positive only            | Positive, zero, negative |\n",
    "| Flexibility             | Less flexible            | More flexible            |\n",
    "| Common usage scenarios  | Positive-only features   | Mixed-sign features      |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### **Implementation in Python (Scikit-learn)**\n",
    "\n",
    "Both transformations can be implemented using the **`PowerTransformer`** class in scikit-learn.\n",
    "\n",
    "#### Example:\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1], [2], [3], [4], [5]])\n",
    "\n",
    "# Box-Cox Transformation\n",
    "pt_boxcox = PowerTransformer(method='box-cox', standardize=True)\n",
    "data_boxcox = pt_boxcox.fit_transform(data)\n",
    "\n",
    "# Yeo-Johnson Transformation (works with negative values too)\n",
    "data_neg = np.array([[-5], [-2], [0], [2], [5]])\n",
    "pt_yeojohnson = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "data_yeojohnson = pt_yeojohnson.fit_transform(data_neg)\n",
    "\n",
    "print(\"Box-Cox Transformed Data:\\n\", data_boxcox)\n",
    "print(\"Yeo-Johnson Transformed Data:\\n\", data_yeojohnson)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Power Transformers**\n",
    "1. Reduces skewness, making data more Gaussian-like.\n",
    "2. Improves the performance of models requiring normality.\n",
    "3. Stabilizes variance for better statistical analysis.\n",
    "\n",
    "\n",
    "\n",
    "### **Limitations**\n",
    "1. Requires numerical data.\n",
    "2. Box-Cox cannot handle zero or negative values.\n",
    "3. Choosing the wrong transformation method can introduce bias.\n",
    "\n",
    "\n",
    "\n",
    "### **When to Use Power Transformers**\n",
    "- Use when your data is skewed and variance stabilization is required.\n",
    "- Use Yeo-Johnson if data contains negative values.\n",
    "- Use Box-Cox for strictly positive data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization (Binning):\n",
    "\n",
    "Binning, also known as **discretization**, is a preprocessing technique in machine learning used to group continuous numerical data into discrete intervals, or \"bins.\" This can help make the data easier to interpret and use, especially for models or analyses that benefit from categorical data. Here's a detailed explanation:\n",
    "\n",
    "\n",
    "\n",
    "### **Why Binning is Useful**\n",
    "1. **Reducing Noise**: By grouping values into bins, small variations or noise in the data can be smoothed out.\n",
    "2. **Handling Non-linear Relationships**: Binning can help reveal patterns or relationships that might be non-linear or harder to detect in continuous data.\n",
    "3. **Improving Model Performance**: Some algorithms, like decision trees, work better with categorical data derived from binning.\n",
    "4. **Feature Engineering**: Binned data can be used to create new features that add value to the model.\n",
    "\n",
    "\n",
    "\n",
    "### **How Binning Works**\n",
    "The process involves:\n",
    "1. **Defining Bin Boundaries**: Dividing the range of continuous values into non-overlapping intervals.\n",
    "2. **Assigning Values to Bins**: Each data point is assigned to a bin based on which interval its value falls into.\n",
    "\n",
    "\n",
    "\n",
    "### **Types of Binning**\n",
    "\n",
    "#### 1. **Fixed-width Binning**\n",
    "   - **Equal-width Binning**: The range of the data is divided into intervals of equal size.\n",
    "     - Example: If data ranges from 0 to 100, dividing it into 5 bins would create bins like `[0-20, 20-40, 40-60, 60-80, 80-100]`.\n",
    "   - **Equal-frequency Binning**: Each bin contains the same number of data points (but the width of the bins may vary).\n",
    "     - Example: If there are 100 data points and 5 bins, each bin will contain 20 points.\n",
    "\n",
    "#### 2. **Custom Binning**\n",
    "   - Bins are defined manually based on domain knowledge.\n",
    "     - Example: Age ranges like `[0-18, 19-35, 36-50, 51+]`.\n",
    "\n",
    "#### 3. **Dynamic Binning**\n",
    "   - Binning is determined algorithmically, often using methods like **k-means clustering** or decision tree splits, to create bins that optimize some objective (e.g., reducing information loss).\n",
    "\n",
    "\n",
    "\n",
    "### **Binning Techniques in Practice**\n",
    "1. **Label Encoding**: Assign each bin a label or number (e.g., `[0, 1, 2...]`).\n",
    "2. **One-hot Encoding**: Represent each bin as a separate binary feature (e.g., `[1, 0, 0]` for the first bin).\n",
    "\n",
    "\n",
    "\n",
    "### **Advantages of Binning**\n",
    "- Makes data simpler and easier to interpret.\n",
    "- Handles outliers by grouping extreme values into a single bin.\n",
    "- Can reduce the impact of noise on models.\n",
    "\n",
    "\n",
    "\n",
    "### **Disadvantages of Binning**\n",
    "- May lead to loss of information, especially if bins are too wide.\n",
    "- Choosing inappropriate bin boundaries can introduce bias.\n",
    "- Fixed-width binning can result in uneven distribution, especially with skewed data.\n",
    "\n",
    "\n",
    "\n",
    "### **Use Case Example**\n",
    "Suppose you have a dataset of customer ages, and you want to predict purchasing behavior. Instead of using the raw `age` values, you can bin the ages into categories:\n",
    "- `[0-18]: Teen`\n",
    "- `[19-35]: Young Adult`\n",
    "- `[36-50]: Middle-aged`\n",
    "- `[51+]: Senior`\n",
    "\n",
    "By binning, you can observe purchasing patterns across these age groups, which may be more insightful than analyzing raw ages.\n",
    "\n",
    "\n",
    "\n",
    "Binning is especially effective when combined with data visualization (e.g., histograms) to understand the distribution and refine the bins accordingly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
