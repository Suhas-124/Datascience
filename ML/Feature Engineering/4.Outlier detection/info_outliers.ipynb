{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier:\n",
    "\n",
    "An **outlier** is an observation in a dataset that significantly differs from other observations. It can be much higher or much lower than the majority of the data points and can distort statistical analyses, potentially affecting the accuracy of models. Outliers may result from variability in the data, errors, or genuine anomalies that need special attention.\n",
    "\n",
    "### Techniques to Detect Outliers:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - **Z-Score (Standard Score):**\n",
    "     - Measures how many standard deviations a data point is from the mean. A Z-score greater than 3 or less than -3 typically indicates an outlier.\n",
    "   - **IQR (Interquartile Range):**\n",
    "     - The IQR measures the range between the 25th and 75th percentiles (Q1 and Q3). An outlier is often defined as any value outside the range \\( Q1 - 1.5 \\times IQR \\) to \\( Q3 + 1.5 \\times IQR \\).\n",
    "   \n",
    "2. **Visualization Methods:**\n",
    "   - **Box Plot:**\n",
    "     - A box plot can visually highlight outliers as points outside the \"whiskers\" (IQR bounds).\n",
    "   - **Scatter Plot:**\n",
    "     - In two-dimensional data, scatter plots can visually indicate points that are distant from the majority of the data.\n",
    "   - **Histogram:**\n",
    "     - A histogram might show a long tail or sparse regions that can be indicative of outliers.\n",
    "\n",
    "3. **Machine Learning Methods:**\n",
    "   - **Isolation Forest:**\n",
    "     - A model that isolates outliers by randomly selecting features and splitting the data based on them. It is well-suited for high-dimensional data.\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n",
    "     - A clustering algorithm that groups closely packed data points and considers points in low-density regions as outliers.\n",
    "   - **Local Outlier Factor (LOF):**\n",
    "     - Measures the local density deviation of data points. Points that have a substantially lower density than their neighbors are considered outliers.\n",
    "\n",
    "4. **Model-Based Methods:**\n",
    "   - **One-Class SVM (Support Vector Machine):**\n",
    "     - A variant of SVM that is used for anomaly detection. It works by learning a decision function for outlier detection in high-dimensional spaces.\n",
    "   - **Autoencoders (for Deep Learning):**\n",
    "     - Anomaly detection can also be done using neural networks, where an autoencoder is trained to learn a compressed representation of the data. Data points with high reconstruction errors are treated as outliers.\n",
    "\n",
    "These techniques vary in complexity and applicability, and the choice of method depends on the type of data and the context of the analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-Score Method:\n",
    "\n",
    "The **Z-score** method is a statistical technique used to detect outliers in a dataset. The Z-score represents how many standard deviations a data point is away from the mean of the dataset. It is used to identify extreme values (outliers) that are significantly different from other observations in the data.\n",
    "\n",
    "### Formula for Z-Score:\n",
    "The Z-score for a data point $ x $ is calculated using the following formula:\n",
    "\n",
    "$$\n",
    "Z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ x $ = Data point you want to evaluate.\n",
    "- $ \\mu $ = Mean of the dataset.\n",
    "- $ \\sigma $ = Standard deviation of the dataset.\n",
    "\n",
    "### Steps to Detect Outliers Using Z-Score:\n",
    "\n",
    "1. **Calculate the Mean and Standard Deviation:**\n",
    "   - First, calculate the **mean** ($ \\mu $) and **standard deviation** ($ \\sigma $) of your dataset. The mean represents the average value of the data, and the standard deviation measures how spread out the data is.\n",
    "\n",
    "   - **Mean** is calculated as:\n",
    "     $$\n",
    "     \\mu = \\frac{\\sum_{i=1}^{n} x_i}{n}\n",
    "     $$\n",
    "     Where $ x_i $ are the individual data points, and $ n $ is the number of data points.\n",
    "\n",
    "   - **Standard Deviation** is calculated as:\n",
    "     $$\n",
    "     \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{n}}\n",
    "     $$\n",
    "     It represents the average deviation of each data point from the mean.\n",
    "\n",
    "2. **Compute Z-Scores for Each Data Point:**\n",
    "   - For each data point $ x $ in the dataset, calculate the Z-score using the formula provided. This will give you the number of standard deviations each data point is away from the mean.\n",
    "\n",
    "3. **Determine a Threshold for Outliers:**\n",
    "   - Generally, a Z-score greater than 3 or less than -3 is considered an outlier, meaning the data point is more than 3 standard deviations away from the mean. However, this threshold can be adjusted depending on the context or distribution of the data.\n",
    "   - For example, you might consider a threshold of 2 or 2.5 for more sensitive outlier detection.\n",
    "\n",
    "4. **Identify Outliers:**\n",
    "   - Once you have calculated the Z-scores, any data point with a Z-score above the chosen threshold (e.g., $ |Z| > 3 $) is considered an **outlier**. Similarly, values with Z-scores below -3 (or the negative of your threshold) are also outliers.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Consider a small dataset of test scores:  \n",
    "$$ 55, 58, 61, 62, 65, 98, 120, 130 $$\n",
    "\n",
    "1. **Calculate the Mean:**\n",
    "   $$\n",
    "   \\mu = \\frac{55 + 58 + 61 + 62 + 65 + 98 + 120 + 130}{8} = 76.625\n",
    "   $$\n",
    "\n",
    "2. **Calculate the Standard Deviation:**\n",
    "   $$\n",
    "   \\sigma = \\sqrt{\\frac{(55 - 76.625)^2 + (58 - 76.625)^2 + \\cdots + (130 - 76.625)^2}{8}} \\approx 25.99\n",
    "   $$\n",
    "\n",
    "3. **Compute Z-Scores for Each Data Point:**\n",
    "\n",
    "   For 55:\n",
    "   $$\n",
    "   Z = \\frac{55 - 76.625}{25.99} = \\frac{-21.625}{25.99} \\approx -0.83\n",
    "   $$\n",
    "   For 130:\n",
    "   $$\n",
    "   Z = \\frac{130 - 76.625}{25.99} = \\frac{53.375}{25.99} \\approx 2.05\n",
    "   $$\n",
    "\n",
    "   Similar calculations would be done for the other data points.\n",
    "\n",
    "4. **Determine Outliers:**\n",
    "   If the threshold is set at 3 (for $ |Z| > 3 $), none of the data points would be considered outliers in this case, as all Z-scores are within this range.\n",
    "\n",
    "   However, if the threshold were set at 2.5 or 2, you might find that values such as 98, 120, and 130 might be flagged as outliers because their Z-scores are greater than the threshold.\n",
    "\n",
    "### Advantages and Disadvantages of Z-Score Method:\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to compute and easy to understand.\n",
    "- Effective for detecting outliers in normally distributed data.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Assumes data follows a normal distribution. The method may not work well for skewed or non-normal data.\n",
    "- Sensitive to the presence of outliers themselves, as extreme values can influence the mean and standard deviation.\n",
    "\n",
    "This method is best suited for small to moderate datasets, and when the data is approximately normally distributed. For non-normal data, other techniques (like IQR, DBSCAN, or machine learning models) might be more effective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "``` python\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example random data points (e.g., test scores)\n",
    "data = np.array([55, 58, 61, 62, 65, 98, 120, 130, 150, 200]).reshape(-1, 1)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data (Z-score normalization)\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "\n",
    "# Identify outliers (e.g., Z-scores > 3 or Z-scores < -3)\n",
    "outliers = data[np.abs(data_scaled) > 3]\n",
    "\n",
    "# Print the results\n",
    "print(\"Original Data Points:\", data.flatten())\n",
    "print(\"Scaled Data (Z-scores):\", data_scaled.flatten())\n",
    "print(\"Outliers:\", outliers.flatten())\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IQR (Interquartile Range):\n",
    "\n",
    "The **Interquartile Range (IQR)** method is another statistical technique used to detect outliers in a dataset. It is based on the idea that outliers are values that are significantly higher or lower than most of the data points. IQR is a measure of statistical dispersion, and it represents the range between the first quartile (Q1) and the third quartile (Q3) of the dataset.\n",
    "\n",
    "### Steps for IQR-based Outlier Detection:\n",
    "\n",
    "1. **Calculate Quartiles**:\n",
    "   - **First Quartile (Q1)**: This is the 25th percentile, which means 25% of the data points fall below this value.\n",
    "   - **Third Quartile (Q3)**: This is the 75th percentile, meaning 75% of the data points fall below this value.\n",
    "\n",
    "2. **Calculate the IQR**:\n",
    "   - The **IQR** is the difference between the third quartile (Q3) and the first quartile (Q1):\n",
    "     $$\n",
    "     \\text{IQR} = Q3 - Q1\n",
    "     $$\n",
    "\n",
    "3. **Define Outlier Boundaries**:\n",
    "   - Data points that lie outside the boundaries defined by the following formulas are considered outliers:\n",
    "     $$\n",
    "     \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "     $$\n",
    "     $$\n",
    "     \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "     $$\n",
    "   - Any data point below the lower bound or above the upper bound is considered an outlier.\n",
    "\n",
    "### Steps to Apply IQR Method in Python:\n",
    "\n",
    "Here’s how to apply the IQR method to detect outliers in a dataset using Python:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example data (e.g., test scores)\n",
    "data = [55, 58, 61, 62, 65, 98, 120, 130, 150, 200]\n",
    "\n",
    "# Convert the data to a NumPy array for easier calculations\n",
    "data_array = np.array(data)\n",
    "\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = np.percentile(data_array, 25)\n",
    "Q3 = np.percentile(data_array, 75)\n",
    "\n",
    "# Calculate the IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = data_array[(data_array < lower_bound) | (data_array > upper_bound)]\n",
    "\n",
    "# Print the results\n",
    "print(\"Data points:\", data)\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n",
    "print(\"Outliers:\", outliers)\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "1. **Data**: The dataset of test scores is provided.\n",
    "2. **Q1 and Q3**: The first and third quartiles are calculated using `np.percentile()`. This function computes the 25th and 75th percentiles.\n",
    "3. **IQR Calculation**: The interquartile range is computed by subtracting Q1 from Q3.\n",
    "4. **Outlier Boundaries**: The lower and upper bounds are calculated using the IQR formula. Data points outside these bounds are considered outliers.\n",
    "5. **Outlier Detection**: Data points that are less than the lower bound or greater than the upper bound are flagged as outliers.\n",
    "6. **Results**: The code prints the original data, quartiles, IQR, lower and upper bounds, and any detected outliers.\n",
    "\n",
    "### Example Output:\n",
    "\n",
    "For the example dataset:\n",
    "\n",
    "```\n",
    "Data points: [55, 58, 61, 62, 65, 98, 120, 130, 150, 200]\n",
    "Q1: 61.5\n",
    "Q3: 130.0\n",
    "IQR: 68.5\n",
    "Lower Bound: -56.75\n",
    "Upper Bound: 248.25\n",
    "Outliers: [150 200]\n",
    "```\n",
    "\n",
    "Here:\n",
    "- **Q1** = 61.5, **Q3** = 130.0, and **IQR** = 68.5.\n",
    "- **Lower Bound** is -56.75 and **Upper Bound** is 248.25.\n",
    "- The values **150** and **200** are considered outliers because they exceed the upper bound of 248.25.\n",
    "\n",
    "### Advantages of IQR Method:\n",
    "- **Simple** and easy to understand.\n",
    "- **Robust** to outliers themselves, as it focuses on the central 50% of the data.\n",
    "- Works well for **non-normal** distributions as it doesn’t rely on the assumption of normality like the Z-score method.\n",
    "\n",
    "### Disadvantages:\n",
    "- **Fixed Threshold**: The multiplier of 1.5 is arbitrary and might not work well for all datasets.\n",
    "- **Doesn't work well for skewed data**: The IQR method may not work well if the data has a heavy skew or is multimodal.\n",
    "  \n",
    "The IQR method is widely used in exploratory data analysis to quickly identify extreme values in datasets, especially when data isn't normally distributed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Percentile (Winsorization) Method:\n",
    "\n",
    "**Percentile-based (or Winsorization) method** is another technique for handling outliers, which modifies the extreme values (outliers) in a dataset by replacing them with a value within a defined range. This helps to reduce the impact of extreme values on the analysis and model-building process.\n",
    "\n",
    "### What is Percentile or Winsorization?\n",
    "\n",
    "- **Percentile-based** approach involves setting a threshold for the data values at certain percentiles. For example, the 1st percentile and the 99th percentile could be used to define the lower and upper bounds of your data.\n",
    "- **Winsorization** refers to the process of **capping** or **truncating** outliers at a specified percentile value instead of removing them. For instance, any data point greater than the 99th percentile is replaced by the value at the 99th percentile, and any data point less than the 1st percentile is replaced by the value at the 1st percentile.\n",
    "\n",
    "The goal of Winsorization is to make the dataset less sensitive to extreme values while keeping all observations, which can improve model performance, especially in cases where the data has skewed distributions or outliers that can't be easily removed.\n",
    "\n",
    "### Steps in Winsorization:\n",
    "\n",
    "1. **Choose Percentiles**: You need to select the percentiles that will be used to cap the data. Typically, the 1st and 99th percentiles, or the 5th and 95th percentiles, are chosen to reduce the influence of outliers.\n",
    "   \n",
    "2. **Identify Outliers**: Data points above the upper percentile or below the lower percentile are considered outliers.\n",
    "\n",
    "3. **Replace Outliers**: Replace outlier values with the corresponding percentile values (i.e., values below the lower percentile are replaced with the value at the lower percentile, and values above the upper percentile are replaced with the value at the upper percentile).\n",
    "\n",
    "### Formula for Winsorization:\n",
    "- **Lower Bound** = Value at the $ p\\% $ percentile (for example, 1st percentile).\n",
    "- **Upper Bound** = Value at the $ (100 - p)\\% $ percentile (for example, 99th percentile).\n",
    "\n",
    "### Example of Winsorization:\n",
    "\n",
    "Let's say you have a dataset of test scores and you want to apply Winsorization using the 5th and 95th percentiles.\n",
    "\n",
    "- **Step 1**: Calculate the 5th and 95th percentiles.\n",
    "- **Step 2**: Replace values below the 5th percentile with the 5th percentile value and values above the 95th percentile with the 95th percentile value.\n",
    "\n",
    "### Example Code in Python:\n",
    "\n",
    "Here’s how you can perform Winsorization in Python using `numpy` and `scipy`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# Example data (e.g., test scores)\n",
    "data = [55, 58, 61, 62, 65, 98, 120, 130, 150, 200]\n",
    "\n",
    "# Apply Winsorization (capping the data at 5th and 95th percentiles)\n",
    "winsorized_data = mstats.winsorize(data, limits=[0.05, 0.05])\n",
    "\n",
    "# Print original and winsorized data\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Winsorized Data:\", winsorized_data)\n",
    "```\n",
    "\n",
    "### Explanation of Code:\n",
    "1. **Data**: A dataset of test scores is given.\n",
    "2. **Winsorization**: The `mstats.winsorize()` function from the `scipy.stats` module is used to apply Winsorization to the data. The `limits=[0.05, 0.05]` argument means that the 5% of data at the lower and upper ends will be replaced by the values at the 5th and 95th percentiles, respectively.\n",
    "3. **Output**: The original data and the Winsorized data are printed.\n",
    "\n",
    "### Output Example:\n",
    "\n",
    "For the dataset:\n",
    "\n",
    "```\n",
    "Original Data: [55, 58, 61, 62, 65, 98, 120, 130, 150, 200]\n",
    "Winsorized Data: [ 55.  58.  61.  62.  65.  98. 120. 130. 130. 130.]\n",
    "```\n",
    "\n",
    "Here, the value 150 and 200 have been capped at 130, which is the value at the 95th percentile.\n",
    "\n",
    "### When to Use Winsorization:\n",
    "\n",
    "1. **Skewed Data**: When data is highly skewed, Winsorization helps in limiting the effect of extreme values on the overall analysis.\n",
    "2. **Preserving All Data**: Unlike other methods (such as removing outliers), Winsorization retains all data points by capping the extreme values. This is useful when you want to keep the full dataset but minimize the effect of outliers.\n",
    "3. **Regression Models**: In regression models, extreme outliers can disproportionately affect the results. Winsorization helps make the regression coefficients more stable by reducing the influence of extreme values.\n",
    "\n",
    "### Advantages of Winsorization:\n",
    "- **Prevents Loss of Data**: Instead of removing outliers, it modifies them to a more reasonable value, retaining all the data points for analysis.\n",
    "- **Reduces Influence of Outliers**: Winsorization reduces the impact of outliers while still allowing them to be part of the dataset, unlike methods like trimming or removing outliers.\n",
    "- **Works Well with Skewed Data**: It is particularly useful for skewed datasets, where the presence of outliers can unduly influence the model.\n",
    "\n",
    "### Disadvantages:\n",
    "- **Arbitrary Cutoffs**: Choosing the correct percentiles (like 5% and 95%) can be subjective, and different cutoffs may lead to different results.\n",
    "- **Potential Information Loss**: Although outliers are modified rather than removed, you may still lose some information that might be important if those outliers were valid data points.\n",
    "- **Does Not Work in All Cases**: If the data is highly non-normal or contains systematic outliers, Winsorization may not be effective.\n",
    "\n",
    "### Summary:\n",
    "Winsorization is a powerful technique for handling outliers by limiting the range of extreme values in the data. It is especially useful in preserving the dataset while reducing the influence of extreme values, making it a preferred method in certain applications like regression modeling or exploratory data analysis where preserving all data points is important.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
