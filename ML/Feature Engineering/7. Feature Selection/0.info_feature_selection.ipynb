{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection parameters\n",
    "\n",
    "## **ttest-ind** : For only two variables 0,1 (mainly for numerical columns)\n",
    "## **f_oneway** : For the Multiclass calssification (mainly for the numerical columns)\n",
    "## **chi2_contengency** : For only two variable and Multiclass classification (for categorical colums)\n",
    "\n",
    "## **Spearmanr** : If Output is numerical columns (apply for numerical columns)\n",
    "## **Kruskal** : If output is numerical columns (apply for categorical columns)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ **Simplified Feature Selection Guide with Examples**\n",
    "\n",
    "| **Test** | **Use Case** | **Feature Example** | **Target Example** | **When to Use** | **Example Scenario** |\n",
    "|----------|--------------|---------------------|---------------------|------------------|------------------------|\n",
    "| **ttest_ind** | Binary Classification | `age`, `salary` (numerical) | `churn` = 0 or 1 (binary) | When target has 2 categories | Compare avg. age of churned vs non-churned users |\n",
    "| **f_oneway** | Multiclass Classification | `income`, `score` (numerical) | `education_level` = ['high school', 'bachelor', 'master'] | When target has >2 categories | Check if income differs across education levels |\n",
    "| **chi2** / `chi2_contingency` | Binary/Multiclass Classification | `gender`, `region` (categorical) | `purchased` = 0/1 or multiple classes | Both feature and target are categorical | Test if purchase behavior differs by gender |\n",
    "| **spearmanr** | Regression | `age`, `hours_spent` (numerical) | `monthly_spend` (numerical) | When target is continuous | Correlation between age and spending |\n",
    "| **kruskal** | Regression | `product_type`, `location` (categorical) | `revenue` (numerical) | When feature is categorical & target is numerical | Does revenue differ by product type? |\n",
    "\n",
    "\n",
    "\n",
    "### üîë Summary Cheatsheet\n",
    "\n",
    "| **If target is...** | **And feature is...** | **Use...** |\n",
    "|---------------------|-----------------------|-------------|\n",
    "| Binary categorical | Numerical | `ttest_ind` |\n",
    "| Multiclass categorical | Numerical | `f_oneway` |\n",
    "| Categorical (any) | Categorical | `chi2` |\n",
    "| Numerical | Numerical | `spearmanr` |\n",
    "| Numerical | Categorical | `kruskal` |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. What is Feature Selection?**\n",
    "- **Goal**: Identify and retain features that contribute most to the predictive power of your model while discarding irrelevant or redundant ones.\n",
    "- **Why it matters**:\n",
    "  - Reduces model complexity.\n",
    "  - Speeds up training.\n",
    "  - Improves generalization (avoids overfitting).\n",
    "  - Enhances interpretability.\n",
    "\n",
    "\n",
    "\n",
    "### **2. Types of Feature Selection Methods**\n",
    "Feature selection techniques can be grouped into **three categories**:\n",
    "\n",
    "#### **A. Filter Methods**\n",
    "- **How it works**: Select features based on statistical measures (e.g., correlation, variance) **before** training the model.\n",
    "- **Pros**: Fast, computationally efficient.\n",
    "- **Cons**: Ignores feature interactions.\n",
    "- **Common Techniques**:\n",
    "  1. **Univariate Selection**: Rank features using statistical tests (e.g., ANOVA, chi-squared).\n",
    "     ```python\n",
    "     from sklearn.feature_selection import SelectKBest, chi2\n",
    "     X_new = SelectKBest(chi2, k=5).fit_transform(X, y)\n",
    "     ```\n",
    "  2. **Correlation Analysis**: Remove features highly correlated with others (redundancy).\n",
    "     ```python\n",
    "     corr_matrix = X.corr().abs()\n",
    "     upper_tri = corr_matrix.where(np.triu(np.ones_like(corr_matrix), k=1).astype(bool))\n",
    "     to_drop = [col for col in upper_tri.columns if any(upper_tri[col] > 0.8)]\n",
    "     X = X.drop(to_drop, axis=1)\n",
    "     ```\n",
    "  3. **Variance Threshold**: Remove low-variance features (constant or near-constant values).\n",
    "     ```python\n",
    "     from sklearn.feature_selection import VarianceThreshold\n",
    "     selector = VarianceThreshold(threshold=0.1)\n",
    "     X_new = selector.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "#### **B. Wrapper Methods**\n",
    "- **How it works**: Use a machine learning model to evaluate feature subsets (e.g., forward/backward selection).\n",
    "- **Pros**: Considers feature interactions.\n",
    "- **Cons**: Computationally expensive.\n",
    "- **Common Techniques**:\n",
    "  1. **Recursive Feature Elimination (RFE)**: Iteratively remove the least important features.\n",
    "     ```python\n",
    "     from sklearn.feature_selection import RFE\n",
    "     from sklearn.linear_model import LogisticRegression\n",
    "     model = LogisticRegression()\n",
    "     rfe = RFE(model, n_features_to_select=5)\n",
    "     X_new = rfe.fit_transform(X, y)\n",
    "     ```\n",
    "  2. **Forward Selection**: Start with 0 features, add one at a time based on model performance.\n",
    "  3. **Backward Elimination**: Start with all features, remove one at a time.\n",
    "\n",
    "#### **C. Embedded Methods**\n",
    "- **How it works**: Feature selection is built into the model training process.\n",
    "- **Pros**: Balances speed and accuracy.\n",
    "- **Common Techniques**:\n",
    "  1. **Lasso (L1 Regularization)**: Penalizes non-important features by shrinking their coefficients to zero.\n",
    "     ```python\n",
    "     from sklearn.linear_model import Lasso\n",
    "     lasso = Lasso(alpha=0.01)\n",
    "     lasso.fit(X, y)\n",
    "     selected_features = X.columns[lasso.coef_ != 0]\n",
    "     ```\n",
    "  2. **Tree-Based Models**: Use feature importance scores from algorithms like Random Forest or XGBoost.\n",
    "     ```python\n",
    "     from sklearn.ensemble import RandomForestClassifier\n",
    "     model = RandomForestClassifier()\n",
    "     model.fit(X, y)\n",
    "     importance = model.feature_importances_\n",
    "     ```\n",
    "\n",
    "#### **D. Hybrid Methods**\n",
    "- Combine filter and wrapper methods (e.g., use correlation to shortlist features, then apply RFE).\n",
    "\n",
    "\n",
    "\n",
    "### **3. Key Considerations for Feature Selection**\n",
    "1. **Domain Knowledge**: Use expert insights to retain meaningful features (e.g., \"proline\" in the Wine dataset may be critical).\n",
    "2. **Feature Interactions**: Some features may only be useful when combined (e.g., BMI = weight/height¬≤).\n",
    "3. **Class Imbalance**: Ensure selection methods account for imbalance (e.g., use stratified sampling).\n",
    "4. **Avoid Data Leakage**: Perform feature selection **after** splitting data into train/test sets.\n",
    "\n",
    "\n",
    "\n",
    "### **4. Step-by-Step Workflow**\n",
    "1. **Preprocess Data**: Handle missing values, encode categorical variables.\n",
    "2. **Filter Methods**: Use correlation, variance, or univariate tests to remove obvious noise.\n",
    "3. **Wrapper/Embedded Methods**: Refine selection using model-based techniques.\n",
    "4. **Validate**: Compare model performance (e.g., accuracy, F1-score) before and after selection.\n",
    "\n",
    "\n",
    "\n",
    "### **5. Example with the Wine Dataset**\n",
    "**Goal**: Select features to classify wines into 3 classes.\n",
    "- **Filter Method Example**:\n",
    "  ```python\n",
    "  from sklearn.datasets import load_wine\n",
    "  from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "  data = load_wine()\n",
    "  X, y = data.data, data.target\n",
    "\n",
    "  # Select top 5 features using ANOVA F-test\n",
    "  selector = SelectKBest(score_func=f_classif, k=5)\n",
    "  X_new = selector.fit_transform(X, y)\n",
    "  print(\"Selected features:\", data.feature_names[selector.get_support()])\n",
    "  ```\n",
    "- **Embedded Method Example** (Random Forest):\n",
    "  ```python\n",
    "  from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "  model = RandomForestClassifier()\n",
    "  model.fit(X, y)\n",
    "  importance = model.feature_importances_\n",
    "  top_features = [data.feature_names[i] for i in importance.argsort()[-5:]]\n",
    "  print(\"Top 5 features:\", top_features)\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### **6. Common Mistakes to Avoid**\n",
    "- Using the entire dataset (including test data) for feature selection.\n",
    "- Ignoring feature scaling (e.g., for methods like SVM or k-NN).\n",
    "- Over-relying on a single technique (combine methods for robustness).\n",
    "\n",
    "\n",
    "\n",
    "### **7. Advanced Techniques**\n",
    "- **PCA (Dimensionality Reduction)**: Not strictly feature selection, but reduces dimensions.\n",
    "- **Mutual Information**: Measures dependency between features and target.\n",
    "  ```python\n",
    "  from sklearn.feature_selection import mutual_info_classif\n",
    "  mi = mutual_info_classif(X, y)\n",
    "  ```\n",
    "- **SHAP Values**: Explain model predictions and feature importance.\n",
    "  ```python\n",
    "  import shap\n",
    "  explainer = shap.TreeExplainer(model)\n",
    "  shap_values = explainer.shap_values(X)\n",
    "  ```\n",
    "\n",
    "\n",
    "\n",
    "### **8. When to Use Which Method?**\n",
    "- **Small datasets**: Filter methods (fast).\n",
    "- **Large datasets**: Embedded/wrapper methods (accuracy).\n",
    "- **Interpretability**: Filter/embedded methods (e.g., Lasso, Random Forest).\n",
    "\n",
    "\n",
    "\n",
    "### **9. Tools & Libraries**\n",
    "- **Scikit-learn**: `SelectKBest`, `RFE`, `VarianceThreshold`.\n",
    "- **Statsmodels**: For detailed statistical tests.\n",
    "- **MLxtend**: Sequential feature selection.\n",
    "\n",
    "\n",
    "\n",
    "### **10. Practice Exercise**\n",
    "**Task**: Use the **Wine Dataset** and compare model performance (e.g., accuracy) using:\n",
    "1. All 13 features.\n",
    "2. Top 5 features selected via `SelectKBest`.\n",
    "3. Top 5 features selected via Random Forest importance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection (t-test) (T-test for independent samples) (For Numerical Values)**\n",
    "\n",
    "In feature selection, `ttest_ind` is used to identify features that are significantly different between two groups. It comes from **scipy.stats** and performs an **independent two-sample t-test**, which checks whether the means of two independent groups are statistically different.\n",
    "\n",
    "### How `ttest_ind` Helps in Feature Selection:\n",
    "- It helps determine whether a feature (column) is useful for distinguishing between two classes in classification problems.\n",
    "- A small p-value (typically < 0.05) indicates a significant difference in the feature values between the two groups, meaning the feature is important for classification.\n",
    "- A large p-value suggests the feature might not be useful for distinguishing between classes.\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Sample data: Two groups (Class 0 and Class 1) for a single feature\n",
    "class_0 = np.array([2.5, 3.0, 3.2, 4.1, 3.8])\n",
    "class_1 = np.array([5.2, 5.8, 6.1, 5.9, 6.5])\n",
    "\n",
    "# Perform t-test\n",
    "stat, p_value = ttest_ind(class_0, class_1)\n",
    "\n",
    "print(f\"t-statistic: {stat}, p-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"Feature is significant for classification.\")\n",
    "else:\n",
    "    print(\"Feature is NOT significant for classification.\")\n",
    "```\n",
    "\n",
    "### When to Use `ttest_ind` in Feature Selection:\n",
    "- When dealing with **continuous numerical features**.\n",
    "- When you have **two classes** in classification problems.\n",
    "- If the feature distribution is approximately **normal** (since t-test assumes normality).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's break down the **manual calculation** of the independent two-sample t-test (`ttest_ind`) step by step.\n",
    "\n",
    "\n",
    "\n",
    "## **Formula for t-test (Independent)**\n",
    "The independent t-test checks whether two groups have significantly different means. The formula is:\n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\bar{X}_1, \\bar{X}_2$ = Means of the two groups\n",
    "- $s_1^2, s_2^2$ = Variances of the two groups\n",
    "- $n_1, n_2$ = Sample sizes of the two groups\n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step Calculation**\n",
    "### **Example Data**\n",
    "Let's say we have two groups (Class 0 and Class 1) with the following values:\n",
    "\n",
    "| Class 0 | Class 1 |\n",
    "|---------|---------|\n",
    "| 2.5     | 5.2     |\n",
    "| 3.0     | 5.8     |\n",
    "| 3.2     | 6.1     |\n",
    "| 4.1     | 5.9     |\n",
    "| 3.8     | 6.5     |\n",
    "\n",
    "### **Step 1: Calculate the Means ($\\bar{X}_1, \\bar{X}_2$)**\n",
    "#### Mean of Class 0:\n",
    "$$\n",
    "\\bar{X}_1 = \\frac{2.5 + 3.0 + 3.2 + 4.1 + 3.8}{5} = \\frac{16.6}{5} = 3.32\n",
    "$$\n",
    "\n",
    "#### Mean of Class 1:\n",
    "$$\n",
    "\\bar{X}_2 = \\frac{5.2 + 5.8 + 6.1 + 5.9 + 6.5}{5} = \\frac{29.5}{5} = 5.9\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Calculate the Variance ($s_1^2, s_2^2$)**\n",
    "The formula for variance is:\n",
    "\n",
    "$$\n",
    "s^2 = \\frac{\\sum (X - \\bar{X})^2}{n - 1}\n",
    "$$\n",
    "\n",
    "#### Variance of Class 0:\n",
    "$$\n",
    "s_1^2 = \\frac{(2.5 - 3.32)^2 + (3.0 - 3.32)^2 + (3.2 - 3.32)^2 + (4.1 - 3.32)^2 + (3.8 - 3.32)^2}{5 - 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(-0.82)^2 + (-0.32)^2 + (-0.12)^2 + (0.78)^2 + (0.48)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.6724 + 0.1024 + 0.0144 + 0.6084 + 0.2304}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{1.628}{4} = 0.407\n",
    "$$\n",
    "\n",
    "#### Variance of Class 1:\n",
    "$$\n",
    "s_2^2 = \\frac{(5.2 - 5.9)^2 + (5.8 - 5.9)^2 + (6.1 - 5.9)^2 + (5.9 - 5.9)^2 + (6.5 - 5.9)^2}{5 - 1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{(-0.7)^2 + (-0.1)^2 + (0.2)^2 + (0.0)^2 + (0.6)^2}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.49 + 0.01 + 0.04 + 0.0 + 0.36}{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.9}{4} = 0.225\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Compute the t-statistic**\n",
    "$$\n",
    "t = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "$$\n",
    "\n",
    "Substituting values:\n",
    "\n",
    "$$\n",
    "t = \\frac{3.32 - 5.9}{\\sqrt{\\frac{0.407}{5} + \\frac{0.225}{5}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{-2.58}{\\sqrt{0.0814 + 0.045}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{-2.58}{\\sqrt{0.1264}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = \\frac{-2.58}{0.3556}\n",
    "$$\n",
    "\n",
    "$$\n",
    "t = -7.25\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Compute the p-value (Using t-table or Python)**\n",
    "To get the **p-value**, we can use statistical tables or Python's `scipy.stats.t.cdf` function.\n",
    "\n",
    "If we check the t-distribution table for **degrees of freedom (df = n1 + n2 - 2 = 5 + 5 - 2 = 8)**, a t-score of **-7.25** gives a very small p-value (typically <0.0001), meaning the feature is significant.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Conclusion**\n",
    "Since the **p-value is very small (p < 0.05)**, we conclude that **this feature significantly differs between the two groups** and is **useful for classification**.\n",
    "\n",
    "\n",
    "\n",
    "## **Summary of Steps**\n",
    "1. Compute means for both groups ($\\bar{X}_1, \\bar{X}_2$).\n",
    "2. Compute variances ($s_1^2, s_2^2$).\n",
    "3. Plug values into the t-test formula.\n",
    "4. Calculate the **t-statistic**.\n",
    "5. Find the **p-value** (using a t-table or Python).\n",
    "6. If **p < 0.05**, the feature is significant.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's break it down even **simpler** using a real-life analogy.  \n",
    "\n",
    "\n",
    "\n",
    "## **Think of It Like a Taste Test üçï vs üçî**  \n",
    "Imagine you are running a **food competition** between **Pizza Lovers** and **Burger Lovers**. You want to check:  \n",
    "\n",
    "üëâ *Do people in the Pizza group eat significantly fewer/more calories than people in the Burger group?*  \n",
    "\n",
    "You collect data on how many calories each person eats in both groups.\n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step in Layman Terms**  \n",
    "\n",
    "### **Step 1: Find the Average Calories Each Group Eats (Mean)**\n",
    "- You **add up all calories** each person eats in the Pizza group and divide by the number of people ‚Üí **this is the average (mean) for Pizza Lovers**.  \n",
    "- Do the same for Burger Lovers ‚Üí **this is the average (mean) for Burger Lovers**.  \n",
    "- If these averages are very different, it suggests that one group eats more than the other.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Check How Much People in Each Group Vary (Spread/Variance)**\n",
    "- Not everyone eats the same amount of calories! Some Pizza Lovers eat a lot; some eat less.  \n",
    "- The same happens with Burger Lovers.  \n",
    "- So, we **measure how spread out the numbers are** in each group. This is called **variance**.  \n",
    "\n",
    "If everyone in the Pizza group eats around **2000 calories**, and everyone in the Burger group eats around **3000 calories**, there is **low variance**.  \n",
    "But if some Pizza Lovers eat **1000 calories** and some eat **3000 calories**, there is **high variance**.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Compare the Two Groups with a Formula (t-test)**\n",
    "Now, we use a **formula** to compare:  \n",
    "üëâ **How different are the averages (step 1)?**  \n",
    "üëâ **How spread out is the data in each group (step 2)?**  \n",
    "\n",
    "The **t-test formula** gives a **t-score** (a number that tells us how different the two groups are).  \n",
    "\n",
    "- **If the t-score is big**, it means the groups are very different.  \n",
    "- **If the t-score is small**, the groups are similar.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Get the Final Answer (p-value)**\n",
    "The **p-value** tells us:  \n",
    "- **Small p-value (< 0.05)** ‚Üí The groups are **very different** ‚Üí The feature (calories) is important!  \n",
    "- **Large p-value (> 0.05)** ‚Üí The groups are **similar** ‚Üí The feature is **not useful**.  \n",
    "\n",
    "## **Example with Numbers** üéØ  \n",
    "\n",
    "| Pizza Lovers (Calories) | Burger Lovers (Calories) |\n",
    "|-------------------------|-------------------------|\n",
    "| 2000                    | 3000                    |\n",
    "| 2100                    | 3100                    |\n",
    "| 1900                    | 2900                    |\n",
    "| 2050                    | 3050                    |\n",
    "| 1950                    | 2950                    |\n",
    "\n",
    "\n",
    "\n",
    "- **Average for Pizza Lovers** = **2000 calories**  \n",
    "- **Average for Burger Lovers** = **3000 calories**  \n",
    "- The difference is **1000 calories**.  \n",
    "- The variance is low (numbers in each group are close to their mean).  \n",
    "- The t-test gives a **big t-score** and a **small p-value** (p < 0.05).  \n",
    "\n",
    "üëâ **Conclusion:** Pizza and Burger lovers eat **very different** calories, so \"Calories\" is an important feature!  \n",
    "\n",
    "\n",
    "\n",
    "## **Now Relate This to Feature Selection**  \n",
    "- Instead of **Pizza vs. Burger**, we have **Class 0 vs. Class 1** in a machine learning dataset.  \n",
    "- Instead of **Calories**, we check each feature (e.g., height, weight, salary).  \n",
    "- If a feature (e.g., salary) is **significantly different** between two groups, it is **important for classification**!  \n",
    "\n",
    "\n",
    "\n",
    "## **Final Takeaway**  \n",
    "- `ttest_ind` helps check **if a feature is useful for distinguishing two groups**.  \n",
    "- A **small p-value** means the feature **is important**.  \n",
    "- A **large p-value** means the feature **doesn‚Äôt help** separate the groups.  \n",
    "\n",
    "\n",
    "\n",
    "### **Super Simple Conclusion:**  \n",
    "`ttest_ind` is like a **food competition** ‚Äì it checks if two groups are really different. üçï vs üçî If they are, we use that feature for classification. üöÄ  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's go step by step and break down the **Chi-Square Test for Independence (`chi2_contingency`)** in a **super simple** way.  \n",
    "\n",
    "\n",
    "\n",
    "## **What is `chi2_contingency`?**\n",
    "It checks whether **two categorical variables** are **related** or **independent** of each other.  \n",
    "\n",
    "üëâ In simple terms: **\"Does one category affect the other?\"**  \n",
    "\n",
    "For example:  \n",
    "- **Does gender (Male/Female) affect preference for a product (Yes/No)?**  \n",
    "- **Does education level (High School/College) affect job type (Tech/Non-Tech)?**  \n",
    "- **Does smoking (Yes/No) affect lung disease (Yes/No)?**  \n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step Process**\n",
    "\n",
    "### **Step 1: Create a Contingency Table**  \n",
    "A **contingency table** is like a frequency table that shows how many times each combination occurs.  \n",
    "\n",
    "For example, let's say we surveyed **100 people** to check whether **gender** affects their **preference for a product**.  \n",
    "\n",
    "|               | Prefers Product | Doesn't Prefer Product | Total |\n",
    "|--------------|----------------|------------------------|------|\n",
    "| **Male**     | 30             | 20                     | 50   |\n",
    "| **Female**   | 40             | 10                     | 50   |\n",
    "| **Total**    | 70             | 30                     | 100  |\n",
    "\n",
    "Each cell in this table tells how many people **fall into a specific category combination**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Calculate Expected Counts**\n",
    "We now calculate what the numbers **should be** if gender and product preference were completely **independent** (not related).  \n",
    "\n",
    "Formula for **expected count** for each cell:\n",
    "\n",
    "$$\n",
    "E_{ij} = \\frac{(Row\\ Total) \\times (Column\\ Total)}{\\text{Grand Total}}\n",
    "$$\n",
    "\n",
    "Let‚Äôs calculate for **Male - Prefers Product**:\n",
    "\n",
    "$$\n",
    "E_{Male,Prefers} = \\frac{(Row\\ Total\\ for\\ Male) \\times (Column\\ Total\\ for\\ Prefers)}{\\text{Grand Total}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{Male,Prefers} = \\frac{50 \\times 70}{100} = \\frac{3500}{100} = 35\n",
    "$$\n",
    "\n",
    "Similarly, we calculate for other cells:\n",
    "\n",
    "|               | Prefers Product (Expected) | Doesn't Prefer (Expected) |\n",
    "|--------------|----------------|------------------------|\n",
    "| **Male**     | 35             | 15                     |\n",
    "| **Female**   | 35             | 15                     |\n",
    "\n",
    "Now, we compare the **actual observed values** vs **expected values**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Compute the Chi-Square Statistic**\n",
    "The Chi-Square formula is:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $O$ = Observed count (actual values from survey)\n",
    "- $E$ = Expected count (calculated from step 2)\n",
    "\n",
    "Now, calculate for each cell:\n",
    "\n",
    "For **Male - Prefers Product**:\n",
    "\n",
    "$$\n",
    "\\frac{(30 - 35)^2}{35} = \\frac{(-5)^2}{35} = \\frac{25}{35} = 0.714\n",
    "$$\n",
    "\n",
    "For **Male - Doesn‚Äôt Prefer Product**:\n",
    "\n",
    "$$\n",
    "\\frac{(20 - 15)^2}{15} = \\frac{(5)^2}{15} = \\frac{25}{15} = 1.667\n",
    "$$\n",
    "\n",
    "For **Female - Prefers Product**:\n",
    "\n",
    "$$\n",
    "\\frac{(40 - 35)^2}{35} = \\frac{(5)^2}{35} = \\frac{25}{35} = 0.714\n",
    "$$\n",
    "\n",
    "For **Female - Doesn‚Äôt Prefer Product**:\n",
    "\n",
    "$$\n",
    "\\frac{(10 - 15)^2}{15} = \\frac{(-5)^2}{15} = \\frac{25}{15} = 1.667\n",
    "$$\n",
    "\n",
    "Now sum them all up:\n",
    "\n",
    "$$\n",
    "\\chi^2 = 0.714 + 1.667 + 0.714 + 1.667 = 4.76\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Find the p-value**\n",
    "To determine if the relationship is **significant**, we compare our Chi-Square value to a **critical value from a Chi-Square table**, or we calculate the **p-value** using Python.\n",
    "\n",
    "The **degrees of freedom** (df) is calculated as:\n",
    "\n",
    "$$\n",
    "df = (\\text{rows} - 1) \\times (\\text{columns} - 1)\n",
    "$$\n",
    "\n",
    "For our table:\n",
    "\n",
    "$$\n",
    "df = (2 - 1) \\times (2 - 1) = 1\n",
    "$$\n",
    "\n",
    "Now, using a **Chi-Square table or Python**, we find the **p-value** for $\\chi^2 = 4.76$ with $df = 1$.  \n",
    "Let's assume **p = 0.029**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 5: Interpret the Results**\n",
    "- **If p-value < 0.05**, reject the null hypothesis ‚Üí *There is a significant relationship between gender and product preference!*  \n",
    "- **If p-value > 0.05**, fail to reject the null hypothesis ‚Üí *No significant relationship. Gender does not affect product preference!*  \n",
    "\n",
    "Since **p = 0.029 < 0.05**, we conclude:  \n",
    "‚úÖ **Gender and product preference are related!**  \n",
    "\n",
    "\n",
    "\n",
    "## **How is `chi2_contingency` Used in Feature Selection?**\n",
    "- If a **categorical feature** (e.g., \"Gender\") is **highly related** to the target variable (e.g., \"Buys Product\"), it is an **important feature**.  \n",
    "- If **p-value < 0.05**, the feature is useful for prediction.  \n",
    "- If **p-value > 0.05**, the feature **does not help** and can be removed.\n",
    "\n",
    "\n",
    "\n",
    "## **Summary**\n",
    "| Step | What You Do |\n",
    "|------|------------|\n",
    "| 1Ô∏è‚É£ | Make a **contingency table** (count how often each category appears). |\n",
    "| 2Ô∏è‚É£ | Calculate **expected counts** (what would happen if categories were independent). |\n",
    "| 3Ô∏è‚É£ | Use **Chi-Square formula** to compare observed vs. expected. |\n",
    "| 4Ô∏è‚É£ | Find the **p-value** (probability that the difference is due to chance). |\n",
    "| 5Ô∏è‚É£ | If **p < 0.05**, feature is important! |\n",
    "\n",
    "\n",
    "\n",
    "## **Real Example in Python**\n",
    "Here‚Äôs how you do it in Python using **scipy.stats.chi2_contingency**:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Creating the observed frequency table\n",
    "data = np.array([[30, 20],  # Male\n",
    "                 [40, 10]]) # Female\n",
    "\n",
    "# Performing Chi-Square test\n",
    "chi2, p, dof, expected = chi2_contingency(data)\n",
    "\n",
    "# Output results\n",
    "print(f\"Chi-Square Value: {chi2}\")\n",
    "print(f\"p-value: {p}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "print(f\"Expected Table:\\n{expected}\")\n",
    "\n",
    "# Check significance\n",
    "if p < 0.05:\n",
    "    print(\"Feature is significant!\")\n",
    "else:\n",
    "    print(\"Feature is NOT significant.\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## **Final Takeaway**\n",
    "üîπ `chi2_contingency` helps check if **two categorical variables are related**.  \n",
    "üîπ If **p < 0.05**, the feature is useful for classification.  \n",
    "üîπ If **p > 0.05**, the feature can be removed.  \n",
    "üîπ **Used in feature selection** for categorical data in machine learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ANOVA (Analysis of Variance) Test ‚Äì Full Explanation**  \n",
    "\n",
    "### **What is ANOVA?**\n",
    "ANOVA (**Analysis of Variance**) is a **statistical test** used to determine **if there is a significant difference between the means of three or more independent groups**. It helps us check whether the variations in the groups are **due to random chance or actual differences** in the population.\n",
    "\n",
    "üîπ **Example Use Case**:  \n",
    "Suppose you want to compare the **average test scores** of students from three different schools. ANOVA can tell you **whether at least one school has a significantly different mean score** compared to the others.\n",
    "\n",
    "\n",
    "\n",
    "## **Types of ANOVA**\n",
    "1. **One-Way ANOVA** ‚Üí Compares means across **one independent variable** (one factor).  \n",
    "   - Example: Comparing **test scores** of students from **three schools**.  \n",
    "   \n",
    "2. **Two-Way ANOVA** ‚Üí Compares means across **two independent variables** (two factors).  \n",
    "   - Example: Comparing test scores based on **school** (factor 1) and **teaching method** (factor 2).  \n",
    "\n",
    "3. **Repeated Measures ANOVA** ‚Üí Used when the **same subjects** are tested multiple times under different conditions (like before and after a treatment).  \n",
    "\n",
    "\n",
    "\n",
    "## **1. One-Way ANOVA Formula**  \n",
    "The **ANOVA test statistic (F-statistic)** is given by:\n",
    "\n",
    "$$\n",
    "F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}}\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- **Between-group variance** measures how much the group means differ from the overall mean.  \n",
    "- **Within-group variance** measures how much the data points within each group differ from their group mean.  \n",
    "\n",
    "A **higher F-value** means the groups are likely to have significantly different means.\n",
    "\n",
    "\n",
    "\n",
    "## **Step-by-Step Calculation of One-Way ANOVA**\n",
    "Let's consider **three groups (A, B, and C)** with test scores:\n",
    "\n",
    "| **Group A** | **Group B** | **Group C** |\n",
    "|------------|------------|------------|\n",
    "| 85         | 78         | 90         |\n",
    "| 88         | 82         | 94         |\n",
    "| 92         | 84         | 89         |\n",
    "| 94         | 80         | 96         |\n",
    "| 90         | 86         | 91         |\n",
    "\n",
    "### **Step 1: Compute Group Means ($\\bar{X}$)**\n",
    "#### Mean of Group A:\n",
    "$$\n",
    "\\bar{X}_A = \\frac{85 + 88 + 92 + 94 + 90}{5} = 89.8\n",
    "$$\n",
    "\n",
    "#### Mean of Group B:\n",
    "$$\n",
    "\\bar{X}_B = \\frac{78 + 82 + 84 + 80 + 86}{5} = 82.0\n",
    "$$\n",
    "\n",
    "#### Mean of Group C:\n",
    "$$\n",
    "\\bar{X}_C = \\frac{90 + 94 + 89 + 96 + 91}{5} = 92.0\n",
    "$$\n",
    "\n",
    "#### Overall Mean ($\\bar{X}_{\\text{overall}}$)\n",
    "$$\n",
    "\\bar{X}_{\\text{overall}} = \\frac{(85+88+92+94+90) + (78+82+84+80+86) + (90+94+89+96+91)}{15} = 87.93\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Compute Between-Group Variance (SSB)**\n",
    "The formula for **Sum of Squares Between Groups (SSB)**:\n",
    "\n",
    "$$\n",
    "SSB = n_A (\\bar{X}_A - \\bar{X}_{\\text{overall}})^2 + n_B (\\bar{X}_B - \\bar{X}_{\\text{overall}})^2 + n_C (\\bar{X}_C - \\bar{X}_{\\text{overall}})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSB = 5(89.8 - 87.93)^2 + 5(82 - 87.93)^2 + 5(92 - 87.93)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSB = 5(3.47) + 5(35.22) + 5(16.54) = 17.35 + 176.1 + 82.7 = 276.15\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Compute Within-Group Variance (SSW)**\n",
    "The formula for **Sum of Squares Within Groups (SSW)**:\n",
    "\n",
    "$$\n",
    "SSW = \\sum (X - \\bar{X})^2 \\text{ for each group}\n",
    "$$\n",
    "\n",
    "#### **For Group A:**\n",
    "$$\n",
    "(85 - 89.8)^2 + (88 - 89.8)^2 + (92 - 89.8)^2 + (94 - 89.8)^2 + (90 - 89.8)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 23.04 + 3.24 + 4.84 + 17.64 + 0.04 = 48.8\n",
    "$$\n",
    "\n",
    "#### **For Group B:**\n",
    "$$\n",
    "(78 - 82)^2 + (82 - 82)^2 + (84 - 82)^2 + (80 - 82)^2 + (86 - 82)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 16 + 0 + 4 + 4 + 16 = 40\n",
    "$$\n",
    "\n",
    "#### **For Group C:**\n",
    "$$\n",
    "(90 - 92)^2 + (94 - 92)^2 + (89 - 92)^2 + (96 - 92)^2 + (91 - 92)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 4 + 4 + 9 + 16 + 1 = 34\n",
    "$$\n",
    "\n",
    "$$\n",
    "SSW = 48.8 + 40 + 34 = 122.8\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Compute F-Statistic**\n",
    "$$\n",
    "F = \\frac{\\text{MSB}}{\\text{MSW}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Mean Square Between Groups (MSB)** = $ \\frac{SSB}{df_B} $  \n",
    "  - Degrees of freedom **df_B** = Number of groups - 1 = 3 - 1 = 2  \n",
    "  - $ MSB = \\frac{276.15}{2} = 138.08 $  \n",
    "\n",
    "- **Mean Square Within Groups (MSW)** = $ \\frac{SSW}{df_W} $  \n",
    "  - Degrees of freedom **df_W** = Total samples - Number of groups = 15 - 3 = 12  \n",
    "  - $ MSW = \\frac{122.8}{12} = 10.23 $  \n",
    "\n",
    "$$\n",
    "F = \\frac{138.08}{10.23} = 13.5\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 5: Find the p-value**\n",
    "The **p-value** is obtained from the **F-distribution table** or using Python:\n",
    "\n",
    "```python\n",
    "from scipy.stats import f\n",
    "\n",
    "# Degrees of freedom\n",
    "df_between = 2\n",
    "df_within = 12\n",
    "\n",
    "# Compute p-value\n",
    "p_value = 1 - f.cdf(13.5, df_between, df_within)\n",
    "\n",
    "print(f\"P-value: {p_value}\")\n",
    "```\n",
    "\n",
    "üìå **If p-value < 0.05**, we reject the null hypothesis and conclude that at least one group has a significantly different mean.\n",
    "\n",
    "\n",
    "\n",
    "## **Final Conclusion**\n",
    "- If **p < 0.05**, at least one group's mean is significantly different.\n",
    "- If **p > 0.05**, the differences between groups are likely **due to chance**.\n",
    "\n",
    "\n",
    "\n",
    "## **Summary of Steps**\n",
    "1. **Compute group means** and **overall mean**.\n",
    "2. **Calculate SSB** (Between-group variance).\n",
    "3. **Calculate SSW** (Within-group variance).\n",
    "4. **Compute the F-statistic**.\n",
    "5. **Find the p-value**.\n",
    "6. **Interpret results** ‚Üí If **p < 0.05**, at least one group is different.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a **comparison table** that explains when to use **t-test (independent), ANOVA, and Chi-Square test (chi2_contingency)** along with their use cases, assumptions, and examples:  \n",
    "\n",
    "| **Test Name**          | **Type of Data**  | **Purpose**  | **Number of Groups** | **Assumptions** | **Example Use Case** | **Hypothesis Tested** | **Python Function** |\n",
    "|------------------------|------------------|-------------|----------------------|-----------------|----------------------|------------------------|----------------------|\n",
    "| **t-test (Independent)** <br> `ttest_ind()` | **Numerical (continuous) vs. Categorical** | Compare means between **two** independent groups | **2 groups only** | - Data is **normally distributed** <br> - Groups have **equal variance** (for Student‚Äôs t-test) <br> - Observations are **independent** | **Example:** Compare **average exam scores** of **male vs. female students** | **H‚ÇÄ:** No difference in means between the two groups <br> **H‚ÇÅ:** Means are significantly different | `scipy.stats.ttest_ind(group1, group2)` |\n",
    "| **ANOVA (Analysis of Variance)** <br> `f_oneway()` | **Numerical (continuous) vs. Categorical** | Compare means across **three or more** independent groups | **3+ groups** | - Data is **normally distributed** <br> - Groups have **equal variance** <br> - Observations are **independent** | **Example:** Compare **average salaries** across job roles (**Engineer, Manager, Director, VP**) | **H‚ÇÄ:** All groups have the same mean <br> **H‚ÇÅ:** At least one group mean is different | `scipy.stats.f_oneway(group1, group2, group3, ...)` |\n",
    "| **Chi-Square Test** <br> `chi2_contingency()` | **Categorical vs. Categorical** | Test **association** or **independence** between two categorical variables | **Any number of categories** | - Data is in **frequency/count format** <br> - Expected frequencies > 5 in most cells | **Example:** Check if **education level** is related to **preferred social media platform** | **H‚ÇÄ:** No association between the categorical variables <br> **H‚ÇÅ:** Variables are dependent | `scipy.stats.chi2_contingency(table)` |\n",
    "\n",
    "\n",
    "\n",
    "## **How to Choose the Right Test?**\n",
    "- **If comparing means between two groups ‚Üí Use t-test (`ttest_ind`)**\n",
    "- **If comparing means across 3+ groups ‚Üí Use ANOVA (`f_oneway`)**\n",
    "- **If testing association between two categorical variables ‚Üí Use Chi-Square (`chi2_contingency`)**\n",
    "\n",
    "\n",
    "\n",
    "## **Example Code for Each Test**\n",
    "### **1Ô∏è‚É£ Independent t-test (Comparing Two Groups)**\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data: Exam scores of males and females\n",
    "male_scores = [85, 88, 90, 92, 86]\n",
    "female_scores = [78, 82, 84, 80, 79]\n",
    "\n",
    "# Perform independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(male_scores, female_scores)\n",
    "\n",
    "print(f\"t-statistic: {t_stat}, p-value: {p_value}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ ANOVA (Comparing 3+ Groups)**\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data: Salaries of different job roles\n",
    "engineers = [60000, 62000, 58000, 59000, 61000]\n",
    "managers = [80000, 82000, 78000, 81000, 79000]\n",
    "directors = [120000, 125000, 118000, 122000, 121000]\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_stat, p_value = stats.f_oneway(engineers, managers, directors)\n",
    "\n",
    "print(f\"F-statistic: {f_stat}, p-value: {p_value}\")\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ Chi-Square Test (Categorical vs. Categorical)**\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Contingency table: (Education Level x Social Media Preference)\n",
    "data = np.array([[50, 30, 20],  # High School\n",
    "                 [60, 40, 30],  # Bachelor's\n",
    "                 [70, 50, 40]]) # Master's\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, expected = stats.chi2_contingency(data)\n",
    "\n",
    "print(f\"Chi-Square Value: {chi2}, p-value: {p}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü **Spearman's Rank Correlation: The Complete Guide** üéØ\n",
    "\n",
    "### **üîπ What is Spearman‚Äôs Rank Correlation?**\n",
    "Spearman‚Äôs Rank Correlation, denoted as **Spearman's œÅ (rho)**, is a **non-parametric** statistical test that measures the **monotonic** relationship between two variables.\n",
    "\n",
    "üöÄ Unlike **Pearson‚Äôs correlation**, which captures **linear relationships**, Spearman‚Äôs correlation checks **whether one variable increases (or decreases) as another does, even if the relationship is not linear**.\n",
    "\n",
    "\n",
    "### **üîπ When Should You Use Spearman's Correlation?**\n",
    "‚úÖ **Your data is numerical** (continuous or ordinal).  \n",
    "‚úÖ **Your data is NOT normally distributed**.  \n",
    "‚úÖ **You suspect a monotonic (but possibly nonlinear) relationship**.  \n",
    "‚úÖ **You have ranked or ordinal data** (like ratings, preferences, etc.).  \n",
    "‚úÖ **You want a more robust alternative to Pearson's correlation** (less sensitive to outliers).\n",
    "\n",
    "üî¥ **Do NOT use Spearman‚Äôs if:**\n",
    "‚ùå The relationship is NOT monotonic.  \n",
    "‚ùå You specifically need to measure a **linear** relationship (use Pearson instead).\n",
    "\n",
    "\n",
    "### **üîπ How Does Spearman‚Äôs Correlation Work?**\n",
    "Spearman‚Äôs correlation works by converting the data into **ranks** and then calculating the **Pearson correlation** on the ranks.\n",
    "\n",
    "üëÄ **Step-by-Step Process:**\n",
    "1Ô∏è‚É£ **Convert values into ranks** (smallest value gets rank 1, second smallest gets rank 2, etc.).  \n",
    "2Ô∏è‚É£ **Compute the difference between ranks** for each pair of data points.  \n",
    "3Ô∏è‚É£ **Calculate Spearman‚Äôs correlation using the formula**:\n",
    "\n",
    "$$\n",
    "œÅ = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ d_i $ = difference between the ranks of each pair\n",
    "- $ n $ = number of observations\n",
    "\n",
    "### **üîπ Spearman‚Äôs vs. Pearson‚Äôs vs. Mutual Information**\n",
    "| Method | Relationship Type | Handles Nonlinear? | Affected by Outliers? | Works with Ordinal Data? |\n",
    "|--------|------------------|--------------------|----------------------|------------------------|\n",
    "| **Spearman‚Äôs (œÅ)** | Monotonic | ‚úÖ Yes | ‚úÖ No (more robust) | ‚úÖ Yes |\n",
    "| **Pearson‚Äôs (r)** | Linear | ‚ùå No | ‚ùå Yes | ‚ùå No |\n",
    "| **Mutual Information** | Any | ‚úÖ Yes | ‚úÖ No | ‚úÖ Yes |\n",
    "\n",
    "üîπ **Spearman's is a great choice when Pearson's fails due to nonlinearity or outliers!**\n",
    "\n",
    "\n",
    "### **üîπ Spearman‚Äôs Correlation in Python**\n",
    "Here‚Äôs how you can calculate Spearman‚Äôs correlation using `scipy`:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Sample Data\n",
    "data = {\n",
    "    'Feature1': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    'Feature2': [1, 3, 2, 5, 4, 7, 6, 9, 8, 10]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Spearman's Correlation\n",
    "rho, p_value = spearmanr(df['Feature1'], df['Feature2'])\n",
    "\n",
    "# Print Results\n",
    "print(f\"Spearman's Correlation: {rho:.4f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "\n",
    "### **üîπ Interpreting the Results**\n",
    "- **œÅ (Spearman's correlation) value:**\n",
    "  - **+1** ‚Üí Perfect positive monotonic relationship üìà\n",
    "  - **-1** ‚Üí Perfect negative monotonic relationship üìâ\n",
    "  - **0** ‚Üí No correlation ‚ùå\n",
    "\n",
    "- **p-value:**\n",
    "  - **p < 0.05** ‚Üí The correlation is **statistically significant** ‚úÖ\n",
    "  - **p > 0.05** ‚Üí No strong evidence of correlation ‚ùå\n",
    "\n",
    "\n",
    "### **üîπ Example Scenarios**\n",
    "üîµ **Example 1 (Perfect Monotonic Relationship)**\n",
    "```python\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [10, 20, 30, 40, 50]\n",
    "```\n",
    "üìä **œÅ = 1.0** (Perfect Positive Correlation)\n",
    "\n",
    "üî¥ **Example 2 (Nonlinear but Monotonic)**\n",
    "```python\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [1, 4, 9, 16, 25]  # Squared values\n",
    "```\n",
    "üìä **œÅ ‚âà 1.0** (Still strong correlation!)\n",
    "\n",
    "üü¢ **Example 3 (Non-monotonic)**\n",
    "```python\n",
    "x = [1, 2, 3, 4, 5, 6]\n",
    "y = [10, 5, 20, 15, 30, 25]  # No clear increasing or decreasing trend\n",
    "```\n",
    "üìä **œÅ ‚âà 0.0** (No correlation)\n",
    "\n",
    "\n",
    "### **üîπ Conclusion: When to Choose Spearman?**\n",
    "‚úÖ Use **Spearman‚Äôs correlation** when:\n",
    "- Your data has a **nonlinear but monotonic** trend.  \n",
    "- You have **ranked/ordinal** data (e.g., survey ratings).  \n",
    "- You want a **robust** method less affected by **outliers**.\n",
    "\n",
    "üöÄ Spearman's is **more flexible than Pearson‚Äôs** and is often used in **feature selection** when working with numerical variables.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step and manually calculate **Spearman's Rank Correlation Coefficient (œÅ)** using an example.  \n",
    "\n",
    "\n",
    "\n",
    "### **üìå Example Dataset**\n",
    "We have two variables **X** and **Y**:\n",
    "\n",
    "| X  | Y  |\n",
    "|----|----|\n",
    "| 10 | 200 |\n",
    "| 20 | 150 |\n",
    "| 30 | 300 |\n",
    "| 40 | 250 |\n",
    "| 50 | 400 |\n",
    "\n",
    "We will calculate **Spearman's rank correlation coefficient** step by step.\n",
    "\n",
    "\n",
    "\n",
    "## **üî¢ Step 1: Rank the Data**\n",
    "Spearman‚Äôs correlation works on **ranks** instead of actual values.\n",
    "\n",
    "### **Ranking X and Y:**\n",
    "The smallest value gets **rank 1**, the second smallest gets **rank 2**, and so on.\n",
    "\n",
    "| X  | Rank(X) | Y  | Rank(Y) |\n",
    "|----|--------|----|--------|\n",
    "| 10 | 1      | 200 | 2      |\n",
    "| 20 | 2      | 150 | 1      |\n",
    "| 30 | 3      | 300 | 3      |\n",
    "| 40 | 4      | 250 | 4      |\n",
    "| 50 | 5      | 400 | 5      |\n",
    "\n",
    "Now, we will **calculate the difference** between the ranks of each pair.\n",
    "\n",
    "\n",
    "\n",
    "## **üî¢ Step 2: Compute Rank Differences (d) and Square Them (d¬≤)**\n",
    "\n",
    "$$\n",
    "d_i = \\text{Rank}(X) - \\text{Rank}(Y)\n",
    "$$\n",
    "\n",
    "| X  | Rank(X) | Y  | Rank(Y) | $ d_i $ | $ d_i^2 $ |\n",
    "|----|--------|----|--------|------|------|\n",
    "| 10 | 1      | 200 | 2      | -1   | 1    |\n",
    "| 20 | 2      | 150 | 1      | 1    | 1    |\n",
    "| 30 | 3      | 300 | 3      | 0    | 0    |\n",
    "| 40 | 4      | 250 | 4      | 0    | 0    |\n",
    "| 50 | 5      | 400 | 5      | 0    | 0    |\n",
    "\n",
    "### **Sum of $ d_i^2 $ values:**\n",
    "$$\n",
    "\\sum d_i^2 = 1 + 1 + 0 + 0 + 0 = 2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üî¢ Step 3: Apply Spearman‚Äôs Correlation Formula**\n",
    "$$\n",
    "œÅ = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\sum d_i^2 = 2 $ (Sum of squared rank differences)\n",
    "- $ n = 5 $ (Number of data points)\n",
    "\n",
    "Now, plug in the values:\n",
    "\n",
    "$$\n",
    "œÅ = 1 - \\frac{6(2)}{5(5^2 - 1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "œÅ = 1 - \\frac{12}{5(25 - 1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "œÅ = 1 - \\frac{12}{5 \\times 24}\n",
    "$$\n",
    "\n",
    "$$\n",
    "œÅ = 1 - \\frac{12}{120}\n",
    "$$\n",
    "\n",
    "$$\n",
    "œÅ = 1 - 0.1\n",
    "$$\n",
    "\n",
    "$$\n",
    "œÅ = 0.9\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **üéØ Interpretation of Spearman's Correlation**\n",
    "- **œÅ = 0.9** means **strong positive correlation**.\n",
    "- As **X increases, Y also increases in a monotonic way**.\n",
    "- Even if the relationship is not perfectly linear, the ranking order is **mostly preserved**.\n",
    "\n",
    "\n",
    "\n",
    "## **üî¢ Verify with Python**\n",
    "Let‚Äôs verify our manual calculation with `scipy.stats.spearmanr`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "X = [10, 20, 30, 40, 50]\n",
    "Y = [200, 150, 300, 250, 400]\n",
    "\n",
    "rho, _ = spearmanr(X, Y)\n",
    "print(f\"Spearman's correlation coefficient: {rho:.4f}\")\n",
    "```\n",
    "\n",
    "‚úÖ **Output:**  \n",
    "```\n",
    "Spearman's correlation coefficient: 0.9\n",
    "```\n",
    "Matches our manual calculation! üéØ\n",
    "\n",
    "\n",
    "\n",
    "### **üéØ Summary**\n",
    "‚úÖ **Spearman‚Äôs correlation works by ranking the data first**.  \n",
    "‚úÖ **It measures how well the rank order is preserved** between two variables.  \n",
    "‚úÖ **œÅ = 0.9 means strong positive correlation**, even if the actual values don‚Äôt follow a perfect straight line.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like you're asking about the **Kruskal-Wallis test**, which is a **non-parametric statistical test** used to compare three or more independent groups to determine if they come from the same distribution. This test is an alternative to the one-way ANOVA when the assumption of normality isn‚Äôt met.\n",
    "\n",
    "### **üöÄ Kruskal-Wallis Test: A Deep Dive**\n",
    "The Kruskal-Wallis test is based on **ranks** rather than actual data values, making it robust against outliers and non-normal distributions.\n",
    "\n",
    "üîπ **When to Use?**  \n",
    "- When comparing **three or more** independent groups  \n",
    "- When data is **not normally distributed**  \n",
    "- When sample sizes are small  \n",
    "\n",
    "üîπ **How It Works?**\n",
    "1. **Rank all values** from all groups together, from lowest to highest.  \n",
    "2. Compute the **sum of ranks** for each group.  \n",
    "3. Use the **Kruskal-Wallis H statistic** formula:  \n",
    "\n",
    "   $$\n",
    "   H = \\frac{12}{N(N+1)} \\sum \\frac{R_i^2}{n_i} - 3(N+1)\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "   - $ N $ = Total number of observations  \n",
    "   - $ n_i $ = Number of observations in each group  \n",
    "   - $ R_i $ = Sum of ranks for each group  \n",
    "\n",
    "4. Compare **H** to the critical value from the chi-square distribution with $ k-1 $ degrees of freedom.\n",
    "\n",
    "5. If **p-value < significance level (0.05)**, we **reject the null hypothesis** ‚Üí At least one group is different.\n",
    "\n",
    "\n",
    "\n",
    "### **üåü Example Scenario**\n",
    "Imagine you're a **data scientist analyzing customer satisfaction** across three different stores (**A, B, and C**). You collect ratings from customers and want to check if satisfaction levels differ.\n",
    "\n",
    "1. **Step 1: Collect Data**  \n",
    "   - Store A: [4, 5, 6, 7, 8]  \n",
    "   - Store B: [2, 3, 4, 5, 6]  \n",
    "   - Store C: [7, 8, 9, 10, 10]  \n",
    "\n",
    "2. **Step 2: Rank Data Across All Stores**\n",
    "   - Rank values from 1 (lowest) to highest.\n",
    "   \n",
    "3. **Step 3: Compute H-statistic**  \n",
    "\n",
    "4. **Step 4: Compare with Chi-Square Table**  \n",
    "\n",
    "5. **Step 5: Interpret Results**  \n",
    "   - If **p < 0.05**, at least one store‚Äôs satisfaction level is significantly different.\n",
    "\n",
    "\n",
    "\n",
    "### **üéØ Key Takeaways**\n",
    "‚úÖ Non-parametric ‚Üí Works even if assumptions of normality fail  \n",
    "‚úÖ Compares multiple groups efficiently  \n",
    "‚úÖ Doesn‚Äôt require equal sample sizes  \n",
    "‚úÖ If **significant**, follow up with post-hoc tests like **Dunn‚Äôs Test** to identify which groups differ.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's go step by step through a **manual calculation** of the **Kruskal-Wallis Test** with an example.  \n",
    "\n",
    "\n",
    "\n",
    "## **üìå Example: Comparing Exam Scores of 3 Classes**  \n",
    "Suppose we have exam scores from **three different classes**:  \n",
    "\n",
    "| **Class A** | **Class B** | **Class C** |  \n",
    "|------------|------------|------------|  \n",
    "| 85         | 88         | 90         |  \n",
    "| 80         | 75         | 95         |  \n",
    "| 78         | 85         | 98         |  \n",
    "| 92         | 82         | 89         |  \n",
    "\n",
    "Our goal is to check **if there is a significant difference in scores** across the three classes.  \n",
    "\n",
    "\n",
    "\n",
    "### **Step 1: Rank All Data Together**\n",
    "We **combine all values** and **rank them from lowest to highest**. If two values are the same, assign them the **average rank**.\n",
    "\n",
    "| Score | Rank | Group |\n",
    "|--------|------|--------|\n",
    "| 75     | 1    | B      |\n",
    "| 78     | 2    | A      |\n",
    "| 80     | 3    | A      |\n",
    "| 82     | 4    | B      |\n",
    "| 85     | 5.5  | A      |\n",
    "| 85     | 5.5  | B      |\n",
    "| 88     | 7    | B      |\n",
    "| 89     | 8    | C      |\n",
    "| 90     | 9    | C      |\n",
    "| 92     | 10   | A      |\n",
    "| 95     | 11   | C      |\n",
    "| 98     | 12   | C      |\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Calculate Rank Sums for Each Group**  \n",
    "Now, sum up the ranks for each class:\n",
    "\n",
    "- **Class A**: $ R_A = 2 + 3 + 5.5 + 10 = 20.5 $  \n",
    "- **Class B**: $ R_B = 1 + 4 + 5.5 + 7 = 17.5 $  \n",
    "- **Class C**: $ R_C = 8 + 9 + 11 + 12 = 40 $  \n",
    "\n",
    "Total number of observations:  \n",
    "$$\n",
    "N = 12\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Apply Kruskal-Wallis Formula**\n",
    "$$\n",
    "H = \\frac{12}{N(N+1)} \\sum \\frac{R_i^2}{n_i} - 3(N+1)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ N = 12 $ (total number of observations)  \n",
    "- $ n_A = 4 $, $ n_B = 4 $, $ n_C = 4 $ (each group has 4 values)  \n",
    "- $ R_A = 20.5 $, $ R_B = 17.5 $, $ R_C = 40 $  \n",
    "\n",
    "First, calculate each term:\n",
    "\n",
    "$$\n",
    "\\frac{(R_A)^2}{n_A} = \\frac{(20.5)^2}{4} = \\frac{420.25}{4} = 105.06\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{(R_B)^2}{n_B} = \\frac{(17.5)^2}{4} = \\frac{306.25}{4} = 76.56\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{(R_C)^2}{n_C} = \\frac{(40)^2}{4} = \\frac{1600}{4} = 400\n",
    "$$\n",
    "\n",
    "Now, compute:\n",
    "\n",
    "$$\n",
    "\\sum \\frac{R_i^2}{n_i} = 105.06 + 76.56 + 400 = 581.62\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{12(13)} (581.62) - 3(13)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{156} \\times 581.62 - 39\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = 44.7 - 39\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = 5.7\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Compare with Chi-Square Critical Value**\n",
    "The Kruskal-Wallis test statistic $ H $ follows a **chi-square distribution** with $ k - 1 $ degrees of freedom.  \n",
    "- Here, $ k = 3 $ (number of groups), so $ df = 3 - 1 = 2 $.  \n",
    "- At **Œ± = 0.05**, the **chi-square critical value** for $ df = 2 $ is **5.99**.\n",
    "\n",
    "Since **H = 5.7** is **less than 5.99**, we **fail to reject the null hypothesis**.  \n",
    "üëâ **Conclusion**: No significant difference exists between the three classes.\n",
    "\n",
    "\n",
    "### **üîπ Summary**\n",
    "‚úÖ We ranked data, summed ranks per group, and applied the Kruskal-Wallis formula.  \n",
    "‚úÖ We compared the test statistic to a chi-square table.  \n",
    "‚úÖ Since **H < critical value**, we conclude that **no significant difference** exists.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let‚Äôs break this down **step by step in the simplest way possible** with a real-world example.  \n",
    "\n",
    "\n",
    "\n",
    "### **üçï Imagine You Are Comparing Pizza Quality in 3 Restaurants**\n",
    "Let's say you and your friends visit **three different pizza places (A, B, and C)** and rate their pizzas.  \n",
    "Each of you gives a score from 1 to 10.  \n",
    "\n",
    "Here are the ratings:  \n",
    "\n",
    "| **Pizza Place A** | **Pizza Place B** | **Pizza Place C** |  \n",
    "|-----------------|-----------------|-----------------|  \n",
    "| 8.5           | 8.8           | 9.0           |  \n",
    "| 8.0           | 7.5           | 9.5           |  \n",
    "| 7.8           | 8.5           | 9.8           |  \n",
    "| 9.2           | 8.2           | 8.9           |  \n",
    "\n",
    "Our goal: **Do these pizza places have the same quality, or is at least one significantly better or worse?**  \n",
    "\n",
    "\n",
    "\n",
    "### **üìå Step 1: Convert Raw Scores to Ranks**\n",
    "Instead of using the actual ratings, we **rank all scores** together from lowest to highest.  \n",
    "\n",
    "| Score | Rank | Pizza Place |\n",
    "|--------|------|------------|\n",
    "| 7.5  | 1    | B |\n",
    "| 7.8  | 2    | A |\n",
    "| 8.0  | 3    | A |\n",
    "| 8.2  | 4    | B |\n",
    "| 8.5  | 5.5  | A |\n",
    "| 8.5  | 5.5  | B |\n",
    "| 8.8  | 7    | B |\n",
    "| 8.9  | 8    | C |\n",
    "| 9.0  | 9    | C |\n",
    "| 9.2  | 10   | A |\n",
    "| 9.5  | 11   | C |\n",
    "| 9.8  | 12   | C |\n",
    "\n",
    "üîπ **Why do we rank?**  \n",
    "Because Kruskal-Wallis doesn‚Äôt care about actual numbers, only their order. It looks at whether one group consistently has higher or lower ranks than others.\n",
    "\n",
    "\n",
    "\n",
    "### **üìå Step 2: Add Up the Ranks for Each Pizza Place**\n",
    "Now, let‚Äôs sum up the ranks for each restaurant:\n",
    "\n",
    "- **Pizza Place A:**  \n",
    "  $ 2 + 3 + 5.5 + 10 = 20.5 $  \n",
    "- **Pizza Place B:**  \n",
    "  $ 1 + 4 + 5.5 + 7 = 17.5 $  \n",
    "- **Pizza Place C:**  \n",
    "  $ 8 + 9 + 11 + 12 = 40 $  \n",
    "\n",
    "Now we have total rank sums for each place.\n",
    "\n",
    "\n",
    "\n",
    "### **üìå Step 3: Plug the Numbers into the Kruskal-Wallis Formula**\n",
    "We use the formula:\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{N(N+1)} \\sum \\frac{R_i^2}{n_i} - 3(N+1)\n",
    "$$\n",
    "\n",
    "Where:  \n",
    "- $ N = 12 $ (total ratings)  \n",
    "- $ n_A = 4 $, $ n_B = 4 $, $ n_C = 4 $ (each place has 4 ratings)  \n",
    "- $ R_A = 20.5 $, $ R_B = 17.5 $, $ R_C = 40 $  \n",
    "\n",
    "Let‚Äôs calculate:\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{12(13)} ( \\frac{20.5^2}{4} + \\frac{17.5^2}{4} + \\frac{40^2}{4}) - 3(13)\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{156} (105.06 + 76.56 + 400) - 39\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = \\frac{12}{156} \\times 581.62 - 39\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = 44.7 - 39\n",
    "$$\n",
    "\n",
    "$$\n",
    "H = 5.7\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **üìå Step 4: Compare to the Chi-Square Table**\n",
    "We now check if **5.7 is a big enough number** to say there‚Äôs a real difference.\n",
    "\n",
    "- The Kruskal-Wallis test follows a **chi-square distribution**.\n",
    "- For **three groups (A, B, C)**, we look at **df = 3 - 1 = 2** in the chi-square table.\n",
    "- **Critical value at Œ± = 0.05** is **5.99**.\n",
    "\n",
    "Since **5.7 < 5.99**, we **fail to reject the null hypothesis**.  \n",
    "\n",
    "\n",
    "\n",
    "### **üì¢ Final Answer: No Significant Difference in Pizza Quality!**\n",
    "Even though the numbers looked a little different, the Kruskal-Wallis test tells us that **there is no strong evidence that one pizza place is better than the others**.  \n",
    "\n",
    "‚úÖ **If the H value was greater than 5.99, we would say at least one place is significantly different.**  \n",
    "‚úÖ **If we got a significant result, we‚Äôd do a \"post-hoc test\" to find which pizza place is better.**  \n",
    "\n",
    "\n",
    "\n",
    "### **üîπ In the Simplest Terms:**\n",
    "1. **Sort all scores from lowest to highest** and assign ranks.  \n",
    "2. **Add up the ranks for each group.**  \n",
    "3. **Use the formula to compute H.**  \n",
    "4. **Compare H to a threshold (chi-square table).**  \n",
    "5. **If H is too small ‚Üí no difference. If H is big ‚Üí at least one group is different.**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
