{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìö **POS Tagging (Part-of-Speech Tagging) Explained in Simple Terms:**\n",
    "\n",
    "\n",
    "\n",
    "POS Tagging is a **natural language processing (NLP)** technique where we **label each word in a sentence with its corresponding part of speech**, such as noun, verb, adjective, etc.\n",
    "\n",
    "For example:\n",
    "\n",
    "üí¨ **Input Sentence:**  \n",
    "`Suhas is learning machine learning.`\n",
    "\n",
    "üîñ **POS Tagged Sentence:**  \n",
    "`Suhas/NN is/VBZ learning/VBG machine/NN learning/NN.`\n",
    "\n",
    "Here‚Äôs what each tag means:  \n",
    "- **NN** ‚Üí Noun (e.g., person, place, thing)  \n",
    "- **VBZ** ‚Üí Verb (third person singular, present tense)  \n",
    "- **VBG** ‚Üí Verb (present participle/gerund)  \n",
    "\n",
    "Let‚Äôs dive deeper!\n",
    "\n",
    "\n",
    "\n",
    "### üß† **Why is POS Tagging Important?**\n",
    "1. **Understanding Sentence Structure**:  \n",
    "   It helps machines understand how words function in a sentence, making it easier to process text.\n",
    "   \n",
    "2. **Key for NLP Applications**:  \n",
    "   POS tagging is a core step in many NLP tasks like **named entity recognition (NER)**, **sentiment analysis**, **chatbots**, **text summarization**, etc.\n",
    "\n",
    "3. **Disambiguating Words**:  \n",
    "   Some words have **multiple meanings** based on context. For example:  \n",
    "   - **\"Book a room\"** (verb) vs **\"Read a book\"** (noun).  \n",
    "   POS tagging helps resolve this ambiguity.\n",
    "\n",
    "\n",
    "\n",
    "### üîç **How Does POS Tagging Work?**\n",
    "There are two main approaches to POS tagging:\n",
    "\n",
    "#### ‚úÖ **1. Rule-Based POS Tagging**:\n",
    "- Based on a set of **predefined grammatical rules**.\n",
    "- Example Rule:  \n",
    "  - If a word ends with **\"-ed\"**, it‚Äôs likely a **past tense verb (VBD)**.  \n",
    "\n",
    "üí° Example:  \n",
    "`He played football.`  \n",
    "- Rule-Based Tagger applies a rule: Since **\"played\"** ends in **\"-ed\"**, it‚Äôs tagged as **VBD (past tense verb)**.\n",
    "\n",
    "#### ‚úÖ **2. Statistical (Machine Learning) POS Tagging**:\n",
    "- Uses **machine learning algorithms** to tag words based on **probabilities**.\n",
    "- **HMM (Hidden Markov Model)** is one of the popular models used for statistical POS tagging.\n",
    "  \n",
    "üí° Example:  \n",
    "- The tagger is trained on a **large dataset** of sentences with POS tags.\n",
    "- It learns patterns like **\"I/PRP am/VBP\"** is more likely than **\"I/NN am/NN\"**.\n",
    "\n",
    "### üìä **Common POS Tags and Their Meanings (Based on Penn Treebank)**\n",
    "\n",
    "| **Tag**  | **Part of Speech**     | **Example Words**             |\n",
    "|---------|------------------------|--------------------------------|\n",
    "| NN      | Noun                   | dog, car, idea                |\n",
    "| VB      | Verb (base form)       | eat, run, write               |\n",
    "| VBG     | Verb (gerund)          | eating, running, writing      |\n",
    "| JJ      | Adjective              | blue, fast, beautiful         |\n",
    "| RB      | Adverb                 | quickly, silently, very       |\n",
    "| PRP     | Pronoun                | he, she, it                   |\n",
    "| DT      | Determiner             | the, a, an                    |\n",
    "| IN      | Preposition            | in, on, at                    ||\n",
    "\n",
    "\n",
    "### ü§ñ **POS Tagging in Python (Using NLTK Library)**\n",
    "```python\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Sample Sentence\n",
    "sentence = \"Suhas is learning machine learning.\"\n",
    "\n",
    "# Tokenizing and POS Tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Output\n",
    "print(pos_tags)\n",
    "```\n",
    "\n",
    "üîç **Output:**  \n",
    "`[('Suhas', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('machine', 'NN'), ('learning', 'NN')]`\n",
    "\n",
    "\n",
    "### üõ† **POS Tagging Algorithms**  \n",
    "Here are some popular algorithms for POS tagging:\n",
    "\n",
    "| **Algorithm**          | **Description**                                           | **Example Tools**                  |\n",
    "|------------------------|-----------------------------------------------------------|------------------------------------|\n",
    "| Rule-Based Tagger      | Uses hand-crafted rules                                   | NLTK, Stanford POS Tagger          |\n",
    "| HMM (Hidden Markov)    | Based on probabilities and sequences                      | NLTK, spaCy                        |\n",
    "| CRF (Conditional RF)   | Uses conditional random fields                            | Stanford NLP                       |\n",
    "| Neural Networks        | Uses deep learning models (RNNs, LSTMs)                   | spaCy, Flair  \n",
    "\n",
    "\n",
    "\n",
    "### üöÄ **Real-Life Applications of POS Tagging**\n",
    "1. **Chatbots and Virtual Assistants**  \n",
    "   - POS tagging helps assistants like **Siri** or **Alexa** understand user commands.\n",
    "\n",
    "2. **Search Engines**  \n",
    "   - POS tagging improves **search accuracy** by understanding user queries better.\n",
    "\n",
    "3. **Grammar Checkers**  \n",
    "   - Tools like **Grammarly** use POS tagging to identify grammatical errors.\n",
    "\n",
    "\n",
    "\n",
    "### üìñ **Example in a Real Dataset**\n",
    "Consider a sentence from a dataset:\n",
    "\n",
    "üí¨ **Sentence:**  \n",
    "`The quick brown fox jumps over the lazy dog.`\n",
    "\n",
    "üîñ **POS Tags:**  \n",
    "`[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]`\n",
    "\n",
    "\n",
    "\n",
    "### üß© **Challenges in POS Tagging**\n",
    "1. **Ambiguity**:  \n",
    "   - Some words have **multiple tags** depending on the context.  \n",
    "   Example:  \n",
    "   - **\"They can fish\"** ‚Üí \"can\" could be a **modal verb** or a **noun**.\n",
    "\n",
    "2. **Out-of-Vocabulary Words**:  \n",
    "   - Words not present in the training data may not be tagged correctly.\n",
    "\n",
    "\n",
    "\n",
    "### üéØ **Summary**\n",
    "- **POS Tagging** assigns a **part of speech** to each word in a sentence.\n",
    "- It helps machines understand **sentence structure** and improve **NLP applications**.\n",
    "- There are **rule-based** and **statistical approaches** to POS tagging.\n",
    "- Python libraries like **NLTK**, **spaCy**, and **Stanford NLP** make it easy to implement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Examples of POS Tagging:**\n",
    "\n",
    "## üß© **Think of a Sentence as a Group of People in a Room**  \n",
    "\n",
    "Imagine you're at a **party**. There are different people there ‚Äî some are **doctors**, some are **teachers**, some are **students**, and so on. To better understand who is who, you need to **label each person with their role**.\n",
    "\n",
    "Similarly, in a **sentence**, each word plays a role:\n",
    "- Some words are **nouns** (people or things),\n",
    "- Some are **verbs** (actions),\n",
    "- Some are **adjectives** (describing words),\n",
    "- Some are **prepositions** (words that show direction or position).\n",
    "\n",
    "**POS Tagging** is just a way to give a **label** to each word so that we know what role it plays in the sentence.\n",
    "\n",
    "\n",
    "\n",
    "### üìù **Example of POS Tagging in Action**\n",
    "\n",
    "üí¨ **Sentence:**  \n",
    "`Suhas loves pizza.`\n",
    "\n",
    "üîñ **POS Tags:**  \n",
    "- **Suhas** ‚Üí **Noun** (because it's a name)  \n",
    "- **loves** ‚Üí **Verb** (because it's an action)  \n",
    "- **pizza** ‚Üí **Noun** (because it's a thing)\n",
    "\n",
    "Basically, you're saying, **\"Hey, 'Suhas' is a noun, 'loves' is a verb, and 'pizza' is a noun.\"**\n",
    "\n",
    "\n",
    "\n",
    "## ü§î **Why Do We Need POS Tagging?**\n",
    "\n",
    "Think about **Google Assistant** or **Alexa**.  \n",
    "When you say something like, **\"Play happy songs\"**, Alexa needs to know:\n",
    "- **\"Play\"** is an action (verb),\n",
    "- **\"happy\"** is a description (adjective),\n",
    "- **\"songs\"** is a thing (noun).\n",
    "\n",
    "Without understanding these roles, Alexa wouldn't know what you're asking.\n",
    "\n",
    "\n",
    "\n",
    "## üè∑Ô∏è **POS Tags in Everyday Sentences (with Easy Explanations)**\n",
    "\n",
    "Here are a few simple sentences to show how words are tagged:\n",
    "\n",
    "| **Sentence**             | **Word**     | **POS Tag (Role)** | **Explanation**                      |\n",
    "|--------------------------|--------------|--------------------|--------------------------------------|\n",
    "| I am Suhas.              | I            | Pronoun            | Refers to a person (you).            |\n",
    "|                          | am           | Verb               | An action (being).                   |\n",
    "|                          | Suhas        | Noun               | A name (person).                     |\n",
    "| Suhas eats pizza.        | Suhas        | Noun               | A name (person).                     |\n",
    "|                          | eats         | Verb               | An action (eating).                  |\n",
    "|                          | pizza        | Noun               | A thing (food).                      |\n",
    "| The car is red.          | The          | Determiner         | Specifies which car.                 |\n",
    "|                          | car          | Noun               | A thing (vehicle).                   |\n",
    "|                          | is           | Verb               | An action (being).                   |\n",
    "|                          | red          | Adjective          | Describes the car (color).           |\n",
    "\n",
    "\n",
    "\n",
    "## üéâ **Think of It Like Labels on Items in a Store**\n",
    "\n",
    "Imagine you walk into a **supermarket**. There are **labels** on everything:\n",
    "- **Milk** ‚Äì **Dairy**  \n",
    "- **Apples** ‚Äì **Fruits**  \n",
    "- **Bread** ‚Äì **Bakery**\n",
    "\n",
    "POS tagging is like putting **labels** on words in a sentence.\n",
    "\n",
    "\n",
    "\n",
    "### üíª **POS Tagging in Python (Simple Example)**\n",
    "\n",
    "Here‚Äôs how it works in Python using **NLTK**:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "sentence = \"Suhas loves pizza.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "```\n",
    "\n",
    "üîç **Output:**  \n",
    "`[('Suhas', 'NNP'), ('loves', 'VBZ'), ('pizza', 'NN')]`\n",
    "\n",
    "\n",
    "\n",
    "## üòé **Still Confused? Let‚Äôs Use a Simple Story!**\n",
    "\n",
    "### üßô **Story: A Day in Suhas's Life**\n",
    "\n",
    "Let‚Äôs write a small story and see how POS Tagging works.\n",
    "\n",
    "üí¨ **Story:**  \n",
    "`Suhas went to the market and bought apples.`\n",
    "\n",
    "üîñ **POS Tagged Story:**  \n",
    "- **Suhas/NNP** ‚Üí **Proper Noun** (It's your name).  \n",
    "- **went/VBD** ‚Üí **Verb** (past action of going).  \n",
    "- **to/IN** ‚Üí **Preposition** (shows direction).  \n",
    "- **the/DT** ‚Üí **Determiner** (points to something specific).  \n",
    "- **market/NN** ‚Üí **Noun** (a place).  \n",
    "- **and/CC** ‚Üí **Conjunction** (joins two ideas).  \n",
    "- **bought/VBD** ‚Üí **Verb** (past action of buying).  \n",
    "- **apples/NNS** ‚Üí **Noun (plural)** (things you bought).\n",
    "\n",
    "\n",
    "\n",
    "### üéØ **Why Do We Need It in Real Life?**\n",
    "\n",
    "Imagine you're building a **chatbot** for a **pizza ordering app**.  \n",
    "When a user says:  \n",
    "> \"I want a large pizza with extra cheese.\"\n",
    "\n",
    "The chatbot needs to know:\n",
    "- **\"I\"** ‚Üí Pronoun (referring to the user).  \n",
    "- **\"want\"** ‚Üí Verb (action).  \n",
    "- **\"large\"** ‚Üí Adjective (describing the size).  \n",
    "- **\"pizza\"** ‚Üí Noun (the thing they want).  \n",
    "- **\"with\"** ‚Üí Preposition (shows addition).  \n",
    "- **\"extra cheese\"** ‚Üí Noun (what they want on top).\n",
    "\n",
    "Without POS tagging, the bot wouldn't know how to respond.\n",
    "\n",
    "\n",
    "\n",
    "## üìö **Quick Summary**\n",
    "\n",
    "| **Concept**         | **Explanation**                                                  |\n",
    "|---------------------|------------------------------------------------------------------|\n",
    "| POS Tagging         | Labeling each word in a sentence with its part of speech.        |\n",
    "| Why It's Important  | Helps machines understand sentences better.                      |\n",
    "| Common Tags         | Noun, Verb, Adjective, Pronoun, Preposition, Conjunction, etc.   |\n",
    "| Real-Life Use Cases | Chatbots, Google Search, Alexa, Grammar Checkers, etc.           |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé **What is a Hidden Markov Model (HMM)?**\n",
    "\n",
    "A **Hidden Markov Model** is a **statistical model** that helps us predict a **sequence of hidden states** (like POS tags) based on a sequence of **observed events** (like words in a sentence).\n",
    "\n",
    "HMM is widely used in:\n",
    "- **POS tagging**\n",
    "- **Speech recognition**\n",
    "- **Machine translation**\n",
    "- **Named Entity Recognition (NER)**\n",
    "\n",
    "\n",
    "\n",
    "## üß© **Key Concepts of HMM**\n",
    "\n",
    "There are **two main sequences** in HMM:\n",
    "\n",
    "1. **Observed Sequence (Visible Events)**  \n",
    "   The words we see in a sentence.  \n",
    "   Example: `[\"Suhas\", \"eats\", \"pizza\"]`\n",
    "\n",
    "2. **Hidden Sequence (Hidden States)**  \n",
    "   The corresponding POS tags we want to predict.  \n",
    "   Example: `[\"NNP\", \"VBZ\", \"NN\"]`  \n",
    "   - `NNP` = Proper Noun  \n",
    "   - `VBZ` = Verb (third-person singular)  \n",
    "   - `NN` = Noun\n",
    "\n",
    "\n",
    "\n",
    "## üß† **How HMM Works (In Simple Terms)**\n",
    "\n",
    "HMM assumes:\n",
    "1. **The next state (POS tag) depends only on the current state.**  \n",
    "   Example: If you see a **noun**, the next word is likely to be a **verb** or an **adjective**.\n",
    "\n",
    "2. **Each word (observation) is generated by a hidden state (POS tag).**  \n",
    "   Example: The word **\"pizza\"** is more likely to be a **noun**.\n",
    "\n",
    "\n",
    "\n",
    "## üìã **Mathematical Components of HMM**\n",
    "\n",
    "HMM consists of **three probabilities**:\n",
    "\n",
    "1Ô∏è‚É£ **Transition Probability (A)**:  \n",
    "   - The probability of moving from one POS tag to another.  \n",
    "   - Example: P(Verb ‚Üí Noun)\n",
    "\n",
    "2Ô∏è‚É£ **Emission Probability (B)**:  \n",
    "   - The probability of a word being generated by a specific POS tag.  \n",
    "   - Example: P(\"eats\" | Verb)\n",
    "\n",
    "3Ô∏è‚É£ **Initial Probability (œÄ)**:  \n",
    "   - The probability of starting with a specific POS tag.  \n",
    "   - Example: P(Noun at the start of the sentence)\n",
    "\n",
    "\n",
    "\n",
    "### üßÆ **HMM Formula:**\n",
    "\n",
    "For a given sentence **W = [w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]** and hidden states **T = [t‚ÇÅ, t‚ÇÇ, ..., t‚Çô]**, the probability of a POS tag sequence is:\n",
    "\n",
    "$$\n",
    "P(T, W) = P(T_1) \\times P(W_1 | T_1) \\times \\prod_{i=2}^{n} P(T_i | T_{i-1}) \\times P(W_i | T_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ P(T_1) $ = Initial probability of the first tag.  \n",
    "- $ P(T_i | T_{i-1}) $ = Transition probability from the previous tag to the current tag.  \n",
    "- $ P(W_i | T_i) $ = Emission probability of a word given a tag.\n",
    "\n",
    "\n",
    "\n",
    "## ‚öôÔ∏è **Step-by-Step Example of HMM for POS Tagging**\n",
    "\n",
    "Let‚Äôs tag the sentence:  \n",
    "**\"Suhas eats pizza.\"**\n",
    "\n",
    "| Word      | Possible Tags     |\n",
    "|-----------|-------------------|\n",
    "| Suhas     | NNP (Proper Noun) |\n",
    "| eats      | VBZ (Verb)        |\n",
    "| pizza     | NN (Noun)         |\n",
    "\n",
    "\n",
    "\n",
    "### üß© **Step 1: Initial Probabilities (œÄ)**\n",
    "\n",
    "The first word in the sentence is **\"Suhas\"**.\n",
    "\n",
    "- P(NNP) = 0.6  \n",
    "- P(VBZ) = 0.2  \n",
    "- P(NN) = 0.2\n",
    "\n",
    "So, the model starts with **NNP** because it has the highest probability.\n",
    "\n",
    "\n",
    "\n",
    "### üß© **Step 2: Transition Probabilities (A)**\n",
    "\n",
    "Now, the model checks the transition from **NNP ‚Üí VBZ ‚Üí NN**.\n",
    "\n",
    "| From \\ To | NNP   | VBZ   | NN    |\n",
    "|-----------|-------|-------|-------|\n",
    "| NNP       | 0.1   | 0.8   | 0.1   |\n",
    "| VBZ       | 0.2   | 0.1   | 0.7   |\n",
    "| NN        | 0.4   | 0.4   | 0.2   |\n",
    "\n",
    "- P(NNP ‚Üí VBZ) = 0.8  \n",
    "- P(VBZ ‚Üí NN) = 0.7\n",
    "\n",
    "\n",
    "\n",
    "### üß© **Step 3: Emission Probabilities (B)**\n",
    "\n",
    "The model checks how likely a word is to be generated by a tag.\n",
    "\n",
    "| Word    | NNP   | VBZ   | NN    |\n",
    "|---------|-------|-------|-------|\n",
    "| Suhas   | 0.9   | 0.1   | 0.0   |\n",
    "| eats    | 0.1   | 0.8   | 0.1   |\n",
    "| pizza   | 0.0   | 0.1   | 0.9   |\n",
    "\n",
    "- P(\"Suhas\" | NNP) = 0.9  \n",
    "- P(\"eats\" | VBZ) = 0.8  \n",
    "- P(\"pizza\" | NN) = 0.9\n",
    "\n",
    "\n",
    "\n",
    "### üß© **Step 4: Calculating the Most Probable Path**\n",
    "\n",
    "The HMM calculates the most likely sequence of tags using the **Viterbi Algorithm**, which is a dynamic programming approach to find the best path.\n",
    "\n",
    "For our example:  \n",
    "- The most likely sequence of tags is **[NNP, VBZ, NN]**.  \n",
    "- Probability = P(NNP) √ó P(\"Suhas\" | NNP) √ó P(NNP ‚Üí VBZ) √ó P(\"eats\" | VBZ) √ó P(VBZ ‚Üí NN) √ó P(\"pizza\" | NN)\n",
    "\n",
    "\n",
    "\n",
    "## ü§ñ **Viterbi Algorithm (Dynamic Programming for HMM)**\n",
    "\n",
    "The **Viterbi Algorithm** is used to find the **most likely sequence of tags**. It works by:\n",
    "\n",
    "1. Building a **trellis** (grid) of all possible paths.  \n",
    "2. Calculating the **maximum probability** path to each tag.  \n",
    "3. Tracing back the path to find the best sequence.\n",
    "\n",
    "\n",
    "\n",
    "### üí° **Why Use HMM?**\n",
    "\n",
    "1. **Handles Ambiguity**:  \n",
    "   It can resolve ambiguous words based on context.  \n",
    "   Example: **\"run\"** can be a noun or a verb.\n",
    "\n",
    "2. **Considers Dependencies**:  \n",
    "   It looks at **previous words and tags** to predict the next tag.\n",
    "\n",
    "\n",
    "\n",
    "## üõ†Ô∏è **Challenges with HMM**\n",
    "\n",
    "1. **Requires a Lot of Data**:  \n",
    "   HMM needs large amounts of labeled data to estimate probabilities.\n",
    "\n",
    "2. **Assumes Markov Property**:  \n",
    "   It assumes that the next tag depends only on the **previous tag**, which isn‚Äôt always true.\n",
    "\n",
    "3. **Doesn‚Äôt Handle Long-Term Dependencies Well**:  \n",
    "   HMM struggles with **long sentences** where tags depend on distant words.\n",
    "\n",
    "\n",
    "\n",
    "## üìà **HMM vs. Modern Methods**\n",
    "\n",
    "| **Aspect**         | **HMM**            | **Neural Networks**     |\n",
    "|--------------------|--------------------|------------------------|\n",
    "| Accuracy           | Medium             | High                   |\n",
    "| Handles Ambiguity  | Yes                | Yes                    |\n",
    "| Handles Unknown Words | Poor              | Good                   |\n",
    "| Complexity         | Medium             | High                   |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Summary of HMM in POS Tagging**\n",
    "\n",
    "| **Step**            | **Description**                                         |\n",
    "|---------------------|---------------------------------------------------------|\n",
    "| **Step 1**          | Calculate **initial probabilities** (œÄ).                |\n",
    "| **Step 2**          | Calculate **transition probabilities** (A).             |\n",
    "| **Step 3**          | Calculate **emission probabilities** (B).               |\n",
    "| **Step 4**          | Use the **Viterbi Algorithm** to find the best sequence.|\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down **Hidden Markov Models (HMMs)** step by step with a **manual calculation example** for a single sentence.  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 1: Understanding Hidden Markov Models (HMMs)**  \n",
    "An **HMM** consists of:  \n",
    "- **Hidden states** (e.g., `Noun`, `Verb`, `Adjective`)  \n",
    "- **Observations** (words in a sentence)  \n",
    "- **Transition probabilities** (probability of moving from one state to another)  \n",
    "- **Emission probabilities** (probability of a word being generated by a particular state)  \n",
    "- **Initial probabilities** (probability of starting in a given state)  \n",
    "\n",
    "We use **Viterbi Algorithm** to find the most probable sequence of hidden states (POS tags) for a given sentence.\n",
    "\n",
    "\n",
    "\n",
    "## **Step 2: Define an Example Sentence**  \n",
    "Let's take the sentence:  \n",
    "üëâ `\"Suhas works hard\"`  \n",
    "\n",
    "We assume that words can belong to the following **hidden states (POS tags)**:  \n",
    "- **N** (Noun)  \n",
    "- **V** (Verb)  \n",
    "- **Adj** (Adjective)  \n",
    "\n",
    "\n",
    "\n",
    "## **Step 3: Define the HMM Parameters**  \n",
    "\n",
    "### **1Ô∏è‚É£ Initial Probabilities (œÄ)**  \n",
    "Probability of starting with each POS tag:  \n",
    "\n",
    "| POS Tag | Probability |\n",
    "|---------|------------|\n",
    "| N (Noun) | 0.6 |\n",
    "| V (Verb) | 0.3 |\n",
    "| Adj (Adjective) | 0.1 |\n",
    "\n",
    "### **2Ô∏è‚É£ Transition Probabilities (A)**\n",
    "Probability of transitioning from one POS tag to another:  \n",
    "\n",
    "| From ‚Üí To | N (Noun) | V (Verb) | Adj (Adjective) |\n",
    "|-----------|---------|---------|---------|\n",
    "| **N (Noun)** | 0.3 | 0.5 | 0.2 |\n",
    "| **V (Verb)** | 0.2 | 0.3 | 0.5 |\n",
    "| **Adj (Adjective)** | 0.4 | 0.4 | 0.2 |\n",
    "\n",
    "### **3Ô∏è‚É£ Emission Probabilities (B)**\n",
    "Probability of a word being emitted from a given POS tag:  \n",
    "\n",
    "| Word ‚Üí POS | N (Noun) | V (Verb) | Adj (Adjective) |\n",
    "|------------|---------|---------|---------|\n",
    "| **Suhas** | 0.8 | 0.1 | 0.1 |\n",
    "| **works** | 0.1 | 0.7 | 0.2 |\n",
    "| **hard** | 0.1 | 0.2 | 0.7 |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 4: Apply the Viterbi Algorithm (Manual Calculation)**  \n",
    "We will compute probabilities for each step:  \n",
    "\n",
    "### **Step 1: Compute Initial Probabilities for \"Suhas\"**\n",
    "$$\n",
    "V_1(N) = \\pi(N) \\times B(N, Suhas) = 0.6 \\times 0.8 = 0.48\n",
    "$$\n",
    "$$\n",
    "V_1(V) = \\pi(V) \\times B(V, Suhas) = 0.3 \\times 0.1 = 0.03\n",
    "$$\n",
    "$$\n",
    "V_1(Adj) = \\pi(Adj) \\times B(Adj, Suhas) = 0.1 \\times 0.1 = 0.01\n",
    "$$\n",
    "\n",
    "| POS | Probability |\n",
    "|-----|------------|\n",
    "| Noun (N) | 0.48 |\n",
    "| Verb (V) | 0.03 |\n",
    "| Adjective (Adj) | 0.01 |\n",
    "\n",
    "### **Step 2: Compute Probabilities for \"works\"**  \n",
    "For **N ‚Üí works**:  \n",
    "$$\n",
    "V_2(N) = \\max [V_1(N) \\times A(N \\to N), V_1(V) \\times A(V \\to N), V_1(Adj) \\times A(Adj \\to N)] \\times B(N, works)\n",
    "$$\n",
    "$$\n",
    "= \\max [0.48 \\times 0.3, 0.03 \\times 0.2, 0.01 \\times 0.4] \\times 0.1\n",
    "$$\n",
    "$$\n",
    "= \\max [0.144, 0.006, 0.004] \\times 0.1 = 0.0144\n",
    "$$\n",
    "\n",
    "For **V ‚Üí works**:  \n",
    "$$\n",
    "V_2(V) = \\max [0.48 \\times 0.5, 0.03 \\times 0.3, 0.01 \\times 0.4] \\times 0.7\n",
    "$$\n",
    "$$\n",
    "= \\max [0.24, 0.009, 0.004] \\times 0.7 = 0.168\n",
    "$$\n",
    "\n",
    "For **Adj ‚Üí works**:  \n",
    "$$\n",
    "V_2(Adj) = \\max [0.48 \\times 0.2, 0.03 \\times 0.5, 0.01 \\times 0.2] \\times 0.2\n",
    "$$\n",
    "$$\n",
    "= \\max [0.096, 0.015, 0.002] \\times 0.2 = 0.0192\n",
    "$$\n",
    "\n",
    "| POS | Probability |\n",
    "|-----|------------|\n",
    "| Noun (N) | 0.0144 |\n",
    "| Verb (V) | 0.168 |\n",
    "| Adjective (Adj) | 0.0192 |\n",
    "\n",
    "### **Step 3: Compute Probabilities for \"hard\"**  \n",
    "For **N ‚Üí hard**:  \n",
    "$$\n",
    "V_3(N) = \\max [0.0144 \\times 0.3, 0.168 \\times 0.2, 0.0192 \\times 0.4] \\times 0.1\n",
    "$$\n",
    "$$\n",
    "= \\max [0.00432, 0.0336, 0.00768] \\times 0.1 = 0.00336\n",
    "$$\n",
    "\n",
    "For **V ‚Üí hard**:  \n",
    "$$\n",
    "V_3(V) = \\max [0.0144 \\times 0.5, 0.168 \\times 0.3, 0.0192 \\times 0.4] \\times 0.2\n",
    "$$\n",
    "$$\n",
    "= \\max [0.0072, 0.0504, 0.00768] \\times 0.2 = 0.01008\n",
    "$$\n",
    "\n",
    "For **Adj ‚Üí hard**:  \n",
    "$$\n",
    "V_3(Adj) = \\max [0.0144 \\times 0.2, 0.168 \\times 0.5, 0.0192 \\times 0.2] \\times 0.7\n",
    "$$\n",
    "$$\n",
    "= \\max [0.00288, 0.084, 0.00384] \\times 0.7 = 0.0588\n",
    "$$\n",
    "\n",
    "| POS | Probability |\n",
    "|-----|------------|\n",
    "| Noun (N) | 0.00336 |\n",
    "| Verb (V) | 0.01008 |\n",
    "| Adjective (Adj) | 0.0588 |\n",
    "\n",
    "\n",
    "\n",
    "## **Step 5: Determine the Most Likely Sequence**  \n",
    "The most probable final state is **Adj (Adjective)** (highest probability **0.0588**).  \n",
    "We backtrack to find the path:  \n",
    "- `\"hard\"` ‚Üí **Adj**  \n",
    "- `\"works\"` ‚Üí **Verb**  \n",
    "- `\"Suhas\"` ‚Üí **Noun**  \n",
    "\n",
    "**Final POS Tag Sequence:**  \n",
    "üëâ `Noun ‚Üí Verb ‚Üí Adjective`  \n",
    "\n",
    "**Predicted POS tags for `\"Suhas works hard\"`**:  \n",
    "- `Suhas (Noun)`  \n",
    "- `works (Verb)`  \n",
    "- `hard (Adjective)`\n",
    "\n",
    "\n",
    "\n",
    "## **Conclusion**\n",
    "- We manually applied the **Viterbi Algorithm** to decode the sequence.\n",
    "- **HMMs** use **transition, emission, and initial probabilities** to find the most probable sequence of hidden states.\n",
    "- This is how **speech recognition, POS tagging, and NLP models** use HMMs!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
