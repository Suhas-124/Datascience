{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing:\n",
    "\n",
    "\n",
    "\n",
    "Sure! Here's a detailed explanation of each **text preprocessing step in NLP**. These steps are crucial to clean and prepare text data for further analysis or modeling.\n",
    "\n",
    "\n",
    "\n",
    "## üî§ **1. Lowercasing**  \n",
    "**What it is:**  \n",
    "Lowercasing involves converting all text in a dataset to lowercase.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Reduces the number of unique words by treating words like \"Apple\" and \"apple\" as the same.\n",
    "- Helps models focus on word meaning instead of case differences.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"Hello World!\"  \n",
    "- Lowercased: \"hello world!\"\n",
    "\n",
    "\n",
    "\n",
    "## üè∑Ô∏è **2. Remove HTML Tags**  \n",
    "**What it is:**  \n",
    "Removing HTML tags is essential when text data is scraped from websites.\n",
    "\n",
    "**Why it's important:**  \n",
    "- HTML tags don‚Äôt carry useful information for text analysis.\n",
    "- Ensures clean text without formatting or metadata.\n",
    "\n",
    "**Example:**  \n",
    "- Original: `<p>Hello <b>World</b>!</p>`  \n",
    "- Cleaned: \"Hello World!\"\n",
    "\n",
    "\n",
    "\n",
    "## üåê **3. Remove URLs**  \n",
    "**What it is:**  \n",
    "URLs are often present in text, especially in social media or web data.\n",
    "\n",
    "**Why it's important:**  \n",
    "- URLs are typically not useful for sentiment or topic analysis.\n",
    "- Removing them avoids noise in the data.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"Check out https://example.com for more info.\"  \n",
    "- Cleaned: \"Check out for more info.\"\n",
    "\n",
    "\n",
    "\n",
    "## ‚úèÔ∏è **4. Remove Punctuation**  \n",
    "**What it is:**  \n",
    "Removing punctuation marks like commas, periods, exclamation points, etc.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Punctuation doesn‚Äôt contribute to the meaning of words in most text analysis tasks.\n",
    "- Helps standardize the text data.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"Hello, world!\"  \n",
    "- Cleaned: \"Hello world\"\n",
    "\n",
    "\n",
    "\n",
    "## üí¨ **5. Chat Word Treatment**  \n",
    "**What it is:**  \n",
    "Handling informal text that contains abbreviations or slang words commonly used in chats.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Chat words like \"gr8\" (great) or \"u\" (you) can confuse the model.\n",
    "- Normalizing such text improves model performance.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"Hey, hw r u?\"  \n",
    "- Cleaned: \"Hey, how are you?\"\n",
    "\n",
    "\n",
    "\n",
    "## ‚úçÔ∏è **6. Spelling Correction**  \n",
    "**What it is:**  \n",
    "Automatically correcting spelling mistakes in the text.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Misspelled words can reduce the accuracy of NLP models.\n",
    "- Ensures consistent and accurate text representation.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"I am hapy to be here.\"  \n",
    "- Corrected: \"I am happy to be here.\"\n",
    "\n",
    "\n",
    "\n",
    "## üõë **7. Removing Stop Words**  \n",
    "**What it is:**  \n",
    "Stop words are common words like \"the,\" \"is,\" \"and,\" that don‚Äôt carry significant meaning.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Removing stop words reduces noise and helps focus on important terms.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"The cat is on the mat.\"  \n",
    "- Cleaned: \"cat mat\"\n",
    "\n",
    "\n",
    "\n",
    "## üòä **8. Handling Emojis**  \n",
    "**What it is:**  \n",
    "Dealing with emojis in text, either by removing them or converting them to text descriptions.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Emojis can convey sentiment or emotions.\n",
    "- Ignoring emojis might lose valuable information.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"I am so happy üòä\"  \n",
    "- Converted: \"I am so happy [smiling_face]\"\n",
    "\n",
    "\n",
    "\n",
    "## üß© **9. Tokenization**  \n",
    "**What it is:**  \n",
    "Breaking down text into smaller units (tokens), usually words or sentences.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Tokenization is the first step in converting text into a format that models can process.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"Hello world!\"  \n",
    "- Tokenized: [\"Hello\", \"world\"]\n",
    "\n",
    "\n",
    "\n",
    "## üå± **10. Stemming**  \n",
    "**What it is:**  \n",
    "Stemming reduces words to their root form by chopping off suffixes.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Reduces the number of unique words in a dataset.\n",
    "- Helps in grouping words with similar meanings.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"running, runner, runs\"  \n",
    "- Stemmed: \"run\"\n",
    "\n",
    "\n",
    "\n",
    "## üåø **11. Lemmatization**  \n",
    "**What it is:**  \n",
    "Lemmatization reduces words to their base form (lemma) using a vocabulary and rules.\n",
    "\n",
    "**Why it's important:**  \n",
    "- Unlike stemming, lemmatization returns meaningful words.\n",
    "- Ensures grammatical correctness.\n",
    "\n",
    "**Example:**  \n",
    "- Original: \"running, better, cars\"  \n",
    "- Lemmatized: \"run, good, car\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example:\n",
    "\n",
    "Sure! Here's a **complete code example** in Python for each text preprocessing step using common libraries like **re**, **nltk**, **spacy**, and **beautifulsoup**. I'll show each step in a clean and understandable way.\n",
    "\n",
    "Let's get started! üöÄ\n",
    "\n",
    "\n",
    "\n",
    "### üíª **Complete Code Example for Text Preprocessing in NLP**\n",
    "\n",
    "```python\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load Spacy model for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample text for preprocessing\n",
    "text = \"Hello <b>world!</b> Visit us at https://example.com. I'm soooo happy üòä!!! How r u? It's gr8.\"\n",
    "\n",
    "# Step 1: Lowercasing\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Step 2: Remove HTML Tags\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# Step 3: Remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "# Step 4: Remove Punctuation\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Step 5: Chat Word Treatment\n",
    "chat_words_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"r\": \"are\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"hw\": \"how\",\n",
    "    \"hpy\": \"happy\"\n",
    "}\n",
    "\n",
    "def treat_chat_words(text):\n",
    "    words = text.split()\n",
    "    new_text = [chat_words_dict.get(word, word) for word in words]\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# Step 6: Spelling Correction (Simple Example)\n",
    "def correct_spelling(text):\n",
    "    corrections = {\n",
    "        \"hapy\": \"happy\",\n",
    "        \"soooo\": \"so\"\n",
    "    }\n",
    "    words = text.split()\n",
    "    corrected_text = [corrections.get(word, word) for word in words]\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "# Step 7: Removing Stop Words\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Step 8: Handling Emojis\n",
    "def handle_emojis(text):\n",
    "    emoji_dict = {\n",
    "        \"üòä\": \"[smiling_face]\",\n",
    "        \"üò¢\": \"[sad_face]\"\n",
    "    }\n",
    "    for emoji, meaning in emoji_dict.items():\n",
    "        text = text.replace(emoji, meaning)\n",
    "    return text\n",
    "\n",
    "# Step 9: Tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Step 10: Stemming\n",
    "def stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word) for word in words])\n",
    "\n",
    "# Step 11: Lemmatization\n",
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "# Apply all steps\n",
    "text = lowercase_text(text)\n",
    "text = remove_html_tags(text)\n",
    "text = remove_urls(text)\n",
    "text = remove_punctuation(text)\n",
    "text = treat_chat_words(text)\n",
    "text = correct_spelling(text)\n",
    "text = remove_stopwords(text)\n",
    "text = handle_emojis(text)\n",
    "tokens = tokenize_text(text)\n",
    "stemmed_text = stemming(text)\n",
    "lemmatized_text = lemmatization(text)\n",
    "\n",
    "# Print final results\n",
    "print(\"Tokenized Text:\", tokens)\n",
    "print(\"Stemmed Text:\", stemmed_text)\n",
    "print(\"Lemmatized Text:\", lemmatized_text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìã **Explanation of Each Step in the Code:**\n",
    "\n",
    "| Step                  | Function Name           | Description                                     |\n",
    "|-----------------------|-------------------------|-------------------------------------------------|\n",
    "| 1. Lowercasing         | `lowercase_text()`      | Converts text to lowercase                     |\n",
    "| 2. Remove HTML Tags    | `remove_html_tags()`    | Removes HTML tags using BeautifulSoup          |\n",
    "| 3. Remove URLs         | `remove_urls()`         | Removes URLs using regex                       |\n",
    "| 4. Remove Punctuation  | `remove_punctuation()`  | Removes punctuation using regex                |\n",
    "| 5. Chat Word Treatment | `treat_chat_words()`    | Replaces chat words with their full forms      |\n",
    "| 6. Spelling Correction | `correct_spelling()`    | Corrects common spelling errors (custom dict)  |\n",
    "| 7. Removing Stop Words | `remove_stopwords()`    | Removes stop words using NLTK                  |\n",
    "| 8. Handling Emojis     | `handle_emojis()`       | Replaces emojis with text descriptions         |\n",
    "| 9. Tokenization        | `tokenize_text()`       | Tokenizes the text using NLTK                  |\n",
    "| 10. Stemming           | `stemming()`            | Applies stemming using NLTK‚Äôs PorterStemmer    |\n",
    "| 11. Lemmatization      | `lemmatization()`       | Applies lemmatization using Spacy              |\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Output Example:**\n",
    "\n",
    "```shell\n",
    "Tokenized Text: ['hello', 'world', 'visit', 'us', 'im', 'so', 'happy', '[smiling_face]', 'how', 'are', 'you', 'its', 'great']\n",
    "Stemmed Text: hello world visit us im so happi smilingfac how are you it great\n",
    "Lemmatized Text: hello world visit we be so happy [smiling_face] how be you it great\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
