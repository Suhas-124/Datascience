{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the punkt tokenizer from NLTK (for tokenization)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text corpus (usually, you would use a much larger corpus)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"The dog is man's best friend\",\n",
    "    \"The fox is quick and cunning\",\n",
    "    \"The king the protector\",\n",
    "    \"The queen is the most humble\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['never', 'jump', 'over', 'the', 'lazy', 'dog', 'quickly'], ['the', 'dog', 'is', 'man', \"'s\", 'best', 'friend'], ['the', 'fox', 'is', 'quick', 'and', 'cunning'], ['the', 'king', 'the', 'protector'], ['the', 'queen', 'is', 'the', 'most', 'humble']]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize each sentence into words\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec model using the skip-gram method (default in Word2Vec)\n",
    "model = Word2Vec(tokenized_corpus,vector_size=100,window=5,min_count=1,sg=1)\n",
    "\n",
    "# Train Word2Vec model using the CBOW method (sg=0)\n",
    "\n",
    "\n",
    "# Save the model to disk (optional)\n",
    "model.save('word2vec_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most Similar Words to 'dog':\n",
      "humble: 0.18952025473117828\n",
      "quickly: 0.16077181696891785\n",
      "best: 0.15949507057666779\n",
      "jump: 0.13699346780776978\n"
     ]
    }
   ],
   "source": [
    "# Let's now check some basic operations:\n",
    "\n",
    "# 1. **Find similar words** to a target word\n",
    "similar_words = model.wv.most_similar('dog',topn=4)\n",
    "print(\"\\nMost Similar Words to 'dog':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. **Find word analogy**: king - man + woman = ?\n",
    "# analogy = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "# print(\"\\nAnalogy: king - man + woman = ?\")\n",
    "# print(f\"Result: {analogy[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vector representation of 'fox':\n",
      "[-0.00713902  0.00124103 -0.00717672 -0.00224462  0.0037193   0.00583312\n",
      "  0.00119818  0.00210273 -0.00411039  0.00722533 -0.00630704  0.00464721\n",
      " -0.00821997  0.00203647 -0.00497705 -0.00424769 -0.00310899  0.00565521\n",
      "  0.0057984  -0.00497465  0.00077333 -0.00849578  0.00780981  0.00925729\n",
      " -0.00274233  0.00080022  0.00074665  0.00547788 -0.00860608  0.00058445\n",
      "  0.00686942  0.00223159  0.00112468 -0.00932216  0.00848237 -0.00626413\n",
      " -0.00299237  0.00349379 -0.00077263  0.00141129  0.00178199 -0.0068289\n",
      " -0.00972481  0.00904058  0.00619805 -0.00691293  0.00340348  0.00020606\n",
      "  0.00475374 -0.00711994  0.00402695  0.00434743  0.00995737 -0.00447374\n",
      " -0.00138927 -0.00731732 -0.00969783 -0.00908026 -0.00102276 -0.00650329\n",
      "  0.00484973 -0.00616403  0.00251919  0.00073944 -0.00339216 -0.00097922\n",
      "  0.00997912  0.00914589 -0.00446183  0.00908303 -0.00564176  0.00593092\n",
      " -0.00309722  0.00343175  0.00301723  0.00690046 -0.00237388  0.00877504\n",
      "  0.00758943 -0.00954765 -0.00800821 -0.0076379   0.00292326 -0.00279472\n",
      " -0.00692952 -0.00812826  0.00830918  0.00199049 -0.00932802 -0.00479272\n",
      "  0.00313674 -0.00471321  0.00528084 -0.00423344  0.00264179 -0.00804569\n",
      "  0.00620989  0.00481889  0.00078719  0.00301345]\n"
     ]
    }
   ],
   "source": [
    "# 3. **Get vector representation of a word** (e.g., \"fox\")\n",
    "\n",
    "fox_vector = model.wv['fox']\n",
    "print(\"\\nVector representation of 'fox':\")\n",
    "print(fox_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity between 'dog' and 'fox': 0.004465762991458178\n"
     ]
    }
   ],
   "source": [
    "# 4. **Check similarity between two words**: dog and fox\n",
    "\n",
    "similarity = model.wv.similarity('dog','fox')\n",
    "print(f\"\\nSimilarity between 'dog' and 'fox': {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'cat' not found in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# 5. **Check the vector of a word** that may not appear in the corpus\n",
    "# Note: This won't work if the word doesn't appear in the vocabulary\n",
    "try:\n",
    "    cat_vector = model.wv['cat']\n",
    "    print(\"\\nVector representation of 'cat':\")\n",
    "    print(cat_vector)\n",
    "except KeyError:\n",
    "    print(\"\\n'cat' not found in the vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Corpus: [['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog'], ['never', 'jump', 'over', 'the', 'lazy', 'dog', 'quickly'], ['the', 'dog', 'is', 'man', \"'s\", 'best', 'friend'], ['the', 'fox', 'is', 'quick', 'and', 'cunning']]\n",
      "\n",
      "Most Similar Words to 'dog':\n",
      "'s: 0.16072362661361694\n",
      "man: 0.1373775154352188\n",
      "best: 0.12302176654338837\n",
      "\n",
      "Vector representation of 'fox':\n",
      "[-8.2442528e-03  9.3009099e-03 -1.9279649e-04 -1.9608135e-03\n",
      "  4.6023987e-03 -4.0986156e-03  2.7443201e-03  6.9370107e-03\n",
      "  6.0579553e-03 -7.5167711e-03  9.3802791e-03  4.6714144e-03\n",
      "  3.9629950e-03 -6.2402389e-03  8.4646065e-03 -2.1446063e-03\n",
      "  8.8347802e-03 -5.3538652e-03 -8.1350775e-03  6.8160607e-03\n",
      "  1.6739438e-03 -2.1977786e-03  9.5240278e-03  9.4875079e-03\n",
      " -9.7718211e-03  2.5138054e-03  6.1488538e-03  3.8735813e-03\n",
      "  2.0183569e-03  4.4099652e-04  6.7934365e-04 -3.8170179e-03\n",
      " -7.1389196e-03 -2.0966420e-03  3.9263223e-03  8.8186637e-03\n",
      "  9.2534628e-03 -5.9735756e-03 -9.4031645e-03  9.7659072e-03\n",
      "  3.4380513e-03  5.1579997e-03  6.2808064e-03 -2.8026453e-03\n",
      "  7.3252083e-03  2.8296341e-03  2.8678267e-03 -2.3856002e-03\n",
      " -3.1219830e-03 -2.3717002e-03  4.2749033e-03  6.7538196e-05\n",
      " -9.5843347e-03 -9.6664038e-03 -6.1540646e-03 -1.1920498e-04\n",
      "  1.9976143e-03  9.4343210e-03  5.5811293e-03 -4.2829695e-03\n",
      "  2.8142531e-04  4.9655857e-03  7.6959552e-03 -1.1389224e-03\n",
      "  4.3282970e-03 -5.8075045e-03 -8.0304703e-04  8.1041995e-03\n",
      " -2.3571099e-03 -9.6642831e-03  5.7781776e-03 -3.9341091e-03\n",
      " -1.2217636e-03  9.9858195e-03 -2.2539855e-03 -4.7562118e-03\n",
      " -5.3291605e-03  6.9810147e-03 -5.7051871e-03  2.1073644e-03\n",
      " -5.2611805e-03  6.1188792e-03  4.3562236e-03  2.6074224e-03\n",
      " -1.4895225e-03 -2.7479446e-03  9.0010948e-03  5.2169478e-03\n",
      " -2.1589508e-03 -9.4760470e-03 -7.4218549e-03 -1.0649411e-03\n",
      " -7.9942483e-04 -2.5653637e-03  9.6768867e-03 -4.5769481e-04\n",
      "  5.8814469e-03 -7.4551087e-03 -2.5115497e-03 -5.5463086e-03]\n",
      "\n",
      "Similarity between 'dog' and 'fox': 0.06841891258955002\n",
      "\n",
      "'cat' not found in the vocabulary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer from NLTK (for tokenization)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text corpus (usually, you would use a much larger corpus)\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Never jump over the lazy dog quickly\",\n",
    "    \"The dog is man's best friend\",\n",
    "    \"The fox is quick and cunning\"\n",
    "]\n",
    "\n",
    "# Tokenize each sentence into words\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Print the tokenized corpus to understand the structure\n",
    "print(\"Tokenized Corpus:\", tokenized_corpus)\n",
    "\n",
    "# Train Word2Vec model using the CBOW method (sg=0) # sg=1 for skip gram\n",
    "model = Word2Vec(tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the model to disk (optional)\n",
    "model.save(\"word2vec_model_cbow\")\n",
    "\n",
    "# Let's now check some basic operations:\n",
    "\n",
    "# 1. **Find similar words** to a target word\n",
    "similar_words = model.wv.most_similar(\"dog\", topn=3)\n",
    "print(\"\\nMost Similar Words to 'dog':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n",
    "\n",
    "# # 2. **Find word analogy**: king - man + woman = ?\n",
    "# analogy = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
    "# print(\"\\nAnalogy: king - man + woman = ?\")\n",
    "# print(f\"Result: {analogy[0][0]}\")\n",
    "\n",
    "# 3. **Get vector representation of a word** (e.g., \"fox\")\n",
    "fox_vector = model.wv['fox']\n",
    "print(\"\\nVector representation of 'fox':\")\n",
    "print(fox_vector)\n",
    "\n",
    "# 4. **Check similarity between two words**: dog and fox\n",
    "similarity = model.wv.similarity('dog', 'fox')\n",
    "print(f\"\\nSimilarity between 'dog' and 'fox': {similarity}\")\n",
    "\n",
    "# 5. **Check the vector of a word** that may not appear in the corpus\n",
    "# Note: This won't work if the word doesn't appear in the vocabulary\n",
    "try:\n",
    "    cat_vector = model.wv['cat']\n",
    "    print(\"\\nVector representation of 'cat':\")\n",
    "    print(cat_vector)\n",
    "except KeyError:\n",
    "    print(\"\\n'cat' not found in the vocabulary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
