{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Data:\n",
    "\n",
    "Let‚Äôs dive deep into the types of techniques used to handle **imbalanced data** in machine learning. I'll explain **undersampling**, **oversampling**, **SMOTE**, **ensemble methods**, and **cost-sensitive learning** in a **simple, layman-friendly manner with examples and code snippets** to help you get a complete understanding.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **1. Undersampling (Reducing Majority Class)**\n",
    "### üß© What is it?\n",
    "In **undersampling**, we reduce the number of samples from the majority class to balance the dataset. This is done by **randomly removing data points** from the majority class so that the number of instances is closer to the minority class.\n",
    "\n",
    "### üìå **When to Use?**\n",
    "- When the dataset is **large**, and you can afford to lose some majority class samples without losing too much information.\n",
    "\n",
    "\n",
    "\n",
    "### **üìñ Example (Code)**\n",
    "```python\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "print(f'Original Dataset: {Counter(y)}')\n",
    "\n",
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "print(f'Resampled Dataset: {Counter(y_res)}')\n",
    "```\n",
    "\n",
    "### ‚ö° **Advantages:**\n",
    "- Simple and easy to implement.\n",
    "- Reduces computation time.\n",
    "\n",
    "### ‚ö†Ô∏è **Disadvantages:**\n",
    "- Can **lose important information** by removing data.\n",
    "- Not suitable when the dataset is small.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **2. Oversampling (Increasing Minority Class)**\n",
    "### üß© What is it?\n",
    "In **oversampling**, we increase the number of samples in the minority class by **duplicating existing samples** or **creating synthetic samples**.\n",
    "\n",
    "### üìå **When to Use?**\n",
    "- When the dataset is **small**, and you need more data to help the model learn patterns.\n",
    "\n",
    "\n",
    "\n",
    "### **üìñ Example (Code)**\n",
    "```python\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Apply Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "print(f'Resampled Dataset: {Counter(y_res)}')\n",
    "```\n",
    "\n",
    "### ‚ö° **Advantages:**\n",
    "- Helps improve the model's ability to detect the minority class.\n",
    "- Prevents the model from becoming biased toward the majority class.\n",
    "\n",
    "### ‚ö†Ô∏è **Disadvantages:**\n",
    "- **Overfitting risk**: Repeated samples can lead to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **3. SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "### üß© What is it?\n",
    "**SMOTE** is a more advanced version of oversampling. Instead of duplicating existing samples, it **creates synthetic samples** by interpolating between existing minority class samples.\n",
    "\n",
    "Think of it like this: if you have a few minority class samples, SMOTE creates new samples by **drawing lines between them and generating points along those lines**.\n",
    "\n",
    "### üìå **When to Use?**\n",
    "- When you want to avoid overfitting caused by simple oversampling.\n",
    "- When the minority class is small.\n",
    "\n",
    "\n",
    "\n",
    "### **üìñ Example (Code)**\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "print(f'Resampled Dataset: {Counter(y_res)}')\n",
    "```\n",
    "\n",
    "### ‚ö° **Advantages:**\n",
    "- Reduces overfitting.\n",
    "- Generates more realistic data points.\n",
    "\n",
    "### ‚ö†Ô∏è **Disadvantages:**\n",
    "- Can introduce **noise** by creating unrealistic samples.\n",
    "- Computationally more expensive than simple oversampling.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **4. Ensemble Methods**\n",
    "### üß© What is it?\n",
    "Ensemble methods combine **multiple models** to improve the overall performance. These methods can be modified to handle imbalanced data by **giving more importance to the minority class** or by **combining predictions from different balanced subsets of the data**.\n",
    "\n",
    "### üìå **Types of Ensemble Methods for Imbalanced Data:**\n",
    "1. **Balanced Random Forest**:\n",
    "   - Modifies Random Forest by **undersampling** the majority class for each tree.\n",
    "   \n",
    "2. **EasyEnsemble**:\n",
    "   - Uses **multiple undersampled subsets** of the majority class to train multiple models and **combine their predictions**.\n",
    "\n",
    "3. **Boosting Algorithms** (e.g., XGBoost, LightGBM, CatBoost):\n",
    "   - These algorithms focus more on **misclassified samples**, which often belong to the minority class.\n",
    "\n",
    "\n",
    "\n",
    "### **üìñ Example (Code) - Balanced Random Forest**\n",
    "```python\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply Balanced Random Forest\n",
    "clf = BalancedRandomForestClassifier(random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X)\n",
    "```\n",
    "\n",
    "### ‚ö° **Advantages:**\n",
    "- Improves the model's performance on the minority class.\n",
    "- Reduces bias toward the majority class.\n",
    "\n",
    "### ‚ö†Ô∏è **Disadvantages:**\n",
    "- More computationally expensive than other methods.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **5. Cost-Sensitive Learning**\n",
    "### üß© What is it?\n",
    "In **cost-sensitive learning**, we modify the model to **assign different penalties (costs)** for misclassifying the minority and majority classes.\n",
    "\n",
    "For example:\n",
    "- Misclassifying a **minority class** instance (e.g., fraud detection) is more **costly** than misclassifying a majority class instance.\n",
    "\n",
    "Most algorithms allow you to set a **`class_weight`** parameter to make the model pay more attention to the minority class.\n",
    "\n",
    "\n",
    "\n",
    "### **üìñ Example (Code) - Cost-Sensitive Logistic Regression**\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply Cost-Sensitive Logistic Regression\n",
    "clf = LogisticRegression(class_weight='balanced', random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X)\n",
    "```\n",
    "\n",
    "### ‚ö° **Advantages:**\n",
    "- No need to modify the dataset.\n",
    "- Prevents bias toward the majority class.\n",
    "\n",
    "### ‚ö†Ô∏è **Disadvantages:**\n",
    "- Requires careful tuning of the cost/weight values.\n",
    "\n",
    "# üí° **Summary Comparison Table**\n",
    "\n",
    "| **Method**             | **Description**                                         | **Advantages**                      | **Disadvantages**                  |\n",
    "|------------------------|---------------------------------------------------------|-------------------------------------|------------------------------------|\n",
    "| **Undersampling**       | Reduce majority class samples                           | Simple to implement, fast           | May lose valuable data             |\n",
    "| **Oversampling**        | Duplicate or create new samples in minority class       | Improves minority class detection   | Risk of overfitting                |\n",
    "| **SMOTE**               | Create synthetic samples for the minority class         | Reduces overfitting risk            | Can introduce noise                |\n",
    "| **Ensemble Methods**    | Combine multiple models to improve performance          | Effective for imbalanced data       | Computationally expensive          |\n",
    "| **Cost-Sensitive Learning** | Modify the model to penalize misclassifications more | No need to modify the dataset       | Requires careful tuning            |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **Conclusion**\n",
    "- **Undersampling**: Works well when the dataset is large.\n",
    "- **Oversampling**: Good when the dataset is small but can cause overfitting.\n",
    "- **SMOTE**: More advanced oversampling that creates synthetic samples.\n",
    "- **Ensemble Methods**: Combine multiple models to improve performance.\n",
    "- **Cost-Sensitive Learning**: Adjusts the model to focus more on the minority class.\n",
    "\n",
    "Handling **imbalanced data** properly is essential for building models that can **generalize well** and make **accurate predictions** on both majority and minority classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemle Methods:\n",
    "\n",
    "Let‚Äôs dive **deep** into **ensemble methods for imbalanced data** with a clear and **layman-friendly explanation**. I'll cover **why ensemble methods work**, **different types of ensemble techniques**, and provide **step-by-step examples** with **code snippets** to make it easy for you to grasp.\n",
    "\n",
    "\n",
    "\n",
    "# üß© **What Are Ensemble Methods?**\n",
    "An **ensemble method** combines **multiple models** (weak learners) to create a **stronger model** that performs better than individual models. \n",
    "\n",
    "For **imbalanced data**, ensemble methods are modified to:\n",
    "- **Focus more on the minority class.**\n",
    "- **Reduce bias toward the majority class.**\n",
    "\n",
    "Ensemble methods are particularly useful for imbalanced data because **simple models often fail to detect the minority class**, but combining multiple models can improve the performance on both classes.\n",
    "\n",
    "\n",
    "\n",
    "# üîé **Why Use Ensemble Methods for Imbalanced Data?**\n",
    "When dealing with **imbalanced data**, traditional algorithms like **Logistic Regression** or **Decision Trees** tend to predict the **majority class more often**, ignoring the minority class. Ensemble methods solve this by:\n",
    "- **Sampling data smartly** (undersampling/oversampling within the ensemble).\n",
    "- **Adjusting the model's focus** toward the minority class.\n",
    "- **Combining predictions from multiple models** to get better accuracy.\n",
    "\n",
    "\n",
    "\n",
    "# üéØ **Types of Ensemble Methods for Imbalanced Data**\n",
    "\n",
    "Here are the **four main types** of ensemble methods you can use for imbalanced data:\n",
    "\n",
    "| **Method**                  | **Description**                                      |\n",
    "|-----------------------------|------------------------------------------------------|\n",
    "| **Balanced Random Forest**   | Uses undersampling of the majority class in each tree. |\n",
    "| **Easy Ensemble**            | Uses multiple undersampled subsets to train different models. |\n",
    "| **Boosting (e.g., AdaBoost, XGBoost)** | Focuses more on misclassified samples, often the minority class. |\n",
    "| **Bagging with SMOTE**       | Combines oversampling (SMOTE) with bagging to improve minority class detection. |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üèãÔ∏è **1. Balanced Random Forest**\n",
    "\n",
    "### üîé **What Is It?**\n",
    "The **Balanced Random Forest** is a variation of the **Random Forest** algorithm designed specifically for imbalanced data. \n",
    "\n",
    "### ‚öôÔ∏è **How It Works:**\n",
    "- For each tree in the forest, it **randomly undersamples** the majority class to balance the dataset.\n",
    "- This prevents the model from becoming biased toward the majority class.\n",
    "\n",
    "\n",
    "\n",
    "### üìñ **Balanced Random Forest Code Example**\n",
    "```python\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply Balanced Random Forest\n",
    "clf = BalancedRandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Advantages of Balanced Random Forest:**\n",
    "- Reduces bias toward the majority class.\n",
    "- Works well with large datasets.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- Can be **computationally expensive** for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üèãÔ∏è **2. Easy Ensemble**\n",
    "\n",
    "### üîé **What Is It?**\n",
    "**Easy Ensemble** is a **bagging-based technique** that uses **multiple undersampled subsets** of the majority class to train multiple models.\n",
    "\n",
    "Think of it like this:\n",
    "- Instead of using one large dataset, **split the majority class into smaller balanced subsets**.\n",
    "- Train a model on each subset and **combine their predictions**.\n",
    "\n",
    "\n",
    "\n",
    "### üìñ **Easy Ensemble Code Example**\n",
    "```python\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply Easy Ensemble\n",
    "eec = EasyEnsembleClassifier(n_estimators=10, random_state=42)\n",
    "eec.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = eec.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Advantages of Easy Ensemble:**\n",
    "- Combines the power of **multiple undersampled datasets**.\n",
    "- **Less prone to overfitting** compared to simple undersampling.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- Computationally more expensive than simple undersampling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üèãÔ∏è **3. Boosting (e.g., AdaBoost, XGBoost, LightGBM)**\n",
    "\n",
    "### üîé **What Is It?**\n",
    "Boosting is an **iterative technique** where models are trained sequentially, and each new model focuses on **correcting the mistakes** made by the previous models. \n",
    "\n",
    "For **imbalanced data**, boosting algorithms:\n",
    "- Focus more on **misclassified samples** (which are often from the minority class).\n",
    "- Adjust the **sample weights** so that the minority class gets more attention.\n",
    "\n",
    "\n",
    "\n",
    "### üìñ **Boosting Code Example (Using XGBoost)**\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply XGBoost Classifier\n",
    "xgb = XGBClassifier(scale_pos_weight=9, random_state=42)  # Adjust weight for the minority class\n",
    "xgb.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Advantages of Boosting:**\n",
    "- Works well for both **small and large datasets**.\n",
    "- Focuses on **hard-to-classify samples**, improving minority class detection.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- Can be **prone to overfitting** if not tuned properly.\n",
    "- **Requires careful tuning** of hyperparameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üèãÔ∏è **4. Bagging with SMOTE**\n",
    "\n",
    "### üîé **What Is It?**\n",
    "This method combines **bagging** (bootstrap aggregation) with **SMOTE** (Synthetic Minority Over-sampling Technique). \n",
    "\n",
    "Here‚Äôs how it works:\n",
    "1. Apply **bagging** to split the dataset into multiple subsets.\n",
    "2. Use **SMOTE** to oversample the minority class in each subset.\n",
    "3. Train a model on each subset and combine their predictions.\n",
    "\n",
    "\n",
    "\n",
    "### üìñ **Bagging with SMOTE Code Example**\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], n_samples=1000, random_state=42)\n",
    "\n",
    "# Apply Bagging with SMOTE\n",
    "pipeline = make_pipeline(SMOTE(random_state=42), DecisionTreeClassifier())\n",
    "bagging_clf = BaggingClassifier(base_estimator=pipeline, n_estimators=10, random_state=42)\n",
    "bagging_clf.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = bagging_clf.predict(X)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y, y_pred))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Advantages of Bagging with SMOTE:**\n",
    "- Handles **imbalanced data effectively** by creating synthetic samples.\n",
    "- Reduces **overfitting** risk.\n",
    "\n",
    "### ‚ùå **Disadvantages:**\n",
    "- More computationally expensive.\n",
    "\n",
    "---\n",
    "\n",
    "# üí° **Summary Comparison Table**\n",
    "\n",
    "| **Method**             | **Description**                                    | **When to Use?**                         | **Example Algorithms**             |\n",
    "|------------------------|----------------------------------------------------|-----------------------------------------|------------------------------------|\n",
    "| **Balanced Random Forest** | Randomly undersamples the majority class per tree | For large datasets                      | BalancedRandomForestClassifier     |\n",
    "| **Easy Ensemble**       | Trains multiple models on different undersampled sets | When you want multiple models combined  | EasyEnsembleClassifier             |\n",
    "| **Boosting**            | Focuses on misclassified samples                  | For both small and large datasets       | XGBoost, AdaBoost, LightGBM        |\n",
    "| **Bagging with SMOTE**  | Combines SMOTE with bagging                      | When you need synthetic samples         | BaggingClassifier + SMOTE          |\n",
    "\n",
    "\n",
    "\n",
    "# üöÄ **Final Thoughts**\n",
    "- **Ensemble methods** are powerful for handling **imbalanced data** because they combine multiple models to make better predictions.\n",
    "- Techniques like **Balanced Random Forest**, **Easy Ensemble**, **Boosting**, and **Bagging with SMOTE** ensure that **minority classes get enough attention**.\n",
    "- These methods can significantly **improve model performance** for tasks like **fraud detection**, **disease diagnosis**, and more!\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
