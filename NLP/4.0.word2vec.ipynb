{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟 **Full Explanation of Word2Vec** 🌟\n",
    "\n",
    "**Word2Vec** is a revolutionary technique in Natural Language Processing (NLP) that creates **vector representations** (embeddings) of words, capturing their **semantic relationships** based on the contexts in which they appear. Developed by **Tomas Mikolov** and his team at **Google** in 2013, Word2Vec allows us to map words to **dense, low-dimensional vectors** such that words with similar meanings are represented by vectors that are **close together** in a vector space.\n",
    "\n",
    "#### Why is Word2Vec Important?\n",
    "\n",
    "Before Word2Vec, traditional methods like **Bag-of-Words (BoW)** used **sparse vectors** for word representation, making it difficult to capture relationships between words. Word2Vec’s **dense vectors** enable us to capture these relationships, making it a game-changer in NLP. \n",
    "\n",
    "For example:\n",
    "- The words **\"king\"** and **\"queen\"** will have **similar vector representations** because they appear in similar **contextual relationships**, whereas **\"king\"** and **\"dog\"** will be quite different.\n",
    "\n",
    "### Key Concepts of Word2Vec ✨\n",
    "\n",
    "#### 🧠 **Word Embeddings**\n",
    "- **Embeddings** are the real-valued vectors that represent words in a **low-dimensional space**. \n",
    "- Words with **similar meanings** (like **\"man\"** and **\"woman\"**) will be **close** in this vector space.\n",
    "\n",
    "#### 🧳 **Context and Target Words**\n",
    "- In Word2Vec, we focus on words in **context**. \n",
    "- A **target word** is the word we want to predict or represent, and the **context words** are the words surrounding the target word.\n",
    "\n",
    "#### 🌍 **Vector Space**\n",
    "- Word2Vec embeds words into a **vector space** where semantically similar words are **closer together**. For instance, **\"man\"** and **\"woman\"** will have similar vectors because they often appear in similar contexts (e.g., \"king\" and \"queen\" in a **royalty context**).\n",
    "\n",
    "\n",
    "\n",
    "### Word2Vec Training Models 🎓\n",
    "\n",
    "There are **two primary models** in Word2Vec that determine how the word embeddings are learned:\n",
    "\n",
    "#### 🏗️ **1. Continuous Bag of Words (CBOW)**\n",
    "\n",
    "- **Goal**: Predict the **target word** using **surrounding context words**.\n",
    "- **How it works**: CBOW takes a context window of surrounding words and tries to predict the word in the middle. For example:\n",
    "  - Given the sentence: **\"The cat sat on the mat\"**\n",
    "  - If **\"sat\"** is the target word, the context words would be **[\"The\", \"cat\", \"on\", \"the\", \"mat\"]**.\n",
    "  \n",
    "#### 🚀 **2. Skip-Gram Model**\n",
    "\n",
    "- **Goal**: Predict the **context words** given a **target word**.\n",
    "- **How it works**: The Skip-Gram model takes a **target word** and predicts the surrounding context words. For example:\n",
    "  - Given the word **\"sat\"**, it tries to predict the context words like **[\"The\", \"cat\", \"on\", \"the\", \"mat\"]**.\n",
    "\n",
    "\n",
    "\n",
    "### **Training Objective Function (Mathematical Deep Dive)** 🔢\n",
    "\n",
    "Word2Vec works by optimizing a **loss function** that encourages the model to **predict context words** for each target word (or vice versa). This involves calculating the **probability** of context words appearing around a target word, based on their **vector representations**.\n",
    "\n",
    "#### **Softmax vs. Negative Sampling**\n",
    "- Instead of computing the probability over the entire vocabulary, Word2Vec uses **Negative Sampling**, which randomly selects a few **negative examples** (words that aren’t context words) to simplify training.\n",
    "\n",
    "\n",
    "\n",
    "### **Word2Vec Advantages** 🌈\n",
    "\n",
    "1. **Semantic Relationships**: \n",
    "   - Word2Vec can discover **meaningful relationships** between words. For example, it can capture the relationship **\"king\" - \"man\" + \"woman\" = \"queen\"**. Amazing, right? 💡\n",
    "\n",
    "2. **Efficient**:\n",
    "   - It’s much more efficient than older methods like **TF-IDF** or **One-Hot Encoding**. Word2Vec can handle **large corpora** with **dense embeddings** that save space and computational resources.\n",
    "\n",
    "3. **Contextual Understanding**:\n",
    "   - Word2Vec isn’t just about counting word occurrences. It **understands context**, meaning that words appearing in similar contexts will have **similar representations**.\n",
    "\n",
    "4. **Transfer Learning**:\n",
    "   - You can take **pre-trained embeddings** and use them for downstream tasks like **sentiment analysis**, **named entity recognition**, and **machine translation**, boosting their performance.\n",
    "\n",
    "\n",
    "\n",
    "### **Applications of Word2Vec** 🚀\n",
    "\n",
    "1. **Word Similarity & Analogy**: \n",
    "   - Want to know how **similar** two words are? Word2Vec helps you find out! For instance, **\"dog\"** and **\"cat\"** will be closer in the vector space than **\"dog\"** and **\"car\"**.\n",
    "   - You can also solve analogies like **\"man\" - \"woman\" = \"king\" - \"queen\"**.\n",
    "\n",
    "2. **Sentiment Analysis**: \n",
    "   - Word2Vec can capture **the meaning of words** in a sentence, making it useful for understanding whether text is positive, negative, or neutral.\n",
    "\n",
    "3. **Recommendation Systems**: \n",
    "   - By comparing **embeddings** of words or products, Word2Vec can help recommend similar items, just like how **Amazon** suggests products based on what you’ve bought before. \n",
    "\n",
    "4. **Machine Translation**: \n",
    "   - Word2Vec can be used to map words between languages by learning that similar words in different languages will have **similar vector representations**.\n",
    "\n",
    "5. **Document Clustering**: \n",
    "   - By averaging the embeddings of words in a document, Word2Vec can represent an entire document and help group similar documents together.\n",
    "\n",
    "\n",
    "\n",
    "### **Word2Vec vs GloVe vs FastText** 🔥\n",
    "\n",
    "- **GloVe**: Unlike Word2Vec, which focuses on local context, **GloVe** captures **global co-occurrence statistics** from the corpus.\n",
    "- **FastText**: This is a **word representation extension** to Word2Vec that represents words as **character n-grams**, enabling it to handle **out-of-vocabulary words** more effectively.\n",
    "\n",
    "\n",
    "\n",
    "### 🌟 **Conclusion**\n",
    "\n",
    "Word2Vec is **transforming** the way we understand and work with language. By using **dense embeddings** to represent words, it can capture complex **semantic relationships** between words and provide better models for tasks like **translation**, **sentiment analysis**, and **recommendation systems**.\n",
    "\n",
    "It's a powerful tool for anyone working with text data, and its ability to **learn from context** is a breakthrough in NLP. Whether you're solving analogies or improving recommendation engines, Word2Vec has got your back! 🚀💬\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Word2vec:\n",
    "\n",
    "Absolutely! Let me break it down in simple terms and add some color for clarity.\n",
    "\n",
    "\n",
    "\n",
    "### What is **Word2Vec**? 🤔\n",
    "\n",
    "Imagine you're trying to understand the **meaning of words** by looking at the **words around them** in a sentence. For example:\n",
    "\n",
    "- **\"The dog is barking.\"** 🐕\n",
    "- **\"The cat is purring.\"** 🐱\n",
    "\n",
    "Here, the words **dog** and **cat** are quite similar in meaning, right? They both refer to animals, and their **actions** (barking, purring) are verbs related to animals. Even though they are **different words**, they have **similar meanings**.\n",
    "\n",
    "This is the basic idea behind **Word2Vec**.\n",
    "\n",
    "### What does Word2Vec do? 🤖\n",
    "\n",
    "Word2Vec helps a computer **understand the meaning of words** by turning them into numbers (called **vectors**). These vectors are like **coordinates** in a **2D map**, where similar words are placed **closer together**.\n",
    "\n",
    "So, in the **vector space** of Word2Vec:\n",
    "\n",
    "- The word **\"dog\"** 🐕 will be closer to **\"cat\"** 🐱 than to **\"car\"** 🚗, because dogs and cats are animals and share similar meanings.\n",
    "- But **\"dog\"** 🐕 will be far away from **\"car\"** 🚗 because they are different types of things.\n",
    "\n",
    "### How does it work? ⚙️\n",
    "\n",
    "Word2Vec is like a **learning machine** that looks at large amounts of text and tries to figure out which words appear together in similar contexts. \n",
    "\n",
    "There are **two main ways** Word2Vec learns this:\n",
    "\n",
    "\n",
    "\n",
    "### **1. CBOW (Continuous Bag of Words)** 🧠\n",
    "\n",
    "- **Goal**: Predict the middle word in a sentence, given the surrounding words.\n",
    "  \n",
    "  **Example**:\n",
    "  - If we look at the sentence:  \n",
    "    **\"The dog is barking.\"**\n",
    "  - We try to **predict** the word **\"is\"** by looking at the context words: **\"The\"**, **\"dog\"**, **\"barking\"**.\n",
    "\n",
    "In simple terms: **Given the surrounding words, we try to guess the center word.** \n",
    "\n",
    "\n",
    "\n",
    "### **2. Skip-Gram** 🔄\n",
    "\n",
    "- **Goal**: Predict the surrounding words, given a middle word.\n",
    "\n",
    "  **Example**:\n",
    "  - If the middle word is **\"dog\"** 🐕, we try to predict the context: **\"The\"**, **\"is\"**, **\"barking\"**.\n",
    "\n",
    "In simple terms: **Given one word, we try to figure out what words are likely to appear around it.**\n",
    "\n",
    "\n",
    "\n",
    "### **How does this help?** 🎯\n",
    "\n",
    "When Word2Vec learns this from **thousands or millions of sentences**, it creates **word embeddings** (the vectors), which are just **numbers** that capture the **meaning** of each word.\n",
    "\n",
    "- Words like **\"dog\"** 🐕 and **\"cat\"** 🐱, which share similar meanings, will have **similar numbers** (embeddings) and be close together.\n",
    "- Words like **\"dog\"** 🐕 and **\"car\"** 🚗, which are very different, will have **different numbers** and be far apart.\n",
    "\n",
    "\n",
    "\n",
    "### **Why is Word2Vec special?** 🌟\n",
    "\n",
    "#### **1. Captures Relationships between Words** 📏\n",
    "\n",
    "Word2Vec doesn't just look at individual words. It looks at how words **relate to each other**. \n",
    "\n",
    "For example:\n",
    "- **\"king\"** 👑 - **\"man\"** 👨 + **\"woman\"** 👩 = **\"queen\"** 👑\n",
    "  - Here, Word2Vec understands the relationship between **king** and **man**, and between **queen** and **woman**. It can **subtract** the relationship of **man** from **king** and add the relationship of **woman** to get **queen**.\n",
    "\n",
    "#### **2. Helps with Text Understanding** 📚\n",
    "\n",
    "With Word2Vec, we can use **vectors** to measure the **similarity** between two words, which helps us:\n",
    "\n",
    "- **Find synonyms** (words with similar meanings).\n",
    "- **Understand the context** (how words are related to each other in sentences).\n",
    "- **Perform tasks** like **translation**, **sentiment analysis**, and more!\n",
    "\n",
    "#### **3. Faster and Better than Old Methods** 🚀\n",
    "\n",
    "In the past, methods like **One-Hot Encoding** made each word a giant vector full of **zeros**, which wasn't very efficient.\n",
    "\n",
    "Word2Vec, on the other hand, creates **compact** and **efficient** representations where **similar words** are grouped together. \n",
    "\n",
    "\n",
    "\n",
    "### **Where is Word2Vec used?** 🔧\n",
    "\n",
    "1. **Text Classification** 📝: Classifying text as positive, negative, etc.\n",
    "2. **Sentiment Analysis** ❤️💔: Understanding whether a sentence is happy or sad.\n",
    "3. **Machine Translation** 🌍: Translating words from one language to another.\n",
    "4. **Recommendation Systems** 💡: Suggesting items based on words people have used (like movie titles).\n",
    "5. **Search Engines** 🔍: Ranking pages based on the words used in the page and the search query.\n",
    "\n",
    "\n",
    "\n",
    "### **Visualizing Word Embeddings** 🎨\n",
    "\n",
    "Imagine you're in a **3D world**, and each word is a point in that world. Words with similar meanings are **close together**, while words with different meanings are **far apart**.\n",
    "\n",
    "- **\"dog\"** 🐕 and **\"cat\"** 🐱 will be **near each other**.\n",
    "- **\"dog\"** 🐕 and **\"car\"** 🚗 will be **far apart**.\n",
    "\n",
    "Word2Vec turns words into **points in this world**, making it easier to understand how words are related.\n",
    "\n",
    "\n",
    "\n",
    "### **A Fun Example** ✨\n",
    "\n",
    "Let's say you're learning to write funny sentences using **Word2Vec**!\n",
    "\n",
    "You take a sentence like:\n",
    "\n",
    "**\"The dog is happy.\"** 🐕❤️\n",
    "\n",
    "Word2Vec will create a vector for **dog**, **happy**, and other words. Now, you want to change the word **happy** to something else that still makes sense, like **excited** or **joyful**. Word2Vec will suggest these words because it knows they are **similar**.\n",
    "\n",
    "\n",
    "\n",
    "### **Summary** 🎉\n",
    "\n",
    "- **Word2Vec** is like teaching a computer how to understand words by looking at the words around them.\n",
    "- It helps us turn words into numbers (vectors) and places similar words **closer together** in a **vector space**.\n",
    "- Word2Vec captures **relationships** between words, like **king** to **queen** or **dog** to **cat**.\n",
    "- It's used for tasks like **search engines**, **translation**, **recommendations**, and more!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
