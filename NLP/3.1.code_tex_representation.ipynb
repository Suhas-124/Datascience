{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2\n",
      "0  0.0  0.0  1.0\n",
      "1  0.0  1.0  0.0\n",
      "2  1.0  0.0  0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# sample data\n",
    "data = {'Color':['Red','Green','Blue']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "#Intialize Onehotencoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df[['Color']])\n",
    "\n",
    "#Convert to a Dataframe for better visualization\n",
    "# encoder_df = pd.DataFrame(encoded_data.toarray(),columns=encoder.get_feature_names_out())\n",
    "encoder_df = pd.DataFrame(encoded_data.toarray(),)\n",
    "\n",
    "print(encoder_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1]\n",
      " [1 1 0 0 0 1]\n",
      " [0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# step 1: Create a Corpus\n",
    "\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love machine learning\"\n",
    "\n",
    "]\n",
    "\n",
    "# step 2: Intialize countvectorizer\n",
    "vecotrizer = CountVectorizer()\n",
    "\n",
    "# step 3: Fit the vectorizer to the corpus\n",
    "X = vecotrizer.fit_transform(corpus)\n",
    "\n",
    "# step 4: Convert the result into matrix\n",
    "print(X.toarray())\n",
    "# print(vecotrizer.get_feature_names_out())\n",
    "\n",
    "# Get the words in the Bag of Words model\n",
    "print(\"Vocabulary:\", vecotrizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 1 0 0 1 0]\n",
      " [1 1 1 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 1 1 1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Create a corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "# step 2: Intialize countvectorizer with n-grams\n",
    "vecotrizer = CountVectorizer(ngram_range=(1,2)) # unigrams and bigrams\n",
    "X = vecotrizer.fit_transform(corpus)\n",
    "\n",
    "# step 3: Print the n-grams\n",
    "# print(vecotrizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "\n",
    "vecotrizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.72033345 0.         0.42544054 0.         0.\n",
      "  0.54783215]\n",
      " [0.         0.         0.72033345 0.42544054 0.         0.\n",
      "  0.54783215]\n",
      " [0.54645401 0.         0.         0.32274454 0.54645401 0.54645401\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"NLP is fun\",\n",
    "    \"NLP is interesting\",\n",
    "    \"Machine learning is amazing\"\n",
    "]\n",
    "\n",
    "# Create TF - IDF Vectorizer\n",
    "vecotrizer = TfidfVectorizer()\n",
    "tfidf_matrix = vecotrizer.fit_transform(documents)\n",
    "\n",
    "# Get the TF - IDF matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Get feature names\n",
    "print(vecotrizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "/usr/lib/python3/dist-packages/secretstorage/dhcrypto.py:15: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/usr/lib/python3/dist-packages/secretstorage/util.py:19: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting emoji\n",
      "  Using cached emoji-2.11.1-py2.py3-none-any.whl (433 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Text_Length  Word_Count  \\\n",
      "0  I love NLP! üòç It's amazing to learn machine le...           53          10   \n",
      "1     Get a free offer now!!! Win exciting prizes. üéâ           46           9   \n",
      "2  Machine learning is the future of AI. It's so ...           58          10   \n",
      "\n",
      "   Special_Char_Count  Exclamation_Mark_Count  Emoji_Count  \n",
      "0                   0                       1            1  \n",
      "1                   0                       3            1  \n",
      "2                   0                       1            0  \n"
     ]
    }
   ],
   "source": [
    "# Sample text dataset\n",
    "documents = [\n",
    "    \"I love NLP! üòç It's amazing to learn machine learning.\",\n",
    "    \"Get a free offer now!!! Win exciting prizes. üéâ\",\n",
    "    \"Machine learning is the future of AI. It's so interesting!\"\n",
    "]\n",
    "\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Function to extract text length\n",
    "def get_text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "# Function to extract word count\n",
    "def get_word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to count special characters\n",
    "def get_special_char_count(text):\n",
    "    return len(re.findall(r'[@#]', text))\n",
    "\n",
    "# Function to count exclamation marks\n",
    "def get_exclamation_mark_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "# Function to count emojis\n",
    "def get_emoji_count(text):\n",
    "    return sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "features = pd.DataFrame(documents, columns=['Text'])\n",
    "\n",
    "# Apply custom feature functions\n",
    "features['Text_Length'] = features['Text'].apply(get_text_length)\n",
    "features['Word_Count'] = features['Text'].apply(get_word_count)\n",
    "features['Special_Char_Count'] = features['Text'].apply(get_special_char_count)\n",
    "features['Exclamation_Mark_Count'] = features['Text'].apply(get_exclamation_mark_count)\n",
    "features['Emoji_Count'] = features['Text'].apply(get_emoji_count)\n",
    "\n",
    "# Display the feature matrix\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 27)\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Combine Custom Features with Standard NLP Features (e.g., TF-IDF)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(features['Text'])\n",
    "\n",
    "# Normalize custom features\n",
    "scaler = MinMaxScaler()\n",
    "custom_features = scaler.fit_transform(features[['Text_Length', 'Word_Count', 'Special_Char_Count', 'Exclamation_Mark_Count', 'Emoji_Count']])\n",
    "\n",
    "# Combine TF-IDF with custom features\n",
    "import numpy as np\n",
    "final_features = np.hstack((tfidf_matrix.toarray(), custom_features))\n",
    "\n",
    "print(final_features.shape)  # Check final feature matrix shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Train a Model Using Custom Features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample labels (spam detection example)\n",
    "labels = [0, 1, 0]  # 0 = Not Spam, 1 = Spam\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
