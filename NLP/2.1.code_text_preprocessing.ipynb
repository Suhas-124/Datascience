{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK Package\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for preprocessing\n",
    "text = \"Hello <b>world!</b> Visit us at https://example.com. I'm soooo happy ðŸ˜Š!!! How r u? It's gr8.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello <b>world!</b> visit us at https://example.com. i'm soooo happy ðŸ˜Š!!! how r u? it's gr8.\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = lowercase_text(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! visit us at https://example.com. i'm soooo happy ðŸ˜Š!!! how r u? it's gr8.\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text,'html.parser').get_text()\n",
    "\n",
    "text = remove_html_tags(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! visit us at  i'm soooo happy ðŸ˜Š!!! how r u? it's gr8.\n"
     ]
    }
   ],
   "source": [
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+','',text,flags=re.MULTILINE)\n",
    "\n",
    "text = remove_urls(text)\n",
    "print(text)\n",
    "\n",
    "#or using default method\n",
    "\n",
    "# import preprocessor as p\n",
    "\n",
    "# text = \"Hello <b>world!</b> Visit us at https://example.com. I'm soooo happy ðŸ˜Š!!! How r u? It's gr8.\"\n",
    "# clean_text = p.clean(text)  # Removes URLs, mentions, hashtags, emojis, and special characters\n",
    "# print(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world visit us at  im soooo happy  how r u its gr8\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]','',text)\n",
    "\n",
    "text = remove_punctuation(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Chat word Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world visit us at im soooo happy how are you its great\n"
     ]
    }
   ],
   "source": [
    "chat_words_dict = {\n",
    "    'u': 'you',\n",
    "    'r': 'are',\n",
    "    'gr8': 'great',\n",
    "    'hw': 'how',\n",
    "    'hpy': 'happy'\n",
    "}\n",
    "\n",
    "def treat_chat_word(text):\n",
    "    words = text.split()\n",
    "    new_text = [chat_words_dict.get(word,word) for word in words]\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "text = treat_chat_word(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spelling Correction (simple Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! visit us at i'm so happy ðŸ˜Š!!! how r u? it's gr8.\n"
     ]
    }
   ],
   "source": [
    "textsp = \"hello world! visit us at  i'm soooo hapy ðŸ˜Š!!! how r u? it's gr8.\"\n",
    "\n",
    "def correct_spelling(text):\n",
    "    corections = {\n",
    "        'hapy': 'happy',\n",
    "        'soooo': 'so'\n",
    "    }\n",
    "    words = text.split()\n",
    "    corrected_text = [corections.get(word,word) for word in words]\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "text = correct_spelling(textsp)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! visit us 'm happy ðŸ˜Š ! ! ! r u ? 's gr8 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/suhas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "text = remove_stop_words(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handle Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! visit us 'm happy [smiling_face] ! ! ! r u ? 's gr8 .\n"
     ]
    }
   ],
   "source": [
    "def handle_emojis(text):\n",
    "    emoji_dict = {\n",
    "        \"ðŸ˜Š\": \"[smiling_face]\",\n",
    "        \"ðŸ˜¢\": \"[sad_face]\"\n",
    "    }\n",
    "    for emoji,meaning in emoji_dict.items():\n",
    "        text = text.replace(emoji,meaning)\n",
    "        return text\n",
    "    \n",
    "text = handle_emojis(text)\n",
    "print(text)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'enjoys', 'programming', ',', 'playing', ',', 'and', 'running', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"He enjoys programming, playing, and running.\"\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "text = tokenize_text(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he enjoy program , play and run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text = \"he enjoys programming,playing and running\"\n",
    "\n",
    "def stemming_info(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join([stemmer.stem(word) for word in words])\n",
    "\n",
    "text = stemming_info(text)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he enjoy programming , play and running\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"he enjoys programming,playing and running\"\n",
    "\n",
    "# load spacy model for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def lemmatization_info(text):\n",
    "    doc = nlp(text)\n",
    "    return \" \".join(token.lemma_ for token in doc)\n",
    "\n",
    "text = lemmatization_info(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full process at one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from cleantext import clean\n",
    "import preprocessor as p\n",
    "from textblob import TextBlob\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Load spaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample text for preprocessing\n",
    "text = \"Hetllo <b>world!</b> Vdisit us at https://example.com. I'm soooo happy ðŸ˜Š!!! How r u? It's gr8.\"\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "\n",
    "html_text = remove_html_tags(text)\n",
    "print(\"Text after removing HTML tags:\")\n",
    "print(html_text)\n",
    "\n",
    "# Step 1: Clean the text with preprocessor (removes URLs, mentions, hashtags, emojis, and special characters)\n",
    "clean_text = p.clean(html_text)\n",
    "\n",
    "# Step 2: Remove punctuation\n",
    "clean_text = clean(clean_text, no_punct=True)\n",
    "\n",
    "# Step 3: Spell check\n",
    "textblb = TextBlob(clean_text)\n",
    "clean_text = textblb.correct().string\n",
    "\n",
    "# Step 4: Use spaCy to remove stop words\n",
    "doc = nlp(clean_text)\n",
    "cleaned_words = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "# Join the cleaned words back into a string\n",
    "cleaned_text = \" \".join(cleaned_words)\n",
    "\n",
    "# Step 5: Tokenization\n",
    "tokenized_text = word_tokenize(cleaned_text)\n",
    "\n",
    "# Step 6: Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in tokenized_text]\n",
    "\n",
    "# Join the stemmed words back into a string\n",
    "stemmed_text = \" \".join(stemmed_words)\n",
    "\n",
    "# Step 7: Lemmatization\n",
    "lemmatized_words = [nlp(word)[0].lemma_ for word in tokenized_text]\n",
    "\n",
    "# Join the lemmatized words back into a string\n",
    "lemmatized_text = \" \".join(lemmatized_words)\n",
    "\n",
    "print(\"\\nCleaned text with stop words removed:\")\n",
    "print(cleaned_text)\n",
    "\n",
    "print(\"\\nTokenized text:\")\n",
    "print(tokenized_text)\n",
    "\n",
    "print(\"\\nStemmed text:\")\n",
    "print(stemmed_text)\n",
    "\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
