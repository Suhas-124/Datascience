{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **What is a Corpus?** üìö\n",
    "A **corpus** is a **collection of text data** used for NLP tasks.\n",
    "\n",
    "Think of it as a **library of documents** that you want to analyze.\n",
    "\n",
    "üëâ **Example:**\n",
    "- A folder containing all the books written by an author.\n",
    "- A database of customer reviews on an e-commerce site.\n",
    "- A collection of news articles for text classification.\n",
    "\n",
    "In NLP, you need a **corpus** as the **starting point** to train your model.\n",
    "\n",
    "üìå **Analogy:**  \n",
    "If you're building a recipe recommendation system, the **corpus** is like your **collection of all recipe books.**\n",
    "\n",
    "\n",
    "\n",
    "## 2Ô∏è‚É£ **What is a Document?** üìÑ\n",
    "A **document** is a **single piece of text** within the corpus.\n",
    "\n",
    "üëâ **Example:**\n",
    "- Each **news article** in a collection of news articles is a **document.**\n",
    "- Each **customer review** in a dataset of reviews is a **document.**\n",
    "- Each **chapter** in a book can be treated as a **document.**\n",
    "\n",
    "In NLP, we often split the corpus into **individual documents** for analysis.\n",
    "\n",
    "üìå **Analogy:**  \n",
    "If the corpus is a **recipe book collection**, then each **recipe** is a **document.**\n",
    "\n",
    "\n",
    "\n",
    "## 3Ô∏è‚É£ **What is a Vocabulary?** üìñ\n",
    "The **vocabulary** is the **set of unique words** present in your corpus.\n",
    "\n",
    "üëâ **Example:**\n",
    "If your corpus contains these three documents:\n",
    "- Doc 1: \"I love NLP.\"\n",
    "- Doc 2: \"NLP is fun.\"\n",
    "- Doc 3: \"I love machine learning.\"\n",
    "\n",
    "The **vocabulary** would be:\n",
    "`{'I', 'love', 'NLP', 'is', 'fun', 'machine', 'learning'}`\n",
    "\n",
    "üìå **Vocabulary Size:**  \n",
    "In the above example, the **vocabulary size** is **7** (because there are 7 unique words).\n",
    "\n",
    "üìå **Analogy:**  \n",
    "If the corpus is a **collection of recipes**, the **vocabulary** is like the **list of all unique ingredients** used across all recipes.\n",
    "\n",
    "\n",
    "\n",
    "## 4Ô∏è‚É£ **What is a Word (or Token)?** üìù\n",
    "A **word** (also called a **token** in NLP) is **each individual term or unit of text** in your document.\n",
    "\n",
    "üëâ **Example:**\n",
    "In the sentence **\"I love NLP\"**, there are **3 words** (or tokens):\n",
    "- \"I\"\n",
    "- \"love\"\n",
    "- \"NLP\"\n",
    "\n",
    "üìå **Tokenization:**  \n",
    "The process of splitting text into individual **words (tokens)** is called **tokenization.**\n",
    "\n",
    "## üéØ **Summary Table:**\n",
    "\n",
    "| Concept      | Definition                             | Example                     | Analogy                    |\n",
    "|--------------|----------------------------------------|-----------------------------|----------------------------|\n",
    "| **Corpus**   | Collection of text documents            | All news articles on a site | A library of recipe books  |\n",
    "| **Document** | A single piece of text from the corpus  | One news article            | One recipe from a book     |\n",
    "| **Vocabulary** | Set of unique words in the corpus      | {'I', 'love', 'NLP'}        | List of unique ingredients |\n",
    "| **Word (Token)** | Each individual word in a document   | \"I\", \"love\", \"NLP\"          | Each ingredient in a recipe|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üîç **Why are these concepts important in NLP?**\n",
    "\n",
    "1. **Corpus**: Provides the text data you need to train your NLP model.\n",
    "2. **Document**: Helps divide the text data into smaller chunks for analysis.\n",
    "3. **Vocabulary**: Defines the list of unique words your model can work with.\n",
    "4. **Word (Token)**: The fundamental unit of text that your model analyzes.\n",
    "\n",
    "\n",
    "\n",
    "## üíª **In Code (Example Using Python)**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "# Create a vocabulary\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.vocabulary_)\n",
    "\n",
    "# Document-Term Matrix\n",
    "print(\"Document-Term Matrix:\\n\", X.toarray())\n",
    "\n",
    "# output\n",
    "Vocabulary: {'love': 3, 'nlp': 5, 'is': 1, 'fun': 0, 'machine': 4, 'learning': 2}\n",
    "Document-Term Matrix:\n",
    " [[0 0 0 1 0 1]\n",
    " [1 1 0 0 0 1]\n",
    " [0 0 1 1 1 0]]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### ü§î **Still Confused? Here's a Story!**\n",
    "Imagine you're a **chef** and you have:\n",
    "- A **library (corpus)** of recipe books.\n",
    "- Each **recipe (document)** is a different dish.\n",
    "- The **list of ingredients (vocabulary)** shows all the unique ingredients across all recipes.\n",
    "- Each **ingredient (word/token)** is an individual item in a recipe.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí° **What is One-Hot Encoding?**\n",
    "One-Hot Encoding is a technique used to convert categorical data (like colors, cities, or product categories) into a format that can be used by machine learning models. Since most models only understand **numerical data**, we need to transform these categories into numbers.\n",
    "\n",
    "Let‚Äôs break it down step by step in a **simple and easy way.** üòä\n",
    "\n",
    "\n",
    "\n",
    "## üõë **Why can't we use categories directly?**\n",
    "Imagine you have a dataset with a **\"Color\"** column:\n",
    "\n",
    "| Color   |\n",
    "|---------|\n",
    "| Red     |\n",
    "| Green   |\n",
    "| Blue    |\n",
    "\n",
    "If you assign numbers like this:\n",
    "\n",
    "| Color   | Number |\n",
    "|---------|--------|\n",
    "| Red     | 1      |\n",
    "| Green   | 2      |\n",
    "| Blue    | 3      |\n",
    "\n",
    "üëâ **Problem:** The model might think there‚Äôs a mathematical relationship between the numbers (e.g., Blue > Green > Red). But that‚Äôs not true! Colors don‚Äôt have any inherent order.\n",
    "\n",
    "**Solution:** Use **One-Hot Encoding** to represent each category as a binary (0/1) vector.\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **How does One-Hot Encoding work?**\n",
    "\n",
    "For the **\"Color\"** column, you create a new column for each unique value (Red, Green, Blue), and mark them as **1** or **0** depending on whether that row has that value.\n",
    "\n",
    "| Color   | Red | Green | Blue |\n",
    "|---------|-----|-------|------|\n",
    "| Red     | 1   | 0     | 0    |\n",
    "| Green   | 0   | 1     | 0    |\n",
    "| Blue    | 0   | 0     | 1    |\n",
    "\n",
    "Each row has **only one \"hot\" (1)** value, while the others are **\"cold\" (0).**\n",
    "\n",
    "\n",
    "\n",
    "## üîß **Steps to Perform One-Hot Encoding (Example with Scikit-Learn)**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {'Color': ['Red', 'Green', 'Blue']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df[['Color']])\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out())\n",
    "\n",
    "print(encoded_df)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "   Color_Blue  Color_Green  Color_Red\n",
    "0        0.0          0.0        1.0\n",
    "1        0.0          1.0        0.0\n",
    "2        1.0          0.0        0.0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üéØ **Real-Life Use Case**\n",
    "If you have a dataset of customer purchases, the \"Product\" column might contain categories like:\n",
    "\n",
    "| Product      |\n",
    "|--------------|\n",
    "| Laptop       |\n",
    "| Smartphone   |\n",
    "| Tablet       |\n",
    "\n",
    "One-Hot Encoding would transform this into:\n",
    "\n",
    "| Laptop | Smartphone | Tablet |\n",
    "|--------|------------|--------|\n",
    "| 1      | 0          | 0      |\n",
    "| 0      | 1          | 0      |\n",
    "| 0      | 0          | 1      |\n",
    "\n",
    "\n",
    "\n",
    "## üß† **When to Use One-Hot Encoding?**\n",
    "- ‚úÖ When your categorical data **doesn‚Äôt have any order or ranking.**\n",
    "- ‚úÖ When your machine learning model requires **numerical input.**\n",
    "\n",
    "\n",
    "\n",
    "## ‚ö†Ô∏è **Disadvantages of One-Hot Encoding**\n",
    "1. **High dimensionality:** If your categorical feature has too many unique values (e.g., thousands of cities), it creates a large number of columns, which can make your model slow and memory-intensive.\n",
    "2. **Sparse representation:** One-Hot Encoding creates a lot of 0s, which leads to a sparse matrix.\n",
    "\n",
    "\n",
    "\n",
    "## üí° **When NOT to use One-Hot Encoding?**\n",
    "If your categorical data has an inherent order (like \"Low\", \"Medium\", \"High\"), you should use **Ordinal Encoding** instead.\n",
    "\n",
    "## üöÄ **Summary (Layman's Terms)**\n",
    "Think of One-Hot Encoding as a way to **tell your model \"Yes or No\" for each category** instead of confusing it with numbers that imply order.\n",
    "\n",
    "For example:\n",
    "- Is the color Red? ‚úÖ Yes (1)\n",
    "- Is the color Green? ‚ùå No (0)\n",
    "- Is the color Blue? ‚ùå No (0)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö **What is Bag of Words (BoW)?**\n",
    "\n",
    "The **Bag of Words (BoW)** is a way of representing **text data as numerical features** that machine learning models can understand.\n",
    "\n",
    "In simple terms:\n",
    "- It **counts how often each word appears** in a document.\n",
    "- The order of the words doesn‚Äôt matter.\n",
    "- It creates a **document-term matrix** (a table) where each row is a document, and each column is a unique word.\n",
    "\n",
    "\n",
    "\n",
    "### üí° **Why is it Called \"Bag of Words\"?**\n",
    "Imagine you have a **bag** full of words from a document.  \n",
    "- You **don‚Äôt care about the order** of the words.  \n",
    "- You only care about **what words are in the bag** and **how many times each word appears**.\n",
    "\n",
    "For example:\n",
    "> **Sentence:** \"NLP is fun and NLP is useful\"  \n",
    "> **Bag of Words:** {NLP: 2, is: 2, fun: 1, and: 1, useful: 1}\n",
    "\n",
    "\n",
    "\n",
    "## üîé **How Does Bag of Words Work?**\n",
    "\n",
    "Let‚Äôs break it down into **4 steps**:\n",
    "\n",
    "### **Step 1: Create a Corpus**\n",
    "A **corpus** is a collection of text documents.\n",
    "\n",
    "Example corpus:\n",
    "```text\n",
    "Doc 1: \"I love NLP\"\n",
    "Doc 2: \"NLP is fun\"\n",
    "Doc 3: \"I love machine learning\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Step 2: Build a Vocabulary**\n",
    "The **vocabulary** is a list of all the **unique words** in the entire corpus.\n",
    "\n",
    "Example:\n",
    "```text\n",
    "Vocabulary: {'I', 'love', 'NLP', 'is', 'fun', 'machine', 'learning'}\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Count Word Frequencies**\n",
    "For each document, **count how many times each word in the vocabulary appears**.\n",
    "\n",
    "| Document    | I  | love | NLP | is | fun | machine | learning |\n",
    "|-------------|----|------|-----|----|-----|---------|----------|\n",
    "| Doc 1       | 1  | 1    | 1   | 0  | 0   | 0       | 0        |\n",
    "| Doc 2       | 0  | 0    | 1   | 1  | 1   | 0       | 0        |\n",
    "| Doc 3       | 1  | 1    | 0   | 0  | 0   | 1       | 1        |\n",
    "\n",
    "This table is your **Document-Term Matrix (DTM)**.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 4: Create the Document-Term Matrix**\n",
    "The **Document-Term Matrix** represents the **Bag of Words** model.\n",
    "\n",
    "**Matrix Representation:**\n",
    "\n",
    "| Vocabulary     | Doc 1 | Doc 2 | Doc 3 |\n",
    "|----------------|-------|-------|-------|\n",
    "| 'I'            | 1     | 0     | 1     |\n",
    "| 'love'         | 1     | 0     | 1     |\n",
    "| 'NLP'          | 1     | 1     | 0     |\n",
    "| 'is'           | 0     | 1     | 0     |\n",
    "| 'fun'          | 0     | 1     | 0     |\n",
    "| 'machine'      | 0     | 0     | 1     |\n",
    "| 'learning'     | 0     | 0     | 1     |\n",
    "\n",
    "\n",
    "\n",
    "## üí° **How to Interpret the Matrix:**\n",
    "- **Rows** = Words from the vocabulary.\n",
    "- **Columns** = Documents.\n",
    "- **Values** = The number of times each word appears in each document.\n",
    "\n",
    "For example:\n",
    "- In **Doc 1**:\n",
    "  - **\"I\"** appears **1 time**.\n",
    "  - **\"love\"** appears **1 time**.\n",
    "  - **\"NLP\"** appears **1 time**.\n",
    "  - Other words appear **0 times**.\n",
    "\n",
    "\n",
    "\n",
    "## üß© **Example in Python (Using CountVectorizer)**\n",
    "\n",
    "Let‚Äôs see how to implement Bag of Words in Python using **CountVectorizer** from scikit-learn.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Create a corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "# Step 2: Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Step 3: Fit the vectorizer to the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 4: Convert the result to a matrix\n",
    "print(X.toarray())  # Document-Term Matrix\n",
    "print(vectorizer.get_feature_names_out())  # Vocabulary\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üß™ **Output:**\n",
    "```text\n",
    "[[0 0 0 1 0 1]\n",
    " [1 1 0 0 0 1]\n",
    " [0 0 1 1 1 0]]\n",
    "\n",
    "Vocabulary: ['fun', 'is', 'learning', 'love', 'machine', 'nlp']\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üí° **Advantages of Bag of Words:**\n",
    "‚úÖ Simple and easy to implement.  \n",
    "‚úÖ Works well for text classification tasks.  \n",
    "‚úÖ Gives a basic numerical representation of text data.\n",
    "\n",
    "\n",
    "\n",
    "## ‚ùå **Disadvantages of Bag of Words:**\n",
    "‚ùå **Ignores word order** (e.g., \"NLP is fun\" vs. \"Fun is NLP\" are treated the same).  \n",
    "‚ùå **Doesn‚Äôt capture the meaning of words**.  \n",
    "‚ùå **Results in a sparse matrix** (lots of zeros if the vocabulary is large).  \n",
    "‚ùå **Fails to handle synonyms and context** (e.g., \"good\" and \"great\" are treated as different words).\n",
    "\n",
    "\n",
    "\n",
    "## üß† **Real-World Analogy:**\n",
    "Think of Bag of Words like a **grocery list**.\n",
    "\n",
    "Imagine you have 3 people with different grocery lists:\n",
    "\n",
    "| Item         | Person 1 | Person 2 | Person 3 |\n",
    "|--------------|----------|----------|----------|\n",
    "| Apples       | 2        | 1        | 3        |\n",
    "| Bananas      | 1        | 2        | 0        |\n",
    "| Carrots      | 0        | 0        | 1        |\n",
    "\n",
    "- You don‚Äôt care about the order in which they wrote the items.\n",
    "- You just care about **what items they need** and **how many of each item they want**.\n",
    "\n",
    "\n",
    "\n",
    "## üßê **When to Use Bag of Words:**\n",
    "- When building a **text classification model** (e.g., spam detection, sentiment analysis).\n",
    "- When you need a simple way to **convert text into numbers**.\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Key Takeaways:**\n",
    "- **Bag of Words** is a simple way to represent text data as a numerical matrix.\n",
    "- It works by **counting the frequency** of words in each document.\n",
    "- The resulting **Document-Term Matrix** is used as input for machine learning models.\n",
    "- It **ignores word order and context**, which can be a limitation for more complex tasks.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö **What are N-Grams in NLP?**\n",
    "\n",
    "In NLP, an **N-Gram** is a **sequence of N words** from a given text or sentence.\n",
    "\n",
    "- **N = 1** ‚Üí Unigram (1-word sequence)  \n",
    "- **N = 2** ‚Üí Bigram (2-word sequence)  \n",
    "- **N = 3** ‚Üí Trigram (3-word sequence)  \n",
    "- **N = 4 or more** ‚Üí Higher-order N-Grams (e.g., 4-grams, 5-grams, etc.)\n",
    "\n",
    "N-Grams help capture **word combinations** and **context** in text data.\n",
    "\n",
    "\n",
    "\n",
    "### üîß **Why Use N-Grams?**\n",
    "\n",
    "- **Unigrams** capture individual words but miss context.\n",
    "- **Bigrams and Trigrams** capture **phrases** and **word combinations**, providing more context.\n",
    "- N-Grams are useful for **text analysis**, **sentiment analysis**, **language modeling**, etc.\n",
    "\n",
    "\n",
    "\n",
    "### üìù **Example Sentence:**\n",
    "\n",
    "Let‚Äôs take a simple sentence:\n",
    "\n",
    "> **\"I love NLP\"**\n",
    "\n",
    "Let‚Äôs see how to form N-Grams from this sentence.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Unigram (1-Gram)**\n",
    "A **Unigram** captures **one word at a time**.\n",
    "\n",
    "| Unigram  |\n",
    "|----------|\n",
    "| I        |\n",
    "| love     |\n",
    "| NLP      |\n",
    "\n",
    "üëâ **Unigrams** focus only on individual words and ignore the relationship between words.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Bigram (2-Gram)**\n",
    "A **Bigram** captures **two consecutive words** at a time.\n",
    "\n",
    "| Bigram       |\n",
    "|--------------|\n",
    "| I love       |\n",
    "| love NLP     |\n",
    "\n",
    "üëâ **Bigrams** capture simple word combinations and some context.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **Trigram (3-Gram)**\n",
    "A **Trigram** captures **three consecutive words** at a time.\n",
    "\n",
    "| Trigram      |\n",
    "|--------------|\n",
    "| I love NLP   |\n",
    "\n",
    "üëâ **Trigrams** capture even more context by considering **three-word phrases**.\n",
    "\n",
    "\n",
    "\n",
    "### ‚úÖ **4-Gram (Quadgram)**\n",
    "A **4-Gram** captures **four consecutive words** at a time.  \n",
    "If the sentence is shorter than 4 words, no 4-Grams can be created.\n",
    "\n",
    "\n",
    "\n",
    "## üîé **How to Generate N-Grams in Python**\n",
    "\n",
    "Here‚Äôs how to generate N-Grams using Python‚Äôs **CountVectorizer** from scikit-learn.\n",
    "\n",
    "### **Example Code:**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Step 1: Create a corpus\n",
    "corpus = [\n",
    "    \"I love NLP\",\n",
    "    \"NLP is fun\",\n",
    "    \"I love machine learning\"\n",
    "]\n",
    "\n",
    "# Step 2: Initialize CountVectorizer with n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # Unigrams and Bigrams\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Step 3: Print the n-grams\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())  # Document-Term Matrix\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### üß™ **Output:**\n",
    "\n",
    "```text\n",
    "['is', 'is fun', 'learning', 'love', 'love machine', 'machine', 'machine learning', 'nlp', 'nlp is']\n",
    "\n",
    "[[0 0 0 1 0 0 0 1 0]\n",
    " [1 1 0 0 0 0 0 1 1]\n",
    " [0 0 1 1 1 1 1 0 0]]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üß† **Understanding N-Gram Importance**\n",
    "\n",
    "Here‚Äôs how N-Grams can improve text analysis:\n",
    "\n",
    "| **N-Gram Type** | **Captures**                          | **Example**            |\n",
    "|-----------------|---------------------------------------|------------------------|\n",
    "| **Unigram**     | Individual words                      | \"I\", \"love\", \"NLP\"     |\n",
    "| **Bigram**      | Simple word pairs                     | \"I love\", \"love NLP\"   |\n",
    "| **Trigram**     | More context with three-word phrases   | \"I love NLP\"           |\n",
    "| **4-Gram**      | Even more context                     | Longer phrases         |\n",
    "\n",
    "\n",
    "\n",
    "## üìä **When to Use Different N-Grams**\n",
    "\n",
    "| **N-Gram Type** | **Use Case**                         | **Advantages**                                   | **Disadvantages**                               |\n",
    "|-----------------|--------------------------------------|-------------------------------------------------|------------------------------------------------|\n",
    "| **Unigram**     | Text classification                  | Simple to implement                             | Misses word context                             |\n",
    "| **Bigram**      | Sentiment analysis                   | Captures basic context                          | Still limited context                           |\n",
    "| **Trigram**     | Chatbots, language modeling          | Captures more context                           | Can become sparse for small datasets            |\n",
    "| **4-Gram+**     | Advanced language models             | Very detailed context                           | Computationally expensive and sparse            |\n",
    "\n",
    "\n",
    "\n",
    "## üìö **Real-World Example: Sentiment Analysis**\n",
    "\n",
    "Suppose we are analyzing the sentiment of these sentences:\n",
    "\n",
    "1. \"The movie was **not good**.\"\n",
    "2. \"The movie was **good**.\"\n",
    "\n",
    "If we use **Unigrams**, both sentences will have the word \"good,\" making it hard to distinguish the negative sentiment in sentence 1.\n",
    "\n",
    "But with **Bigrams**, we can capture the phrase \"not good,\" which clearly shows negative sentiment.\n",
    "\n",
    "\n",
    "\n",
    "## üìã **Summary of Key Concepts:**\n",
    "\n",
    "| **Concept**      | **Explanation**                                  |\n",
    "|------------------|--------------------------------------------------|\n",
    "| **Unigram**      | A single word (e.g., \"I\", \"love\", \"NLP\")         |\n",
    "| **Bigram**       | Two consecutive words (e.g., \"I love\", \"love NLP\") |\n",
    "| **Trigram**      | Three consecutive words (e.g., \"I love NLP\")     |\n",
    "| **Document-Term Matrix** | A table showing N-Gram counts for each document |\n",
    "| **CountVectorizer** | A scikit-learn tool to generate N-Grams        |\n",
    "\n",
    "\n",
    "\n",
    "## üîç **When to Use N-Grams in NLP:**\n",
    "\n",
    "‚úÖ Text classification  \n",
    "‚úÖ Sentiment analysis  \n",
    "‚úÖ Chatbots  \n",
    "‚úÖ Language modeling  \n",
    "‚úÖ Spelling correction  \n",
    "‚úÖ Machine translation  \n",
    "\n",
    "\n",
    "\n",
    "### üí° **Real-World Analogy:**\n",
    "Think of N-Grams as **phrases** in a conversation.\n",
    "\n",
    "For example:\n",
    "- **Unigram** = Single words (like keywords).  \n",
    "- **Bigram** = Common phrases people use (like \"Good morning\").  \n",
    "- **Trigram** = Longer phrases with more meaning (like \"How are you doing?\").\n",
    "\n",
    "\n",
    "\n",
    "### üéØ **Key Takeaways:**\n",
    "\n",
    "- **N-Grams capture sequences of words** to provide more context.\n",
    "- **Unigrams** are good for simple tasks, but they miss context.\n",
    "- **Bigrams and Trigrams** help capture **phrases and relationships** between words.\n",
    "- The choice of **N** depends on your task and dataset size.\n",
    "\n",
    "## üìä **Summary Table:**\n",
    "\n",
    "| **Advantages**                      | **Disadvantages**                             |\n",
    "|-------------------------------------|----------------------------------------------|\n",
    "| Simple and easy to implement        | Data sparsity for higher N                   |\n",
    "| Captures word relationships         | Needs large data for higher N-Grams          |\n",
    "| Works well for short texts          | Misses long-range dependencies               |\n",
    "| Preserves word order                | Computationally expensive for large N        |\n",
    "| Useful for text classification      | Vocabulary explosion                        |\n",
    "| Handles misspellings and typos      | Does not capture semantic meaning            |\n",
    "| Can improve performance in tasks    | Fails with out-of-vocabulary words           |\n",
    "\n",
    "## üí° **When to Use N-Grams:**\n",
    "\n",
    "| **Use Case**               | **Recommended N-Gram** |\n",
    "|----------------------------|------------------------|\n",
    "| Spam detection              | Bigrams or Trigrams    |\n",
    "| Sentiment analysis          | Bigrams or Trigrams    |\n",
    "| Chatbots                    | Trigrams or 4-Grams    |\n",
    "| Text classification         | Unigrams + Bigrams     |\n",
    "| Language modeling           | Trigrams or higher     |\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç **What is TF-IDF in NLP?**\n",
    "\n",
    "**TF-IDF (Term Frequency-Inverse Document Frequency)** is a numerical statistic used to evaluate the **importance of a word in a document** relative to a collection of documents (called a **corpus**). It helps identify the **most relevant words** by reducing the weight of **common words** (like \"the\", \"is\", \"and\") and increasing the importance of **unique words** in a document.\n",
    "\n",
    "\n",
    "\n",
    "## üìö **Breaking Down TF-IDF:**\n",
    "\n",
    "TF-IDF is a combination of two values:\n",
    "\n",
    "1Ô∏è‚É£ **TF (Term Frequency)**  \n",
    "2Ô∏è‚É£ **IDF (Inverse Document Frequency)**  \n",
    "\n",
    "The final **TF-IDF score** is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ t $ = term (word)  \n",
    "- $ d $ = document  \n",
    "- $ N $ = total number of documents  \n",
    "\n",
    "\n",
    "\n",
    "### üß© **1. Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures how often a word appears in a document.\n",
    "\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "$$\n",
    "\n",
    "üîé **Example**:  \n",
    "- Document: **\"NLP is fun. NLP is interesting.\"**  \n",
    "- Term: **\"NLP\"**  \n",
    "- TF for \"NLP\" = $ \\frac{2}{5} = 0.4 $\n",
    "\n",
    "\n",
    "\n",
    "### üß© **2. Inverse Document Frequency (IDF)**\n",
    "\n",
    "IDF measures how **unique** or **rare** a word is across the entire corpus.  \n",
    "A **rare word** gets a **higher IDF score**, while a **common word** gets a **lower score**.\n",
    "\n",
    "$$\n",
    "\\text{IDF}(t) = \\log{\\left(\\frac{N}{1 + n_t}\\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ N $ = Total number of documents  \n",
    "- $ n_t $ = Number of documents containing the term $ t $\n",
    "\n",
    "üëâ **Why Add 1?**  \n",
    "We add **1** to avoid dividing by zero when the term doesn‚Äôt appear in any document.\n",
    "\n",
    "\n",
    "\n",
    "### üîé **Example of IDF Calculation**:\n",
    "\n",
    "| Document ID | Document Text                |\n",
    "|-------------|------------------------------|\n",
    "| D1          | \"NLP is fun\"                 |\n",
    "| D2          | \"NLP is interesting\"         |\n",
    "| D3          | \"Machine learning is amazing\"|\n",
    "\n",
    "For the term **\"NLP\"**:\n",
    "- $ N = 3 $ (total documents)  \n",
    "- $ n_t = 2 $ (appears in D1 and D2)\n",
    "\n",
    "$$\n",
    "\\text{IDF}(\\text{\"NLP\"}) = \\log{\\left(\\frac{3}{1 + 2}\\right)} = \\log{(1)} = 0\n",
    "$$\n",
    "\n",
    "So, **\"NLP\"** gets a low IDF score because it‚Äôs common in the corpus.\n",
    "\n",
    "\n",
    "\n",
    "### üßÆ **Calculating TF-IDF (Example)**\n",
    "\n",
    "Let‚Äôs calculate the **TF-IDF score** for the word **\"learning\"** in **D3**.\n",
    "\n",
    "| Document ID | Document Text                      |\n",
    "|-------------|------------------------------------|\n",
    "| D1          | \"NLP is fun\"                       |\n",
    "| D2          | \"NLP is interesting\"               |\n",
    "| D3          | \"Machine learning is amazing\"      |\n",
    "\n",
    "- **TF(\"learning\", D3)** = $ \\frac{1}{4} = 0.25 $  \n",
    "- **IDF(\"learning\")** = $ \\log{\\left(\\frac{3}{1 + 1}\\right)} = \\log{1.5} \\approx 0.176 $  \n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}(\\text{\"learning\"}, D3) = 0.25 \\times 0.176 = 0.044\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## üß† **Why Use TF-IDF in NLP?**\n",
    "\n",
    "TF-IDF is used to **identify important words** in documents and ignore **common words** (stopwords) that don‚Äôt add much value.\n",
    "\n",
    "### üîß **Common Use Cases:**\n",
    "1. **Text Classification**  \n",
    "2. **Information Retrieval** (search engines)  \n",
    "3. **Keyword Extraction**  \n",
    "4. **Text Summarization**  \n",
    "\n",
    "\n",
    "\n",
    "## üÜö **TF-IDF vs Bag of Words (BoW)**\n",
    "\n",
    "| **Feature**       | **Bag of Words (BoW)**                     | **TF-IDF**                                 |\n",
    "|-------------------|-------------------------------------------|-------------------------------------------|\n",
    "| **Definition**    | Counts the occurrence of each word         | Weighs words based on importance          |\n",
    "| **Focus**         | Frequency only                             | Frequency + Rarity                       |\n",
    "| **Issue**         | Gives equal importance to all words        | Reduces the weight of common words        |\n",
    "| **Example**       | ‚Äúthe‚Äù, ‚Äúis‚Äù are treated equally important  | ‚Äúthe‚Äù gets less weight than unique words  |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Advantages of TF-IDF**\n",
    "\n",
    "1Ô∏è‚É£ **Reduces the Importance of Stopwords**  \n",
    "Words like \"the\", \"is\", \"and\" have low TF-IDF scores because they appear frequently in many documents.\n",
    "\n",
    "2Ô∏è‚É£ **Highlights Important Words**  \n",
    "Words unique to a document get higher scores, making them more relevant for tasks like **text classification** or **topic modeling**.\n",
    "\n",
    "3Ô∏è‚É£ **Easy to Implement**  \n",
    "TF-IDF is easy to calculate using libraries like **scikit-learn**.\n",
    "\n",
    "4Ô∏è‚É£ **Improves Search Engine Performance**  \n",
    "Search engines use TF-IDF to rank documents based on the **relevance of search terms**.\n",
    "\n",
    "\n",
    "\n",
    "## ‚ùå **Disadvantages of TF-IDF**\n",
    "\n",
    "1Ô∏è‚É£ **Ignores Word Order**  \n",
    "TF-IDF treats documents as a **bag of words**, ignoring the order in which words appear.\n",
    "\n",
    "2Ô∏è‚É£ **Fails to Capture Semantic Meaning**  \n",
    "It doesn‚Äôt understand the **meaning** of words or their **relationship** to each other.\n",
    "\n",
    "3Ô∏è‚É£ **Sparse Matrix**  \n",
    "In large corpora, the **TF-IDF matrix** becomes **sparse**, leading to higher memory usage.\n",
    "\n",
    "4Ô∏è‚É£ **Cannot Handle Synonyms**  \n",
    "TF-IDF doesn‚Äôt account for **synonyms** or **different word forms** (e.g., \"run\" vs. \"running\").\n",
    "\n",
    "\n",
    "\n",
    "## üõ†Ô∏è **How to Implement TF-IDF in Python**\n",
    "\n",
    "Using **scikit-learn**:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"NLP is fun\",\n",
    "    \"NLP is interesting\",\n",
    "    \"Machine learning is amazing\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Get feature names\n",
    "print(vectorizer.get_feature_names_out())\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[[0.70710678 0.         0.         0.70710678 0.        ]\n",
    " [0.70710678 0.         0.         0.70710678 0.        ]\n",
    " [0.         0.         0.57735027 0.         0.57735027]]\n",
    "['amazing' 'fun' 'interesting' 'is' 'learning' 'machine' 'nlp']\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## üßë‚Äçüéì **Summary of TF-IDF:**\n",
    "\n",
    "| **Component**   | **Definition**                                |\n",
    "|-----------------|----------------------------------------------|\n",
    "| **TF**          | Frequency of a word in a document             |\n",
    "| **IDF**         | Importance of a word across all documents     |\n",
    "| **TF-IDF**      | Combines TF and IDF to weigh word importance  |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí° **Custom Features?**\n",
    "\n",
    "To apply **custom features** in NLP, you can go beyond standard approaches like **TF-IDF** or **Bag of Words** and engineer **domain-specific features** to enhance your model's performance. Custom features help capture **semantic, syntactic, or contextual nuances** that general methods might miss.\n",
    "\n",
    "Let‚Äôs dive into a **step-by-step guide** on **what custom features are**, **how to create them**, and **how to apply them in an NLP pipeline**.\n",
    "\n",
    "\n",
    "\n",
    "# üìö **What are Custom Features in NLP?**\n",
    "\n",
    "Custom features are **manually created attributes** that provide more information about the text beyond simple word counts or embeddings. These features can be based on **domain knowledge**, **text structure**, or **contextual cues**.\n",
    "\n",
    "Examples of custom features:\n",
    "- **Length of the text**\n",
    "- **Number of special characters (e.g., @, #)**\n",
    "- **Presence of keywords**\n",
    "- **Sentiment scores**\n",
    "- **POS tags (Parts of Speech)**\n",
    "- **Readability scores**\n",
    "- **Named Entity Recognition (NER) counts**\n",
    "\n",
    "\n",
    "\n",
    "# üí° **Why Use Custom Features?**\n",
    "\n",
    "- **Incorporates domain knowledge**  \n",
    "- **Improves model performance**  \n",
    "- **Captures semantic or syntactic information**  \n",
    "- **Gives more control over feature engineering**  \n",
    "\n",
    "For instance, in **spam detection**, the presence of words like **\"free\", \"offer\", \"win\"** can be an important feature. Similarly, in **sentiment analysis**, **emoji counts** or **exclamation marks** might provide valuable insights.\n",
    "\n",
    "\n",
    "\n",
    "# üîß **How to Create and Apply Custom Features in NLP?**\n",
    "\n",
    "Let‚Äôs build a **custom feature extraction pipeline** with practical examples.\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 1: Load and Preprocess the Data**\n",
    "\n",
    "```python\n",
    "# Sample text dataset\n",
    "documents = [\n",
    "    \"I love NLP! üòç It's amazing to learn machine learning.\",\n",
    "    \"Get a free offer now!!! Win exciting prizes. üéâ\",\n",
    "    \"Machine learning is the future of AI. It's so interesting!\"\n",
    "]\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 2: Define Custom Feature Extraction Functions**\n",
    "\n",
    "Here are a few custom features to add:\n",
    "\n",
    "| **Feature**             | **Description**                                |\n",
    "|-------------------------|------------------------------------------------|\n",
    "| **Text Length**          | Total number of characters in the text         |\n",
    "| **Word Count**           | Total number of words in the text              |\n",
    "| **Special Characters**   | Number of special characters (e.g., `@`, `#`)  |\n",
    "| **Exclamation Marks**    | Number of exclamation marks (`!`)              |\n",
    "| **Emoji Count**          | Number of emojis in the text                   |\n",
    "\n",
    "```python\n",
    "import re\n",
    "import emoji\n",
    "\n",
    "# Function to extract text length\n",
    "def get_text_length(text):\n",
    "    return len(text)\n",
    "\n",
    "# Function to extract word count\n",
    "def get_word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Function to count special characters\n",
    "def get_special_char_count(text):\n",
    "    return len(re.findall(r'[@#]', text))\n",
    "\n",
    "# Function to count exclamation marks\n",
    "def get_exclamation_mark_count(text):\n",
    "    return text.count('!')\n",
    "\n",
    "# Function to count emojis\n",
    "def get_emoji_count(text):\n",
    "    return sum(1 for char in text if char in emoji.EMOJI_DATA)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 3: Apply Custom Features to the Dataset**\n",
    "\n",
    "We can apply these functions to each document to create a **feature matrix**.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the features\n",
    "features = pd.DataFrame(documents, columns=['Text'])\n",
    "\n",
    "# Apply custom feature functions\n",
    "features['Text_Length'] = features['Text'].apply(get_text_length)\n",
    "features['Word_Count'] = features['Text'].apply(get_word_count)\n",
    "features['Special_Char_Count'] = features['Text'].apply(get_special_char_count)\n",
    "features['Exclamation_Mark_Count'] = features['Text'].apply(get_exclamation_mark_count)\n",
    "features['Emoji_Count'] = features['Text'].apply(get_emoji_count)\n",
    "\n",
    "# Display the feature matrix\n",
    "print(features)\n",
    "```\n",
    "\n",
    "### üîç **Output:**\n",
    "\n",
    "| Text                                                    | Text_Length | Word_Count | Special_Char_Count | Exclamation_Mark_Count | Emoji_Count |\n",
    "|---------------------------------------------------------|-------------|------------|--------------------|-----------------------|-------------|\n",
    "| \"I love NLP! üòç It's amazing to learn machine learning.\" | 51          | 10         | 0                  | 1                     | 1           |\n",
    "| \"Get a free offer now!!! Win exciting prizes. üéâ\"        | 45          | 9          | 0                  | 3                     | 1           |\n",
    "| \"Machine learning is the future of AI. It's so...\"       | 50          | 11         | 0                  | 1                     | 0           |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 4: Combine Custom Features with Standard NLP Features (e.g., TF-IDF)**\n",
    "\n",
    "You can combine your custom features with **TF-IDF** or **Bag of Words** to create a richer feature set for your model.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Apply TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(features['Text'])\n",
    "\n",
    "# Normalize custom features\n",
    "scaler = MinMaxScaler()\n",
    "custom_features = scaler.fit_transform(features[['Text_Length', 'Word_Count', 'Special_Char_Count', 'Exclamation_Mark_Count', 'Emoji_Count']])\n",
    "\n",
    "# Combine TF-IDF with custom features\n",
    "import numpy as np\n",
    "final_features = np.hstack((tfidf_matrix.toarray(), custom_features))\n",
    "\n",
    "print(final_features.shape)  # Check final feature matrix shape\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 5: Train a Model Using Custom Features**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Sample labels (spam detection example)\n",
    "labels = [0, 1, 0]  # 0 = Not Spam, 1 = Spam\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(\"Model Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Step 6: Example of Custom Features for Sentiment Analysis**\n",
    "\n",
    "| **Feature**            | **Description**                                 |\n",
    "|------------------------|-------------------------------------------------|\n",
    "| **Positive Words Count** | Count of positive words (e.g., \"love\", \"great\") |\n",
    "| **Negative Words Count** | Count of negative words (e.g., \"bad\", \"hate\")   |\n",
    "| **Polarity Score**       | Overall sentiment score based on word polarity  |\n",
    "\n",
    "\n",
    "\n",
    "## ‚úÖ **Other Ideas for Custom Features:**\n",
    "\n",
    "1. **POS (Parts of Speech) Tag Count**  \n",
    "   - Count of nouns, verbs, adjectives, etc.\n",
    "\n",
    "2. **Named Entity Count**  \n",
    "   - Number of entities (e.g., people, places, organizations) in the text.\n",
    "\n",
    "3. **Sentiment Scores**  \n",
    "   - Use a sentiment analysis library like **TextBlob** or **VADER**.\n",
    "\n",
    "4. **Readability Score**  \n",
    "   - Calculate how easy or difficult a text is to read.\n",
    "\n",
    "\n",
    "\n",
    "## üìä **Advantages of Custom Features:**\n",
    "\n",
    "| **Advantages**                             | **Explanation**                                    |\n",
    "|--------------------------------------------|----------------------------------------------------|\n",
    "| **Captures Domain Knowledge**              | Helps incorporate specific insights for better performance |\n",
    "| **Improves Model Interpretability**        | Custom features are easier to interpret and explain |\n",
    "| **Can Enhance Model Performance**          | When designed well, custom features can significantly boost accuracy |\n",
    "\n",
    "\n",
    "\n",
    "## ‚ùå **Disadvantages of Custom Features:**\n",
    "\n",
    "| **Disadvantages**                         | **Explanation**                                    |\n",
    "|-------------------------------------------|----------------------------------------------------|\n",
    "| **Time-Consuming to Create**              | Requires manual effort and domain knowledge        |\n",
    "| **May Overfit to Training Data**          | Custom features may capture noise instead of signal |\n",
    "| **Hard to Generalize**                    | Some features may not work well across different datasets |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
